doi,title,abstract,label_included
https://doi.org/10.1002/9780470723586,Statistical Issues in Drug Development,"Drug development is the process of finding and producing therapeutically useful pharmaceuticals, turning them into safe and effective medicine, and producing reliable information regarding the appropriate dosage and dosing intervals. With regulatory authorities demanding increasingly higher standards in such developments, statistics has become an intrinsic and critical element in the design and conduct of drug development programmes. Statistical Issues in Drug Development presents an essential and thought provoking guide to the statistical issues and controversies involved in drug development. This highly readable second edition has been updated to include: Comprehensive coverage of the design and interpretation of clinical trials. Expanded sections on missing data, equivalence, meta-analysis and dose finding. An examination of both Bayesian and frequentist methods. A new chapter on pharmacogenomics and expanded coverage of pharmaco-epidemiology and pharmaco-economics. Coverage of the ICH guidelines, in particular ICH E9, Statistical Principles for Clinical Trials. It is hoped that the book will stimulate dialogue between statisticians and life scientists working within the pharmaceutical industry. The accessible and wide-ranging coverage make it essential reading for both statisticians and non-statisticians working in the pharmaceutical industry, regulatory bodies and medical research institutes. There is also much to benefit undergraduate and postgraduate students whose courses include a medical statistics component. Ã‚Â© 2007 John Wiley & Sons, Ltd.",0
https://doi.org/10.1007/s11031-015-9499-5,Kindness reduces avoidance goals in socially anxious individuals,"Social avoidance goals have been linked to negative social outcomes and may contribute to the social impairment experienced by socially anxious individuals. In this study, we examined whether engaging in acts of kindness, a technique designed to increase happiness, decreases social avoidance goals in socially anxious participants and whether social anxiety reduction and hedonic enhancement (i.e., increased positive affect) mediate this effect. Socially anxious undergraduates were randomly assigned to three conditions: performing acts of kindness (AK; NÂ =Â 38); exposure only (EO; NÂ =Â 41); and recording life details (LD; NÂ =Â 36), a neutral control condition. Participants engaged in these activities for 4Â weeks. AK resulted in the greatest decrease in social avoidance goals by post-intervention. EO also reduced avoidance goals over time relative to LD. The effect of task condition on avoidance goals over time was fully mediated by social anxiety reduction over time. Neither AK nor EO increased positive affect. Implications for social anxiety treatment are discussed. Â© 2015, Springer Science+Business Media New York.",0
https://doi.org/10.1016/j.healthplace.2014.08.009,Individual and province inequalities in health among older people in China: Evidence and policy implications,"This paper uses multi-level modelling to analyse data from the nationally-representative Chinese Health and Retirement Longitudinal Study (CHARLS) in order to investigate the characteristics associated with poor health among older people, including individual and household characteristics as well as the characteristics of the provinces in which the older person lives (contextual effects). The results show that older Chinese women, rural residents, those with an education level lower than high school, without individual income sources, who are ex-smokers, and those from poor economic status households are more likely to report disability and poor self-rated health. Differentials in the health outcomes remain substantial between provinces even after controlling for a number of individual and household characteristics.",0
https://doi.org/10.1002/hec.804,Comprehensive decision analytical modelling in economic evaluation: a Bayesian approach,"Decision analytical models are widely used in economic evaluation of health care interventions with the objective of generating valuable information to assist health policy decision-makers to allocate scarce health care resources efficiently. The whole decision modelling process can be summarised in four stages: (i) a systematic review of the relevant data (including meta-analyses), (ii) estimation of all inputs into the model (including effectiveness, transition probabilities and costs), (iii) sensitivity analysis for data and model specifications, and (iv) evaluation of the model. The aim of this paper is to demonstrate how the individual components of decision modelling, outlined above, may be addressed simultaneously in one coherent Bayesian model (sometimes known as a comprehensive decision analytical model) and evaluated using Markov Chain Monte Carlo simulation implemented in the specialist software WinBUGS. To illustrate the method described, it is applied to two illustrative examples: (1) The prophylactic use of neurominidase inhibitors for the prevention of influenza. (2) The use of taxanes for the second-line treatment of advanced breast cancer. The advantages of integrating the four stages outlined into one comprehensive decision analytical model, compared to the conventional 'two-stage' approach, are discussed.",0
https://doi.org/10.1007/s11336-008-9101-0,"On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha","This discussion paper argues that both the use of Cronbach's alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score's reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test's internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals' test performance. The paper ends with a list of conclusions about the usefulness of alpha.",0
https://doi.org/10.1016/j.jmp.2010.08.009,The form of the forgetting curve and the fate of memories,"Psychologists have debated the form of the forgetting curve for over a century. We focus on resolving three problems that have blocked a clear answer on this issue. First, we analyzed data from a longitudinal experiment measuring cued recall and stem completion from 1 min to 28 days after study, with more observations per interval per participant than in previous studies. Second, we analyzed the data using hierarchical models, avoiding distortions due to averaging over participants. Third, we implemented the models in a Bayesian framework, enabling our analysis to account for the ability of candidate forgetting functions to imitate each other. An exponential function provided the best fit to individual participant data collected under both explicit and implicit retrieval instructions, but Bayesian model selection favored a power function. All analysis supported above chance asymptotic retention, suggesting that, despite quite brief study, storage of some memories was effectively permanent.",0
https://doi.org/10.1016/j.jspi.2003.09.026,A class of shrinkage priors for the dependence structure in longitudinal data,"We review a general class of priors for the dependence in longitudinal (temporal) data in settings where a parametric form is often assumed and place them in the context of the literature. The idea is to embed priors on the parameters of the structure within a richer, more flexible class of priors. These priors are shown to contain standard objective priors for structured and unstructured dependence models as special cases under certain conditions and parameterizations. Recommendations and specific details regarding their use are provided.",0
https://doi.org/10.1016/j.paid.2015.02.023,Examining the association between psychological wellbeing with daily and intra-individual variation in subjective wellbeing,"Abstract A number of studies identify distinct dimensions of psychological and subjective wellbeing. However, few investigations have examined how these distinctive wellbeing dimensions may be related over time. The present study aimed to contribute to this growing body of research by adopting a measurement burst design to examine the association between psychological functioning with daily and intra-individual variation in affect over a 14-day period, controlling for personality. Participants ( N  = 45) comprised a sample of Australian adults from Canberra, Australia who were observed on up to 14 days over a 2-week period (Mobs = 10.9 (SD = 3.1)). Maximum Likelihood (ML) estimates from a multi-level structural equation model identified psychological functioning as only weakly associated with daily positive affect, and unrelated to daily levels of negative affect and intra-individual variation in both affect domains when adjusting for demographic, personality and daily stressors. Positive and negative daily events were most strongly associated with positive and negative feelings, respectively. Post-hoc analysis within a Bayesian context confirmed our ML results whilst a Monte Carlo simulation identified sufficient statistical power of significant parameters. Overall, evidence for an association between psychological functioning and daily affect was not identified.",0
https://doi.org/10.3758/bf03200523,RTSYS: A DOS application for the analysis of reaction time data,"RTSYS is a menu-driven DOS application for the manipulation, analysis, and graphical display of reaction time data. It can be used either in a single-task environment under DOS, with access to a set operating system commands, or as an application under Windows. All functions have context-sensitive help. RTSYS fits the ex-Gaussian distribution to reaction time data without the difficulties usually associated with numerical parameter estimation. Distribution fitting and flexible censoring and rescaling options allow RTSYS to address the problems of reaction time distribution skew and outlying responses with reasonable sample sizes. RTSYS can automatically process multiple input files from experiments with arbitrary designs and produce formatted output of statistics for further processing by graphical and inferential statistical packages. The present article reviews and explains techniques used by RTSYS and provides an overview of the operation of the program.",0
https://doi.org/10.1080/10705510709336745,Comparison of Approaches to Constructing Confidence Intervals for Mediating Effects Using Structural Equation Models,"Abstract Mediators are variables that explain the association between an independent variable and a dependent variable. Structural equation modeling (SEM) is widely used to test models with mediating effects. This article illustrates how to construct confidence intervals (CIs) of the mediating effects for a variety of models in SEM. Specifically, mediating models with 1 mediator, 2 intermediate mediators, 2 specific mediators, and 1 mediator in 2 independent groups are illustrated. By using phantom variables (Rindskopf, 1984), a Wald CI, percentile bootstrap CI, bias-corrected bootstrap CI, and a likelihood-based CI on the mediating effect are easily constructed with some existing SEM packages, such as LISREL, M plus, and Mx. Monte Carlo simulation studies are used to compare the coverage probabilities of these CIs. The results show that the coverage probabilities of these CIs are comparable when the mediating effect is large or when the sample size is large. However, when the mediating effect and the sam...",0
https://doi.org/10.1080/10705511.2014.935266,Inference and Interval Estimation Methods for Indirect Effects With Latent Variable Models,"Although much is known about the performance of recent methods for inference and interval estimation for indirect or mediated effects with observed variables, little is known about their performance in latent variable models. This article presents an extensive Monte Carlo study of 11 different leading or popular methods adapted to structural equation models with latent variables. Manipulated variables included sample size, number of indicators per latent variable, internal consistency per set of indicators, and 16 different path combinations between latent variables. Results indicate that some popular or previously recommended methods, such as the bias-corrected bootstrap and asymptotic standard errors had poorly calibrated Type I error and coverage rates in some conditions. Likelihood-based confidence intervals, the distribution of the product method, and the percentile bootstrap emerged as leading methods for both interval estimation and inference, whereas joint significance tests and the partial poster...",0
https://doi.org/10.1198/108571108x273160,Bayesian multivariate process modeling for prediction of forest attributes,"This article investigates multivariate spatial process models suitable for predicting multiple forest attributes using a multisource forest inventory approach. Such data settings involve several spatially dependent response variables arising in each location. Not only does each variable vary across space, they are likely to be correlated among themselves. Traditional approaches have attempted to model such data using simplifying assumptions, such as a common rate of decay in the spatial correlation or simplified cross-covariance structures among the response variables. Our current focus is to produce spatially explicit, tree species specific, prediction of forest biomass per hectare over a region of interest. Modeling such associations presents challenges in terms of validity of probability distributions as well as issues concerning identifiability and estimability of parameters. Our template encompasses several models with different correlation structures. These models represent different hypotheses whose tenability are assessed using formal model comparisons. We adopt a Bayesian hierarchical approach offering a sampling-based inferential framework using efficient Markov chain Monte Carlo methods for estimating model parameters. Ã‚Â© 2008 American Statistical Association and the International Biometric Society.",0
https://doi.org/10.2307/2533273,Robust Restricted Maximum Likelihood in Mixed Linear Models,"SUMMARY Definitions of robust maximum likelihood (robust ML) and robust restricted maximum likelihood (robust REML) are introduced, and the definitions are applied to data from biological and chemical experiments. A simulation study is undertaken to investigate the asymptotic properties of robust ML and robust REML in small samples and to examine the advantages of using robust methods. 1. Introduction and Definitions Linear models with multiple sources of error are widely used in designed experiments across many scientific fields. An example of such an experiment is described by Patterson and Nabugoomu (1992), from Patterson and Silvey (1980). Six varieties of wheat were grown at ten centres that formed a sample of the main types of growing area for wheat in Scotland, and the yields in tonnes/hectare were recorded. The experiment is unbalanced because, of 60 possible variety-centre combinations, only 46 were used. At seven centres, four varieties were grown and at the remaining three centres, all six varieties were grown. In this paper we fit the simplest mixed linear model proposed for this data by Patterson and Nabugoomu, namely:",0
https://doi.org/10.1207/s15327906mbr3304_5,Multiple Imputation for Multivariate Missing-Data Problems: A Data Analyst's Perspective,"Analyses of multivariate data are frequently hampered by missing values. Until recently, the only missing-data methods available to most data analysts have been relatively ad1 hoc practices such as listwise deletion. Recent dramatic advances in theoretical and computational statistics, however, have produced anew generation of flexible procedures with a sound statistical basis. These procedures involve multiple imputation (Rubin, 1987), a simulation technique that replaces each missing datum with a set of m > 1 plausible values. The rn versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. New computational algorithms and software described in a recent book (Schafer, 1997a) allow us to create proper multiple imputations in complex multivariate settings. This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from the Adolescent Alcohol Prevention Trial (Hansen & Graham, 199 I).",0
https://doi.org/10.1016/j.paid.2015.08.009,Associations of self-compassion and global self-esteem with positive and negative affect and stress reactivity in daily life: Findings from a smart phone study,"The present study examined trait self-compassion and trait self-esteem in relation to positive (PA) and negative affect (NA), as well as their associations with stress reactivity in daily life. One hundred and one subjects completed questionnaires on perceived stress and affect twice a day for 14 consecutive days on smart phones. Results indicated that self-compassion and global self-esteem were positively related to PA and negatively to NA. After controlling for self-esteem, self-compassion remained significantly associated with PA and NA, whereas self-esteem was no longer associated with PA and NA after controlling for self-compassion. Furthermore, results indicated that self-compassion buffered the effect of stress on NA, whereas this was not the case for global self-esteem. Neither self-compassion nor self-esteem moderated the relation of stress on PA in separate models. The results of the present study add to the growing literature regarding beneficial relations of self-compassion and psychological well-being and further emphasize the distinction of self-compassion and global self-esteem.",0
https://doi.org/10.1080/10705511.2011.557337,Testing Measurement Invariance: A Comparison of Multiple-Group Categorical CFA and IRT,"This study investigated two major approaches in testing measurement invariance for ordinal measures: multiple-group categorical confirmatory factor analysis (MCCFA) and item response theory (IRT). Unlike the ordinary linear factor analysis, MCCFA can appropriately model the ordered-categorical measures with a threshold structure. A simulation study under various conditions was conducted for the comparison of MCCFA and IRT with respect to the power to detect the lack of invariance across groups. Both MCCFA and IRT showed reasonable power to identify the noninvariant item when differential item functioning (DIF) was large. The false positive rates were relatively high in both methods, however. The adjustment of critical values improved the performance of MCCFA by reducing false positive rates substantially and yet yielding adequate power. Alternative model fit indexes of MCCFA were also examined and they were found to be reliable to detect DIF, in general.",0
https://doi.org/10.1080/10705510701301602,The Model-Size Effect on Traditional and Modified Tests of Covariance Structures,"According to Kenny and McCoach (2003), chi-square tests of structural equation models produce inflated Type I error rates when the degrees of freedom increase. So far, the amount of this bias in large models has not been quantified. In a Monte Carlo study of confirmatory factor models with a range of 48 to 960 degrees of freedom it was found that the traditional maximum likelihood ratio statistic, T ML , overestimates nominal Type I error rates up to 70% under conditions of multivariate normality. Some alternative statistics for the correction of model-size effects were also investigated: the scaled Satorra–Bentler statistic, T SC ; the adjusted Satorra–Bentler statistic, T AD (Satorra & Bentler, 1988, 1994); corresponding Bartlett corrections, T MLb , T SCb , and T ADb (Bartlett, 1950); and corresponding Swain corrections, T MLs , T SCs , and T ADs (Swain, 1975). The empirical findings indicate that the model test statistic T MLs should be applied when large structural equation models are analyzed and th...",0
https://doi.org/10.1348/000711009x480640,Coping with stress through decisional control: Quantification of negotiating the environment,"Coping with stress through 'decisional control' - positioning oneself in a multifaceted stressing situation so as to minimize the likelihood of an untoward event - is modelled within a tree-structure scenario, whose architecture hierarchically nests elements of varying threat. Analytic and simulation platforms quantify the game-like interplay of cognitive demands and threat reduction. When elements of uncertainty enter the theoretical structure, specifically at more subordinate levels of the hierarchy, the mathematical expectation of threat is particularly exacerbated. As quantified in this model, the exercise of decisional control is demonstrably related to reduction in expected threat (the minimum correlation across comprehensive parameter settings being .55). Disclosure of otherwise intractable stress-coping subtleties, endowed by the quantitative translation of verbal premises, is underscored. Formalization of decisional stress control is seen to usher in linkages to augmenting formal developments from fields of cognitive science, preference and choice modelling, and nonlinear dynamical systems theory. Model-prescribed empirical consequences are stipulated.",0
https://doi.org/10.1177/014662168100500212,Using Simulation Results to Choose a Latent Trait Model,"A latent trait model goodness-of-fit statistic was defined, and its relationships to several other com monly used fit statistics were described. Simulation data were used to examine the behavior of these fit statistics under conditions similar to those found with real data. The simulation data were generated for 36 pseudo-items and 1,000 simulees using three-, two-, and one-parameter logistic latent trait models. The data were analyzed using three-, two-, and one-parameter models. Between-model comparisons were made of the fit statistics, trait es timates, and item parameter estimates. The three generating models produced clearly different pat terns of results. The simulation results were com pared to results for real data involving seventh- and eighth-grade students' performance on eight achievement tests. The achievement test results ap peared most similar to the simulation results based on data generated with the three-parameter model. Some practical problems that can result from using an inappropriate model with multiple-choice tests are discussed.",0
https://doi.org/10.3102/1076998607302631,The D-Optimality Item Selection Criterion in the Early Stage of CAT: A Study With the Graded Response Model,"During the early stage of computerized adaptive testing (CAT), item selection criteria based on Fisher’s information often produce less stable latent trait estimates than the Kullback-Leibler global information criterion. Robustness against early stage instability has been reported for the D-optimality criterion in a polytomous CAT with the Nominal Response Model and is shown herein to be reproducible for the Graded Response Model. For comparative purposes, the A-optimality and the global information criteria are also applied. Their item selection is investigated as a function of test progression and item bank composition. The results indicate how the selection of specific item parameters underlies the criteria performances evaluated via accuracy and precision of estimation. In addition, the criteria item exposure rates are compared, without the use of any exposure controlling measure. On the account of stability, precision, accuracy, numerical simplicity, and less evidently, item exposure rate, the D-opt...",0
,Latent growth mixture modeling : a simulation study,"Latent growth curve modeling (LGM) combined with the latent classes (LGMM) in the SEM context, is the method under investigation in this study. This dynamic way of analyzing longitudinal data takes an increasingly central position in the social sciences, e.g. in psychology. Despite twenty years development of the theory behind the LGM and LGMM, these are novel methods in analyzing data in practice. With limited sample size the functionality of the model is unknown. The aim of this dissertation was to examine the functionality of the linear LGM model with four repeated measurements, which is a typical case in longitudinal research. LGMM parameters were estimated using maximum likelihood estimation with robust standard errors (MLR). The effect of differences between latent classes in mean values of latent components with varying sample sizes is examined in this study. Other affecting factors examined are reliability of observed variables, number of repeated measures, model construct and additional measurement points. The functionality of LGMM was approached from three different viewpoints: 1) problems in estimation of model parameters expressed as number of failed estimations and as the number of negative variance estimates, 2) the ability of AIC, BIC and aBIC information criteria and VLMR, LMR and BLRT statistical tests to decide the number of latent classes, and 3) good parameter estimation, which was evaluated using four different criteria: MSE, proportion of bias in MSE, bias of standard error, and 95 % coverage. The results of Monte Carlo simulations suggest that from information criteria AIC, BIC aBIC and VLMR and LMR tests, BIC is most useful with small sample sizes ( ) and aBIC with large sample sizes ( ). The few results suggest that the BLRT test could be useful in any situation. More investigation is needed to further support the functionality of this test. The study reveals that the estimation of LGMM fails only in a few cases, and problems in estimation appear mainly in the negative variance estimates. The results of the simulations suggest that it is possible to identify the true two-latent classes when SMD is at least 2, in which case reliability of observed variables should be high and the sample size should be relatively large. In that case estimation produce good parameter estimates. When SMD is 4 or 5, the probability in identifying the right two-latent-class solution instead of the wrong one-class solution is greater than .70 with the smallest sample size (n=50) using BIC in models with high reliability. To achieve reliable results in estimation, the sample size should be greater than 50. 500 < n 500 ≥ n",0
https://doi.org/10.1177/004005999202400310,Curriculum-Based Oral Reading Fluency Norms for Students in Grades 2 through 5,,0
https://doi.org/10.1037/a0015858,Evaluating model fit for growth curve models: Integration of fit indices from SEM and MLM frameworks.,"Evaluating overall model fit for growth curve models involves 3 challenging issues. (a) Three types of longitudinal data with different implications for model fit may be distinguished: balanced on time with complete data, balanced on time with data missing at random, and unbalanced on time. (b) Traditional work on fit from the structural equation modeling (SEM) perspective has focused only on the covariance structure, but growth curve models have four potential sources of misspecification: within-individual covariance matrix, between-individuals covariance matrix, marginal mean structure, and conditional mean structure. (c) Growth curve models can be estimated in both the SEM and multilevel modeling (MLM) frameworks; these have different emphases for the evaluation of model fit. In this article, the authors discuss the challenges presented by these 3 issues in the calculation and interpretation of SEM- and MLM-based fit indices for growth curve models and conclude by identifying some lines for future research.",0
https://doi.org/10.1007/s10742-012-0079-9,Variation in New Zealand hospital outcomes: combining hierarchical Bayesian modeling and propensity score methods for hospital performance comparisons,"Two major statistical issues confronting comparative analyses of hospital outcomes are adequacy of case-mix adjustment and proper accounting for random variation. Hierarchical modeling has been proposed to improve precision and reduce the impact of random variation but becomes difficult to implement when there are numerous case-mix factors to control. In this paper we formulate the problem of hospital performance comparisons within the framework of potential outcomes and illustrate an approach to hospital comparisons which combines multiple category propensity score methods for the control of case-mix variations with hierarchical Bayesian modeling of case-mix adjusted summaries. The approach is similar to that proposed by Huang et al. (Health Serv Res 40:253-278, 2005) but extends their approach by using a Bayesian model to accommodate hospital level attributes and to facilitate joint modeling of performance for multiple outcomes. The analytical approach is illustrated by a comparison of 30 day post admission mortality risks for patients treated for acute myocardial infarction, pneumonia or stroke in 34 New Zealand public hospitals. In a small simulation study, reported in electronic supplementary material, hierarchical models outperformed non-hierarchical models, achieving both better credible interval coverage and shorter average interval lengths for measures of between hospital variation based on contrasts between the 90th and 10th percentiles of the mortality risk distribution. Simulation performance of hierarchical and non-hierarchical models in detecting unusual performance was similar. Â© 2012 Springer Science+Business Media, LLC.",0
https://doi.org/10.1177/1094428110391814,More Statistical and Methodological Myths and Urban Legends,"The study of urban legends represents the application of concepts developed in the academic study of traditional folktales to stories circulating in the modern world. Vandenberg adapted the notion of ‘‘urban legends’’ into the area of organizational research methods and coined the term statistical and methodological myths and urban legends (SMMULs) to refer to the collection of various rules of thumb and other ‘‘received doctrines’’ that often guide researchers’ scientific behavior. Various SMMULs have been examined over 7 years in a number of symposia at scientific conferences, in a previous Feature Topic in Organizational Research Methods (2006), and in an edited book. This Feature Topic continues this tradition by presenting five new SMMULs.",0
https://doi.org/10.1207/s15327930pje7704_9,Methodological Advances in the Analysis of Individual Growth With Relevance to Education Policy,"The purpose of this article is to demonstrate how recent methodological developments in the analysis of individual growth can inform important problems in education policy. Specifically, this article focuses on a method referred to as growth mixture modeling. Growth mixture modeling is a relatively new procedure for the analysis of longitudinal data that relaxes many of the assumptions associated with conventional growth curve modeling. In particular, growth mixture modeling tests for the existence of unique growth trajectory classes through a combination of latent class analysis and standard growth curve modeling. Antecedent predictors of the latent classes can be incorporated as well as relations from the latent classes to specific outcomes. This article applies growth mixture modeling to data from the Early Childhood Longitudinal Study-Kindergarten class of 1998-1999. The specific policy question posed in this article focuses on the estimation of latent growth trajectory classes in reading proficiency ...",0
https://doi.org/10.1016/j.cct.2017.06.009,Increasing the efficiency of oncology basket trials using a Bayesian approach,"With the rapid growth of targeted and immune-oncology therapies, novel statistical design approaches are needed to increase the flexibility and efficiency of early phase oncology trials. Basket trials enroll patients with defined biological deficiencies, but with multiple histologic tumor types (or indications), to discover in which indications the drug is active. In such designs different indications are typically analyzed independently. This, however, ignores potential biological similarities among the indications. Our research provides a statistical methodology to enhance such basket trials by assessing the homogeneity of the response rates among indications at an interim analysis, and applying a Bayesian hierarchical modeling approach in the second stage if the efficacy is deemed reasonably homogenous across indications. This increases the power of the study by allowing indications with similar response rates to borrow information from each other. Via simulations, we quantify the efficiency gain of our proposed approach relative to the conventional parallel approach. The operating characteristics of our method depend on the similarity of the response rates between the different indications. If the response rates are comparable in most or all indications after treatment with the investigational drug, a substantial increase in efficiency as compared to the conventional approach can be obtained as fewer patients are required or a higher power is attained. We also demonstrate that efficacy again decreases if the response rates vary considerably among tumor types but it is still better than the conventional approach.",0
https://doi.org/10.3389/fnint.2012.00063,Temporal-order judgment of visual and auditory stimuli: modulations in situations with and without stimulus discrimination,"Temporal-order judgment (TOJ) tasks are an important paradigm to investigate processing times of information in different modalities. There are a lot of studies on how temporal order decisions can be influenced by stimuli characteristics. However, so far it has not been investigated whether the addition of a choice reaction time (RT) task has an influence on TOJ. Moreover, it is not known when during processing the decision about the temporal order of two stimuli is made. We investigated the first of these two questions by comparing a regular TOJ task with a dual task (DT). In both tasks, we manipulated different processing stages to investigate whether the manipulations have an influence on TOJ and to determine thereby the time of processing at which the decision about temporal order is made. The results show that the addition of a choice RT task does have an influence on the TOJ, but the influence seems to be linked to the kind of manipulation of the processing stages that is used. The results of the manipulations indicate that the temporal order decision in the DT paradigm is made after perceptual processing of the stimuli.",0
https://doi.org/10.1002/sim.6342,Statistical methods for dealing with publication bias in meta-analysis,"Publication bias is an inevitable problem in the systematic review and meta-analysis. It is also one of the main threats to the validity of meta-analysis. Although several statistical methods have been developed to detect and adjust for the publication bias since the beginning of 1980s, some of them are not well known and are not being used properly in both the statistical and clinical literature. In this paper, we provided a critical and extensive discussion on the methods for dealing with publication bias, including statistical principles, implementation, and software, as well as the advantages and limitations of these methods. We illustrated a practical application of these methods in a meta-analysis of continuous support for women during childbirth. Copyright © 2014 John Wiley & Sons, Ltd.",0
https://doi.org/10.1167/14.7.9,Quantifying the effect of intertrial dependence on perceptual decisions,"In the perceptual sciences, experimenters study the causal mechanisms of perceptual systems by probing observers with carefully constructed stimuli. It has long been known, however, that perceptual decisions are not only determined by the stimulus, but also by internal factors. Internal factors could lead to a statistical influence of previous stimuli and responses on the current trial, resulting in serial dependencies, which complicate the causal inference between stimulus and response. However, the majority of studies do not take serial dependencies into account, and it has been unclear how strongly they influence perceptual decisions. We hypothesize that one reason for this neglect is that there has been no reliable tool to quantify them and to correct for their effects. Here we develop a statistical method to detect, estimate, and correct for serial dependencies in behavioral data. We show that even trained psychophysical observers suffer from strong history dependence. A substantial fraction of the decision variance on difficult stimuli was independent of the stimulus but dependent on experimental history.We discuss the strong dependence of perceptual decisions on internal factors and its implications for correct data interpretation.",0
https://doi.org/10.3758/brm.40.1.61,Diffusion model analysis with MATLAB: A DMAT primer,"The Ratcliff diffusion model has proved to be a useful tool in reaction time analysis. However, its use has been limited by the practical difficulty of estimating the parameters. We present a software tool, the Diffusion Model Analysis Toolbox (DMAT), intended to make the Ratcliff diffusion model for reaction time and accuracy data more accessible to experimental psychologists. The tool takes the form of a MATLAB toolbox and can be freely downloaded from ppw.kuleuven.be/okp/dmatoolbox. Using the program does not require a background in mathematics, nor any advanced programming experience (but familiarity with MATLAB is useful). We demonstrate the basic use of DMAT with two examples.",0
https://doi.org/10.1111/ajps.12001,How Many Countries for Multilevel Modeling? A Comparison of Frequentist and Bayesian Approaches,"Researchers in comparative research increasingly use multilevel models to test effects of country-level factors on individual behavior and preferences. However, the asymptotic justification of widely employed estimation strategies presumes large samples and applications in comparative politics routinely involve only a small number of countries. Thus, researchers and reviewers often wonder if these models are applicable at all. In other words, how many countries do we need for multilevel modeling? I present results from a large-scale Monte Carlo experiment comparing the performance of multilevel models when few countries are available. I find that maximum likelihood estimates and confidence intervals can be severely biased, especially in models including cross-level interactions. In contrast, the Bayesian approach proves to be far more robust and yields considerably more conservative tests.",1
https://doi.org/10.1037/1082-989x.4.1.84,Sample size in factor analysis.,"The factor analysis literature includes a range of recommendations regarding the minimum sample size necessary to obtain factor solutions that are adequately stable and that correspond closely to population factors. A fundamental misconception about this issue is that the minimum sample size, or the",0
https://doi.org/10.1016/j.bbi.2010.11.018,Adoptive transfer of peripheral immune cells potentiates allodynia in a graded chronic constriction injury model of neuropathic pain,"► Intraperitoneal adoptive transfer of peripheral immune cells potentiates chronic constriction injury-induced allodynia . ► Peripheral immune cells only capable of potentiating existing allodynia, rather than establishing allodynia. ► Intraperitoneal adoptive transfer of high pain splenocytes may induce the migration of host-derived immune cells from the spleen to the CNS. ► Intrathecal transfer of “pain-activated” CD45 + cells results in potentiated allodynia. Recent evidence demonstrates that peripheral immune cells contribute to the nociceptive hypersensitivity associated with neuropathic pain by infiltrating the central nervous system (CNS). We have recently developed a rat model of graded chronic constriction injury (CCI) by varying the exposure of the sciatic nerve and control non-nerve tissue to surgical placement of chromic gut. We demonstrate that splenocytes can contribute significantly to CCI-induced allodynia, as adoptive transfer of these cells from high pain donors to low pain recipients potentiates allodynia ( P < 0.001). The phenomenon was replicated with peripheral blood mononuclear cells ( P < 0.001). Adoptive transfer of allodynia was not achieved in sham recipients, indicating that peripheral immune cells are only capable of potentiating existing allodynia, rather than establishing allodynia. As adoptively transferred cells were found by flow cytometry to migrate to the spleen ( P < 0.05) and potentiation of allodynia was prevented in splenectomised low pain recipients, adoptive transfer of high pain splenocytes may induce the migration of host-derived immune cells from the spleen to the CNS as observed by flow cytometry ( P < 0.05). Importantly, intrathecal transfer of CD45 + cells prepared from spinal cords of high pain donors into low pain recipients led to potentiated allodynia ( P < 0.001), confirming that infiltrating immune cells are not passive bystanders, but actively contribute to nociceptive hypersensitivity in the lumbar spinal cord.",0
https://doi.org/10.1890/08-0219.1,"Upper respiratory tract disease, force of infection, and effects on survival of gopher tortoises","Upper respiratory tract disease (URTD) caused by Mycoplasma agassizii has been hypothesized to contribute to the decline of some wild populations of gopher tortoises (Gopherus polyphemus). However, the force of infection (FOI) and the effect of URTD on survival in free-ranging tortoise populations remain unknown. Using four years (2003-2006) of mark-recapture and epidemiological data collected from 10 populations of gopher tortoises in central Florida, USA, we estimated the FOI (probability per year of a susceptible tortoise becoming infected) and the effect of URTD (i.e., seropositivity to M. agassizii) on apparent survival rates. Sites with high (> or = 25%) seroprevalence had substantially higher FOI (0.22 +/- 0.03; mean +/- SE) than low (< 25%) seroprevalence sites (0.04 +/- 0.01). Our results provide the first quantitative evidence that the rate of transmission of M. agassizii is directly related to the seroprevalence of the population. Seropositive tortoises had higher apparent survival (0.99 +/- 0.0001) than seronegatives (0.88 +/- 0.03), possibly because seropositive tortoises represent individuals that survived the initial infection, developed chronic disease, and experienced lower mortality during the four-year span of our study. However, two lines of evidence suggested possible effects of mycoplasmal URTD on tortoise survival. First, one plausible model suggested that susceptible (seronegative) tortoises in high seroprevalence sites had lower apparent survival rates than did susceptible tortoises in low seroprevalence sites, indicating a possible acute effect of infection. Second, the number of dead tortoise remains detected during annual site surveys increased significantly with increasing site seroprevalence, from approximately 1 to approximately 5 shell remains per 100 individuals. If (as our results suggest) URTD in fact reduces adult survival, it could adversely influence the population dynamics and persistence of this late- maturing, long-lived species.",0
https://doi.org/10.1111/j.2044-8260.2011.02021.x,The dynamic interplay between negative and positive emotions in daily life predicts response to treatment in depression: A momentary assessment study,"Although the treatment of depressive illness aims to restore the imbalance between an excess of negative affect (NA) and a shortage of positive affect (PA), no study has examined how NA and PA may influence each other in depression. This study examines how NA and PA dynamically influence each other in depression and how this may impact on treatment response.Depressed help-seeking individuals participated in the Experience Sampling Method (ESM), which enables visualization of subtle dynamic alterations of momentary affective states over time. Thereafter, participants received a combination of antidepressant treatment and psychotherapy, and were followed up each month.NA and PA were assessed during ESM at 10 random moments per day for 6 days. Depressive symptoms were assessed at baseline and at monthly intervals during treatment.Future response to treatment was associated with altered baseline NA-PA dynamics in individuals with previous depressive episodes. Their daily life boosts of PA were followed by a stronger suppression of NA over subsequent hours than in other depressed groups or controls.Subtle individual differences in daily life emotional dynamics predict future treatment outcome in depression.",0
https://doi.org/10.1002/9781118358887,Basic and Advanced Bayesian Structural Equation Modeling,"This book provides clear instructions to researchers on how to apply Structural Equation Models (SEMs) for analyzing the inter relationships between observed and latent variables. Basic and Advanced Bayesian Structural Equation Modeling introduces basic and advanced SEMs for analyzing various kinds of complex data, such as ordered and unordered categorical data, multilevel data, mixture data, longitudinal data, highly non-normal data, as well as some of their combinations. In addition, Bayesian semiparametric SEMs to capture the true distribution of explanatory latent variables are introduced, whilst SEM with a nonparametric structural equation to assess unspecified functional relationships among latent variables are also explored. Statistical methodologies are developed using the Bayesian approach giving reliable results for small samples and allowing the use of prior information leading to better statistical results. Estimates of the parameters and model comparison statistics are obtained via powerful Markov Chain Monte Carlo methods in statistical computing. Introduces the Bayesian approach to SEMs, including discussion on the selection of prior distributions, and data augmentation. Demonstrates how to utilize the recent powerful tools in statistical computing including, but not limited to, the Gibbs sampler, the Metropolis-Hasting algorithm, and path sampling for producing various statistical results such as Bayesian estimates and Bayesian model comparison statistics in the analysis of basic and advanced SEMs. Discusses the Bayes factor, Deviance Information Criterion (DIC), and $L_\nu$-measure for Bayesian model comparison. Introduces a number of important generalizations of SEMs, including multilevel and mixture SEMs, latent curve models and longitudinal SEMs, semiparametric SEMs and those with various types of discrete data, and nonparametric structural equations. Illustrates how to use the freely available software WinBUGS to produce the results. Provides numerous real examples for illustrating the theoretical concepts and computational procedures that are presented throughout the book. Researchers and advanced level students in statistics, biostatistics, public health, business, education, psychology and social science will benefit from this book. Â© 2012 John Wiley & Sons, Ltd. All rights reserved.",0
https://doi.org/10.1111/1467-9868.00384,Posterior bimodality in the balanced one-way random-effects model,"Summary. Although some researchers have examined posterior multimodality for specific richly parameterized models, multimodality is not well characterized for any such model. The paper characterizes bimodality of the joint and marginal posteriors for a conjugate analysis of the balanced one-way random-effects model with a flat prior on the mean. This apparently simple model has surprisingly complex and even bizarre mode behaviour. Bimodality usually arises when the data indicate a much larger between-groups variance than does the prior. We examine an example in detail, present a graphical display for describing bimodality and use real data sets from a statistical practice to shed light on the practical relevance of bimodality for these models.",0
https://doi.org/10.2307/2983325,League Tables and Their Limitations: Statistical Issues in Comparisons of Institutional Performance,"SUMMARY In the light of an increasing interest in the accountability of public institutions, this paper sets out the statistical issues involved in making quantitative comparisons between institutions in the areas of health and education. We deal in detail with the need to take account of model-based uncertainty in making comparisons. We discuss the need to establish appropriate measures of institutional 'outcomes' and base-line measures and the need to exercise care and sensitivity when interpreting apparent differences. The paper emphasizes that statistical methods exist which can contribute to an understanding of the extent and possible reasons for differences between institutions. It also urges caution by discussing the limitations of such methods.",0
https://doi.org/10.1111/j.0006-341x.1999.00117.x,A General Maximum Likelihood Analysis of Variance Components in Generalized Linear Models,"This paper describes an EM algorithm for nonparametric maximum likelihood (ML) estimation in generalized linear models with variance component structure. The algorithm provides an alternative analysis to approximate MQL and PQL analyses (McGilchrist and Aisbett, 1991, Biometrical Journal 33, 131-141; Breslow and Clayton, 1993; Journal of the American Statistical Association 88, 9-25; McGilchrist, 1994, Journal of the Royal Statistical Society, Series B 56, 61-69; Goldstein, 1995, Multilevel Statistical Models) and to GEE analyses (Liang and Zeger, 1986, Biometrika 73, 13-22). The algorithm, first given by Hinde and Wood (1987, in Longitudinal Data Analysis, 110-126), is a generalization of that for random effect models for overdispersion in generalized linear models, described in Aitkin (1996, Statistics and Computing 6, 251-262). The algorithm is initially derived as a form of Gaussian quadrature assuming a normal mixing distribution, but with only slight variation it can be used for a completely unknown mixing distribution, giving a straightforward method for the fully nonparametric ML estimation of this distribution. This is of value because the ML estimates of the GLM parameters can be sensitive to the specification of a parametric form for the mixing distribution. The nonparametric analysis can be extended straightforwardly to general random parameter models, with full NPML estimation of the joint distribution of the random parameters. This can produce substantial computational saving compared with full numerical integration over a specified parametric distribution for the random parameters. A simple method is described for obtaining correct standard errors for parameter estimates when using the EM algorithm. Several examples are discussed involving simple variance component and longitudinal models, and small-area estimation.",0
https://doi.org/10.1002/eat.1016,A randomized trial of a dissonance-based eating disorder prevention program,"As psychoeducational eating disorder prevention programs have not been shown to reduce bulimic pathology, we developed and evaluated a dissonance-based intervention for high-risk populations.Young women (N = 87) with body image concerns were randomized to this intervention, which involves verbal, written, and behavioral exercises requiring them to critique the thin-ideal, or to a healthy weight management control group. Participants completed a baseline, termination, and 4-week follow-up survey.Participants in the dissonance intervention reported decreased thin-ideal internalization, body dissatisfaction, dieting, negative affect, and bulimic symptoms at termination and at 4-week follow-up. Unexpectedly, participants in the healthy weight management control group also reported some benefits.Taken in conjunction with past findings, these preliminary results suggest that the dissonance intervention, and to a lesser extent the healthy weight management intervention, may reduce bulimic pathology and risk factors for eating disturbances.",0
https://doi.org/10.1016/j.visres.2009.09.021,How do amplitude spectra influence rapid animal detection?,"Amplitude spectra might provide information for natural scene classification. Amplitude does play a role in animal detection because accuracy suffers when amplitude is normalized. However, this effect could be due to an interaction between phase and amplitude, rather than to a loss of amplitude-only information. We used an amplitude-swapping paradigm to establish that animal detection is partly based on an interaction between phase and amplitude. A difference in false alarms for two subsets of our distractor stimuli suggests that the classification of scene environment (man-made versus natural) may also be based on an interaction between phase and amplitude. Examples of interaction between amplitude and phase are discussed.",0
https://doi.org/10.1111/j.2517-6161.1995.tb02031.x,Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing,"SUMMARY The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses -the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.",0
https://doi.org/10.3102/1076998610396895,Mixed and Mixture Regression Models for Continuous Bounded Responses Using the Beta Distribution,"Doubly bounded continuous data are common in the social and behavioral sciences. Examples include judged probabilities, confidence ratings, derived proportions such as percent time on task, and bounded scale scores. Dependent variables of this kind are often difficult to analyze using normal theory models because their distributions may be quite poorly modeled by the normal distribution. The authors extend the beta-distributed generalized linear model (GLM) proposed in Smithson and Verkuilen (2006) to discrete and continuous mixtures of beta distributions, which enables modeling dependent data structures commonly found in real settings. The authors discuss estimation using both deterministic marginal maximum likelihood and stochastic Markov chain Monte Carlo (MCMC) methods. The results are illustrated using three data sets from cognitive psychology experiments.",0
https://doi.org/10.1177/0003122416632212,The Theory of Legal Cynicism and Sunni Insurgent Violence in Post-Invasion Iraq,"We elaborate a cultural framing theory of legal cynicism—previously used to account for neighborhood variation in Chicago homicides—to explain Arab Sunni victimization and insurgent attacks during the U.S. post-invasion occupation of Iraq. Legal cynicism theory has an unrecognized power to explain collective and interpersonal violence in international as well as U.S. settings. We expand on how “double and linked” roles of state and non-state actors can be used to analyze violence against Arab Sunni civilians. Arab Sunnis responded to reports of unnecessary violent attacks by U.S./Coalition soldiers with a legally cynical framing of the U.S./Coalition-led invasion and occupation, the new Shia-dominated Iraqi state, and its military and police. A post-invasion frame amplification of beliefs about state-based illegitimacy, unresponsiveness, and insecurity made it not only possible but predictable that Arab Sunni insurgent attacks would continue against U.S./Coalition forces and transfer to Shia-dominated Iraqi government forces. Violence in Iraq persisted despite U.S. surge efforts to end the Arab Sunni insurgency.",0
https://doi.org/10.1007/978-3-642-20074-8_3,Computerized Adaptive Testing in Computer Assisted Learning?,"AbstractA major goal in computerized learning systems is to optimize learning, while in computerized adaptive tests (CAT) efficient measurement of the proficiency of students is the main focus. There seems to be a common interest to integrate computerized adaptive item selection in learning systems and testing. Item selection is a well founded building block of CAT. However, there are a number of problems that prevent the application of a standard approach, based on item response theory, of computerized adaptive item selection to learning systems. In this work attention will be paid to three unresolved points: item banking, item selection, and choice of IRT model. All problems will be discussed, and an approach to automated item bank generation is presented. Finally some recommendations are given.Keywordsitem-based computer assisted learningcomputer adaptive testingitem bankingitem response theoryitem selection",0
https://doi.org/10.1086/680171,Do Attitudes toward School Influence the Underachievement of Turkish and Moroccan Minority Students in Flanders? The Attitude-Achievement Paradox Revisited,"While many ethnic minority students underachieve compared with their ethnic majority peers, they often hold very positive school attitudes. Mickelson (1990) explained this attitude-achievement paradox by the existence of a double set of attitudes. Abstract attitudes reflect the dominant ideas about schooling, while concrete attitudes refer to a person’s perceptions of reality and originate from the educational benefits people expect to obtain on the labor market. According to Mickelson, only students’ concrete attitudes influence achievement. Applying Mickelson’s theory in Flanders, regarding students of Turkish and Moroccan descent, we could not find evidence that abstract and concrete attitudes play a role in the achievement of ethnic minority students. Qualitative research suggests that this could be due to distinct interpretations of success and ways of dealing with perceived constraints. This contrasts with ethnic majority students, who are more likely to end the school year unsuccessfully if they hold pessimistic concrete attitudes.",0
https://doi.org/10.1590/s0103-90162011000200015,Bayesian analysis of autoregressive panel data model: application in genetic evaluation of beef cattle,"The animal breeding values forecasting at futures times is a relevant technological innovation in the field of Animal Science, since its enables a previous indication of animals that will be either kept by the producer for breeding purposes or discarded. This study discusses an MCMC Bayesian methodology applied to panel data in a time series context. We consider Bayesian analysis of an autoregressive, AR(p), panel data model of order p, using an exact likelihood function, comparative analysis of prior distributions and predictive distributions of future observations. The methodology was tested by a simulation study using three priors: hierarchical Multivariate Normal-Inverse Gamma (model 1), independent Multivariate Student's t Inverse Gamma (model 2) and Jeffrey's (model 3). Comparisons by Pseudo-Bayes Factor favored model 2. The proposed methodology was applied to longitudinal data relative to Expected Progeny Difference (EPD) of beef cattle sires. The forecast efficiency was around 80%. Regarding the mean width of the EPD interval estimation (95%) in a future time, a great advantage was observed for the proposed Bayesian methodology over usual asymptotic frequentist method.",0
https://doi.org/10.1037/met0000028,Meaningful aspects of change as novel random coefficients: A general method for reparameterizing longitudinal models.,"A fundamental goal of longitudinal modeling is to obtain estimates of model parameters that reflect meaningful aspects of change over time. Often, a linear or nonlinear model may be sensible from a theoretical perspective, yet may have parameters that are difficult to interpret in a way that sheds light on substantive hypotheses. Fortunately, such models may be reparameterized to yield more easily interpretable parameters. This article has 3 goals. First, we provide theoretical background and elaboration on Preacher and Hancock's (2012) 4-step method for reparameterizing growth curve models. Second, we extend this method by providing a user-friendly modification of the structured latent curve model in the third step that enables fitting models that are not estimable with the original method. This modification also allows researchers to specify the mean structure without having to determine which parameters enter nonlinearly and without needing to solve complex matrix expressions. Third, we illustrate how this general reparameterization method allows researchers to treat the average rate of change, half-life, and knot (transition point) as random coefficients; these aspects of change have not before been treated as random coefficients in structural equation modeling. We supply Mplus code for illustrative examples in an online supplement. Our core message is that growth curve models are considerably more flexible than most researchers may suspect. Virtually any parameter can be treated as a random coefficient that varies across individuals. Alternative parameterizations of a given model may yield unique insights that are not available with traditional parameterizations.",0
https://doi.org/10.1371/journal.pone.0129293,Advanced Insights into Functional Brain Connectivity by Combining Tensor Decomposition and Partial Directed Coherence,"Quantification of functional connectivity in physiological networks is frequently performed by means of time-variant partial directed coherence (tvPDC), based on time-variant multivariate autoregressive models. The principle advantage of tvPDC lies in the combination of directionality, time variance and frequency selectivity simultaneously, offering a more differentiated view into complex brain networks. Yet the advantages specific to tvPDC also cause a large number of results, leading to serious problems in interpretability. To counter this issue, we propose the decomposition of multi-dimensional tvPDC results into a sum of rank-1 outer products. This leads to a data condensation which enables an advanced interpretation of results. Furthermore it is thereby possible to uncover inherent interaction patterns of induced neuronal subsystems by limiting the decomposition to several relevant channels, while retaining the global influence determined by the preceding multivariate AR estimation and tvPDC calculation of the entire scalp. Finally a comparison between several subjects is considerably easier, as individual tvPDC results are summarized within a comprehensive model equipped with subject-specific loading coefficients. A proof-of-principle of the approach is provided by means of simulated data; EEG data of an experiment concerning visual evoked potentials are used to demonstrate the applicability to real data.",0
https://doi.org/10.1002/sim.1113,"Estimation of bivariate measurements having different change points, with application to cognitive ageing","Longitudinal studies of ageing make repeated observations of multiple measurements on each subject. Change point models are often used to model longitudinal data. We demonstrate the use of Bayesian and profile likelihood methods to simultaneously estimate different change points in the longitudinal course of two different measurements of cognitive function in subjects in the Bronx Aging Study who developed Alzheimer's disease (AD). Analyses show that accelerated memory decline, as measured by Buschke Selective Reminding, begins between seven and eight years before diagnosis of AD, while decline in performance on speeded tasks as measured by WAIS Performance IQ begins slightly more than two years before diagnosis, significantly after the decline in memory.",0
https://doi.org/10.1177/0962280207088026,Optimal Designs for Empirical Bayes Estimators of Individual Linear and Quadratic Growth Curves in Linear Mixed Models,"Many studies on optimal designs for linear mixed model analysis of repeated measures data have focussed on estimating the fixed effects. The present study investigates the optimal number of time points and subjects in case random effects have to be estimated. Linear mixed models with a linear or quadratic trend across equidistant time points are studied. Given a particular cost function, we examine which designs minimise the expected average squared prediction error. Robustness of the optimal design, important when one does not know the underlying model, is also treated.",0
https://doi.org/10.3389/fpsyg.2015.00615,The benefits of looking at intraindividual dynamics in cognitive training data.,"Over the last decade, the prospect of improving or maintaining cognitive functioning has provoked a steadily increasing number of cognitive training studies. Central target populations are individuals at risk for a disadvantageous development, such as older adults exhibiting cognitive decline or children with learning impairments. They rely on cognitive resources to meet the challenges of an independent life in old age or requirements at school.  To support daily cognitive functioning, training outcomes need to generalize to other cognitive abilities. Such transfer effects are, however, highly discussed. For example, recent meta-analyses on working memory training differed in the conclusion on the presence (Au et al., 2015; Karbach and Verhaeghen, 2014) or absence of transfer effects (Melby-Lervag and Hulme, 2013). Usually training-specific design factors such as type, intensity, duration, and feedback routines are discussed as reasons for such inconsistent findings. However, even individuals participating in exactly the same training regime highly differ in their training outcomes. We argue that it is time to study the individual development during trainings to understand these differential outcomes. It is time to have a closer look at the intraindividual training data.",0
https://doi.org/10.1111/j.1467-985x.2011.00685.x,Assessing variance components in multilevel linear models using approximate Bayes factors: a case-study of ethnic disparities in birth weight,"Racial/ethnic disparities in birthweight are a large source of differential morbidity and mortality worldwide and have remained largely unexplained in epidemiologic models. We assess the impact of maternal ancestry and census tract residence on infant birth weights in New York City and the modifying effects of race and nativity by incorporating random effects in a multilevel linear model. Evaluating the significance of these predictors involves the test of whether the variances of the random effects are equal to zero. This is problematic because the null hypothesis lies on the boundary of the parameter space. We generalize an approach for assessing random effects in the two-level linear model to a broader class of multilevel linear models by scaling the random effects to the residual variance and introducing parameters that control the relative contribution of the random effects. After integrating over the random effects and variance components, the resulting integrals needed to calculate the Bayes factor can be efficiently approximated with Laplace's method.",0
https://doi.org/10.1016/j.jmva.2012.11.010,Bayesian modeling of the dependence in longitudinal data via partial autocorrelations and marginal variances,"Many parameters and positive-definiteness are two major obstacles in estimating and modelling a correlation matrix for longitudinal data. In addition, when longitudinal data is incomplete, incorrectly modelling the correlation matrix often results in bias in estimating mean regression parameters. In this paper, we introduce a flexible and parsimonious class of regression models for a covariance matrix parameterized using marginal variances and partial autocorrelations. The partial autocorrelations can freely vary in the interval (-1, 1) while maintaining positive definiteness of the correlation matrix so the regression parameters in these models will have no constraints. We propose a class of priors for the regression coefficients and examine the importance of correctly modeling the correlation structure on estimation of longitudinal (mean) trajectories and the performance of the DIC in choosing the correct correlation model via simulations. The regression approach is illustrated on data from a longitudinal clinical trial.",0
https://doi.org/10.1007/978-1-4939-2104-1_42,A Review of Multilevel Modeling: Some Methodological Issues and Advances,"Multilevel modeling is a recently new class of statistical methods to handle nested data. Mainly thanks to the wide range of applicability and the great increase of statistical softwares, in the last decades multilevel modeling has enjoyed an explosion of published papers and books in both methodological and application field. Currently, there is a need to not only develop the research on multilevel approach for the analysis of complex data, but also to have instructions to properly address the usage. This work aims at summarizing methodological aspects related to multilevel models, illustrating good-practices, advantages, and limits by reviewing applications in various fields, such as socio-economic, educational, health, and medical sciences. We further focus our attention on the latest advances of multilevel modeling towards, e.g., the inclusion of latent variables and the Bayesian approach.",0
https://doi.org/10.2202/1557-4679.1350,Meta-Analysis of Observational Studies with Unmeasured Confounders,"Meta-analysis of observational studies is an exciting new area of innovation in statistical science. Unlike randomized controlled trials, which are the gold standard for proving causation, observational studies are prone to biases including confounding. In this article, we describe a novel Bayesian procedure to control for a confounder that is missing across the sequence of studies in a meta-analysis. We motivate the discussion with the example of a meta-analysis of cohort, case-control and cross-sectional studies examining the relationship between oral contraceptives and endometriosis. An important unmeasured confounder is dysmennoreah, which is an indication for oral contraceptive use. To adjust for unmeasured confounding, we combine random effects models with probabilistic sensitivity analysis techniques. Information about the unmeasured confounder is incorporated into the analysis via prior distributions, and we use MCMC to sample from posterior.",0
https://doi.org/10.1016/j.electstud.2016.01.008,Strange bedfellows: Coalition makeup and perceptions of democratic performance among electoral winners,"We argue that the partisan makeup of governing coalitions affects perceptions of democratic performance among those who voted for a government party. We introduce ambivalence toward the governing parties as the mechanism that drives this relationship, and we argue that such ambivalence, which occurs when favorability ratings of the parties vary, will be more common where the parties are more ideologically diverse. After advancing our theory, we test our expectations with post-election survey data from several countries. Evidence demonstrates that coalition ambivalence is greater where governing parties are ideologically divergent, and, even when controlling for this ideological divergence, ambivalence leads to more negative perceptions of democratic performance, bringing the attitudes of electoral winners closer to those of individuals who did not vote for a party in government.",0
https://doi.org/10.1177/0049124194022003002,Hierarchical Regression Models for Interviewer and Respondent Effects,"It is generally recognized that interviewers may have an important effect on the quality of the data collected in survey research. This article presents an application of the hierarchical regression model in the analysis of interviewer effects. The hierarchical regression model offers an elegant way of analyzing the simultaneous effects of specific interviewer and respondent characteristics. It is especially attractive if the research design does not provide for a random assignment of respondents to interviewers, because it allows the researcher to use statistical rather than experimental control by modeling the interviewer effects conditional on the respondent effects.",0
https://doi.org/10.1161/circgenetics.113.000299,Analysis of Circulating Cholesterol Levels as a Mediator of an Association Between ABO Blood Group and Coronary Heart Disease,"Background— Non-O type of ABO blood group has been associated with a predisposition to coronary heart disease. It is thought that this association is partly mediated by increased cholesterol levels in non–O-type individuals. In this study, we sought to estimate the mediation effect size. Methods and Results— In a group of individuals (n=6476) undergoing coronary angiography, we detected associations of non-O type with significant coronary artery disease with &gt;50% stenosis in ≥1 coronary arteries (odds ratio, 1.24; 95% confidence interval, 1.10–1.39; P =2.6×10 −4 ) and with prevalent or incident myocardial infarction (odds ratio, 1.22; 95% confidence interval, 1.09–1.37; P =1.2×10 −3 ). Subjects of non-O type had higher levels of total cholesterol, low-density lipoprotein cholesterol, and non–high-density lipoprotein cholesterol (mean [SEM] in mmol/L: 4.931[0.021], 3.041 [0.018], and 3.805 [0.020] in non-O type compared with 4.778 [0.026], 2.906 [0.021], and 3.669 [0.024] in O type; P =3.8×10 −7 , P =1.5×10 −7 , and P =3.1×10 −7 , respectively). Mediation analyses indicated that 10% of the effect of non-O type on coronary artery disease susceptibility was mediated by increased low-density lipoprotein cholesterol level ( P =7.8×10 −4 ) and that 11% of the effect of non-O type on myocardial infarction risk was mediated by raised low-density lipoprotein cholesterol level ( P =2.0×10 −3 ). Conclusions— In a model in which it is presumed that cholesterol is a mediator of the associations of ABO group with coronary artery disease and myocardial infarction, around 10% of the effect of non-O type on coronary artery disease and myocardial infarction susceptibility was mediated by its influence on low-density lipoprotein cholesterol level.",0
https://doi.org/10.1079/9780851990002.0235,Estimation of genetic parameters.,,0
https://doi.org/10.1111/j.1460-9568.2005.04470.x,"An initial investigation of spinal mechanisms underlying pain enhancement induced by fractalkine, a neuronally released chemokine","Fractalkine is a chemokine that is tethered to the extracellular surface of neurons. Fractalkine can be released, forming a diffusible signal. Spinal fractalkine (CX3CL1) is expressed by sensory afferents and intrinsic neurons, whereas its receptor (CX3CR1) is predominantly expressed by microglia. Pain enhancement occurs in response both to intrathecally administered fractalkine and to spinal fractalkine endogenously released by peripheral neuropathy. The present experiments examine whether fractalkine-induced pain enhancement is altered by a microglial inhibitor (minocycline) and/or by antagonists/inhibitors of three putative glial products implicated in pain enhancement: interleukin-1 (IL1), interleukin-6 (IL6) and nitric oxide (NO). In addition, it extends a prior study that demonstrated that intrathecal fractalkine-induced mechanical allodynia is blocked by a neutralizing antibody to the rat fractalkine receptor, CX3CR1. Here, intrathecal anti-CX3CR1 also blocked fractalkine-induced thermal hyperalgesia. Furthermore, blockade of microglial activation with minocycline prevented both fractalkine-induced mechanical allodynia (von Frey test) and thermal hyperalgesia (Hargreaves test). Microglial activation appears to lead to the release of IL1, given that pretreatment with IL1 receptor antagonist blocked both fractalkine-induced mechanical allodynia and thermal hyperalgesia. IL1 is not the only proinflammatory cytokine implicated, as a neutralizing antibody to rat IL6 also blocked fractalkine-induced pain facilitation. Lastly, NO appears to be importantly involved, as l-NAME, a broad-spectrum NO synthase inhibitor, also blocked fractalkine-induced effects. Taken together, these data support that neuronally released fractalkine enhances pain via activation of spinal cord glia. Thus, fractalkine may be a neuron-to-glia signal triggering pain facilitation.",0
https://doi.org/10.1016/j.jspi.2005.01.002,Complex elliptical distributions with application to shape analysis,"We introduce a general class of complex elliptical distributions on a complex sphere that includes many of the most commonly used distributions, like the complex Watson, Bingham, angular central Gaussian and several others. We study properties of this family of distributions and apply the distribution theory for modeling shapes in two dimensions. We develop maximum likelihood and Bayesian methods of estimation to describe shape and obtain confidence bounds and credible regions for shapes. The methodology is illustrated through an example where estimation of shape of mouse vertebrae is desired.",0
https://doi.org/10.1037/1082-989x.12.4.434,Allowing for correlations between correlations in random-effects meta-analysis of correlation matrices.,"Practical meta-analysis of correlation matrices generally ignores covariances (and hence correlations) between correlation estimates. The authors consider various methods for allowing for covariances, including generalized least squares, maximum marginal likelihood, and Bayesian approaches, illustrated using a 6-dimensional response in a series of psychological studies concerning prediction of exercise behavior change. Quantities of interest include the overall population mean correlation matrix, the contrast between the mean correlations, the predicted correlation matrix in a new study, and the conflict between the existing studies and a new correlation matrix. The authors conclude that accounting for correlations between correlations is unnecessary when interested in individual correlations but potentially important if concerned with a composite measure involving 2 or more correlations. A simulation study indicates the asymptotic normal assumption appears reasonable. Because of potential instability in the generalized least squares methods, they recommend a model-based approach, either the maximum marginal likelihood approach or a full Bayesian analysis.",0
https://doi.org/10.1177/0146621615591094,Detecting Intervention Effects in a Cluster-Randomized Design Using Multilevel Structural Equation Modeling for Binary Responses,"Multilevel modeling (MLM) is frequently used to detect group differences, such as an intervention effect in a pre-test–post-test cluster-randomized design. Group differences on the post-test scores are detected by controlling for pre-test scores as a proxy variable for unobserved factors that predict future attributes. The pre-test and post-test scores that are most often used in MLM are summed item responses (or total scores). In prior research, there have been concerns regarding measurement error in the use of total scores in using MLM. To correct for measurement error in the covariate and outcome, a theoretical justification for the use of multilevel structural equation modeling (MSEM) has been established. However, MSEM for binary responses has not been widely applied to detect intervention effects (group differences) in intervention studies. In this article, the use of MSEM for intervention studies is demonstrated and the performance of MSEM is evaluated via a simulation study. Furthermore, the consequences of using MLM instead of MSEM are shown in detecting group differences. Results of the simulation study showed that MSEM performed adequately as the number of clusters, cluster size, and intraclass correlation increased and outperformed MLM for the detection of group differences.",0
https://doi.org/10.1002/sim.4780121802,Reporting bayesian analyses of clinical trials,"Many clinicians wrongly interpret p-values as probabilities that treatment has an adverse effect and confidence intervals as probability intervals. Such inferences can be validly drawn from Bayesian analyses of trial results. These analyses use the data to update the prior (or pre-trial) beliefs to give posterior (or post-trial) beliefs about the magnitude of a treatment effect. However, for these methods to gain acceptance in the medical literature, understanding between statisticians and clinicians of the issues involved in choosing appropriate prior distributions for trial reporting needs to be reached. I focus on two types of prior that deserve consideration. The first is the non-informative prior giving standardized likelihood distributions as post-trial probability distributions. Their use is unlikely to be controversial among statisticians whilst being intuitively appealing to clinicians. The second type of prior has a spike of probability mass at the point of no treatment effect. Varying the magnitude of the spike illustrates the sensitivity of the conclusions drawn to the degree of prior scepticism in a treatment effect. With both, graphical displays provide clinical readers with the opportunity to explore the results more fully. An example of how a clinical trial might be reported in the medical literature using these methods is given.",0
https://doi.org/10.1348/014466603322595248,Acting on intentions: The role of anticipated regret,"Three studies tested the hypothesis that anticipated regret (AR) increases consistency between exercise intentions and behaviour. Study 1 employed a longitudinal survey design (N = 384). Measures specified by the theory of planned behaviour, past behaviour, and AR were used to predict self-reported exercise behaviour 2 weeks later. AR moderated the intention-behaviour relationship such that participants were most likely to exercise if they both intended to exercise and anticipated regret if they failed to exercise. Study 2 used an experimental design to examine the effect of focusing on AR prior to reporting intentions (N = 229). Exercise was measured 2 weeks later and the AR-focus manipulation was found to moderate the intention-behaviour relationship in a similar manner to that observed in Study 1. In Study 3 (N = 97), moderation was replicated and was shown to be mediated by the temporal stability of intention.",0
https://doi.org/10.1002/sim.2575,Bayesian meta-analysis and meta-regression for gene–disease associations and deviations from Hardy–Weinberg equilibrium,"Violation of Hardy–Weinberg equilibrium (HWE) can raise doubts about the validity of the conclusions from genetic association studies. However, for most currently performed gene–disease association studies, the available tests have low power to detect deviations from HWE. We consider this issue from a meta-analysis perspective, and suggest an approach to estimate the deviation and investigate its relationship with the observed genetic effects. Different degrees of deviation from HWE have previously been proposed as a potential source of heterogeneity across studies. We present a hierarchical meta-regression model that can be applied to test this assumption, using the concept of the fixation coefficient. We re-analyse seven meta-analyses to illustrate these methods. The uncertainty in the genetic effect estimate tended to increase once the fixation coefficient was taken into account. Dependence of the genetic effect size on the deviation from HWE was found in one meta-analysis, while in the other six examples, deviations from HWE did not clearly explain between-study heterogeneity in the genetic effects. The proposed hierarchical models allow the synthesis of data across gene–disease association studies with appropriate consideration of HWE issues. Copyright © 2006 John Wiley & Sons, Ltd.",0
https://doi.org/10.1080/10705510902751275,Bayesian Analysis of Multivariate Latent Curve Models With Nonlinear Longitudinal Latent Effects,"In longitudinal studies, investigators often measure multiple variables at multiple time points and are interested in investigating individual differences in patterns of change on those variables. Furthermore, in behavioral, social, psychological, and medical research, investigators often deal with latent variables that cannot be observed directly and should be measured by 2 or more manifest variables. Longitudinal latent variables occur when the corresponding manifest variables are measured at multiple time points. Our primary interests are in studying the dynamic change of longitudinal latent variables and exploring the possible interactive effect among the latent variables.Much of the existing research in longitudinal studies focuses on studying change in a single observed variable at different time points. In this article, we propose a novel latent curve model (LCM) for studying the dynamic change of multivariate manifest and latent variables and their linear and interaction relationships. The proposed LCM has the following useful features: First, it can handle multivariate variables for exploring the dynamic change of their relationships, whereas conventional LCMs usually consider change in a univariate variable. Second, it accommodates both first- and second-order latent variables and their interactions to explore how changes in latent attributes interact to produce a joint effect on the growth of an outcome variable. Third, it accommodates both continuous and ordered categorical data, and missing data.",0
https://doi.org/10.1016/s0197-2456(96)00043-8,Comments on Bayesian and frequentist analysis and interpretation of clinical trials,"In this commentary I will advance opinions about the role and usefulness of Bayesian statistical methods in randomized clinical trials (RCTs). This commentary deals with RCTs to establish the efficacy of a treatment-what are often categorized as phase III trials in the U.S. Food and Drug Administration (FDA) regulatory setting. Both frequentist and Bayesian trial designs represent “moving targets”; furthermore, the meaning of frequentist or Bayesian methods has different meanings to different individuals. I shall use the term Bayesian here to refer to methods that use the subjective probability assessments of individuals (often based on considerable data). Thus I will not be speaking to any proposals that would (1) use empirical Bayes prior distributions or (2) use standardized diffuse, improper, minimum information or “noninformative” prior distributions. Such methods of analysis I will call “stylized Bayesian methods” and will address briefly at the end of this commentary. Savage [ll presents a compelling argument that humans should behave in a Bayesian fashion unless (when forced to make bets) one wants to be in a losing position for every possible true state of affairs (i.e., a “state of nature”). Such Bayesian behavior is “coherent“ in Savage’s terminology. Otherwise behavior is “incoherent.” The psychological literature (which is referenced somewhat in the following) refers to “normative” behavior. Here normative means “of, relating or conforming to, or prescribing norms or standards” [21. For reasoning involving probabilities the norm used is that of the usual calculus of probability. In a situation with prior distributions the norm involves an appropriate use of Bayes’s theorem. Speck, quoted above, apparently does not believe that humans follow this normative behavior.",0
https://doi.org/10.1016/s0167-7152(00)00182-6,On the propriety of a modified Jeffreys's prior for variance components in binary random effects models,"This paper proves that a modified Jeffreys's prior on the variance components in binary random effects models is integrable under mild conditions on the link function. These conditions are shown to be satisfied for two commonly used link functions, the logit and probit functions.",0
https://doi.org/10.1186/2046-4053-1-61,Pharmacologic interventions for painful diabetic neuropathy: an umbrella systematic review and comparative effectiveness network meta-analysis (Protocol),"Neuropathic pain can reduce the quality of life and independence of 30% to 50% of patients with diabetes. The comparative effectiveness of analgesics for patients with diabetic neuropathy remains unclear. The aim of the current work, therefore, was to summarize the evidence about the analgesic effectiveness of the most common oral and topical agents used for the treatment of peripheral diabetic neuropathy.We will use an umbrella approach (systematic review of systematic reviews) to identify eligible randomized controlled trials (RCTs) for the most common oral or topical analgesics for painful diabetic neuropathy. Two reviewers will independently determine RCT eligibility. Disagreement will be solved by consensus and arbitrated by a third reviewer. We will extract descriptive, methodological and efficacy data in duplicate. Results will be pooled and analyzed using classic random-effects meta-analyses and network meta-analyses to compute the absolute and relative efficacy of therapeutic options. We will use the I2 statistic and Cochran's Q test to assess heterogeneity. Risk of bias and publication bias, if appropriate, will be evaluated, as well as overall strength of the evidence.This network meta-analysis aims to synthesize available direct and indirect evidence of effectiveness of analgesics in the treatment of painful diabetic neuropathy. The network approach will offer the opportunity to generate a ranking based on efficacy and along with known side effects, costs, and administration burdens will enable patients and clinicians to make choices that best reflect their preferences for treatment of painful diabetic neuropathy.",0
https://doi.org/10.2307/2095413,Does Arrest Really Deter Wife Battery? An Effort to Replicate the Findings of the Minneapolis Spouse Abuse Experiment,"In this paper we try to replicate the findings from the Minneapolis Spouse Abuse Experiment (Sherman and Berk, 1984). Using longitudinal data from the criminal justice system on 783 wife-battery incidents, an ex post factor design coupled with a propensity-score analysis reveals that arrests substantially reduce the number of new incidents of wife battery. In addition, the reductions are greatest for batterers whom the police would ordinarily be especially inclined to arrest. Policy and theoretical implications are discussed. (abstract Adapted from Source: American Sociological Review, 1985. Copyright © 1985 by the American Sociological Association) Domestic Violence Intervention Arrest Effects Domestic Violence Offender Spouse Abuse Offender Spouse Abuse Intervention Deterrence Male Offender Male Violence Adult Male Adult Offender Adult Violence Law Enforcement Intervention Offender Arrest Violence Against Women Partner Violence California Replication Studies 11-01",0
https://doi.org/10.1177/0020715215587772,Work experience during higher education and post-graduation occupational outcomes: A comparative study on four European countries,"This article examines the relationship between work experience acquired during higher education and post-graduation labour market outcomes in four European countries: Germany, Italy, Norway and Spain. A theoretical framework that shows in which institutional contexts work experience may be a ‘competitive advantage’ for young graduates is developed. In the empirical analysis, data from the Higher Education and Graduate Employment in Europe (CHEERS) and Research into Employment and Professional Flexibility (REFLEX) surveys are used to examine the effect of a typology of student employment (accounting for both length and coherence of work experience with the field of study attended) on several occupational outcomes 4–5 years after graduation. The empirical results show that, in Italy and especially in Spain, work activities during tertiary education are associated with better labour market positions after graduation: any type of work experience increases employability and reduces the risk of unemployment, and furthermore, previous work experience – especially when coherent with the field of study – decreases the probability of skill mismatch in future occupations. The effect of student employment, however, is smaller for most outcomes in Germany and negligible in Norway.",0
https://doi.org/10.1016/s0169-7161(06)26021-8,21 Assessing the Fit of Item Response Theory Models,"Publisher Summary Item response theory (IRT) provides a framework for modeling and analyzing item response data. Assessing IRT model fit to item response data is one of the crucial steps before an IRT model can be applied with confidence to estimate proficiency or ability levels of examinees, to link tests across administrations, and to assess adequate yearly progress. Assessing model fit is an important part of the test validation process. It is multifaceted and, as in the verification of any scientific theory, it is an ongoing process where only through the accumulation of empirical evidence can one be confident of the appropriateness of IRT for the solution of a particular measurement problem. The procedures for assessing model–data fit described in this chapter have the potential for addressing the vexing problem of determining if the measurement procedures used are appropriate for addressing the practical measurement problems faced by practitioners.",0
https://doi.org/10.1007/s004170000243,An explicit no response instead of time-out in automated visual-field testing,"Background: To evaluate the effect of response-acquisition technique on psychometric performance in visual-field testing, the conventional one-button yes/time-out method was compared with a two-button yes/no method for responding whether or not the stimulus was detected. There are a number of situations in which the single-button technique leads to ambiguous results. In this study, we thus expected the yes/no method to reduce tendencies towards habituation and automatic responding. Our hypothesis was that the two-button technique could reduce the rate of erroneous responses. Methods: Luminance-difference sensitivity for bright stimuli (32â€²) on a photopic background was evaluated at 26 locations within the central visual field (30Â°) using a specially equalised video display unit and a modified 4/2-dB staircase strategy (six reversals, maximum-likelihood threshold estimation). Sixty-one ophthalmologically normal subjects (aged 20-30 years) were examined twice with each method. Results: Mean sensitivities with the two-button yes/no method were found to be, on average, 0.13 dB above those measured with the one-button yes/time-out technique - a difference without clinical relevance. Within-subject variability did not differ between the two methods. However, the less intuitive two-button yes/no method had a slightly higher number of false responses in catch trials. Conclusion: Compared to the conventional one-button yes/time-out method, the two-button yes/no method in normal young subjects thus showed little difference in mean sensitivities and equivalent within-subject variabilities. Concerning our initial hypothesis, the yes/no method is of somewhat higher complexity and is not able to reduce the rate of erroneous responses. The one-button yes/time-out method fared a little better in error rate. In summary, the yes/no method is an alternative and additional possibility of response acquisition in visual-field testing, which is worthy of being tested in a clinical study with elderly subjects.",0
https://doi.org/10.1121/1.4757740,Bayesian three-dimensional reconstruction of toothed whale trajectories: Passive acoustics assisted with visual and tagging measurements,The author describes and evaluates a Bayesian method to reconstruct three-dimensional toothed whale trajectories from a series of echolocation signals. Localization by using passive acoustic data (time of arrival of source signals at receptors) is assisted by using visual data (coordinates of the whale when diving and resurfacing) and tag information (movement statistics). The efficiency of the Bayesian method is compared to the standard minimum mean squared error statistical approach by comparing the reconstruction results of 48 simulated sperm whale (Physeter macrocephalus) trajectories. The use of the advanced Bayesian method reduces bias (standard deviation) with respect to the standard method up to a factor of 8.9 (13.6). The author provides open-source software which is functional with acoustic data which would be collected in the field from any three-dimensional receptor array design. This approach renews passive acoustics as a valuable tool to study the underwater behavior of toothed whales.,0
https://doi.org/10.1186/s12874-016-0130-6,Mechanisms and mediation in survival analysis: towards an integrated analytical framework,"A wide-ranging debate has taken place in recent years on mediation analysis and causal modelling, raising profound theoretical, philosophical and methodological questions. The authors build on the results of these discussions to work towards an integrated approach to the analysis of research questions that situate survival outcomes in relation to complex causal pathways with multiple mediators. The background to this contribution is the increasingly urgent need for policy-relevant research on the nature of inequalities in health and healthcare.The authors begin by summarising debates on causal inference, mediated effects and statistical models, showing that these three strands of research have powerful synergies. They review a range of approaches which seek to extend existing survival models to obtain valid estimates of mediation effects. They then argue for an alternative strategy, which involves integrating survival outcomes within Structural Equation Models via the discrete-time survival model. This approach can provide an integrated framework for studying mediation effects in relation to survival outcomes, an issue of great relevance in applied health research. The authors provide an example of how these techniques can be used to explore whether the social class position of patients has a significant indirect effect on the hazard of death from colon cancer.The results suggest that the indirect effects of social class on survival are substantial and negative (-0.23 overall). In addition to the substantial direct effect of this variable (-0.60), its indirect effects account for more than one quarter of the total effect. The two main pathways for this indirect effect, via emergency admission (-0.12), on the one hand, and hospital caseload, on the other, (-0.10) are of similar size.The discrete-time survival model provides an attractive way of integrating time-to-event data within the field of Structural Equation Modelling. The authors demonstrate the efficacy of this approach in identifying complex causal pathways that mediate the effects of a socio-economic baseline covariate on the hazard of death from colon cancer. The results show that this approach has the potential to shed light on a class of research questions which is of particular relevance in health research.",0
https://doi.org/10.1177/1094428115588570,Multilevel Latent Polynomial Regression for Modeling (In)Congruence Across Organizational Groups,"This article addresses (in)congruence across different kinds of organizational respondents or “organizational groups”—such as managers versus non-managers or women versus men—and the effects of congruence on organizational outcomes. We introduce a novel multilevel latent polynomial regression model (MLPM) that treats standings of organizational groups as latent “random intercepts” at the organization level while subjecting these to latent interactions that enable response surface modeling to test congruence hypotheses. We focus on the case of organizational culture research, which usually samples managers and excludes non-managers. Reanalyzing data from 67 hospitals with 6,731 managers and non-managers, we find that non-managers perceive their organizations’ cultures as less humanistic and innovative and more controlling than managers, and we find that less congruence between managers and non-managers in these perceptions is associated with lower levels of quality improvement in organizations. Our results call into question the validity of findings from organizational culture and other research that tends to sample one organizational group to the exclusion of others. We discuss our findings and the MLPM, which can be extended to estimate latent interactions for tests of multilevel moderation/interactions.",0
https://doi.org/10.1007/bf02296339,Bayesian factor analysis for multilevel binary observations,"Multilevel covariance structure models have become increasingly popular in the psychometric literature in the past few years to account for population heterogeneity and complex study designs. We develop practical simulation based procedures for Bayesian inference of multilevel binary factor analysis models. We illustrate how Markov Chain Monte Carlo procedures such as Gibbs sampling and Metropolis-Hastings methods can be used to perform Bayesian inference, model checking and model comparison without the need for multidimensional numerical integration. We illustrate the proposed estimation methods using three simulation studies and an application involving student's achievement results in different areas of mathematics.",0
https://doi.org/10.1080/03610739208253917,Modeling incomplete longitudinal and cross-sectional data using latent growth structural models,"In this paper we describe some mathematical and statistical models for identifying and dealing with changes over age. We concentrate specifically on the use of a latent growth structural equation model approach to deal with issues of: (1) latent growth models of change, (2) differences in longitudinal and cross-sectional results, and (3) differences due to longitudinal attrition. This is a methodological paper using simulated data, but we base our models on practical and conceptual principles of modeling change in developmental psychology. Our results illustrate both benefits and limitations using structural models to analyze incomplete longitudinal data.",0
https://doi.org/10.1016/s0191-8869(99)00251-2,Working memory capacity — facets of a cognitive ability construct,"Abstract Working memory capacity is differentiated theoretically along two dimensions: contents and functions. The resulting 3×3 matrix was operationalized by 23 tasks sampled from the literature. Data for these tasks from 128 participants were analyzed by exploratory and confirmatory factor analysis. Regarding the content facet, spatial working memory was clearly distinct from the other two content categories. A distinction between verbal and numerical working memory was not warranted. On the functional dimension the postulated categories of simultaneous storage and transformation and of coordination could not be separated. The third category was clearly separate from the first two functions. This factor could be interpreted to reflect a mixture of variance due to mental speed and to supervisory functions of the central executive.",0
https://doi.org/10.1037/a0029801,Extracting the truth from conflicting eyewitness reports: A formal modeling approach.,"Eyewitnesses often report details of the witnessed crime incorrectly. However, there is usually more than 1 eyewitness observing a crime scene. If this is the case, one approach to reconstruct the details of a crime more accurately is aggregating across individual reports. Although aggregation likely improves accuracy, the degree of improvement largely depends on the method of aggregation. The most straightforward method is the majority rule. This method ignores individual differences between eyewitnesses and selects the answer shared by most eyewitnesses as being correct. We employ an alternative method based on cultural consensus theory (CCT) that accounts for differences in the eyewitnesses' knowledge. To test the validity of this approach, we showed 30 students 1 of 2 versions of a video depicting a heated quarrel between 2 people. The videos differed in the amount of information pertaining to the critical event. Participants then answered questions about the critical event. Analyses based on CCT rendered highly accurate eyewitness competence estimates that mirrored the amount of information available in the video. Moreover, CCT estimates resulted in a more precise reconstruction of the video content than the majority rule did. This was true for group sizes ranging from 4 to 15 eyewitnesses, with the difference being more pronounced for larger groups. Thus, through simultaneous consideration of multiple witness statements, CCT provides a new approach to the assessment of eyewitness accuracy that outperforms standard methods of information aggregation.",0
https://doi.org/10.1002/sim.4326,An informed reference prior for between-study heterogeneity in meta-analyses of binary outcomes,"It is well known that when a Bayesian meta-analysis includes a small number of studies, inference can be sensitive to the choice of prior for the between-study variance. Choosing a vague prior does not solve the problem, as inferences can be substantially different depending on the degree of vagueness. Moreover, because the data provide little information on between-study heterogeneity, posterior inferences for the between-study variance based on vague priors will tend to be unrealistic. It is thus preferable to adopt a reasonable, informed prior for the between-study variance. However, relatively little is known about what constitutes a realistic distribution. On the basis of data from the Cochrane Database of Systematic Reviews, this paper describes the distribution of between-study variance in published meta-analyses, and proposes some realistic, informed priors for use in meta-analyses of binary outcomes. It is hoped that these priors will improve the calibration of inferences from Bayesian meta-analyses.",0
https://doi.org/10.1007/bf02295132,Multidimensional adaptive testing with constraints on test content,"The case of adaptive testing under a multidimensional response model with large numbers of constraints on the content of the test is addressed. The items in the test are selected using a shadow test approach. The 0-1 linear programming model that assembles the shadow tests maximizes posterior expected Kullback-Leibler information in the test. The procedure is illustrated for five different cases of multidimensionality. These cases differ in (a) the numbers of ability dimensions that are intentional or should be considered as ""nuisance dimensions"" and (b) whether the test should or should not display a simple structure with respect to the intentional ability dimensions.",0
https://doi.org/10.1016/j.pscychresns.2013.01.009,An application of item response theory to fMRI data: Prospects and pitfalls,"When using functional brain imaging to study neuropsychiatric patients an important challenge is determining whether the imaging task assesses individual differences with equal precision in healthy control and impaired patient groups. Classical test theory (CTT) requires separate reliability studies of patients and controls to determine equivalent measurement precision with additional studies to determine measurement precision for different levels of disease severity. Unlike CTT, item response theory (IRT) provides estimates of measurement error for different levels of ability, without the need for separate studies, and can determine if different tests are equivalently difficult when investigating differential deficits between groups. To determine the potential value of IRT in functional brain imaging, IRT was applied to behavioral data obtained during a multi-center functional MRI (fMRI) study of working memory (WM). Average item difficulty was approximately one standard deviation below the ability scale mean, supporting the task's sensitivity to individual differences within the ability range of patients with WM impairment, but not within the range of most controls. The correlation of IRT estimated ability with fMRI activation during the task recognition period supported the linkage of the latent IRT scale to brain activation data. IRT can meaningfully contribute to the design of fMRI tasks.",0
https://doi.org/10.1037/0033-295x.84.1.1,"Controlled and automatic human information processing: I. Detection, search, and attention.","A 2-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activation of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically-without S control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a sequence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the S. A series of studies, with approximately 8 Ss, using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search was utilized in varied-mapping paradigms, and in the present studies, it took the form of serial, terminating search. (60 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1977 American Psychological Association.",0
https://doi.org/10.1198/jasa.2009.ap09068,Hierarchical Spatial Process Models for Multiple Traits in Large Genetic Trials,"This article expands upon recent interest in Bayesian hierarchical models in quantitative genetics by developing spatial process models for inference on additive and dominance genetic variance within the context of large spatially referenced trial datasets of multiple traits of interest. Direct application of such multivariate models to large spatial datasets is often computationally infeasible because of cubic order matrix algorithms involved in estimation. The situation is even worse in Markov chain Monte Carlo (MCMC) contexts where such computations are performed for several thousand iterations. Here, we discuss approaches that help obviate these hurdles without sacrificing the richness in modeling. For genetic effects, we demonstrate how an initial spectral decomposition of the relationship matrices negates the expensive matrix inversions required in previously proposed MCMC methods. For spatial effects we discuss a multivariate predictive process that reduces the computational burden by projecting the original process onto a subspace generated by realizations of the original process at a specified set of locations (or knots). We illustrate the proposed methods using a synthetic dataset with multivariate additive and dominant genetic effects and anisotropic spatial residuals, and a large dataset from a scots pine (Pinus sylvestris L.) progeny study conducted in northern Sweden. Our approaches enable us to provide a comprehensive analysis of this large trial which amply demonstrates that, in addition to violating basic assumptions of the linear model, ignoring spatial effects can result in downwardly biased measures of heritability.",0
https://doi.org/10.1007/s11121-007-0070-9,How Many Imputations are Really Needed? Some Practical Clarifications of Multiple Imputation Theory,"Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information (gamma) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which gamma and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on gamma, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.",0
https://doi.org/10.1037/a0033656,Can’t get it out of my mind: Employee rumination after customer mistreatment and negative mood in the next morning.,"Drawing on cognitive rumination theories and conceptualizing customer service interaction as a goal attainment situation for service employees, the current study examined employee rumination about negative service encounters as an intermediate cognitive process that explains the within-person fluctuations in negative emotional reactions resulting from customer mistreatment. Multilevel analyses of 149 call-center employees' 1,189 daily surveys revealed that on days that a service employee received more (vs. less) customer mistreatment, he or she ruminated more (vs. less) at night about negative encounters with customers, which in turn led to higher (vs. lower) levels of negative mood experienced in the next morning. In addition, service rule commitment and perceived organizational support moderated the within-person effect of customer mistreatment on rumination, such that this effect was stronger among those who had higher (vs. lower) levels of service rule commitment but weaker among those who had higher (vs. lower) levels of perceived organizational support. Theoretical and practical implications of these findings are discussed.",0
https://doi.org/10.1177/0013164408322026,On the Use of Nonparametric Item Characteristic Curve Estimation Techniques for Checking Parametric Model Fit,"The purpose of this study was to assess the model fit of a 2PL through comparison with the nonparametric item characteristic curve (ICC) estimation procedures. Results indicate that three nonparametric procedures implemented produced ICCs that are similar to that of the 2PL for items simulated to fit the 2PL. However for misfitting items, especially nonmonotone items, the greatest difference is between the 2PL and kernel smoothing procedures. In general, the differences between ICCs from the nonparametric procedures and the 2PL are reduced as both sample size and test length increase. The false positive rate of the test for model fit is promising for nonparametric ICC estimation methods. Power to detect misfitting items simulated with 4PL is low. Power to detect nonmonotone items is generally much higher. Power is best for kernel smoothing but also good for isotonic regression in the medium to large sample sizes and longer test length conditions. Power for the smoothed isotonic regression is uniformly low.",0
https://doi.org/10.1111/j.2044-8317.1991.tb00966.x,Scaled test statistics and robust standard errors for non-normal data in covariance structure analysis: A Monte Carlo study,"Research studying robustness of maximum likelihood (ML) statistics in covariance structure analysis has concluded that test statistics and standard errors are biased under severe non-normality. An estimation procedure known as asymptotic distribution free (ADF), making no distributional assumption, has been suggested to avoid these biases. Corrections to the normal theory statistics to yield more adequate performance have also been proposed. This study compares the performance of a scaled test statistic and robust standard errors for two models under several non-normal conditions and also compares these with the results from ML and ADF methods. Both ML and ADF test statistics performed rather well in one model and considerably worse in the other. In general, the scaled test statistic seemed to behave better than the ML test statistic and the ADF statistic performed the worst. The robust and ADF standard errors yielded more appropriate estimates of sampling variability than the ML standard errors, which were usually downward biased, in both models under most of the non-normal conditions. ML test statistics and standard errors were found to be quite robust to the violation of the normality assumption when data had either symmetric and platykurtic distributions, or non-symmetric and zero kurtotic distributions.",0
https://doi.org/10.1177/1094428111428356,How Can Significance Tests Be Deinstitutionalized?,"The purpose of this article is to propose possible solutions to the methodological problem of null hypothesis significance testing (NHST), which is framed as deeply embedded in the institutional structure of the social and organizational sciences. The core argument is that, for the deinstitutionalization of statistical significance tests, minor methodological changes within an unreformed epistemology will be as unhelpful as emotive exaggerations of the ill effects of NHST. Instead, several institutional-epistemological reforms affecting cultural-cognitive, normative, and regulative processes and structures in the social sciences are necessary and proposed in this article. In the conclusion, the suggested research reforms, ranging from greater emphasis on inductive and abductive reasoning to statistical modeling and Bayesian epistemology, are classified according to their practical importance and the time horizon expected for their implementation. Individual-level change in researchers' use of NHST is unli...",0
https://doi.org/10.1177/0013164408324460,Item Selection in Computerized Classification Testing,"Several alternatives for item selection algorithms based on item response theory in computerized classification testing (CCT) have been suggested, with no conclusive evidence on the substantial superiority of a single method. It is argued that the lack of sizable effect is because some of the methods actually assess items very similarly through different calculations and will usually select the same item. Consideration of methods that assess information across a wider range is often unnecessary under realistic conditions, although it might be advantageous to utilize them only early in a test. In addition, the efficiency of item selection approaches depend on the termination criteria that are used, which is demonstrated through didactic example and Monte Carlo simulation. Item selection at the cut score, which seems conceptually appropriate for CCT, is not always the most efficient option. A broad framework for item selection in CCT is presented that incorporates these points.",0
https://doi.org/10.1016/j.jml.2006.08.009,A formal model of capacity limits in working memory,"A mathematical model of working-memory capacity limits is proposed on the key assumption of mutual interference between items in working memory. Interference is assumed to arise from overwriting of features shared by these items. The model was fit to time-accuracy data of memory-updating tasks from four experiments using nonlinear mixed effect (NLME) models as a framework. The model gave a good account of the data from a numerical and a spatial task version. The performance pattern in a combination of numerical and spatial updating could be explained by variations in the interference parameter: assuming less feature overlap between contents from different domains than between contents from the same domain, the model can account for double dissociations of content domains in dual-task experiments. Experiment 3 extended this idea to similarity within the verbal domain. The decline of memory accuracy with increasing memory load was steeper with phonologically similar than with dissimilar material, although processing speed was faster for the similar material. The model captured the similarity effects with a higher estimated interference parameter for the similar than for the dissimilar condition. The results are difficult to explain with alternative models, in particular models incorporating time-based decay and models assuming limited resource pools.",0
https://doi.org/10.2307/1912934,A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity,"This paper presents a parameter covariance matrix estimator which is consistent even when the disturbances of a linear regression model are heteroskedastic. This estimator does not depend on a formal model of the structure of the heteroskedasticity. By comparing the elements of the new estimator to those of the usual covariance estimator, one obtains a direct test for heteroskedasticity, since in the absence of heteroskedasticity, the two estimators will be approximately equal, but will generally diverge otherwise. The test has an appealing least squares interpretation.",0
https://doi.org/10.1207/s15327906mbr4001_1,A Comparison of Methods to Test for Mediation in Multisite Experiments,"A Monte Carlo study extended the research of MacKinnon, Lockwood, Hoffman, West, and Sheets (2002) for single-level designs by examining the statistical performance of four methods to test for mediation in a multilevel experimental design. The design studied was a two-group experiment that was replicated across several sites, included a single intervening variable and outcome, and assumed that the effects of the treatment and mediator were constant across sites. The findings provide new evidence of the benefits of and further support for using the asymmetric confidence limits approach to test for mediation. In addition, the authors provide further support for using confidence intervals to assess if treatment effects are completely mediated, as using traditional hypothesis testing may lead to erroneous conclusions.",0
https://doi.org/10.1002/pst.1741,Bayesian robustness in meta-analysis for studies with zero responses,"Statistical meta-analysis is mostly carried out with the help of the random effect normal model, including the case of discrete random variables. We argue that the normal approximation is not always able to adequately capture the underlying uncertainty of the original discrete data. Furthermore, when we examine the influence of the prior distributions considered, in the presence of rare events, the results from this approximation can be very poor. In order to assess the robustness of the quantities of interest in meta-analysis with respect to the choice of priors, this paper proposes an alternative Bayesian model for binomial random variables with several zero responses. Particular attention is paid to the coherence between the prior distributions of the study model parameters and the meta-parameter. Thus, our method introduces a simple way to examine the sensitivity of these quantities to the structure dependence selected for study. For illustrative purposes, an example with real data is analysed, using the proposed Bayesian meta-analysis model for binomial sparse data. Copyright © 2016 John Wiley & Sons, Ltd.",0
https://doi.org/10.1002/jrsm.1167,Automated generation of node‐splitting models for assessment of inconsistency in network meta‐analysis,"Network meta-analysis enables the simultaneous synthesis of a network of clinical trials comparing any number of treatments. Potential inconsistencies between estimates of relative treatment effects are an important concern, and several methods to detect inconsistency have been proposed. This paper is concerned with the node-splitting approach, which is particularly attractive because of its straightforward interpretation, contrasting estimates from both direct and indirect evidence. However, node-splitting analyses are labour-intensive because each comparison of interest requires a separate model. It would be advantageous if node-splitting models could be estimated automatically for all comparisons of interest. We present an unambiguous decision rule to choose which comparisons to split, and prove that it selects only comparisons in potentially inconsistent loops in the network, and that all potentially inconsistent loops in the network are investigated. Moreover, the decision rule circumvents problems with the parameterisation of multi-arm trials, ensuring that model generation is trivial in all cases. Thus, our methods eliminate most of the manual work involved in using the node-splitting approach, enabling the analyst to focus on interpreting the results.",0
https://doi.org/10.1111/j.2041-210x.2011.00131.x,On thinning of chains in MCMC,"Summary   1. Markov chain Monte Carlo (MCMC) is a simulation technique that has revolutionised the analysis of ecological data, allowing the fitting of complex models in a Bayesian framework. Since 2001, there have been nearly 200 papers using MCMC in publications of the Ecological Society of America and the British Ecological Society, including more than 75 in the journal Ecology and 35 in the Journal of Applied Ecology.    2. We have noted that many authors routinely ‘thin’ their simulations, discarding all but every kth sampled value; of the studies we surveyed with details on MCMC implementation, 40% reported thinning.    3. Thinning is often unnecessary and always inefficient, reducing the precision with which features of the Markov chain are summarised. The inefficiency of thinning MCMC output has been known since the early 1990’s, long before MCMC appeared in ecological publications.    4. We discuss the background and prevalence of thinning, illustrate its consequences, discuss circumstances when it might be regarded as a reasonable option and recommend against routine thinning of chains unless necessitated by computer memory limitations.",0
https://doi.org/10.1002/0471746096,Latent Curve Models,"Preface. 1 Introduction. 1.1 Conceptualization and Analysis of Trajectories. 1.2 Three Initial Questions About Trajectories. 1.3 Brief History of Latent Curve Models. 1.4 Organization of the Remainder of the Book. 2 Unconditional Latent Curve Model. 2.1 Repeated Measures. 2.2 General Model and Assumptions. 2.3 Identification. 2.4 Case-By-Case Approach. 2.5 Structural Equation Model Approach. 2.6 Alternative Approaches to the SEM. 2.7 Conclusions. Appendix 2A: Test Statistics, Nonnormality, and Statistical Power. 3 Missing Data and Alternative Metrics of Time. 3.1 Missing Data. 3.2 Missing Data and Alternative Metrics of Time. 3.3 Conclusions. 4 Nonlinear Trajectories and the Coding of Time. 4.1 Modeling Nonlinear Functions of Time. 4.2 Nonlinear Curve Fitting: Estimated Factor Loadings. 4.3 Piecewise Linear Trajectory Models. 4.4 Alternative Parametric Functions. 4.5 Linear Transformations of the Metric of Time. 4.6 Conclusions. Appendix 4A: Identification of Quadratic and Piecewise Latent Curve Models. 4A.1 Quadratic LCM. 4A.2 Piecewise LCM. 5 Conditional Latent Curve Models. 5.1 Conditional Model and Assumptions. 5.2 Identification. 5.3 Structural Equation Modeling Approach. 5.4 Interpretation of Conditional Model Estimates. 5.5 Empirical Example. 5.6 Conclusions. 6 The Analysis of Groups. 6.1 Dummy Variable Approach. 6.2 Multiple-Group Analysis. 6.3 Unknown Group Membership. 6.4 Conclusions. Appendix 6A: Case-by-Case Approach to Analysis of Various Groups. 6A.1 Dummy Variable Method. 6A.2 Multiple-Group Analysis. 6A.3 Unknown Group Membership. 6A.4 Appendix Summary. 7 Multivariate Latent Curve Models. 7.1 Time-Invariant Covariates. 7.2 Time-Varying Covariates. 7.3 Simultaneous Inclusion of Time-Invariant and Time-Varying Covariates. 7.4 Multivariate Latent Curve Models. 7.5 Autoregressive Latent Trajectory Model. 7.6 General Equation for All Models. 7.7 Implied Moment Matrices. 7.8 Conclusions. 8 Extensions of Latent Curve Models. 8.1 Dichotomous and Ordinal Repeated Measures. 8.2 Repeated Latent Variables with Multiple Indicators. 8.3 Latent Covariates. 8.4 Conclusions. References. Author Index. Subject Index.",0
https://doi.org/10.1093/pan/mpm006,A Bayesian Multilevel Modeling Approach to Time-Series Cross-Sectional Data,"The analysis of time-series cross-sectional (TSCS) data has become increasingly popular in political science. Meanwhile, political scientists are also becoming more interested in the use of multilevel models (MLM). However, little work exists to understand the benefits of multilevel modeling when applied to TSCS data. We employ Monte Carlo simulations to benchmark the performance of a Bayesian multilevel model for TSCS data. We find that the MLM performs as well or better than other common estimators for such data. Most importantly, the MLM is more general and offers researchers additional advantages.",0
https://doi.org/10.1037/0278-7393.19.5.1024,Proactive interference and the dynamics of free recall.,"Proactive interference (PI) has long been recognized as a major cause of forgetting. We conducted two experiments that offer another look at the subject by providing a detailed analysis of recall latency distributions during the buildup of and release from PI. These functions were accurately characterized by the convolution of the normal and exponential distributions (viz., the ex-Gaussian), which previously has been shown to describe recognition latency distributions. Further, the fits revealed that the increase in recall latency associated with the buildup of PI results from a slowing of the exponential retrieval stage only. The same result was found even when a short retention interval was used (and recall probability remained constant). These findings suggest that free-recall latency may be a sensitive index of the increased search set size that has often been assumed to accompany the buildup of PI. A central insight emerging from the memory literature of the 1950s and 1960s was that previously learned information can result in the rapid forgetting of more recently learned information. Underwood (1957) argued that this phenomenon, termed proactive interference (PI), was by far the major cause of forgetting in everyday life. Indeed, even in laboratory experiments, the degree of retroactive interference encountered over the course of hours or days was assumed to pale in comparison with the degree of proactive interference resulting from years of prior learning. Although its preeminent (and still unexplained) role in the process of forgetting continues to be recognized, interest in the subject of PI has waned in recent years. The present article contributes a new empirical analysis of this important subject and pursues a detailed theoretical exploration into its underlying nature. In a typical PI experiment, subjects receive blocks of Brown-Peterson trials involving words from a single category (Wickens, 1972). Within a block, free-recall performance declines with each successive trial (the buildup of PI) but recovers each time a new category is introduced (release from PI). In most cases, the dependent variable used in these experiments was the percentage of correct free-recall responses. However, in the research to be presented here, we focus on latency to free recall. Research on free-recall latency in any context is very limited, and in the study of PI it is almost nonexistent. Why might free-recall latency be an interesting variable to investigate? Because such a measure provides important information about the process of retrieval that is likely to be missed by static measures, such as probability of recall. Before addressing the question of exactly what that information might be, we review the scant literature pertaining to the more general and purely empirical question of whether these",0
https://doi.org/10.1016/j.jmp.2012.06.002,Cultural Consensus Theory: Comparing different concepts of cultural truth,"Abstract Cultural Consensus Theory (CCT) is a model-based approach to aggregating the responses of informants (respondents) to questions (items) about some domain of their shared cultural knowledge. The purpose of CCT is to allow a researcher to discover consensus knowledge in cultural groups. This paper compares and contrasts two CCT models for items requiring a dichotomous, true/false answer. The first model is the General Condorcet Model (GCM). A special case of the GCM is already in wide use, especially in cultural anthropology, and this paper generalizes that version and provides new mathematical and statistical results for it. The character of the GCM is that of a general signal detection model, where the item-trial type (signal or noise) as well as the respondents’ hit and false alarm rates, are latent rather than observable. The second model, the Latent Truth Model (LTM), is a new model that allows cultural truth to assume continuous values in the unit interval rather than the two-valued truth assumption of the GCM. Both models are compared analytically, and hierarchical Bayesian inference for each is developed. A posterior predictive model check is established for both models that bears directly on the assumption that there is a single consensus truth. In addition, the similarities and differences between the models are illustrated both with mathematical and statistical results, as well as by analyzing real and simulated data sets, and a second posterior predictive check that tends to differentiate the models is also provided.",0
https://doi.org/10.1037/1196-1961.51.3.181,On the locus of the word frequency effect in visual word recognition.,"Abstract The pattern of factor effects on response time (RT) performance in multi-factor experiments provides a powerful technique for inferring the structure of underlying mental processes. The results of the present lexical decision experiment show that additive effects of stimulus quality and word frequency are observed in mean RTs, variances, and the exGaussian parameters of the RT distribution. These findings are consistent with the conclusion that word frequency and stimulus quality affect separate stages of processing. This is consistent with the conclusion that word frequency effects reflect mapping operations between stages, but, when taken in conjunction with other reports in the literature, is inconsistent with the received view in many activation models that word frequency exerts its effect within the word detector level of representation. Resume Le schema des effets du facteur sur l'execution du temps de reponse (TR) dans les experiences a multiples facteurs fournissent une technique efficace pour fixer la structure des processus mentaux sous-jacents. Les resultats de la presente experience de decision lexicale montrent que les effets additifs de la qualite du stimulus et de la frequence du mot sont observes dans les TR moyens, les variances et les parametres ex-gaussiens de la distribution du TR. Ces conclusions confirment que la frequence du mot et la qualite du stimulus influencent les phases separees du traitement. Elles confirment egalement que les effets de la frequence du mot refletent les operations de correspondance entre les phases, mais, lorsque comparees a d'autres rapports, les conclusions ne correspondent pas a la vision etablie dans plusieurs modeles d'activation qui veut que la frequence du mot exerce ses effets au niveau de representation du detecteur du mot. It is well established that a word's frequency of occurrence in printed English is a strong determinant of performance in word recognition tasks such as lexical decision. For example, high frequency words such as CAT are typically recognized more quickly than lower frequency words such as VAT (e.g., Forster & Chambers, 1973; Frederiksen & Kroll, 1976; see also Monsell, 1991, for a review). One account of this word frequency effect is given by a number of models in the Activation class. Thus, models such as Morton's (1969) logogen model and variants of McClelland and Rumelhart's (1981) interactiveactivation model (e.g., Coltheart, Curtis, Atkins, & Haller, 1993; Grainger & Jacobs, 1996) all assume a word level of representation in the form of orthographic and phonological word detectors for each word the reader knows. These word detectors accumulate activation over time, and both models assume that low frequency words take longer to reach a threshold than do high frequency words. The models differ in that the interactive-activation model assumes that high frequency words start with a higher resting level of activation than do low frequency words whereas the logogen model assumes a common resting level of activation but different recognition thresholds for high and low frequency words One difficulty with the activation explanation is that a simple main effect of word frequency in lexical decision does not allow one to determine the locus of the effect; it merely informs us that word frequency affects performance in this task. A number of investigators have therefore applied Sternberg's (1969) additive factors logic to this issue by manipulating word frequency in conjunction with other factors that affect performance in lexical decision because predictions can be derived from the models as to how the effects of these factors on response time (RT) should combine (e.g., Becker & Killion, 1977; Besner & Smith, 1992; Besner & Swan, 1982). WORD FREQUENCY, STIMULUS QUALITY, AND CONTEXT It is well documented that stimulus quality affects word recognition: Clearly presented stimuli are recognized more quickly than degraded ones. …",0
https://doi.org/10.1111/j.0006-341x.2001.00126.x,A Covariance Estimator for GEE with Improved Small‐Sample Properties,"In this paper, we propose an alternative covariance estimator to the robust covariance estimator of generalized estimating equations (GEE). Hypothesis tests using the robust covariance estimator can have inflated size when the number of independent clusters is small. Resampling methods, such as the jackknife and bootstrap, have been suggested for covariance estimation when the number of clusters is small. A drawback of the resampling methods when the response is binary is that the methods can break down when the number of subjects is small due to zero or near-zero cell counts caused by resampling. We propose a bias-corrected covariance estimator that avoids this problem. In a small simulation study, we compare the bias-corrected covariance estimator to the robust and jackknife covariance estimators for binary responses for situations involving 10-40 subjects with equal and unequal cluster sizes of 16-64 observations. The bias-corrected covariance estimator gave tests with sizes close to the nominal level even when the number of subjects was 10 and cluster sizes were unequal, whereas the robust and jackknife covariance estimators gave tests with sizes that could be 2-3 times the nominal level. The methods are illustrated using data from a randomized clinical trial on treatment for bone loss in subjects with periodontal disease.",0
https://doi.org/10.1177/1536867x0200200403,Estimation of Average Treatment Effects Based on Propensity Scores,"In this paper, we give a short overview of some propensity score matching estimators suggested in the evaluation literature, and we provide a set of Stata programs, which we illustrate using the National Supported Work (NSW) demonstration widely known in labor economics.",0
https://doi.org/10.1002/sim.3043,"Comments on ‘Trying to be precise about vagueness’ by Stephen Senn, Statistics in Medicine 2007; 26 :1417-1430",http://onlinelibrary.wiley.com/doi/10.1002/sim.3043/epdf,0
https://doi.org/10.1111/j.1744-6570.2009.01147.x,USING A META-ANALYTIC PERSPECTIVE TO ENHANCE JOB COMPONENT VALIDATION,"This paper develops synthetic validity estimates based on a metaanalytic-weighted least squares (WLS) approach to job component validity (JCV), using position analysis questionnaire (PAQ) estimates of job characteristics, and the Data, People, & Things ratings from the Dictionary of Occupational Titles as indices of job complexity. For the general aptitude test battery database of 40,487 employees, nine validity coefficients were estimated for 192 positions. The predicted validities from the WLS approach had lower estimated variability than would be obtained from either the classic JCV approach or local criterion-related validity studies. Data, People, & Things summary ratings did not consistently moderate validity coefficients, whereas the PAQ data did moderate validity coefficients. In sum, these results suggest that synthetic validity procedures should incorporate a WLS regression approach. Moreover, researchers should consider a comprehensive set of job characteristics when considering job complexity rather than a single aggregated index. Over half a century ago, Lawshe (1952) coined the term “synthetic validity,” inspired by the analogous engineering process of “synthetic time.” As applied to selection, synthetic validity involves inferring the validity of a selection tool for a specific job based on job analysis data from a variety of other jobs. In other words, synthetic validity facilitates the creation of test batteries and selection systems without the need to conduct a criterion validity study in every situation. Synthetic validity promises many advantages over local validation studies, including drastically reduced costs, improved selection of qualified candidates, and greater legal defensibility (e.g., Hoffman, Rashkovsky, & D’Egidio, 2007; Johnson, 2007; Steel, Huffcutt, & Kammeyer-Mueller, 2006). Synthetic validity can also estimate the validity of a battery of selection instruments for jobs in which there are no incumbents yet. Over the decades since synthetic validity was first proposed, a variety of estimation methods have been designed, with mixed success (Guion, 2006; Scherbaum, 2005). One of",0
https://doi.org/10.3389/fpsyg.2015.01914,Examining the Dynamic Structure of Daily Internalizing and Externalizing Behavior at Multiple Levels of Analysis,"Psychiatric diagnostic covariation suggests that the underlying structure of psychopathology is not one of circumscribed disorders. Quantitative modeling of individual differences in diagnostic patterns has uncovered several broad domains of mental disorder liability, of which the Internalizing and Externalizing spectra have garnered the greatest support. These dimensions have generally been estimated from lifetime or past-year comorbidity patters, which are distal from the covariation of symptoms and maladaptive behavior that ebb and flow in daily life. In this study, structural models are applied to daily diary data (Median = 94 days) of maladaptive behaviors collected from a sample (N = 101) of individuals diagnosed with personality disorders (PDs). Using multilevel and unified structural equation modeling, between-person, within-person, and person-specific structures were estimated from 16 behaviors that are encompassed by the Internalizing and Externalizing spectra. At the between-person level (i.e., individual differences in average endorsement across days) we found support for a two-factor Internalizing-Externalizing model, which exhibits significant associations with corresponding diagnostic spectra. At the within-person level (i.e., dynamic covariation among daily behavior pooled across individuals) we found support for a more differentiated, four-factor, Negative Affect-Detachment-Hostility-Disinhibition structure. Finally, we demonstrate that the person-specific structures of associations between these four domains are highly idiosyncratic.",0
https://doi.org/10.1017/cbo9781107587991,Counterfactuals and Causal Inference,"In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed.",0
https://doi.org/10.1167/8.2.10,Integration of ordinal and metric cues in depth processing,"J. Burge, M. A. Peterson, and S. E. Palmer (2005) reported that ordinal, configural cues of familiarity and convexity influence perceived depth even when unambiguous metric information in the form of binocular disparity is available. In their study, a shape that was both convex and familiar (i.e., a face) increased perceived depth in random dot stereograms if the shape was shown in the foreground and decreased perceived depth if it was shown in the background. It is generally assumed that luminance cues are necessary for pre-figural shape representation to influence figure-ground computations in this way (M. A. Peterson & B. S. Gibson, 1993); thus, Burge et al. (2005) had used a luminance edge. In this research, we asked whether configural cues need to be defined by luminance, contrast, or neither. For a sufficiently large disparity pedestal (about 2.5 arcmin), configural cues influenced perceived depth both for second-order contours and for contours defined only by disparity. The integration of ordinal and metric cues seems to be driven by the general saliency of the contours and not only by luminance information. This challenges the notion that the integration of such cues always needs to arise during figure-ground organization through early combinations of luminance-defined shape and binocular disparity.",0
https://doi.org/10.1080/00273171.2013.784862,A Bayesian Approach for Estimating Mediation Effects With Missing Data,"Methodologists have developed mediation analysis techniques for a broad range of substantive applications, yet methods for estimating mediating mechanisms with missing data have been understudied. This study outlined a general Bayesian missing data handling approach that can accommodate mediation analyses with any number of manifest variables. Computer simulation studies showed that the Bayesian approach produced frequentist coverage rates and power estimates that were comparable to those of maximum likelihood with the bias-corrected bootstrap. We share a SAS macro that implements Bayesian estimation and use two data analysis examples to demonstrate its use.",0
,Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise,"Modern machine learning-based approaches to computer vision require very large databases of hand labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector [9]). New Internet-based services allow for a large number of labelers to collaborate around the world at very low cost. However, using these services brings interesting theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used Majority Vote heuristic for inferring image labels, and is robust to both noisy and adversarial labelers.",0
https://doi.org/10.1016/j.ecolmodel.2006.07.005,Species distribution models and ecological theory: A critical assessment and some possible new approaches,"Given the importance of knowledge of species distribution for conservation and climate change management, continuous and progressive evaluation of the statistical models predicting species distributions is necessary. Current models are evaluated in terms of ecological theory used, the data model accepted and the statistical methods applied. Focus is restricted to Generalised Linear Models (GLM) and Generalised Additive Models (GAM). Certain currently unused regression methods are reviewed for their possible application to species modelling. A review of recent papers suggests that ecological theory is rarely explicitly considered. Current theory and results support species responses to environmental variables to be unimodal and often skewed though process-based theory is often lacking. Many studies fail to test for unimodal or skewed responses and straight-line relationships are often fitted without justification. Data resolution (size of sampling unit) determines the nature of the environmental niche models that can be fitted. A synthesis of differing ecophysiological ideas and the use of biophysical processes models could improve the selection of predictor variables. A better conceptual framework is needed for selecting variables. Comparison of statistical methods is difficult. Predictive success is insufficient and a test of ecological realism is also needed. Evaluation of methods needs artificial data, as there is no knowledge about the true relationships between variables for field data. However, use of artificial data is limited by lack of comprehensive theory. Three potentially new methods are reviewed. Quantile regression (QR) has potential and a strong theoretical justification in Liebig's law of the minimum . Structural equation modelling (SEM) has an appealing conceptual framework for testing causality but has problems with curvilinear relationships. Geographically weighted regression (GWR) intended to examine spatial non-stationarity of ecological processes requires further evaluation before being used. Synthesis and applications: explicit theory needs to be incorporated into species response models used in conservation. For example, testing for unimodal skewed responses should be a routine procedure. Clear statements of the ecological theory used, the nature of the data model and sufficient details of the statistical method are needed for current models to be evaluated. New statistical methods need to be evaluated for compatibility with ecological theory before use in applied ecology . Some recent work with artificial data suggests the combination of ecological knowledge and statistical skill is more important than the precise statistical method used. The potential exists for a synthesis of current species modelling approaches based on their differing ecological insights not their methodology.",0
https://doi.org/10.1007/978-1-4612-4976-4_7,Alternative Methods for Solving the Problem of Selection Bias in Evaluating the Impact of Treatments on Outcomes,"Social scientists never have access to true experimental data of the type sometimes available to laboratory scientists.1 Our inability to use laboratory methods to independently vary treatments to eliminate or isolate spurious channels of causation places a fundamental limitation on the possibility of objective knowledge in the social sciences. In place of laboratory experimental variation, social scientists use subjective thought experiments. Assumptions replace data. In the jargon of modern econometrics, minimal identifying assumptions are invoked.",0
https://doi.org/10.3389/fpsyg.2014.00311,A longitudinal multilevel CFA-MTMM model for interchangeable and structurally different methods,"One of the key interests in the social sciences is the investigation of change and stability of a given attribute. Although numerous models have been proposed in the past for analyzing longitudinal data including multilevel and/or latent variable modeling approaches, only few modeling approaches have been developed for studying the construct validity in longitudinal multitrait-multimethod (MTMM) measurement designs. The aim of the present study was to extend the spectrum of current longitudinal modeling approaches for MTMM analysis. Specifically, a new longitudinal multilevel CFA-MTMM model for measurement designs with structurally different and interchangeable methods (called Latent-State-Combination-Of-Methods model, LS-COM) is presented. Interchangeable methods are methods that are randomly sampled from a set of equivalent methods (e.g., multiple student ratings for teaching quality), whereas structurally different methods are methods that cannot be easily replaced by one another (e.g., teacher, self-ratings, principle ratings). Results of a simulation study indicate that the parameters and standard errors in the LS-COM model are well recovered even in conditions with only five observations per estimated model parameter. The advantages and limitations of the LS-COM model relative to other longitudinal MTMM modeling approaches are discussed.",0
https://doi.org/10.3758/bf03211637,Extracting thresholds from noisy psychophysical data,"Psychophysical studies with infants or with patients often are unable to use pilot data, training, or large numbers of trials. To evaluate threshold estimates under these conditions, computer simulations of experiments with small numbers of trials were performed by using psychometric functions based on a model of two types of noise: stimulus-related noise (affecting slope) and extraneous noise (affecting upper asymptote). Threshold estimates were biased and imprecise when extraneous noise was high, as were the estimates of extraneous noise. Strategies were developed for rejecting data sets as too noisy for unbiased and precise threshold estimation; these strategies were most successful when extraneous noise was low for most of the data sets. An analysis of 1,026 data sets from visual function tests of infants and toddlers showed that extraneous noise is often considerable, that experimental paradigms can be developed that minimize extraneous noise, and that data analysis that does not consider the effects of extraneous noise may underestimate test-retest reliability and overestimate interocular differences.",0
https://doi.org/10.3758/brm.42.4.930,Estimating individual treatment effects from multiple-baseline data: A Monte Carlo study of multilevel-modeling approaches,"While conducting intervention research, researchers and practitioners are often interested in how the intervention functions not only at the group level, but also at the individual level. One way to examine individual treatment effects is through multiple-baseline studies analyzed with multilevel modeling. This analysis allows for the construction of confidence intervals, which are strongly recommended in the reporting guidelines of the American Psychological Association. The purpose of this study was to examine the accuracy of confidence intervals of individual treatment effects obtained from multilevel modeling of multiple-baseline data. Monte Carlo methods were used to examine performance across conditions varying in the number of participants, the number of observations per participant, and the dependency of errors. The accuracy of the confidence intervals depended on the method used, with the greatest accuracy being obtained when multilevel modeling was coupled with the Kenward-Roger method of estimating degrees of freedom.",0
https://doi.org/10.1037/a0023322,Samples in applied psychology: Over a decade of research in review.,"This study examines sample characteristics of articles published in Journal of Applied Psychology (JAP) from 1995 to 2008. At the individual level, the overall median sample size over the period examined was approximately 173, which is generally adequate for detecting the average magnitude of effects of primary interest to researchers who publish in JAP. Samples using higher units of analyses (e.g., teams, departments/work units, and organizations) had lower median sample sizes (Mdn ≈ 65), yet were arguably robust given typical multilevel design choices of JAP authors despite the practical constraints of collecting data at higher units of analysis. A substantial proportion of studies used student samples (~40%); surprisingly, median sample sizes for student samples were smaller than working adult samples. Samples were more commonly occupationally homogeneous (~70%) than occupationally heterogeneous. U.S. and English-speaking participants made up the vast majority of samples, whereas Middle Eastern, African, and Latin American samples were largely unrepresented. On the basis of study results, recommendations are provided for authors, editors, and readers, which converge on 3 themes: (a) appropriateness and match between sample characteristics and research questions, (b) careful consideration of statistical power, and (c) the increased popularity of quantitative synthesis. Implications are discussed in terms of theory building, generalizability of research findings, and statistical power to detect effects.",0
https://doi.org/10.1007/s10648-014-9287-x,The Effect of Small Sample Size on Two-Level Model Estimates: A Review and Illustration,"Multilevel models are an increasingly popular method to analyze data that originate from a clustered or hierarchical structure. To effectively utilize multilevel models, one must have an adequately large number of clusters; otherwise, some model parameters will be estimated with bias. The goals for this paper are to (1) raise awareness of the problems associated with a small number of clusters, (2) review previous studies on multilevel models with a small number of clusters, (3) to provide an illustrative simulation to demonstrate how a simple model becomes adversely affected by small numbers of clusters, (4) to provide researchers with remedies if they encounter clustered data with a small number of clusters, and (5) to outline methodological topics that have yet to be addressed in the literature.",0
https://doi.org/10.3402/ejpt.v6.25216,Analyzing small data sets using Bayesian estimation: the case of posttraumatic stress symptoms following mechanical ventilation in burn survivors,"Background : The analysis of small data sets in longitudinal studies can lead to power issues and often suffers from biased parameter values. These issues can be solved by using Bayesian estimation in conjunction with informative prior distributions. By means of a simulation study and an empirical example concerning posttraumatic stress symptoms (PTSS) following mechanical ventilation in burn survivors, we demonstrate the advantages and potential pitfalls of using Bayesian estimation. Methods : First, we show how to specify prior distributions and by means of a sensitivity analysis we demonstrate how to check the exact influence of the prior (mis-) specification. Thereafter, we show by means of a simulation the situations in which the Bayesian approach outperforms the default, maximum likelihood and approach. Finally, we re-analyze empirical data on burn survivors which provided preliminary evidence of an aversive influence of a period of mechanical ventilation on the course of PTSS following burns. Results : Not suprisingly, maximum likelihood estimation showed insufficient coverage as well as power with very small samples. Only when Bayesian analysis, in conjunction with informative priors, was used power increased to acceptable levels. As expected, we showed that the smaller the sample size the more the results rely on the prior specification. Conclusion : We show that two issues often encountered during analysis of small samples, power and biased parameters, can be solved by including prior information into Bayesian analysis. We argue that the use of informative priors should always be reported together with a sensitivity analysis.",1
https://doi.org/10.1080/10705511.2012.659614,Measurement and Structural Model Class Separation in Mixture CFA: ML/EM Versus MCMC,"Parameter recovery was assessed within mixture confirmatory factor analysis across multiple estimator conditions under different simulated levels of mixture class separation. Mixture class separation was defined in the measurement model (through factor loadings) and the structural model (through factor variances). Maximum likelihood (ML) via the EM algorithm was compared to a Markov chain Monte Carlo (MCMC) estimator condition using weak priors and a condition using tight priors. Results indicated that the MCMC weak condition produced the highest bias, particularly with a weak Dirichlet prior for the mixture class proportions. Specifically, the weak Dirichlet prior affected parameter estimates under all mixture class separation conditions, even with moderate and large sample sizes. With little knowledge about parameters, ML/EM should be used over MCMC weak. However, MCMC tight produced the lowest bias under all mixture class separation conditions and should be used if tight and accurate priors can be plac...",1
https://doi.org/10.3168/jds.2010-3836,A Bayesian approach to analyze energy balance data from lactating dairy cows,"The objective of the present investigation was to develop a Bayesian framework for updating and integrating covariate information into key parameters of metabolizable energy (ME) systems for dairy cows. The study addressed specifically the effects of genetic improvements and feed quality on key parameters in current ME systems. These are net and metabolizable energy for maintenance (NE(M) and ME(M), respectively), efficiency of utilization of ME for milk production (k(L)) and growth (k(G)), and efficiency of utilization of body stores for milk production (k(T)). Data were collated from 38 studies, yielding 701 individual cow observations on milk energy, ME intake, and tissue gain and loss. A function based on a linear relationship between milk energy and ME intake and correcting for tissue energy loss or gain served as the basis of a full Bayesian hierarchical model. The within-study variability was modeled by a Student t-distribution and the between-study variability in the structural parameters was modeled by a multivariate normal distribution. A meaningful relationship between genetic improvements in milk production and the key parameters could not be established. The parameter k(L) was linearly related to feed metabolizability, and the slope predicted a 0.010 (-0.0004; 0.0210) change per 0.1-unit change in metabolizability. The effect of metabolizability on k(L) was smaller than assumed in present feed evaluation systems and its significance was dependent on collection of studies included in the analysis. Three sets of population estimates (with 95% credible interval in parentheses) were generated, reflecting different degrees of prior belief: (1) Noninformative priors yielded 0.28 (0.23; 0.33) MJ/(kg(0.75)d), 0.55 (0.51; 0.58), 0.86 (0.81; 0.93) and 0.66 (0.58; 0.75), for NE(M), k(L), k(G), and k(T), respectively; (2) Introducing an informative prior that was derived from a fasting metabolism study served to combine the most recent information on energy metabolism in modern dairy cows. The new estimates of NE(M), k(L), k(G) and k(T) were 0.34 (0.28; 0.39) MJ/(kg(0.75)d), 0.58 (0.54; 0.62), 0.89 (0.85; 0.95), and 0.69 (0.60; 0.79), respectively; (3) finally, all informative priors were used that were established from literature, yielding estimates for NE(M), k(L), k(G), and k(T) of 0.29 (0.11; 0.46) MJ/(kg(0.75)d), 0.60 (0.54; 0.70), 0.70 (0.50; 0.88), and 0.80 (0.67; 0.97), respectively. Bayesian methods are especially applicable in meta-analytical studies as information can enter at various stages in the hierarchical model.",0
https://doi.org/10.1080/0267257x.2011.609653,Using student-choice behaviour to estimate tuition elasticity in higher education,"Prior research on student response to changes in university prices (tuition) finds that demand is inelastic. We present results, based on separate models for 11 colleges (n = 5606) at a major US un...",0
https://doi.org/10.1016/j.jmp.2009.06.007,Beta-MPT: Multinomial processing tree models for addressing individual differences,"Abstract Traditionally, multinomial processing tree (MPT) models are applied to groups of homogeneous participants, where all participants within a group are assumed to have identical MPT model parameter values. This assumption is unreasonable when MPT models are used for clinical assessment, and it often may be suspect for applications to ordinary psychological experiments. One method for dealing with parameter variability is to incorporate random effects assumptions into a model. This is achieved by assuming that participants’ parameters are drawn independently from some specified multivariate hyperdistribution. In this paper we explore the assumption that the hyperdistribution consists of independent beta distributions, one for each MPT model parameter. These beta-MPT models are ‘hierarchical models’, and their statistical inference is different from the usual approaches based on data aggregated over participants. The paper provides both classical (frequentist) and hierarchical Bayesian approaches to statistical inference for beta-MPT models. In simple cases the likelihood function can be obtained analytically; however, for more complex cases, Markov Chain Monte Carlo algorithms are constructed to assist both approaches to inference. Examples based on clinical assessment studies are provided to demonstrate the advantages of hierarchical MPT models over aggregate analysis in the presence of individual differences.",0
https://doi.org/10.1155/2014/945253,Integrative Genomics and Computational Systems Medicine,"The exponential growth in generation of large amounts of genomic data from biological samples has driven the emerging field of systems medicine. This field is promising because it improves our understanding of disease processes at the systems level. However, the field is still in its young stage. There exists a great need for novel computational methods and approaches to effectively utilize and integrate various omics data.",0
https://doi.org/10.1007/s11222-008-9110-y,A tutorial on adaptive MCMC,"We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem. Ã‚Â© 2008 Springer Science+Business Media, LLC.",0
https://doi.org/10.1207/s15327906mbr4103_4,Bayesian Analysis of Structural Equation Models With Nonlinear Covariates and Latent Variables,"In this article, we formulate a nonlinear structural equation model (SEM) that can accommodate covariates in the measurement equation and nonlinear terms of covariates and exogenous latent variables in the structural equation. The covariates can come from continuous or discrete distributions. A Bayesian approach is developed to analyze the proposed model. Markov chain Monte Carlo methods for obtaining Bayesian estimates and their standard error estimates, highest posterior density intervals, and a PP p value are developed. Results obtained from two simulation studies are reported to respectively reveal the empirical performance of the proposed Bayesian estimation in analyzing complex nonlinear SEMs, and in analyzing nonlinear SEMs with the normal assumption of the exogenous latent variables violated. The proposed methodology is further illustrated by a real example. Detailed interpretation about the interaction terms is presented.",0
https://doi.org/10.1007/s10260-012-0216-1,On the use of MCMC computerized adaptive testing with empirical prior information to improve efficiency,"The paper deals with the introduction of empirical prior information in the estimation of candidate's ability within computerized adaptive testing (CAT). CAT is generally applied to improve efficiency of test administration. In this paper, it is shown how the inclusion of background variables both in the initialization and the ability estimation is able to improve the accuracy of ability estimates. In particular, a Gibbs sampler scheme is proposed in the phases of interim and final ability estimation. By using both simulated and real data, it is proved that the method produces more accurate ability estimates, especially for short tests and when reproducing boundary abilities. This implies that operational problems of CAT related to weak measurement precision under particular conditions, can be reduced as well. In the empirical examples, the methods were applied to CAT for intelligence testing in the area of personnel selection and to educational measurement. Other promising applications would be in the medical world, where testing efficiency is of paramount importance as well. © 2012 Springer-Verlag Berlin Heidelberg.",0
https://doi.org/10.2501/ijmr-53-6-831-857,Market Share Predictions: A New Model with Rating-Based Conjoint Analysis,"Conjoint Analysis (CA) is a technique heavily used by industry in support of product development, pricing and positioning, and market share predictions. This generic term CA encompasses a variety of experimental protocols and estimation models (e.g. rating-based or choice-based), as well as several probabilistic models for predicting market share. As for the rating conjoint, existing probabilistic models from the literature cannot be considered as reliable because they suffer from the Independence of Irrelevant Alternatives (IIA) property, in addition to depending on an arbitrary rating scale selected by the experimenter. In this article, after a brief overview of CA and of models used for market share predictions, we propose a new model for market share predictions, RFC-BOLSE, which avoids the IIA problem, yields convergent results for different rating scales, and outputs predictions that match regression reliability. The model is described in details and simulations and a case study on truck tyres will illustrate the reliability of RFC-BOLSE.",0
https://doi.org/10.1080/10705511.2012.687669,On Obtaining Estimates of the Fraction of Missing Information From Full Information Maximum Likelihood,"Fraction of missing information λ j is a useful measure of the impact of missing data on the quality of estimation of a particular parameter. This measure can be computed for all parameters in the model, and it communicates the relative loss of efficiency in the estimation of a particular parameter due to missing data. It has been recommended that researchers working with incomplete data sets routinely report this measure, as it is more informative than percent missing data (Bodner, 2008 Bodner, T. E. 2008. What improves with increasing missing data imputations?. Structural Equation Modeling, 15: 651–675. [Taylor & Francis Online], [Web of Science ®] , [Google Scholar]; Schafer, 1997 Schafer, J. L. 1997. Analysis of incomplete multivariate data. London: Chapman & Hall.. [Crossref] , [Google Scholar]). However, traditional estimates of λ j require the implementation of multiple imputation (MI). Many researchers prefer to fit structural equation models using full information maximum likelihood rather than MI. This article demonstrates how to obtain an estimate of λ j using maximum likelihood estimation routines only and argues that this estimate is superior to the estimate obtained via MI when the number of imputations is small. Interpretation of λ j is also addressed.",0
https://doi.org/10.1214/06-ba122,Deviance information criteria for missing data models,"The deviance information criterion (DIC) introduced by is directly inspired by linear and generalised linear models, but it is not so naturally defined for missing data models. In this paper, we reassess the criterion for such models, testing the behaviour of various extensions in the cases of mixture and random effect models.",0
https://doi.org/10.1007/s10869-013-9300-2,Meta-analytic Reviews in the Organizational Sciences: Two Meta-analytic Schools on the Way to MARS (the Meta-analytic Reporting Standards),"Purpose: The purpose of this study was to review the Meta-Analysis Reporting Standards (MARS) of the American Psychological Association (APA) and highlight opportunities for improvement of meta-analytic reviews in the organizational sciences. Design/Methodology/Approach: The paper reviews MARS, describes ""best"" meta-analytic practices across two schools of meta-analysis, and shows how implementing such practices helps achieve the aims set forth in MARS. Examples of best practices are provided to aid readers in finding models for their own research. Implications/Value: Meta-analytic reviews are a primary avenue for the accumulation of knowledge in the organizational sciences as well as many other areas of science. Unfortunately, many meta-analytic reviews in the organizational sciences do not fully follow professional guidelines and standards as closely as they should. Such deviations from best practice undermine the transparency and replicability of the reviews and thus their usefulness for the generation of cumulative knowledge and evidence-based practice. This study shows how implementing ""best"" meta-analytic practices helps to achieve the aims set forth in MARS. Although the paper is written primarily for organizational scientists, the paper's recommendations are not limited to any particular scientific domain. Ã‚Â© 2013 Springer Science+Business Media New York.",0
https://doi.org/10.1002/sim.5709,A bias correction for covariance estimators to improve inference with generalized estimating equations that use an unstructured correlation matrix,"Generalized estimating equations (GEEs) are routinely used for the marginal analysis of correlated data. The efficiency of GEE depends on how closely the working covariance structure resembles the true structure, and therefore accurate modeling of the working correlation of the data is important. A popular approach is the use of an unstructured working correlation matrix, as it is not as restrictive as simpler structures such as exchangeable and AR-1 and thus can theoretically improve efficiency. However, because of the potential for having to estimate a large number of correlation parameters, variances of regression parameter estimates can be larger than theoretically expected when utilizing the unstructured working correlation matrix. Therefore, standard error estimates can be negatively biased. To account for this additional finite-sample variability, we derive a bias correction that can be applied to typical estimators of the covariance matrix of parameter estimates. Via simulation and in application to a longitudinal study, we show that our proposed correction improves standard error estimation and statistical inference.",0
https://doi.org/10.1007/bf02291366,Simultaneous factor analysis in several populations,"This paper is concerned with the study of similarities and differences in factor structures between different groups. A common situation occurs when a battery of tests has been administered to samples of examinees from several populations. A very general model is presented, in which any parameter in the factor analysis models (factor loadings, factor variances, factor covariances, and unique variances) for the different groups may be assigned an arbitrary value or constrained to be equal to some other parameter. Given such a specification, the model is estimated by the maximum likelihood method yielding a large sample x2 of goodness of fit. By computing several solutions under different specifications one can test various hypotheses. The method is capable of dealing with any degree of invariance, from the one extreme, where nothing is invariant, to the other extreme, where everything is invariant. Neither the number of tests nor the number of common factors need to be the same for all groups, but to be at all interesting, it is assumed that there is a common core of tests in each battery that is the same or at least content-wise comparable. Ã‚Â© 1971 Psychometric Society.",0
https://doi.org/10.4135/9781446247600.n31,Point-Referenced Spatial Modeling,,0
https://doi.org/10.1007/978-1-4614-1599-2_23,Multilevel Modeling,"Mortality is the most frequently modeled outcome in injury research. It is easy to recognize, relatively free from measurement error, and fundamentally interesting. Injury researchers in public health or clinical medicine have become familiar with logistic regression as a standard way to model a binary outcome like mortality (or alternatively survival). Many other outcomes encountered in injury research can also be considered binary, such as the occurrence of a serious complication or an extended length of stay in hospital. Ã‚Â© Springer Science+Business Media, LLC 2012. All rights reserved.",0
https://doi.org/10.1016/j.tree.2008.10.008,Generalized linear mixed models: a practical guide for ecology and evolution,"How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize ‘best-practice' data analysis procedures for scientists facing this challenge.",0
https://doi.org/10.1167/6.11.13,Spatial four-alternative forced-choice method is the preferred psychophysical method for naive observers,"H. R. Blackwell (1952) investigated the influence of different psychophysical methods and procedures on detection thresholds. He found that the temporal two-interval forced-choice method (2-IFC) combined with feedback, blocked constant stimulus presentation with few different stimulus intensities, and highly trained observers resulted in the ""best"" threshold estimates. This recommendation is in current practice in many psychophysical laboratories and has entered the psychophysicists' ""folk wisdom"" of how to run proper psychophysical experiments. However, Blackwell's recommendations explicitly require experienced observers, whereas many psychophysical studies, particularly with children or within a clinical setting, are performed with naïve observers. In a series of psychophysical experiments, we find a striking and consistent discrepancy between naïve observers' behavior and that reported for experienced observers by Blackwell: Naïve observers show the ""best"" threshold estimates for the spatial four-alternative forced-choice method (4-AFC) and the worst for the commonly employed temporal 2-IFC. We repeated our study with a highly experienced psychophysical observer, and he replicated Blackwell's findings exactly, thus suggesting that it is indeed the difference in psychophysical experience that causes the discrepancy between our findings and those of Blackwell. In addition, we explore the efficiency of different methods and show 4-AFC to be more than 3.5 times more efficient than 2-IFC under realistic conditions. While we have found that 4-AFC consistently gives lower thresholds than 2-IFC in detection tasks, we have found the opposite for discrimination tasks. This discrepancy suggests that there are large extrasensory influences on thresholds--sensory memory for IFC methods and spatial attention for spatial forced-choice methods--that are critical but, alas, not part of theoretical approaches to psychophysics such as signal detection theory.",0
,Random effects models.,,0
https://doi.org/10.1007/bf02294241,A comparison of the efficiency and accuracy of BILOG and LOGIST,"Comparisons are made between BILOG version 2.2 and LOGIST 5.0 Version 2.5 in estimating the item parameters, traits, item characteristic functions (ICFs), and test characteristic functions (TCFs) for the three-parameter logistic model. Data analyzed are simulated item responses for 1000 simulees and one 10-item test, four 20-item tests, and four 40-item tests. LOGIST usually was faster than BILOG in producing maximum likelihood estimates. BILOG almost always produced more accurate estimates of individual item parameters. In estimating ICFs and TCFs BILOG was more accurate for the 10-item test, and the two programs were about equally accurate for the 20- and 40-item tests. Â© 1987 The Psychometric Society.",0
https://doi.org/10.1007/978-1-4613-0893-5_17,Dynamic but Structural Equation Modeling of Repeated Measures Data,"The term “dynamic” is broadly defined as a pattern of change. Many scientists have searched for dynamics by calculating df/dt: the ratio of changes or differences d in a function f relative to changes in time t.This simple dynamic equation was used in the 16th and 17th century motion experiments of Galileo, in the 17th and 18th century gravitation experiments of Newton, and in the 19th century experiments of many physicists and chemists (see Morris, 1985). I also use this dynamic equation, but here I examine multivariate psychological change data using the 20th century developments of latent variable structural equation modeling.",0
https://doi.org/10.1186/s12918-015-0190-y,Genome-scale metabolic model of Rhodococcus jostii RHA1 (iMT1174) to study the accumulation of storage compounds during nitrogen-limited condition,"Rhodococcus jostii RHA1 growing on different substrates is capable of accumulating simultaneously three types of carbon storage compounds: glycogen, polyhydroxyalkanoates (PHA), and triacylglycerols (TAG). Under nitrogen-limited (N-limited) condition, the level of storage increases as is commonly observed for other bacteria. The proportion of each storage compound changes with substrate, but it remains unclear what modelling approach should be adopted to predict the relative composition of the mixture of the storage compounds. We analyzed the growth of R. jostii RHA1 under N-limited conditions using a genome-scale metabolic modelling approach to determine which global metabolic objective function could be used for the prediction.The R. jostii RHA1 model (iMT1174) produced during this study contains 1,243 balanced metabolites, 1,935 unique reactions, and 1,174 open reading frames (ORFs). Seven objective functions used with flux balance analysis (FBA) were compared for their capacity to predict the mixture of storage compounds accumulated after the sudden onset of N-limitation. Predictive abilities were determined using a Bayesian approach. Experimental data on storage accumulation mixture (glycogen, polyhydroxyalkanoates, and triacylglycerols) were obtained for batch cultures grown on glucose or acetate. The best FBA simulation results were obtained using a novel objective function for the N-limited condition which combined the maximization of the storage fluxes and the minimization of metabolic adjustments (MOMA) with the preceding non-limited conditions (max storage + environmental MOMA). The FBA solutions for the non-limited growth conditions were simply constrained by the objective function of growth rate maximization. Measurement of central metabolic fluxes by (13)C-labelling experiments of amino acids further supported the application of the environmental MOMA principle in the context of changing environment. Finally, it was found that the quantitative predictions of the storage mixture during N-limited storage accumulation were fairly sensitive to the biomass composition, as expected.The genome-scale metabolic model analysis of R. jostii RHA1 cultures suggested that the intracellular reaction flux profile immediately after the onset of N-limited condition are impacted by the values of the same fluxes during the period of non-limited growth. PHA turned out to be the main storage pool of the mixture in R. jostii RHA1.",0
https://doi.org/10.1007/bf02293789,A study of algorithms for covariance structure analysis with specific comparisons using factor analysis,"Several algorithms for covariance structure analysis are considered in addition to the Fletcher-Powell algorithm. These include the Gauss-Newton, Newton-Raphson, Fisher Scoring, and Fletcher-Reeves algorithms. Two methods of estimation are considered, maximum likelihood and weighted least squares. It is shown that the Gauss-Newton algorithm which in standard form produces weighted least squares estimates can, in iteratively reweighted form, produce maximum likelihood estimates as well. Previously unavailable standard error estimates to be used in conjunction with the Fletcher-Reeves algorithm are derived. Finally all the algorithms are applied to a number of maximum likelihood and weighted least squares factor analysis problems to compare the estimates and the standard errors produced. The algorithms appear to give satisfactory estimates but there are serious discrepancies in the standard errors. Because it is robust to poor starting values, converges rapidly and conveniently produces consistent standard errors for both maximum likelihood and weighted least squares problems, the Gauss-Newton algorithm represents an attractive alternative for at least some covariance structure analyses. Ã‚Â© 1979 The Psychometric Society.",0
https://doi.org/10.1037/1082-989x.9.1.30,The Role of Coding Time in Estimating and Interpreting Growth Curve Models.,"The coding of time in growth curve models has important implications for the interpretation of the resulting model that are sometimes not transparent. The authors develop a general framework that includes predictors of growth curve components to illustrate how parameter estimates and their standard errors are exactly determined as a function of receding time in growth curve models. Linear and quadratic growth model examples are provided, and the interpretation of estimates given a particular coding of time is illustrated. How and why the precision and statistical power of predictors of lower order growth curve components changes over time is illustrated and discussed. Recommendations include coding time to produce readily interpretable estimates and graphing lower order effects across time with appropriate confidence intervals to help illustrate and understand the growth process.",0
https://doi.org/10.1037/1082-989x.3.3.328,Power estimation in social relations analyses.,"This article is intended for investigators interested in applying the Social Relations Model (SRM) to the study of dyadic processes. On the basis of theory and simulation, the authors provide practical advice concerning optimal number of persons per research group, number of research groups, type of research design, and significance testing method. Power depends more on group size than on number of groups. Power also is greater with the round-robin design than with the block design and greater with an asymptotic test of significance than with the test usually used. The authors discuss the effect of skew on theoretical power.",0
https://doi.org/10.1007/bf02294770,Efficient nonparametric approaches for estimating the operating characteristics of discrete item responses,"Rationale and the actual procedures of two nonparametric approaches, called Bivariate P.D.F. Approach and Conditional P.D.F. Approach, for estimating the operating characteristic of a discrete item response, or the conditional probability, given latent trait, that the examinee's response be that specific response, are introduced and discussed. These methods are featured by the facts that: (a) estimation is made without assuming any mathematical forms, and (b) it is based upon a relatively small sample of several hundred to a few thousand examinees. Some examples of the results obtained by the Simple Sum Procedure and the Differential Weight Procedure of the Conditional P.D.F. Approach are given, using simulated data. The usefulness of these nonparametric methods is also discussed.",0
https://doi.org/10.1080/10409289.2015.1012188,"Boys Might Catch Up, Family Influences Continue: Influences on Behavioral Self-Regulation in Children From an Affluent Region in Germany Before School Entry","Research Findings: Behavioral self-regulation is crucial for school success. Although behavioral self-regulation typically grows rapidly during the preschool period, children in this age group vary widely in their behavioral self-regulation capacities. The present study investigated 3 potential determinants of growth rates in behavioral self-regulation in children from an affluent region in Germany: family educational resources, child gender, and child negative affectivity. Using a longitudinal design, we observed children (N = 60) during the last 2 years before school entry. Children from families with relatively fewer educational resources started off with a lower level of behavioral self-regulation and did not catch up with their more advantaged peers. Boys showed poorer initial behavioral self-regulation than girls, but their gains in behavioral self-regulation were greater over time compared to girls’. Negative affectivity influenced neither the initial level of nor growth in behavioral self-regulati...",0
https://doi.org/10.3102/1076998612461831,A Semiparametric Model for Jointly Analyzing Response Times and Accuracy in Computerized Testing,"The item response times (RTs) collected from computerized testing represent an underutilized type of information about items and examinees. In addition to knowing the examinees’ responses to each item, we can investigate the amount of time examinees spend on each item. Current models for RTs mainly focus on parametric models, which have the advantage of conciseness, but may suffer from reduced flexibility to fit real data. We propose a semiparametric approach, specifically, the Cox proportional hazards model with a latent speed covariate to model the RTs, embedded within the hierarchical framework proposed by van der Linden to model the RTs and response accuracy simultaneously. This semiparametric approach combines the flexibility of nonparametric modeling and the brevity and interpretability of the parametric modeling. A Markov chain Monte Carlo method for parameter estimation is given and may be used with sparse data obtained by computerized adaptive testing. Both simulation studies and real data analysis are carried out to demonstrate the applicability of the new model.",0
https://doi.org/10.3758/bf03192848,Multilevel models for the experimental psychologist: Foundations and illustrative examples,"Although common in the educational and developmental areas, multilevel models are not often utilized in the analysis of data from experimental designs. This article illustrates how multilevel models can be useful with two examples from experimental designs with repeated measurements not involving time. One example demonstrates how to properly examine independent variables for experimental stimuli or individuals that are categorical, continuous, or semicontinuous in the presence of missing data. The second example demonstrates how response times and error rates can be modeled simultaneously within a multivariate model in order to examine speed-accuracy trade-offs at the experimental-condition and individual levels, as well as to examine differences in the magnitude of effects across outcomes. SPSS and SAS syntax for the examples are available electronically.",0
https://doi.org/10.1177/0013164416652441,Fitting Residual Error Structures for Growth Models in SAS PROC MCMC,"In behavioral sciences broadly, estimating growth models with Bayesian methods is becoming increasingly common, especially to combat small samples common with longitudinal data. Although M plus is becoming an increasingly common program for applied research employing Bayesian methods, the limited selection of prior distributions for the elements of covariance structures makes more general software more advantages under certain conditions. However, as a disadvantage of general software’s software flexibility, few preprogrammed commands exist for specifying covariance structures. For instance, PROC MIXED has a few dozen such preprogrammed options, but when researchers divert to a Bayesian framework, software offer no such guidance and requires researchers to manually program these different structures, which is no small task. As such the literature has noted that empirical papers tend to simplify their covariance matrices to circumvent this difficulty, which is not desirable because such a simplification will likely lead to biased estimates of variance components and standard errors. To facilitate wider implementation of Bayesian growth models that properly model covariance structures, this article overviews how to generally program a growth model in SAS PROC MCMC and then demonstrates how to program common residual error structures. Full annotated SAS code and an applied example are provided.",0
https://doi.org/10.1214/aos/1176348671,Affinely Invariant Matching Methods with Ellipsoidal Distributions,"Matched sampling is a common technique used for controlling bias in observational studies. We present a general theoretical framework for studying the performance of such matching methods. Specifically, results are obtained concerning the performance of affinely invariant matching methods with ellipsoidal distributions, which extend previous results on equal percent bias reducing methods. Additional extensions cover conditionally affinely invariant matching methods for covariates with conditionally ellipsoidal distributions. These results decompose the effects of matching into one subspace containing the best linear discriminant, and the subspace of variables uncorrelated with the discriminant. This characterization of the effects of matching provides a theoretical foundation for understanding the performance of specific methods such as matched sampling using estimated propensity scores. Calculations for such methods are given in subsequent articles.",0
https://doi.org/10.1002/bimj.201400232,Augmented Beta rectangular regression models: A Bayesian perspective,"Mixed effects Beta regression models based on Beta distributions have been widely used to analyze longitudinal percentage or proportional data ranging between zero and one. However, Beta distributions are not flexible to extreme outliers or excessive events around tail areas, and they do not account for the presence of the boundary values zeros and ones because these values are not in the support of the Beta distributions. To address these issues, we propose a mixed effects model using Beta rectangular distribution and augment it with the probabilities of zero and one. We conduct extensive simulation studies to assess the performance of mixed effects models based on both the Beta and Beta rectangular distributions under various scenarios. The simulation studies suggest that the regression models based on Beta rectangular distributions improve the accuracy of parameter estimates in the presence of outliers and heavy tails. The proposed models are applied to the motivating Neuroprotection Exploratory Trials in Parkinson's Disease (PD) Long-term Study-1 (LS-1 study, n = 1741), developed by The National Institute of Neurological Disorders and Stroke Exploratory Trials in Parkinson's Disease (NINDS NET-PD) network.",0
https://doi.org/10.3758/bf03203075,Maximum-likelihood psychometric procedures in two-alternative forced-choice: Evaluation and recommendations,"Several new adaptive psychometric procedures have been proposed that use maximum-likelihood methods to estimate points on a subject’s psychophysical function. These developments are summarized, and some psychometric situations that pose special problems for the new approaches are examined. The problem areas include the effects of threshold changes during the testing session, the impact of errors made by the subject, and differences between yes-no and two-alternative forced-choice situations. Data are presented from both human subjects and computer simulations. Strengths and weaknesses of the new procedures are identified and recommendations are made for their use.",0
https://doi.org/10.1037/10244-013,Psychometric procedures for administering CAT-ASVAB.,,0
https://doi.org/10.1201/b14835-15,Hypothesis testing and model selection,,0
https://doi.org/10.1002/sim.991,A comparison of generalized linear mixed model procedures with estimating equations for variance and covariance parameter estimation in longitudinal studies and group randomized trials,"Response data in longitudinal studies and group randomized trials are gathered on units that belong to clusters, within which data are usually positively correlated. Therefore, estimates and confidence intervals for intraclass correlation or variance components are helpful when designing a longitudinal study or group randomized trial. Data simulated from both study designs are used to investigate the estimation of variance and covariance parameters from the following procedures: for continuous outcomes, restricted maximum likelihood (REML) and estimating equations (EE); for binary outcomes, restricted pseudo-likelihood (REPL) and estimating equations (EE). We evaluate these procedures to see which provide valid and precise estimates as well as correct standard errors for the intraclass correlation coefficient or variance components. REML seems the better choice for estimating terms related to correlation for models with normal outcomes, especially in group randomized trial situations. Results for REML and EE are mixed when outcomes are continuous and non-normal. With binary outcomes neither REPL nor EE provides satisfactory estimation or inference in longitudinal study situations, while REPL is preferable for group randomized trials.",0
https://doi.org/10.1111/rssa.12018,Synthesis of evidence on heterogeneous interventions with multiple outcomes recorded over multiple follow-up times reported inconsistently: a smoking cessation case-study,"Health technology assessment requires the synthesis of evidence from multiple sources to assess the cost-effectiveness of competing interventions. However, the format of the available reported evidence is often complex. We present a case-study of electronic aids to smoking cessation, which raises various methodological challenges. The evidence base evaluated highly complex and diverse interventions, reporting one or both of two different, but related, outcome measures. Furthermore, there were differences between studies in the number and timing of follow-up times reported, whereas 12-month continuous abstinence is required in the cost-effectiveness analysis. We develop a categorization system to evaluate the interventions, and we use network meta-analysis of time-to-relapse model parameters to estimate coherent intervention effects for any pair of categories. We compare the fit of alternative time-to-relapse models and explore the effect of joint models for both outcome measures, which can be used to estimate treatment effects when a given outcome is not reported, so that all the available evidence can be combined. © 2013 Royal Statistical Society.",0
https://doi.org/10.1093/aob/mct176,Highly local environmental variability promotes intrapopulation divergence of quantitative traits: an example from tropical rain forest trees,"In habitat mosaics, plant populations face environmental heterogeneity over short geographical distances. Such steep environmental gradients can induce ecological divergence. Lowland rainforests of the Guiana Shield are characterized by sharp, short-distance environmental variations related to topography and soil characteristics (from waterlogged bottomlands on hydromorphic soils to well-drained terra firme on ferralitic soils). Continuous plant populations distributed along such gradients are an interesting system to study intrapopulation divergence at highly local scales. This study tested (1) whether conspecific populations growing in different habitats diverge at functional traits, and (2) whether they diverge in the same way as congeneric species having different habitat preferences.Phenotypic differentiation was studied within continuous populations occupying different habitats for two congeneric, sympatric, and ecologically divergent tree species (Eperua falcata and E. grandiflora, Fabaceae). Over 3000 seeds collected from three habitats were germinated and grown in a common garden experiment, and 23 morphological, biomass, resource allocation and physiological traits were measured.In both species, seedling populations native of different habitats displayed phenotypic divergence for several traits (including seedling growth, biomass allocation, leaf chemistry, photosynthesis and carbon isotope composition). This may occur through heritable genetic variation or other maternally inherited effects. For a sub-set of traits, the intraspecific divergence associated with environmental variation coincided with interspecific divergence.The results indicate that mother trees from different habitats transmit divergent trait values to their progeny, and suggest that local environmental variation selects for different trait optima even at a very local spatial scale. Traits for which differentiation within species follows the same pattern as differentiation between species indicate that the same ecological processes underlie intra- and interspecific variation.",0
https://doi.org/10.1037/h0026141,Statistical significance in psychological research.,"Most theories in the areas of personality, clinical, and social psychology predict no more than the direction of a correlation, group difference, or treatment effect. Since the null hypothesis is never strictly true, such predictions have about a SO-SO chance of being confirmed by experiment when the theory in question is false, since the statistical significance of the result is a function of the sample size. Confirmation of a single directional prediction should usually add little to one's confidence in the theory being tested. Most theories should be tested by multiple corroboration and most empirical generalizations by constructive replication. Statistical significance is perhaps the least important attribute of a good experiment; it is never a sufficient condition for claiming that a theory has been usefully corroborated, that a meaningful empirical fact has been established, or that an experimental report ought to be published. In a recent journal article Sapolsky (1964) developed the following substantive theory: Some psychiatric patients entertain an unconscious belief in the theory of birth which involves the notions of oral impregnation and anal parturition. Such patients should be inclined to manifest eating disorders: compulsive eating in the case of those who wish to get pregnant and anorexia in those who do not. Such patients should also be inclined to see cloacal animals, such as frogs, on the Rorschach. This reasoning led Sapolsky to predict that Rorschach frog responders show",0
https://doi.org/10.1007/s11162-011-9232-5,Effect Sizes and Statistical Methods for Meta-Analysis in Higher Education,"Quantitative meta-analysis is a very useful, yet underutilized, technique for synthesizing research findings in higher education. Meta-analytic inquiry can be more challenging in higher education than in other fields of study as a result of (a) concerns about the use of regression coefficients as a metric for comparing the magnitude of effects across studies, and (b) the non-independence of observations that occurs when a single study contains multiple effect sizes. This methodological note discusses these two important issues and provides concrete suggestions for addressing them. First, meta-analysis scholars have concluded that standardized regression coefficients, which are often provided in higher education manuscripts, constitute an appropriate metric of effect size. Second, hierarchical linear modeling (HLM) analyses provide an effective method for conducting meta-analytic research while accounting for the non-independence of observations, and HLM is generally superior to other proposed methods that attempt to remedy this same problem. A discussion of how to implement these techniques appropriately is provided. Ã‚Â© 2011 Springer Science+Business Media, LLC.",0
https://doi.org/10.1080/00273171.2011.540475,A Systematic Review of Propensity Score Methods in the Social Sciences,"The use of propensity scores in psychological and educational research has been steadily increasing in the last 2 to 3 years. However, there are some common misconceptions about the use of different estimation techniques and conditioning choices in the context of propensity score analysis. In addition, reporting practices for propensity score analyses often lack important details that allow other researchers to confidently judge the appropriateness of reported analyses and potentially to replicate published findings. In this article we conduct a systematic literature review of a large number of published articles in major areas of social science that used propensity scores up until the fall of 2009. We identify common errors in estimation, conditioning, and reporting of propensity score analyses and suggest possible solutions.",0
https://doi.org/10.1214/aos/1176325625,Estimation of a Covariance Matrix Using the Reference Prior,"Estimation of a covariance matrix $\sum$ is a notoriously difficult problem; the standard unbiased estimator can be substantially suboptimal. We approach the problem from a noninformative prior Bayesian perspective, developing the reference noninformative prior for a covariance matrix and obtaining expressions for the resulting Bayes estimators. These expressions involve the computation of high-dimensional posterior expectations, which is done using a recent Markov chain simulation tool, the hit-and-run sampler. Frequentist risk comparisons with previously suggested estimators are also given, and determination of the accuracy of the estimators is addressed.",0
https://doi.org/10.1016/j.jcp.2016.02.056,Bayesian estimation of Karhunen–Loève expansions; A random subspace approach,"One of the most widely-used procedures for dimensionality reduction of high dimensional data is Principal Component Analysis (PCA). More broadly, low-dimensional stochastic representation of random fields with finite variance is provided via the well known Karhunen–Loève expansion (KLE). The KLE is analogous to a Fourier series expansion for a random process, where the goal is to find an orthogonal transformation for the data such that the projection of the data onto this orthogonal subspace is optimal in the L 2 sense, i.e., which minimizes the mean square error. In practice, this orthogonal transformation is determined by performing an SVD (Singular Value Decomposition) on the sample covariance matrix or on the data matrix itself. Sampling error is typically ignored when quantifying the principal components, or, equivalently, basis functions of the KLE. Furthermore, it is exacerbated when the sample size is much smaller than the dimension of the random field. In this paper, we introduce a Bayesian KLE procedure, allowing one to obtain a probabilistic model on the principal components, which can account for inaccuracies due to limited sample size. The probabilistic model is built via Bayesian inference, from which the posterior becomes the matrix Bingham density over the space of orthonormal matrices. We use a modified Gibbs sampling procedure to sample on this space and then build probabilistic Karhunen–Loève expansions over random subspaces to obtain a set of low-dimensional surrogates of the stochastic process. We illustrate this probabilistic procedure with a finite dimensional stochastic process inspired by Brownian motion.",0
https://doi.org/10.1177/014662169602000303,A Global Information Approach to Computerized Adaptive Testing,"Most item selection in computerized adaptive testing is based on Fisher information (or item information). At each stage, an item is selected to maximize the Fisher information at the currently estimated trait level (θ). However, this application of Fisher information could be much less efficient than assumed if the estimators are not close to the true θ, especially at early stages of an adaptive test when the test length (number of items) is too short to provide an accurate estimate for true θ. It is argued here that selection procedures based on global information should be used, at least at early stages of a test when θ estimates are not likely to be close to the true θ. For this purpose, an item selection procedure based on average global information is proposed. Re sults from pilot simulation studies comparing the usual maximum item information item selection with the pro posed global information approach are reported, indicat ing that the new method leads to improvement in terms of bias and mean squared error reduction under many circumstances. Index terms: computerized adaptive testing, Fisher information, global information, infor mation surface, item information, item response theory, Kullback-Leibler information, local information, test in formation.",0
https://doi.org/10.1007/978-3-658-06550-8,Intangible Values in Financial Accounting and Reporting,"Academics and practitioners argue that intangible values have become significant value drivers of today's economy. Major production inputs no longer comprise of property, plant and equipment, but rather of brands, knowledge and other technological innovation. Based on this notion, information on such phenomena is supposedly crucial for existing and potential capital providers in making decisions whether to allocate resources to a company. This thesis examines the information use and needs of financial analysts with respect to intangible values. The purpose is to shed light on the usefulness of such information from the perspective of one of the primary user groups of IFRSs. Ã‚Â© Springer Fachmedien Wiesbaden 2015.",0
https://doi.org/10.1080/10705510802561402,Modeling Dynamic Functional Neuroimaging Data Using Structural Equation Modeling,"The aims of this study were to present a method for developing a path analytic network model using data acquired from positron emission tomography. Regions of interest within the human brain were identified through quantitative activation likelihood estimation meta-analysis. Using this information, a ""true"" or population path model was then developed using Bayesian structural equation modeling. To evaluate the impact of sample size on parameter estimation bias, proportion of parameter replication coverage, and statistical power, a 2 group (clinical/control) × 6 (sample size: N = 10, N = 15, N = 20, N = 25, N = 50, N = 100) Markov chain Monte Carlo study was conducted. Results indicate that using a sample size of less than N = 15 per group will produce parameter estimates exhibiting bias greater than 5% and statistical power below .80.",0
https://doi.org/10.2307/1247354,"Survey Design and Analysis: Principles, Cases, and Procedures",,0
https://doi.org/10.1016/j.pbb.2013.04.012,Peripheral kappa and delta opioid receptors are involved in the antinociceptive effect of crotalphine in a rat model of cancer pain,"Cancer pain is an important clinical problem and may not respond satisfactorily to the current analgesic therapy. We have characterized a novel and potent analgesic peptide, crotalphine, from the venom of the South American rattlesnake Crotalus durissus terrificus . In the present work, the antinociceptive effect of crotalphine was evaluated in a rat model of cancer pain induced by intraplantar injection of Walker 256 carcinoma cells. Intraplantar injection of tumor cells caused the development of hyperalgesia and allodynia, detected on day 5 after tumor cell inoculation. Crotalphine (6 μg/kg), administered p.o., blocked both phenomena. The antinociceptive effect was detected 1 h after treatment and lasted for up to 48 h. Intraplantar injection of nor-binaltorphimine (50 g/paw), a selective antagonist of κ-opioid receptors, antagonized the antinociceptive effect of the peptide, whereas N , N -diallyl-Tyr-Aib-Phe-Leu (ICI 174,864, 10 μg/paw), a selective antagonist of δ-opioid receptors, partially reversed this effect. On the other hand, D-Phe-Cys-Tyr-D-Trp-Orn-Thr-Pen-Thr amide (CTOP, 20 g/paw), an antagonist of μ-opioid receptors, did not modify crotalphine-induced antinociception. These data indicate that crotalphine induces a potent and long lasting opioid-mediated antinociception in cancer pain. • We investigate the antinociceptive effect of crotalphine in cancer pain. • Crotalphine inhibits hyperalgesia, allodynia and spontaneous pain induced by tumor. • Crotalphine induces a potent and long-lasting (2 days) antinociceptive effect. • Peripheral kappa and delta opioid receptors are involved in the antinociceptive.",0
https://doi.org/10.1007/s00231-013-1213-0,Incorporating engineering intuition for parameter estimation in thermal sciences,"This paper proposes a new method of incorporating priors based on engineering intuition for solving inverse problems. The thesis of this paper is that if an asymptote can be found to a problem in applied sciences or engineering, estimation of parameters can be first done for this asymptotic variant, which in principle should be simpler, since one or more parameters of the original problem may vanish for the asymptotic variant. Even so, by solving the inverse problem associated with the asymptotic variant, estimates of key parameters of the full problem can be obtained. This information can then be quantitatively incorporated as priors in the estimation of parameters for the full version of the problem which we call as prior generation through asymptotic variant. The goal is to see if this methodology will significantly reduce the uncertainties in the resulting estimates. To demonstrate this methodology, the classic problem of unsteady heat transfer from a one dimensional fin is chosen. The inverse problem is posed as the simultaneous estimation of the temperature dependent transfer coefficient (h Î¸ ) and the thermal diffusivity (Î±) of the fin material, given experimentally measured temperature-time histories at various locations along the fin. The asymptotic variant Î¸ (x,t) is the steady state problem where the influence of thermal diffusivity vanishes. Using surrogate temperature data generated from assumed values of h Î¸, first the asymptotic variant of the problem is solved using the Markov Chain Monte Carlo method in a Bayesian framework to generate an estimate of h Î¸ . The estimate of h Î¸ is then used as an informative prior for solving the inverse problem of determining h Î¸ and Î± from Î¸ (x,t), and the effect of prior is quantitatively assessed by performing estimation with and without the prior. Finally, for purposes of validation, in-house experiments have been done where Î¸ (x,t) is generated using liquid crystal thermography and these data are used to estimate h Î¸ and Î±. A comparison of experimentally measured temperatures with those that are simulated by using estimated values of (h Î¸, Î±) to solve the governing equation to the problem is also done. Â© 2013 Springer-Verlag Berlin Heidelberg.",0
https://doi.org/10.1371/journal.pone.0025635,Bayesian Hierarchical Models Combining Different Study Types and Adjusting for Covariate Imbalances: A Simulation Study to Assess Model Performance,"Bayesian hierarchical models have been proposed to combine evidence from different types of study designs. However, when combining evidence from randomised and non-randomised controlled studies, imbalances in patient characteristics between study arms may bias the results. The objective of this study was to assess the performance of a proposed Bayesian approach to adjust for imbalances in patient level covariates when combining evidence from both types of study designs.Simulation techniques, in which the truth is known, were used to generate sets of data for randomised and non-randomised studies. Covariate imbalances between study arms were introduced in the non-randomised studies. The performance of the Bayesian hierarchical model adjusted for imbalances was assessed in terms of bias. The data were also modelled using three other Bayesian approaches for synthesising evidence from randomised and non-randomised studies. The simulations considered six scenarios aimed at assessing the sensitivity of the results to changes in the impact of the imbalances and the relative number and size of studies of each type. For all six scenarios considered, the Bayesian hierarchical model adjusted for differences within studies gave results that were unbiased and closest to the true value compared to the other models.Where informed health care decision making requires the synthesis of evidence from randomised and non-randomised study designs, the proposed hierarchical Bayesian method adjusted for differences in patient characteristics between study arms may facilitate the optimal use of all available evidence leading to unbiased results compared to unadjusted analyses.",0
https://doi.org/10.1002/sim.3594,Addressing between-study heterogeneity and inconsistency in mixed treatment comparisons: Application to stroke prevention treatments in individuals with non-rheumatic atrial fibrillation,"Mixed treatment comparison models extend meta-analysis methods to enable comparisons to be made between all relevant comparators in the clinical area of interest. In such modelling it is imperative that potential sources of variability are explored to explain both heterogeneity (variation in treatment effects between trials within pairwise contrasts) and inconsistency (variation in treatment effects between pairwise contrasts) to ensure the validity of the analysis.The objective of this paper is to extend the mixed treatment comparison framework to allow for the incorporation of study-level covariates in an attempt to explain between-study heterogeneity and reduce inconsistency. Three possible model specifications assuming different assumptions are described and applied to a 17-treatment network for stroke prevention treatments in individuals with non-rheumatic atrial fibrillation.The paper demonstrates the feasibility of incorporating covariates within a mixed treatment comparison framework and using model fit statistics to choose between alternative model specifications. Although such an approach may adjust for inconsistencies in networks, as for standard meta-regression, the analysis will suffer from low power if the number of trials is small compared with the number of treatment comparators.",0
https://doi.org/10.2217/cer.14.29,Using indirect comparisons to compare interventions within a Cochrane review: a tool for comparative effectiveness research,"Aim: Assessing relative performance among competing interventions is an important part of comparative effectiveness research. Bayesian indirect comparisons add information to existing Cochrane reviews, such as which intervention is likely to perform best. However, heterogeneity variance priors may influence results and, potentially, clinical guidance. Methods: We highlight the features of Bayesian indirect comparisons using a case study of a Cochrane review update in asthma care. The probability that one self-management educational intervention outperforms others is estimated. Simulation studies investigate the effect of heterogeneity variance prior distributions. Results: Results suggest a 55% probability that individual education is best, followed by combination (39%) and group (6%). The intervention with few trials was sensitive to prior distributions. Conclusion: Bayesian indirect comparisons updates of Cochrane reviews are valuable comparative effectiveness research tools.",0
https://doi.org/10.1207/s15327906mbr4103_5,A Comparison of Single Sample and Bootstrap Methods to Assess Mediation in Cluster Randomized Trials,"A Monte Carlo study examined the statistical performance of single sample and bootstrap methods that can be used to test and form confidence interval estimates of indirect effects in two cluster randomized experimental designs. The designs were similar in that they featured random assignment of clusters to one of two treatment conditions and included a single intervening variable and outcome, but they differed in whether the mediator was measured at the participant or site level. A bias-corrected bootstrap had the best statistical performance for each design and was closely followed by the empirical-Mtest, either of which is recommended for testing and estimating indirect effects in multilevel designs. In addition, consistent with previous research, the commonly used z test had relatively poor performance.",0
https://doi.org/10.1111/j.1541-0420.2010.01547.x,Generalized Causal Mediation Analysis,"The goal of mediation analysis is to assess direct and indirect effects of a treatment or exposure on an outcome. More generally, we may be interested in the context of a causal model as characterized by a directed acyclic graph (DAG), where mediation via a specific path from exposure to outcome may involve an arbitrary number of links (or ""stages""). Methods for estimating mediation (or pathway) effects are available for a continuous outcome and a continuous mediator related via a linear model, while for a categorical outcome or categorical mediator, methods are usually limited to two-stage mediation. We present a method applicable to multiple stages of mediation and mixed variable types using generalized linear models. We define pathway effects using a potential outcomes framework and present a general formula that provides the effect of exposure through any specified pathway. Some pathway effects are nonidentifiable and their estimation requires an assumption regarding the correlation between counterfactuals. We provide a sensitivity analysis to assess the impact of this assumption. Confidence intervals for pathway effect estimates are obtained via a bootstrap method. The method is applied to a cohort study of dental caries in very low birth weight adolescents. A simulation study demonstrates low bias of pathway effect estimators and close-to-nominal coverage rates of confidence intervals. We also find low sensitivity to the counterfactual correlation in most scenarios.",0
https://doi.org/10.1080/00949655.2012.690763,Adaptive fitting of linear mixed-effects models with correlated random effects,"Linear mixed-effects model has been widely used in longitudinal data analyses. In practice, the fitting algorithm can fail to converge due to boundary issues of the estimated random-effects covariance matrix G, i.e., being near-singular, non-positive definite, or both. Current available algorithms are not computationally optimal because the condition number of matrix G is unnecessarily increased when the random-effects correlation estimate is not zero. We propose an adaptive fitting (AF) algorithm using an optimal linear transformation of the random-effects design matrix. It is a data-driven adaptive procedure, aiming at reducing subsequent random-effects correlation estimates down to zero in the optimal transformed estimation space. Simulations show that AF significantly improves the convergent properties, especially under small sample size, relative large noise and high correlation settings. One real data for Insulin-like Growth Factor (IGF) protein is used to illustrate the application of this algorithm implemented with software package R (nlme).",0
https://doi.org/10.1007/bf02296131,On the estimation of polychoric correlations and their asymptotic covariance matrix,A general theory for parametric inference in contingency tables is outlined. Estimation of polychoric correlations is seen as a special case of this theory. The asymptotic covariance matrix of the estimated polychoric correlations is derived for the case when the thresholds are estimated from the univariate marginals and the polychoric correlations are estimated from the bivariate marginals for given thresholds. Computational aspects are also discussed. Â© 1994 The Psychometric Society.,0
https://doi.org/10.1348/000711009x475187,Bayesian analysis of mixtures in structural equation models with non-ignorable missing data,"Structural equation models (SEMs) have become widely used to determine the interrelationships between latent and observed variables in social, psychological, and behavioural sciences. As heterogeneous data are very common in practical research in these fields, the analysis of mixture models has received a lot of attention in the literature. An important issue in the analysis of mixture SEMs is the presence of missing data, in particular of data missing with a non-ignorable mechanism. However, only a limited amount of work has been done in analysing mixture SEMs with non-ignorable missing data. The main objective of this paper is to develop a Bayesian approach for analysing mixture SEMs with an unknown number of components and non-ignorable missing data. A simulation study shows that Bayesian estimates obtained by the proposed Markov chain Monte Carlo methods are accurate and the Bayes factor computed via a path sampling procedure is useful for identifying the correct number of components, selecting an appropriate missingness mechanism, and investigating various effects of latent variables in the mixture SEMs. A real data set on a study of job satisfaction is used to demonstrate the methodology.",0
https://doi.org/10.1002/sim.650,A comparison of statistical methods for meta-analysis,"Meta-analysis may be used to estimate an overall effect across a number of similar studies. A number of statistical techniques are currently used to combine individual study results. The simplest of these is based on a fixed effects model, which assumes the true effect is the same for all studies. A random effects model, however, allows the true effect to vary across studies, with the mean true effect the parameter of interest. We consider three methods currently used for estimation within the framework of a random effects model, and illustrate them by applying each method to a collection of six studies on the effect of aspirin after myocardial infarction. These methods are compared using estimated coverage probabilities of confidence intervals for the overall effect. The techniques considered all generally have coverages below the nominal level, and in particular it is shown that the commonly used DerSimonian and Laird method does not adequately reflect the error associated with parameter estimation, especially when the number of studies is small.",0
https://doi.org/10.2307/2532845,A Modified EM Algorithm for Estimation in Generalized Mixed Models,"Application of the EM algorithm for estimation in the generalized mixed model has been largely unsuccessful because the E-step cannot be determined in most instances. The E-step computes the conditional expectation of the complete data log-likelihood and when the random effect distribution is normal, this expectation remains an intractable integral. The problem can be approached by numerical or analytic approximations; however, the computational burden imposed by numerical integration methods and the absence of an accurate analytic approximation have limited the use of the EM algorithm. In this paper, Laplace's method is adapted for analytic approximation within the E-step. The proposed algorithm is computationally straightforward and retains much of the conceptual simplicity of the conventional EM algorithm, although the usual convergence properties are not guaranteed. The proposed algorithm accommodates multiple random factors and random effect distributions besides the normal, e.g., the log-gamma distribution. Parameter estimates obtained for several data sets and through simulation show that this modified EM algorithm compares favorably with other generalized mixed model methods.",0
https://doi.org/10.1111/bmsp.12051,Multilevel multidimensional item response model with a multilevel latent covariate,"In a pre-test-post-test cluster randomized trial, one of the methods commonly used to detect an intervention effect involves controlling pre-test scores and other related covariates while estimating an intervention effect at post-test. In many applications in education, the total post-test and pre-test scores, ignoring measurement error, are used as response variable and covariate, respectively, to estimate the intervention effect. However, these test scores are frequently subject to measurement error, and statistical inferences based on the model ignoring measurement error can yield a biased estimate of the intervention effect. When multiple domains exist in test data, it is sometimes more informative to detect the intervention effect for each domain than for the entire test. This paper presents applications of the multilevel multidimensional item response model with measurement error adjustments in a response variable and a covariate to estimate the intervention effect for each domain.",0
https://doi.org/10.1007/bf02310555,Coefficient alpha and the internal structure of tests,"A general formula (α) of which a special case is the Kuder-Richardson coefficient of equivalence is shown to be the mean of all split-half coefficients resulting from different splittings of a test. α is therefore an estimate of the correlation between two random samples of items from a universe of items like those in the test. α is found to be an appropriate index of equivalence and, except for very short tests, of the first-factor concentration in the test. Tests divisible into distinct subtests should be so divided before using the formula. The index\(\bar r_{ij} \), derived from α, is shown to be an index of inter-item homogeneity. Comparison is made to the Guttman and Loevinger approaches. Parallel split coefficients are shown to be unnecessary for tests of common types. In designing tests, maximum interpretability of scores is obtained by increasing the first-factor concentration in any separately-scored subtest and avoiding substantial group-factor clusters within a subtest. Scalability is not a requisite.",0
https://doi.org/10.1534/g3.115.019299,Using Bayesian Multilevel Whole Genome Regression Models for Partial Pooling of Training Sets in Genomic Prediction,"Training set size is an important determinant of genomic prediction accuracy. Plant breeding programs are characterized by a high degree of structuring, particularly into populations. This hampers the establishment of large training sets for each population. Pooling populations increases training set size but ignores unique genetic characteristics of each. A possible solution is partial pooling with multilevel models, which allows estimating population-specific marker effects while still leveraging information across populations. We developed a Bayesian multilevel whole-genome regression model and compared its performance with that of the popular BayesA model applied to each population separately (no pooling) and to the joined data set (complete pooling). As an example, we analyzed a wide array of traits from the nested association mapping maize population. There we show that for small population sizes (e.g., <50), partial pooling increased prediction accuracy over no or complete pooling for populations represented in the training set. No pooling was superior; however, when populations were large. In another example data set of interconnected biparental maize populations either partial or complete pooling was superior, depending on the trait. A simulation showed that no pooling is superior when differences in genetic effects among populations are large and partial pooling when they are intermediate. With small differences, partial and complete pooling achieved equally high accuracy. For prediction of new populations, partial and complete pooling had very similar accuracy in all cases. We conclude that partial pooling with multilevel models can maximize the potential of pooling by making optimal use of information in pooled training sets.",0
https://doi.org/10.1177/0146621610392565,Marginal Maximum A Posteriori Item Parameter Estimation for the Generalized Graded Unfolding Model,"A marginal maximum a posteriori (MMAP) procedure was implemented to estimate item parameters in the generalized graded unfolding model (GGUM). Estimates from the MMAP method were compared with those derived from marginal maximum likelihood (MML) and Markov chain Monte Carlo (MCMC) procedures in a recovery simulation that varied sample size, questionnaire length, and number of item response categories. MMAP item parameter estimates were generally the most accurate and had the smallest standard errors on average. In contrast, the accuracy and variability of MML estimates suffered substantially when the number of item response categories was small and the true item locations were extreme. MMAP estimates were also more computationally efficient than corresponding MCMC estimates. Consequently, the MMAP procedure is recommended for estimation of GGUM item parameters.",0
https://doi.org/10.1162/003465304323023697,Finite-Sample Properties of Propensity-Score Matching and Weighting Estimators,"The finite-sample properties of matching and weighting estimators, often used for estimating average treatment effects, are analyzed. Potential and feasible precision gains relative to pair matching are examined. Local linear matching (with and without trimming), k-nearest-neighbor matching, and particularly the weighting estimators performed worst. Ridge matching, on the other hand, leads to an approximately 25% smaller MSE than does pair matching. In addition, ridge matching is least sensitive to the design choice.",0
https://doi.org/10.1038/377059a0,Neural computation of log likelihood in control of saccadic eye movements,"The latency between the appearance of a visual target and the start of the saccadic eye movement made to look at it varies from trial to trial to an extent that is inexplicable in terms of ordinary 'physiological' processes such as synaptic delays and conduction velocities. An alternative interpretation is that it represents the time needed to decide whether a target is in fact present: decision processes are necessarily stochastic, because they depend on extracting information from noisy sensory signals'. In one such model, the presence of a target causes a signal in a decision unit to rise linearly at a rate r from its initial value s, until it reaches a fixed threshold Î¸, when a saccade is initiated, One can regard this decision signal as a neural estimate of the log likelihood of the hypothesis that the target is present, the threshold being the significance criterion or likelihood level at which the target is presumed to be present, Experiments manipulating the prior probability of the target's appearing confirm this notion: the latency distribution then changes in the way expected if so simply reflects the prior log likelihood of the stimulus.",0
https://doi.org/10.1080/03637751.2010.498791,Adolescents' Use of Sexually Explicit Internet Material and Sexual Uncertainty: The Role of Involvement and Gender,"Research has shown that adolescents' use of sexually explicit Internet material (SEIM) is positively associated with an important characteristic of the developing sexual self, sexual uncertainty. However, the causal relation between SEIM use and sexual uncertainty is unclear. Moreover, we do not know which processes underlie this relation and whether gender moderates these processes. Based on a three-wave panel survey among 956 Dutch adolescents, structural equation modeling revealed that more frequent SEIM use increased adolescents' sexual uncertainty. This influence was mediated by adolescents' involvement in SEIM. The impact of SEIM use on involvement was stronger for female than for male adolescents. Future research on the effects of SEIM may benefit from greater attention to experiential states during SEIM use.",0
https://doi.org/10.1037/a0037744,An information capacity limitation of visual short-term memory.,"Research suggests that visual short-term memory (VSTM) has both an item capacity, of around 4 items, and an information capacity. We characterize the information capacity limits of VSTM using a task in which observers discriminated the orientation of a single probed item in displays consisting of 1, 2, 3, or 4 orthogonally oriented Gabor patch stimuli that were presented in noise for 50 ms, 100 ms, 150 ms, or 200 ms. The observed capacity limitations are well described by a sample-size model, which predicts invariance of ∑(i)(d'(i))² for displays of different sizes and linearity of (d'(i))² for displays of different durations. Performance was the same for simultaneous and sequentially presented displays, which implicates VSTM as the locus of the observed invariance and rules out explanations that ascribe it to divided attention or stimulus encoding. The invariance of ∑(i)(d'(i))² is predicted by the competitive interaction theory of Smith and Sewell (2013), which attributes it to the normalization of VSTM traces strengths arising from competition among stimuli entering VSTM.",0
https://doi.org/10.1177/0962280210391076,A review of causal estimation of effects in mediation analyses,"We describe causal mediation methods for analysing the mechanistic factors through which interventions act on outcomes. A number of different mediation approaches have been presented in the biomedical, social science and statistical literature with an emphasis on different aspects of mediation. We review the different sets of assumptions that allow identification and estimation of effects in the simple case of a single intervention, a temporally subsequent mediator and outcome. These assumptions include various no confounding assumptions including sequential ignorability assumptions and also interaction assumptions involving the treatment and mediator. The understanding of such assumptions is crucial since some can be assessed under certain conditions (e.g. treatment–mediator interactions), whereas others cannot (sequential ignorability). These issues become more complex with multiple mediators and longitudinal outcomes. In addressing these assumptions, we review several causal approaches to mediation analyses.",0
https://doi.org/10.1037/a0017057,"Perceiving others’ personalities: Examining the dimensionality, assumed similarity to the self, and stability of perceiver effects.","In interpersonal perception, ""perceiver effects"" are tendencies of perceivers to see other people in a particular way. Two studies of naturalistic interactions examined perceiver effects for personality traits: seeing a typical other as sympathetic or quarrelsome, responsible or careless, and so forth. Several basic questions were addressed. First, are perceiver effects organized as a global evaluative halo, or do perceptions of different traits vary in distinct ways? Second, does assumed similarity (as evidenced by self-perceiver correlations) reflect broad evaluative consistency or trait-specific content? Third, are perceiver effects a manifestation of stable beliefs about the generalized other, or do they form in specific contexts as group-specific stereotypes? Findings indicated that perceiver effects were better described by a differentiated, multidimensional structure with both trait-specific content and a higher order global evaluation factor. Assumed similarity was at least partially attributable to trait-specific content, not just to broad evaluative similarity between self and others. Perceiver effects were correlated with gender and attachment style, but in newly formed groups, they became more stable over time, suggesting that they grew dynamically as group stereotypes. Implications for the interpretation of perceiver effects and for research on personality assessment and psychopathology are discussed.",0
https://doi.org/10.1080/01621459.1977.10480613,The Efficiency of Cox's Likelihood Function for Censored Data,Abstract D.R. Cox has suggested a simple method for the regression analysis of censored data. We carry out an information calculation which shows that Cox's method has full asymptotic efficiency under conditions which are likely to be satisfied in many realistic situations. The connection of Cox's method with the Kaplan-Meier estimator of a survival curve is made explicit.,0
https://doi.org/10.1016/s0169-7161(05)25020-4,Bayesian Thinking in Spatial Statistics,"In the sections below we review basic motivations for spatial statistical analysis, review three general categories of data structure and associated inferential questions, and describe Bayesian methods for achieving inference. Our goal is to highlight similarities across spatial analytic methods, particularly with regards to how hierarchical probability structures often link approaches developed in one area of spatial analysis to components within other areas. By choosing to highlight similarities, we focus on general concepts in spatial inference, and often defer details of several interesting and current threads of development to the relevant literature. We conclude with a listing of some of these developing areas of interest and references for further reading.",0
https://doi.org/10.1007/s10531-014-0741-3,How much are stranding records affected by variation in reporting rates? A case study of small delphinids in the Bay of Biscay,"Marine vertebrate strandings offer an opportunistic sampling scheme that can provide abundant data over long periods. Because the stranding process involves biological, physical and sociological parameters, confounding complicates the interpretation of results. The statistical analysis of these data relies on generalized linear or additive models in order to infer long-term trends, but does not easily account for drift or variation in reporting rates. Here, we capitalized on county-level (administrative) variation following the passing of a law for compulsory reporting of stranded marine mammals in France to investigate how variation in reporting rates may affect the observed trend in stranded small delphinids in the Bay of Biscay. Using a time-series spanning more than 30 years across eight administrative counties, we built variance partitioning models for the analysis of count data. We discussed the choice of an appropriate likelihood to conclude the Negative Binomial useful and interpretable in the context of small delphinid strandings. We expanded the model with a recent methodology to detect structural breaks in the time series, focusing on overdispersion. We performed statistical robustness checks with respect to variations in reporting rates and discuss their causal interpretation in the context of observational data. Stranding frequencies increased on average 7-fold over 30 years. We conclude that reporting rates to the French stranding network have been stable since the early 1990s, and the average 3-fold increase in stranded small delphinids observed in the Bay of Biscay since 1990 is due to other factors, including bycatch. Codes and data are available to replicate the analysis to other national stranding networks. Ã‚Â© 2014 Springer Science+Business Media Dordrecht.",0
https://doi.org/10.1177/01466216000241003,Likelihood-Based Item-Fit Indices for Dichotomous Item Response Theory Models,"New goodness-of-fit indices are introduced for dichotomous item response theory (IRT) models. These indices are based on the likelihoods of number-correct scores derived from the IRT model, and they provide a direct comparison of the modeled and observed frequencies for correct and incorrect responses for each number-correct score. The behavior of Pearson’s X 2 ( S- X 2 ) and the likelihood ratio G 2 ( S- G 2 ) was assessed in a simulation study and compared with two fit indices similar to those currently in use ( Q1- X 2 and Q 1 - G 2 ). The simulations included three conditions in which the simulating and fitting models were identical and three conditions involving model misspecification. S- X 2 performed well, with Type I error rates close to the expected .05 and .01 levels. Performance of this index improved with increased test length. S- G 2 tended to reject the null hypothesis too often, as did Q 1 - X 2 and Q 1 - G 2 . The power of S- X 2 appeared to be similar for all test lengths, but varied depending on the type of model misspecification.",0
https://doi.org/10.1136/bmj.319.7208.508,Methods in health service research: An introduction to bayesian methods in health technology assessment,"This is the third of four articles  Bayes's theorem arose from a posthumous publication in 1763 by Thomas Bayes, a non-conformist minister from Tunbridge Wells. Although it gives a simple and uncontroversial result in probability theory, specific uses of the theorem have been the subject of considerable controversy for more than two centuries. In recent years a more balanced and pragmatic perspective has emerged, and in this paper we review current thinking on the value of the Bayesian approach to health technology assessment.  A concise definition of bayesian methods in health technology assessment has not been established, but we suggest the following: the explicit quantitative use of external evidence in the design, monitoring, analysis, interpretation, and reporting of a health technology assessment. This approach acknowledges that judgments about the benefits of a new technology will rarely be based solely on the results of a single study but should synthesise evidence from multiple sources—for example, pilot studies, trials of similar interventions, and even subjective judgments about the generalisability of the study's results.  A bayesian perspective leads to an approach to clinical trials that is claimed to be more flexible and ethical than traditional methods,1 and to elegant ways of handling multiple substudies—for example, when simultaneously estimating the effects of a treatment on many subgroups.2 Proponents have also argued that a bayesian approach allows conclusions to be provided in a form that is most suitable for decisions specific to patients and decisions affecting public policy.3  #### Summary points  Bayesian methods interpret data from a study in the light of external evidence and judgment, and the form in which conclusions are drawn contributes naturally to decision making  Prior plausibility of hypotheses is taken into account, just as when interpreting the results of a diagnostic test  Scepticism about large treatment effects can be formally …",0
https://doi.org/10.1080/00273171.2015.1065398,A Comparison of Inverse-Wishart Prior Specifications for Covariance Matrices in Multilevel Autoregressive Models,"Multilevel autoregressive models are especially suited for modeling between-person differences in within-person processes. Fitting these models with Bayesian techniques requires the specification of prior distributions for all parameters. Often it is desirable to specify prior distributions that have negligible effects on the resulting parameter estimates. However, the conjugate prior distribution for covariance matrices-the Inverse-Wishart distribution-tends to be informative when variances are close to zero. This is problematic for multilevel autoregressive models, because autoregressive parameters are usually small for each individual, so that the variance of these parameters will be small. We performed a simulation study to compare the performance of three Inverse-Wishart prior specifications suggested in the literature, when one or more variances for the random effects in the multilevel autoregressive model are small. Our results show that the prior specification that uses plug-in ML estimates of the variances performs best. We advise to always include a sensitivity analysis for the prior specification for covariance matrices of random parameters, especially in autoregressive models, and to include a data-based prior specification in this analysis. We illustrate such an analysis by means of an empirical application on repeated measures data on worrying and positive affect.",0
https://doi.org/10.1080/00273171.2013.816235,A New Procedure to Test Mediation With Missing Data Through Nonparametric Bootstrapping and Multiple Imputation,"This article proposes a new procedure to test mediation with the presence of missing data by combining nonparametric bootstrapping with multiple imputation (MI). This procedure performs MI first and then bootstrapping for each imputed data set. The proposed procedure is more computationally efficient than the procedure that performs bootstrapping first and then MI for each bootstrap sample. The validity of the procedure is evaluated using a simulation study under different sample size, missing data mechanism, missing data proportion, and shape of distribution conditions. The result suggests that the proposed procedure performs comparably to the procedure that combines bootstrapping with full information maximum likelihood under most conditions. However, caution needs to be taken when using this procedure to handle missing not-at-random or nonnormal data.",0
https://doi.org/10.3310/hta16110,Management of frozen shoulder: a systematic review and cost-effectiveness analysis.,"Frozen shoulder is condition in which movement of the shoulder becomes restricted. It can be described as either primary (idiopathic) whereby the aetiology is unknown, or secondary, when it can be attributed to another cause. It is commonly a self-limiting condition, of approximately 1 to 3 years' duration, though incomplete resolution can occur.To evaluate the clinical effectiveness and cost-effectiveness of treatments for primary frozen shoulder, identify the most appropriate intervention by stage of condition and highlight any gaps in the evidence.A systematic review was conducted. Nineteen databases and other sources including the Cumulative Index to Nursing and Allied Health (CINAHL), Science Citation Index, BIOSIS Previews and Database of Abstracts of Reviews of Effects (DARE) were searched up to March 2010 and EMBASE and MEDLINE up to January 2011, without language restrictions. MEDLINE, CINAHL and PsycINFO were searched in June 2010 for studies of patients' views about treatment.Randomised controlled trials (RCTs) evaluating physical therapies, arthrographic distension, steroid injection, sodium hyaluronate injection, manipulation under anaesthesia, capsular release or watchful waiting, alone or in combination were eligible for inclusion. Patients with primary frozen shoulder (with or without diabetes) were included. Quasi-experimental studies were included in the absence of RCTs and case series for manipulation under anaesthesia (MUA) and capsular release only. Full economic evaluations meeting the intervention and population inclusion criteria of the clinical review were included. Two researchers independently screened studies for relevance based on the inclusion criteria. One reviewer extracted data and assessed study quality; this was checked by a second reviewer. The main outcomes of interest were pain, range of movement, function and disability, quality of life and adverse events. The analysis comprised a narrative synthesis and pair-wise meta-analysis. A mixed-treatment comparison (MTC) was also undertaken. An economic decision model was intended, but was found to be implausible because of a lack of available evidence. Resource use was estimated from clinical advisors and combined with quality-adjusted life-years obtained through mapping to present tentative cost-effectiveness results.Thirty-one clinical effectiveness studies and one economic evaluation were included. The clinical effectiveness studies evaluated steroid injection, sodium hyaluronate, supervised neglect, physical therapy (mainly physiotherapy), acupuncture, MUA, distension and capsular release. Many of the studies identified were at high risk of bias. Because of variation in the interventions and comparators few studies could be pooled in a meta-analysis. Based on single RCTs, and for some outcomes only, short-wave diathermy may be more effective than home exercise. High-grade mobilisation may be more effective than low-grade mobilisation in a population in which most patients have already had treatment. Data from two RCTs showed that there may be benefit from adding a single intra-articular steroid injection to home exercise in patients with frozen shoulder of < 6 months' duration. The same two trials showed that there may be benefit from adding physiotherapy (including mobilisation) to a single steroid injection. Based on a network of nine studies the MTC found that steroid combined with physiotherapy was the only treatment showing a statistically and clinically significant beneficial treatment effect compared with placebo for short-term pain (standardised mean difference -1.58, 95% credible interval -2.96 to -0.42). This analysis was based on only a subset of the evidence, which may explain why the findings are only partly supportive of the main analysis. No studies of patients' views about the treatments were identified. Average costs ranged from £36.16 for unguided steroid injections to £2204 for capsular release. The findings of the mapping suggest a positive relationship between outcome and European Quality of Life-5 Dimensions (EQ-5D) score: a decreasing visual analogue scale score (less pain) was accompanied by an increasing (better) EQ-5D score. The one published economic evaluation suggested that low-grade mobilisation may be more cost-effective than high-grade mobilisation. Our tentative cost-effectiveness analysis suggested that steroid alone may be more cost-effective than steroid plus physiotherapy or physiotherapy alone. These results are very uncertain.The key limitation was the lack of data available. It was not possible to undertake the planned synthesis exploring the influence of stage of frozen shoulder or the presence of diabetes on treatment effect. As a result of study diversity and poor reporting of outcome data there were few instances where the planned quantitative synthesis was possible or appropriate. Most of the included studies had a small number of participants and may have been underpowered. The lack of available data made the development of a decision-analytic model implausible. We found little evidence on treatment related to stage of condition, treatment pathways, the impact on quality of life, associated resource use and no information on utilities. Without making a number of questionable assumptions modelling was not possible.There was limited clinical evidence on the effectiveness of treatments for primary frozen shoulder. The economic evidence was so limited that no conclusions can be made about the cost-effectiveness of the different treatments. High-quality primary research is required.",0
https://doi.org/10.1007/s00417-006-0474-4,The Freiburg Visual Acuity Test-Variability unchanged by post-hoc re-analysis,"Background: The Freiburg Visual Acuity and Contrast Test (FrACT) has been further developed; it is now available for Macintosh and Windows free of charge at http://www.michaelbach.de/fract.html. The present study sought to reduce the test-retest variability of visual acuity on short runs (18 trials) by post-hoc re-analysis. Methods: The FrACT employs advanced computer graphics to present Landolt Cs over the full range of visual acuity. The sequence of optotypes presented follows an adaptive staircase procedure, the Best-PEST algorithm. The Best-PEST threshold obtained after 18 trials was compared to the result of a post-hoc re-analysis of the acquired data, where both threshold and slope of the psychometric function were estimated via a maximum-likelihood fit. Results: Testing time was 1.7 min per run on average. Test-retest reproducibility was Â±2 lines (or Â±0.2 logMAR) for a 95% confidence band (using 18 optotype presentations per test run). Post-hoc psychometric fitting reproduced the Best-PEST result within 1%, although the individual slopes varied widely; test-retest reproducibility was not improved. Conclusions: The FrACT offers advantages over traditional chart testing with respect to objectivity and reliability. The similarity between the results of the Best-PEST vs. post-hoc analysis, fitting both slope and threshold, suggest that there is no disadvantage to the constant slope assumed by Best PEST. Furthermore, since variability was not reduced by post-hoc analysis, for high reliability more trials should be employed than the 18 trials per run used here. Â© Springer-Verlag 2007.",0
https://doi.org/10.1016/0042-6989(79)90136-6,Probability summation over time,"Abstract Frequency-of-seeing and sensitivity-duration curves were collected for temporal signals of limited spectral extent. A comparison of the two sorts of data suggests that a stimulus is detected whenever the excursions of its linearly filtered, noise-perturbed temporal waveform exceed some fixed magnitude.",0
https://doi.org/10.3758/bf03195574,"QMPE: Estimating Lognormal, Wald, and Weibull RT distributions with a parameter-dependent lower bound","We describe and test quantile maximum probability estimator (QMPE), an open-source ANSI Fortran 90 program for response time distribution estimation. QMPE enables users to estimate parameters for the ex-Gaussian and Gumbel (1958) distributions, along with three ""shifted"" distributions (i.e., distributions with a parameter-dependent lower bound): the Lognormal, Wald, and Weibul distributions. Estimation can be performed using either the standard continuous maximum likelihood (CML) method or quantile maximum probability (QMP; Heathcote & Brown, in press). We review the properties of each distribution and the theoretical evidence showing that CML estimates fail for some cases with shifted distributions, whereas QMP estimates do not. In cases in which CML does not fail, a Monte Carlo investigation showed that QMP estimates were usually as good, and in some cases better, than CML estimates. However, the Monte Carlo study also uncovered problems that can occur with both CML and QMP estimates, particularly when samples are small and skew is low, highlighting the difficulties of estimating distributions with parameter-dependent lower bounds.",0
https://doi.org/10.3758/bf03194544,"The psychometric function: I. Fitting, sampling, and goodness of fit","The psychometric function relates an observer's performance to an independent variable, usually some physical quantity of a stimulus in a psychophysical task. This paper, together with its companion paper (Wichmann & Hill, 2001), describes an integrated approach to (1) fitting psychometric functions, (2) assessing the goodness of fit, and (3) providing confidence intervals for the function's parameters and other estimates derived from them, for the purposes of hypothesis testing. The present paper deals with the first two topics, describing a constrained maximum-likelihood method of parameter estimation and developing several goodness-of-fit tests. Using Monte Carlo simulations, we deal with two specific difficulties that arise when fitting functions to psychophysical data. First, we note that human observers are prone to stimulus-independent errors (or lapses). We show that failure to account for this can lead to serious biases in estimates of the psychometric function's parameters and illustrate how the problem may be overcome. Second, we note that psychophysical data sets are usually rather small by the standards required by most of the commonly applied statistical tests. We demonstrate the potential errors of applying traditional chi2 methods to psychophysical data and advocate use of Monte Carlo resampling techniques that do not rely on asymptotic theory. We have made available the software to implement our methods.",0
https://doi.org/10.1177/0049124110371312,Analysis of a Two-Level Structural Equation Model With Missing Data,"Structural equation models are widely used to model relationships among latent unobservable constructs and observable variables. In some studies, the data set used for analysis is comprised of observations that are drawn from a known hierarchical structure and involves missing entries. A two-level structural equation model can be used to analyze such data sets. Direct maximum likelihood methods for analyzing two-level structural equation models are available in software, such as LISREL and Mplus. These software programs also have options to handle missing observations. The authors develop an alternative procedure that uses an expectation maximization (EM) type algorithm. Using appropriate approximations, the procedure can be implemented using simple statistical software in combination with a basic structural equation modeling program. The authors address the implementation of the procedure in detail and provide syntax codes in R, which is available in the public domain, to implement the proposed procedure. The discussion of the procedure is made with reference to the analysis of a data set that studies job characteristic variables. The authors also use simulation studies to examine the performance of the proposed procedure. The results indicate that the proposed method, which is easily accessible to users, represents a reliable alternative for analyzing two-level structural equation models with missing data.",0
https://doi.org/10.1111/j.2517-6161.1953.tb00135.x,New Light on the Correlation Coefficient and its Transforms,,0
https://doi.org/10.1007/bf02294746,Latent curve analysis,"As a method for representing development, latent trait theory is presented in terms of a statistical model containing individual parameters and a structure on both the first and second moments of the random variables reflecting growth. Maximum likelihood parameter estimates and associated asymptotic tests follow directly. These procedures may be viewed as an alternative to standard repeated measures ANOVA and to first-order auto-regressive methods. As formulated, the model encompasses cohort sequential designs and allow for period or practice effects. A numerical illustration using data initially collected by Nesselroade and Baltes is presented. Â© 1990 The Psychometric Society.",0
,Market Segmentation: Conceptual and Methodological Foundations,Part 1: Introduction. 1. The Historical Development of the Market Segmentation Concept. 2. Segmentation Bases. 3. Segmentation Methods. 4. Tools for Market Segmentation. Part 2: Segmentation Methodology. 5. Clustering Methods. 6. Mixture Models. 7. Mixture Regression Models. 8. Mixture Unfolding Models. 9. Profiling Segments. 10. Dynamic Segmentation. Part 3: Special Topics in Market Segmentation. 11. Joint Segmentation. 12. Market Segmentation with Tailored Interviewing. 13. Model-Based Segmentation Using Structural Equation Models. 14. Segmentation Based on Product Dissimilarity Judgements. Part 4: Applied Market Segmentation. 15. General Observable Bases: Geo-demographics. 16. General Unobservable Bases: Values and Lifestyles. 17. Product-specific observable Bases: Response-based Segmentation. 18. Product-Specific Unobservable Bases: Conjoint Analysis. Part 5: Conclusions and Directions for Future Research. 19. Conclusions: Representations of Heterogeneity. 20. Directions for Future Research. References. Index.,0
https://doi.org/10.1002/jrsm.1153,A Bayesian missing data framework for generalized multiple outcome mixed treatment comparisons,"Bayesian statistical approaches to mixed treatment comparisons (MTCs) are becoming more popular because of their flexibility and interpretability. Many randomized clinical trials report multiple outcomes with possible inherent correlations. Moreover, MTC data are typically sparse (although richer than standard meta-analysis, comparing only two treatments), and researchers often choose study arms based upon which treatments emerge as superior in previous trials. In this paper, we summarize existing hierarchical Bayesian methods for MTCs with a single outcome and introduce novel Bayesian approaches for multiple outcomes simultaneously, rather than in separate MTC analyses. We do this by incorporating partially observed data and its correlation structure between outcomes through contrast-based and arm-based parameterizations that consider any unobserved treatment arms as missing data to be imputed. We also extend the model to apply to all types of generalized linear model outcomes, such as count or continuous responses. We offer a simulation study under various missingness mechanisms (e.g., missing completely at random, missing at random, and missing not at random) providing evidence that our models outperform existing models in terms of bias, mean squared error, and coverage probability then illustrate our methods with a real MTC dataset. We close with a discussion of our results, several contentious issues in MTC analysis, and a few avenues for future methodological development.",0
https://doi.org/10.3758/bf03196751,Modeling individual differences in cognition,"Many evaluations of cognitive models rely on data that have been averaged or aggregated across all experimental subjects, and so fail to consider the possibility of important individual differences between subjects. Other evaluations are done at the single-subject level, and so fail to benefit from the reduction of noise that data averaging or aggregation potentially provides. To overcome these weaknesses, we have developed a general approach to modeling individual differences using families of cognitive models in which different groups of subjects are identified as having different psychological behavior. Separate models with separate parameterizations are applied to each group of subjects, and Bayesian model selection is used to determine the appropriate number of groups. We evaluate this individual differences approach in a simulation study and show that it is superior in terms of the key modeling goals of prediction and understanding. We also provide two practical demonstrations of the approach, one using the ALCOVE model of category learning with data from four previously analyzed category learning experiments, the other using multidimensional scaling representational models with previously analyzed similarity data for colors. In both demonstrations, meaningful individual differences are found and the psychological models are able to account for this variation through interpretable differences in parameterization. The results highlight the potential of extending cognitive models to consider individual differences.",0
https://doi.org/10.1002/cjs.11245,Pseudo-empirical Bayes estimation of small area means based on James-Stein estimation in linear regression models with functional measurement error,en,0
,History and rationale of longitudinal research,,0
https://doi.org/10.1037//1082-989x.7.1.83,A comparison of methods to test mediation and other intervening variable effects.,A Monte Carlo study compared 14 methods to test the statistical significance of the intervening variable effect. An intervening variable (mediator) transmits the effect of an independent variable to a dependent variable. The commonly used R. M. Baron and D. A. Kenny (1986) approach has low statistical power. Two methods based on the distribution of the product and 2 difference-in-coefficients methods have the most accurate Type I error rates and greatest statistical power except in 1 important case in which Type I error rates are too high. The best balance of Type I error and statistical power across all cases is the test of the joint significance of the two effects comprising the intervening variable effect.,0
https://doi.org/10.1177/001316448004000213,Maximally Reliable Composites for Unidimensional Measures,"A general approach to obtaining weights and reliability coefficients of maximally reliable composites is offered for a variety of test theoretic models which assume a single common underlying dimension. The reliability maximizing weights are related to the theoretically specified true score scaling weights to show there is a constant relationship between them that is invariant under separate linear transformations on each variable in the system. Furthermore, an argument is made that test theoretic relations should be derived for the most general model available and not for unnecessarily constrained models.",0
https://doi.org/10.1037/h0040633,A theoretico-historical review of the threshold concept.,"This paper traces the concept of threshold from its classical beginnings and shows the relation of the concept to selected issues in contemporary psychology. Emphasis is placed on 3 main problems: the designation of the origin point on a psychological continuum; the interpretation of the sensory threshold as an intervening variable and the issue of sensory continuity-noncontinuity; and the specification of the response threshold as a dependent variable of behavior, rather than an index of organismic sensitivity. Reference is made to adaptation level theory and the theory of signal detection as possible approaches to the development of a complete psychophysics which does not start from the concept of threshold. (74 ref.) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1963 American Psychological Association.",0
https://doi.org/10.1177/01421602026002007,An EM Approach to Parameter Estimation for the Zinnes and Griggs Paired Comparison IRT Model,"Borman et al. recently proposed a computer adaptive performance appraisal system called CARS II that utilizes paired comparison judgments of behavioral stimuli. To implement this approach,the paired comparison ideal point model developed by Zinnes and Griggs was selected. In this article,the authors describe item response and information functions for the Zinnes and Griggs model and present procedures for estimating stimulus and person parameters. Monte carlo simulations were conducted to assess the accuracy of the parameter estimation procedures. The results indicated that at least 400 ratees (i.e.,ratings) are required to obtain reasonably accurate estimates of the stimulus parameters and their standard errors. In addition,latent trait estimation improves as test length increases. The implications of these results for test construction are also discussed.",0
https://doi.org/10.1177/0193841x9301700202,Estimating Mediated Effects in Prevention Studies,The purpose of this article is to describe statistical procedures to assess how prevention and intervention programs achieve their effects. The analyses require the measurement of intervening or mediating variables hypothesized to represent the causal mechanism by which the prevention program achieves its effects. Methods to estimate mediation are illustrated in the evaluation of a health promotion program designed to reduce dietary cholesterol and a school-based drug prevention program. The methods are relatively easy to apply and the information gained from such analyses should add to our understanding of prevention.,0
https://doi.org/10.1027/1614-2241/a000162,Pushing the Limits,"Abstract. Longitudinal developmental research is often focused on patterns of change or growth across different (sub)groups of individuals. Particular to some research contexts, developmental inquiries may involve one or more (sub)groups that are small in nature and therefore difficult to properly capture through statistical analysis. The current study explores the lower-bound limits of subsample sizes in a multiple group latent growth modeling by means of a simulation study. We particularly focus on how the maximum likelihood (ML) and Bayesian estimation approaches differ when (sub)sample sizes are small. The results show that Bayesian estimation resolves computational issues that occur with ML estimation and that the addition of prior information can be the key to detect a difference between groups when sample and effect sizes are expected to be limited. The acquisition of prior information with respect to the smaller group is especially influential in this context.",1
https://doi.org/10.1080/10503309112331335511,Meta‐Analysis of Therapist Effects in Psychotherapy Outcome Studies,"In a meta-analysis, we examined factors that could account for the differences in therapist efficacy evidenced in psychotherapy outcome studies. The factors investigated were: (1) the use of a treatment manual, (2) the average level of therapist experience, (3) the length of treatment, and (4) the type of treatment (cognitive/behavioral versus psychodynamic). Data were obtained from fifteen psychotherapy outcome studies that produced 27 separate treatment groups. For each treatment group, the amount of outcome variance due to differences between therapists was calculated and served as the dependent variable for the meta-analysis. Each separate treatment group was coded on the above four variables, and multiple regression analyses related the independent variables to the size of therapist effects. Results indicated that the use of a treatment manual and more experienced therapists were associated with small differences between therapists, whereas more inexperienced therapists and no treatment manual were a...",0
https://doi.org/10.3390/su71215810,Firm Sustainability Performance Index Modeling,"The main objective of this paper is to bring a model for firm sustainability performance index by applying both classical and Bayesian structural equation modeling (parametric and semi-parametric modeling). Both techniques are considered to the research data collected based on a survey directed to the China, Taiwan, and Malaysia food manufacturing industry. For estimating firm sustainability performance index we consider three main indicators include knowledge management, organizational learning, and business strategy. Based on the both Bayesian and classical methodology, we confirmed that knowledge management and business strategy have significant impact on firm sustainability performance index.",0
https://doi.org/10.1111/j.1475-6811.2009.01211.x,Attachment avoidance and commitment aversion: A script for relationship failure,"Do avoidantly attached individuals, with fears of closeness and dependency, expect their relationships to fail? Moreover, do they expect specific events that will lead to failure? Canadian students outlined their expectations for a new romantic relationship by ordering a series of dating events using a card sorting procedure. Avoidance was associated with both expectations of relationship failure and commitment aversion (the absence of positive commitment-enhancing events and presence of negative commitment-undermining events). Commitment aversion mediated the relationship between avoidance and expected failure, and a multiple mediation model showed unique paths for positive and negative events. This suggests that avoidantly attached individuals enter into new relationships with detailed scripts for commitment aversion that lead them to expect relationship failure.",0
https://doi.org/10.1016/j.jmva.2012.05.010,A criterion-based model comparison statistic for structural equation models with heterogeneous data,"Heterogeneous data are common in social, educational, medical and behavioral sciences. Recently, finite mixture structural equation models (SEMs) and two-level SEMs have been respectively proposed to analyze different kinds of heterogeneous data. Due to the complexity of these two kinds of SEMs, model comparison is difficult. For instance, the computational burden in evaluating the Bayes factor is heavy, and the Deviance Information Criterion may not be appropriate for mixture SEMs. In this paper, a Bayesian criterion-based method called the L v measure, which involves a component related to the variability of the prediction and a component related to the discrepancy between the data and the prediction, is proposed. Moreover, the calibration distribution is introduced for formal comparison of competing models. Two simulation studies, and two applications based on real data sets are presented to illustrate the satisfactory performance of the L v measure in model comparison.",0
https://doi.org/10.1523/jneurosci.23-03-01026.2003,Spinal Glia and Proinflammatory Cytokines Mediate Mirror-Image Neuropathic Pain in Rats,"Mirror-image allodynia is a mysterious phenomenon that occurs in association with many clinical pain syndromes. Allodynia refers to pain in response to light touch/pressure stimuli, which normally are perceived as innocuous. Mirror-image allodynia arises from the healthy body region contralateral to the actual site of trauma/inflammation. Virtually nothing is known about the mechanisms underlying such pain. A recently developed animal model of inflammatory neuropathy reliably produces mirror-image allodynia, thus allowing this pain phenomenon to be analyzed. In this sciatic inflammatory neuropathy (SIN) model, decreased response threshold to tactile stimuli (mechanical allodynia) develops in rats after microinjection of immune activators around one healthy sciatic nerve at mid-thigh level. Low level immune activation produces unilateral allodynia ipsilateral to the site of sciatic inflammation; more intense immune activation produces bilateral (ipsilateral + mirror image) allodynia. The present studies demonstrate that both ipsilateral and mirror-image SIN-induced allodynias are (1) reversed by intrathecal (peri-spinal) delivery of fluorocitrate, a glial metabolic inhibitor; (2) prevented and reversed by intrathecal CNI-1493, an inhibitor of p38 mitogen-activated kinases implicated in proinflammatory cytokine production and signaling; and (3) prevented or reversed by intrathecal proinflammatory cytokine antagonists specific for interleukin-1, tumor necrosis factor, or interleukin-6. Reversal of ipsilateral and mirror-image allodynias was rapid and complete even when SIN was maintained constantly for 2 weeks before proinflammatory cytokine antagonist administration. These results provide the first evidence that ipsilateral and mirror-image inflammatory neuropathy pain are created both acutely and chronically through glial and proinflammatory cytokine actions.",0
https://doi.org/10.1016/j.cmpb.2006.02.006,Using SAS to conduct nonparametric residual bootstrap multilevel modeling with a small number of groups,"In multilevel modeling, researchers often encounter data with a relatively small number of units at the higher levels. As a result, of this and/or non-normality of the residuals, model parameter estimates, particularly the variance components and standard errors of parameter estimates at the group level, may be biased, thus the corresponding statistical inferences may not be trustworthy. This problem can be addressed by using bootstrap methods to estimate the standard errors of the parameter estimates for significance testing. This study illustrates how to use statistical analysis system (SAS) to conduct nonparametric residual bootstrap multilevel modeling. Specific SAS programs for such modeling are provided.",0
https://doi.org/10.1214/aoms/1177732541,On the Frequency Function of $xy$,"Given the distribution function of x and y, what can be said of the distribution of the product xy? The author has had two inquiries during the last two years, one from an investigator in business statistics and the other from a psychologist, concerning the probable error of the product of two quantities, each of known probable error. There seems to be very little in the literature of mathematical statistics on this question. If x and y are independent and are each distributed according to the same normal frequency law, it is well known that the distribution function of",0
https://doi.org/10.1212/wnl.37.6.963,A controlled study of progabide in partial seizures: Methodology and results,"The results of a multicenter, double-blind, placebo-controlled clinical trial of the efficacy and safety of progabide (PGB) in the treatment of partial seizures are presented. This study was performed with a number of rigorous controls not usually present in clinical trials. These included uniform co-medication in which all patients received only phenytoin and carbamazepine; concentrations of these two drugs were maintained within narrow, predefined concentration ranges. There was no statistically significant difference between PGB and placebo in seizure frequency and seizure duration for most of the analyses performed. One patient was withdrawn from the study because of hepatotoxicity. PGB was associated with a significant inhibition of phenytoin but not carbamazepine clearance. The results of this study indicate that PGB was not a potent antiepileptic drug in this population of persons with intractable epilepsy.",0
https://doi.org/10.1002/bimj.201100055,Hierarchical Bayesian modeling of heterogeneous cluster- and subject-level associations between continuous and binary outcomes in dairy production,"The augmentation of categorical outcomes with underlying Gaussian variables in bivariate generalized mixed effects models has facilitated the joint modeling of continuous and binary response variables. These models typically assume that random effects and residual effects (co)variances are homogeneous across all clusters and subjects, respectively. Motivated by conflicting evidence about the association between performance outcomes in dairy production systems, we consider the situation where these (co)variance parameters may themselves be functions of systematic and/or random effects. We present a hierarchical Bayesian extension of bivariate generalized linear models whereby functions of the (co)variance matrices are specified as linear combinations of fixed and random effects following a square-root-free Cholesky reparameterization that ensures necessary positive semidefinite constraints. We test the proposed model by simulation and apply it to the analysis of a dairy cattle data set in which the random herd-level and residual cow-level effects (co)variances between a continuous production trait and binary reproduction trait are modeled as functions of fixed management effects and random cluster effects.",0
https://doi.org/10.1002/9780470609927,"Survey Methods in Multinational, Multiregional, and Multicultural Contexts",This book provides up-to-date insight into key aspects of methodological research for comparative surveys. It discusses methodological considerations for surveys that are deliberately designed for ...,0
https://doi.org/10.1016/0022-2496(75)90053-x,The relative judgment theory of two choice response time,Choice reaction times and response probabilities are analyzed in terms of a random walk decision process. The theory of relative judgment which gives rise to this analysis assumes that a presented stimulus is compared by the subject to an internal psychological standard or referent. Discrepancies between the stimulus and the referent are accumulated until one or the other of two response thresholds is exceeded. Results from three experiments are examined in detail. In each experiment a particular feature of the theory is tested and shown to be supported by the data. Methods of estimating the two parameters of the theory are given and an appendix presents a summary of theoretical results.,0
https://doi.org/10.1016/j.jretconser.2011.09.003,"Message content in keyword campaigns, click behavior, and price-consciousness: A study of millennial consumers","Abstract Building upon the expectancy theory, this study suggests that message content in keyword advertising influences click behavior. The moderating role of price consciousness is also examined. An online experiment using an ex-post filter sampling method is implemented (final n=165). The results of binary logistic regressions indicate that descriptive message content is more clicked than commercial. Price-consciousness appears to moderate the relationship, with high price-conscious consumers being more influenced by descriptive content than less price-conscious consumers. The obtained results are meaningful and significant for the millennial consumers under study.",0
https://doi.org/10.1109/tnsre.2003.816874,Pattern identification as a function of stimulation on a fingertip-scanned electrotactile display,"Two studies were conducted to determine the effect of stimulation current on pattern perception on a 49-point fingertip-scanned electrotactile (electrocutaneous) display. Performance increased monotonically from near chance levels at the lowest sub-threshold current levels tested to approximately 90% at the highest comfortable current levels. This suggests the existence of a tradeoff between spatial performance and usable ""gray scale"" range in electrotactile presentation of graphical information.",0
https://doi.org/10.1007/bf02294660,Structural equation models with continuous and polytomous variables,"A two-stage procedure is developed for analyzing structural equation models with continuous and polytomous variables. At the first stage, the maximum likelihood estimates of the thresholds, polychoric covariances and variances, and polyserial covariances are simultaneously obtained with the help of an appropriate transformation that significantly simplifies the computation. An asymptotic covariance matrix of the estiates is also computed. At the second stage, the parameters in the structural covariance model are obtained via the generalized least squares approach. Basic statistical properties of the estimates are derived and some illustrative examples and a small simulation study are reported. Ã‚Â© 1992 The Psychometric Society.",0
https://doi.org/10.1348/000711003321645331,A method of moments technique for fitting interaction effects in structural equation models,"The desire to fit structural equation models containing an interaction term has received much methodological attention in the social science literature. This paper presents a technique for the cross-product structural model that utilizes factor score estimates and results in closed-form moments-type estimators. The technique, which does not require normality for the underlying factors, was originally introduced in a very general form by Wall and Amemiya (2000) for any polynomial structural model. In this paper, the practical implementation of this method, including standard error estimation, is presented specifically for the cross-product model. The procedure is applied to an example from social/behavioural epidemiology where the flexibility of the cross-product model provides a useful description of the underlying theory. A simulation study is also presented comparing the method of moments for the cross-product model with three other procedures.",0
https://doi.org/10.1155/2009/537139,The Rise of Markov Chain Monte Carlo Estimation for Psychometric Modeling,"Markov chain Monte Carlo (MCMC) estimation strategies represent a powerful approach to estimation in psychometric models. Popular MCMC samplers and their alignment with Bayesian approaches to modeling are discussed. Key historical and current developments of MCMC are surveyed, emphasizing how MCMC allows the researcher to overcome the limitations of other estimation paradigms, facilitates the estimation of models that might otherwise be intractable, and frees the researcher from certain possible misconceptions about the models.",0
https://doi.org/10.1037/a0035904,Determining the impact of prenatal tobacco exposure on self-regulation at 6 months.,"Our goal in the present study was to examine the effects of maternal smoking during pregnancy on infant self-regulation, exploring birth weight as a mediator and sex as a moderator of risk. A prospective sample of 218 infants was assessed at 6 months of age. Infants completed a battery of tasks assessing working memory/inhibition, attention, and emotional reactivity and regulation. Propensity scores were used to statistically control for confounding risk factors associated with maternal smoking during pregnancy. After prenatal and postnatal confounds were controlled, prenatal tobacco exposure was related to reactivity to frustration and control of attention during stimulus encoding. Birth weight did not mediate the effect of prenatal exposure but was independently related to reactivity and working memory/inhibition. The effect of tobacco exposure was not moderated by sex.",0
https://doi.org/10.1017/s0003055411000414,Unpacking the Black Box of Causality: Learning about Causal Mechanisms from Experimental and Observational Studies,"Identifying causal mechanisms is a fundamental goal of social science. Researchers seek to study not only whether one variable affects another but also how such a causal relationship arises. Yet commonly used statistical methods for identifying causal mechanisms rely upon untestable assumptions and are often inappropriate even under those assumptions. Randomizing treatment and intermediate variables is also insufficient. Despite these difficulties, the study of causal mechanisms is too important to abandon. We make three contributions to improve research on causal mechanisms. First, we present a minimum set of assumptions required under standard designs of experimental and observational studies and develop a general algorithm for estimating causal mediation effects. Second, we provide a method for assessing the sensitivity of conclusions to potential violations of a key assumption. Third, we offer alternative research designs for identifying causal mechanisms under weaker assumptions. The proposed approach is illustrated using media framing experiments and incumbency advantage studies.",0
https://doi.org/10.1016/b978-044452044-9/50018-5,Nonlinear Structural Equation Modeling as a Statistical Method,"Structural equation analysis allows exploring and modeling relationships among latent variables. The most traditional analysis deals only with linear relationships. However, in applied problems, relevant research questions can be associated with some form of nonlinear relations. Also, from a statistical modeling or data analysis point of view, capabilities to address unrestricted nonlinear relations would be welcome. A review is given for the existing approaches that have been developed and designed for specific nonlinear models. Then, a statistical formulation of general nonlinear structural equation analysis is introduced, and a general model fitting procedure applicable under weak assumptions on latent variable distributions is developed. An example with a nonpolynomial nonlinear structural model is discussed using the new method.",0
https://doi.org/10.1080/01621459.1982.10477856,The Well-Calibrated Bayesian,"Abstract Suppose that a forecaster sequentially assigns probabilities to events. He is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent. We prove a theorem to the effect that a coherent Bayesian expects to be well calibrated, and consider its destructive implications for the theory of coherence.",0
https://doi.org/10.1146/annurev.soc.25.1.659,THE ESTIMATION OF CAUSAL EFFECTS FROM OBSERVATIONAL DATA,"▪ Abstract When experimental designs are infeasible, researchers must resort to the use of observational data from surveys, censuses, and administrative records. Because assignment to the independent variables of observational data is usually nonrandom, the challenge of estimating causal effects with observational data can be formidable. In this chapter, we review the large literature produced primarily by statisticians and econometricians in the past two decades on the estimation of causal effects from observational data. We first review the now widely accepted counterfactual framework for the modeling of causal effects. After examining estimators, both old and new, that can be used to estimate causal effects from cross-sectional data, we present estimators that exploit the additional information furnished by longitudinal data. Because of the size and technical nature of the literature, we cannot offer a fully detailed and comprehensive presentation. Instead, we present only the main features of methods that are accessible and potentially of use to quantitatively oriented sociologists.",0
,Estimation of a covariance matrix,,0
https://doi.org/10.1097/ajp.0b013e3182201983,Effect of Tapentadol Extended Release on Productivity,"To compare the effects of tapentadol-extended release versus oxycodone-controlled release for pain relief on productivity by combining evidence from different sources.Multiparameter evidence synthesis. Three sources were used. The first consisted of 3 randomized double-blind controlled trials that evaluated the efficacy and safety of tapentadol and oxycodone for the management of chronic pain. The second was published data on the incidence of constipation in patients exposed to opioids, and the third was a published survey that evaluated the effect of opioid-induced constipation on productivity. In the trials, a patient was classified as constipated if constipation was reported at any time during the 15 weeks of double-blinded assessment after randomization. In the survey, the effect of constipation on productivity was measured using the Work Productivity and Activity Impairment Questionnaire. All analyses were performed using Bayesian Markov chain Monte Carlo simulations in WinBUGS.The odds of developing constipation were 60% lower with tapentadol than with oxycodone (odds ratio=0.40, 95% credible interval, 0.32-0.50). Tapentadol was associated with less time missed from work, less impairment while working, and a lower overall loss in work productivity compared with oxycodone. The gain in overall work productivity with tapentadol was 1.92% compared with oxycodone (95% credible interval, 1.32-2.59), which translates to a gain of almost 1 hour per week worked.Tapentadol was associated with increases in all productivity dimensions compared with oxycodone. Multiparameter evidence synthesis capitalizes on available evidence, so that better informed medical decisions can be made.",0
https://doi.org/10.1186/s40337-014-0028-9,Optimizing prediction of binge eating episodes: a comparison approach to test alternative conceptualizations of the affect regulation model,"Although a wealth of studies have tested the link between negative mood states and likelihood of a subsequent binge eating episode, the assumption that this relationship follows a typical linear dose-response pattern (i.e., that risk of a binge episode increases in proportion to level of negative mood) has not been challenged. The present study demonstrates the applicability of an alternative, non-linear conceptualization of this relationship, in which the strength of association between negative mood and probability of a binge episode increases above a threshold value for the mood variable relative to the slope below this threshold value (threshold dose response model).A sample of 93 women aged 18 to 40 completed an online survey at random intervals seven times per day for a period of one week. Participants self-reported their current mood state and whether they had recently engaged in an eating episode symptomatic of a binge.As hypothesized, the threshold approach was a better predictor than the linear dose-response modeling of likelihood of a binge episode. The superiority of the threshold approach was found even at low levels of negative mood (3 out of 10, with higher scores reflecting more negative mood). Additionally, severity of negative mood beyond this threshold value appears to be useful for predicting time to onset of a binge episode.Present findings suggest that simple dose-response formulations for the association between negative mood and onset of binge episodes miss vital aspects of this relationship. Most notably, the impact of mood on binge eating appears to depend on whether a threshold value of negative mood has been breached, and elevation in mood beyond this point may be useful for clinicians and researchers to identify time to onset.",0
,Family community ethnic identity and the use of formal health care services in Guatemala.,In this paper we investigate family choices about type of prenatal care and assistance at delivery (childbirth) and the use of childhood immunizations in Guatemala during the early and mid-1980s. Our objective is to investigate the reasons underlying relatively low use of formal health care services in Guatemala particularly among the large indigenous population. (EXCERPT),0
https://doi.org/10.1198/1085711032291,A Bayesian hierarchical model for risk assessment of methylmercury,"This article uses a Bayesian hierarchical model to quantify the adverse health effects associated with in-utero exposure to methylmercury. By allowing for study-to-study as well as outcome-to-outcome variability, the approach provides a useful meta-analytic tool for multi-outcome, multi-study environmental risk assessments. The analysis presented here expands on the findings of a National Academy of Sciences (NAS) committee, charged with advising the United States Environmental Protection Agency (EPA) on an appropriate approach to conducting a risk assessment for methylmercury. The NAS committee, for which the senior author (Ryan) was a committee member, reviewed the findings from several conflicting studies and reported the results from a Bayesian hierarchical model that synthesized information across several studies and for several outcomes. Although the NAS committee did not suggest that the hierarchical model be used as the actual basis for a methylmercury risk assessment, the results from the model were used to justify and support the final recommendation that the risk analysis be based on data from a study conducted in the Faroe Islands, which had found an association between in-utero exposure to methylmercury and impaired neurological development. We consider a variety of statistical issues, but particularly sensitivity to model specification. Ã‚Â© 2003 American Statistical Association and the International Biometric Society.",0
https://doi.org/10.1177/1525822x13520280,Bayesian Cultural Consensus Theory,"In this article, we present a Bayesian inference framework for cultural consensus theory (CCT) models for dichotomous (True/False) response data and provide an associated, user-friendly software package along with a detailed user’s guide to carry out the inference. We believe that the time is ripe for Bayesian statistical inference to become the default choice in the field of CCT. Unfortunately, a lack of publications presenting a practical description of the Bayesian framework in the context of CCT models as well as a dearth of accessible software to apply Bayesian inference to CCT data has so far prevented this from happening. We introduce the Bayesian treatment of several CCT models, focusing on the various merits of Bayesian parameter estimation and interpretation of results, and also introduce the Bayesian Cultural Consensus Toolbox software package.",0
https://doi.org/10.1111/1467-9868.00236,Bayesian latent variable models for clustered mixed outcomes,"A general framework is proposed for modelling clustered mixed outcomes. A mixture of generalized linear models is used to describe the joint distribution of a set of underlying variables, and an arbitrary function relates the underlying variables to be observed outcomes. The model accommodates multilevel data structures, general covariate effects and distinct link functions and error distributions for each underlying variable. Within the framework proposed, novel models are developed for clustered multiple binary, unordered categorical and joint discrete and continuous outcomes. A Markov chain Monte Carlo sampling algorithm is described for estimating the posterior distributions of the parameters and latent variables. Because of the flexibility of the modelling framework and estimation procedure, extensions to ordered categorical outcomes and more complex data structures are straightforward. The methods are illustrated by using data from a reproductive toxicity study.",0
https://doi.org/10.1007/s11336-010-9172-6,Hierarchical Bayes Models for Response Time Data,"Human response time (RT) data are widely used in experimental psychology to evaluate theories of mental processing. Typically, the data constitute the times taken by a subject to react to a succession of stimuli under varying experimental conditions. Because of the sequential nature of the experiments there are trends (due to learning, fatigue, fluctuations in attentional state, etc.) and serial dependencies in the data. The data also exhibit extreme observations that can be attributed to lapses, intrusions from outside the experiment, and errors occurring during the experiment. Any adequate analysis should account for these features and quantify them accurately. Recognizing that Bayesian hierarchical models are an excellent modeling tool, we focus on the elaboration of a realistic likelihood for the data and on a careful assessment of the quality of fit that it provides. We judge quality of fit in terms of the predictive performance of the model. We demonstrate how simple Bayesian hierarchical models can be built for several RT sequences, differentiating between subject-specific and condition-specific effects. Â© The Psychometric Society 2010.",0
https://doi.org/10.5665/sleep.2004,Deterioration of Neurobehavioral Performance in Resident Physicians During Repeated Exposure to Extended Duration Work Shifts,"Although acute sleep loss during 24- to 30-h extended duration work shifts (EDWS) has been shown to impair the performance of resident physicians, little is known about the effects of cumulative sleep deficiency on performance during residency training. Chronic sleep restriction induces a gradual degradation of neurobehavioral performance and exacerbates the effects of acute sleep loss in the laboratory, yet the extent to which this occurs under real-world conditions is unknown. In this study, the authors quantify the time course of neurobehavioral deterioration due to repeated exposure to EDWS during a 3-week residency rotation.A prospective, repeated-measures, within-subject design.Medical and cardiac intensive care units, Brigham and Women's Hospital, Boston, MA.Thirty-four postgraduate year one resident physicians (23 males; age 28.0 ± 1.83 (standard deviation) years)Residents working a 3-week Q3 schedule (24- to 30-h work shift starts every 3(rd) day), consisting of alternating 24- to 30-h (EDWS) and approximately 8-h shifts, underwent psychomotor vigilance testing before, during, and after each work shift. Mean response time, number of lapses, and slowest 10% of responses were calculated for each test. Residents also maintained daily sleep/wake/work logs. EDWS resulted in cumulative sleep deficiency over the 21-day rotation (6.3 h sleep obtained per day; average 2.3 h sleep obtained per extended shift). Response times deteriorated over a single 24- to 30-h shift (P < 0.0005), and also cumulatively with each successive EDWS: Performance on the fifth and sixth shift was significantly worse than on the first shift (P < 0.01). Controlling for time of day, there was a significant acute (time on shift) and chronic (successive EDWS) interaction on psychomotor vigilance testing response times (P < 0.05).Chronic sleep deficiency caused progressive degradation in residents' neurobehavioral performance and exacerbated the effects of acute sleep loss inherent in the 24- to 30-h EDWS that are commonly used in resident schedules.",0
https://doi.org/10.3758/bf03194546,Fast and accurate measurement of taste and smell thresholds using a maximum-likelihood adaptive staircase procedure,"This paper evaluates the use of a maximum-likelihood adaptive staircase psychophysical procedure (ML-PEST), originally developed in vision and audition, for measuring detection thresholds in gustation and olfaction. The basis for the psychophysical measurement of thresholds with the ML-PEST procedure is developed. Then, two experiments and four simulations are reported. In the first experiment, ML-PEST was compared with the Wetherill and Levitt up-down staircase method and with the Cain ascending method of limits in the measurement of butyl alcohol thresholds. The four Monte Carlo simulations compared the three psychophysical procedures. In the second experiment, the test-retest reliability of MLPEST for measuring NaCl and butyl alcohol thresholds was assessed. The results indicate that the ML-PEST method gives reliable and precise threshold measurements. Its ability to detect malingerers shows considerable promise. It is recommended for use in clinical testing.",0
https://doi.org/10.1007/978-3-658-08385-4,Therapist Effects on Attrition in Psychotherapy Outpatients,Dirk Zimmermann illustrates that some therapists significantly differ concerning their average dropout rates. He points out that initial impairment is a strong predictor of early termination. Different dropout criteria as well as various explaining variables on patient and on therapist level were assessed. Premature treatment termination is a common phenomenon in psychotherapy with mean dropout rates of about 20%. Therapist effects account for 3%-4% of the variation in dropout. Ã‚Â© Springer Fachmedien Wiesbaden 2015.,0
https://doi.org/10.1002/psp4.12041,The Many Flavors of Model-Based Meta-Analysis: Part I-Introduction and Landmark Data,Meta-analysis is an increasingly important aspect of drug development as companies look to benchmark their own compounds with the competition. There is scope to carry out a wide range of analyses addressing key research questions from preclinical through to postregistration. This set of tutorials will take the reader through key model-based meta-analysis (MBMA) methods with this first installment providing a general introduction before concentrating on classical and Bayesian methods for landmark data.,0
https://doi.org/10.1080/01621459.1986.10478354,Statistics and Causal Inference,"Abstract Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.",0
https://doi.org/10.1177/0013164410387340,How Are the Form and Magnitude of DIF Effects in Multiple-Choice Items Determined by Distractor-Level Invariance Effects?,"This article explores how the magnitude and form of differential item functioning (DIF) effects in multiple-choice items are determined by the underlying differential distractor functioning (DDF) effects, as modeled under the nominal response model. The results of a numerical investigation indicated that (a) the presence of one or more nonzero DDF effects implies a nonzero DIF effect; (b) the magnitude of the DDF effects creates an upper bound to the magnitude of the resulting DIF effect; (c) the presence of a large DDF effect does not necessarily imply a large DIF effect; (d) under the condition of constant DDF effects the resulting DIF effect is independent of other item properties (i.e., location and discrimination parameters), but under the condition of varying DDF effects the resulting DIF effect magnitude is dependent on other item properties; and (e) although crossing DIF can only exist with divergent (varying in sign) DDF effects, divergent DDF effects do not always yield crossing DIF. Implications of these findings for evaluating DIF effects and interpreting the causes of DIF effects are discussed.",0
https://doi.org/10.1080/10705511.2011.582408,Advanced Nonlinear Latent Variable Modeling: Distribution Analytic LMS and QML Estimators of Interaction and Quadratic Effects,"Interaction and quadratic effects in latent variable models have to date only rarely been tested in practice. Traditional product indicator approaches need to create product indicators (e.g., x 1 2, x 1 x 4) to serve as indicators of each nonlinear latent construct. These approaches require the use of complex nonlinear constraints and additional model specifications and do not directly address the nonnormal distribution of the product terms. In contrast, recently developed, easy-to-use distribution analytic approaches do not use product indicators, but rather directly model the nonlinear multivariate distribution of the measured indicators. This article outlines the theoretical properties of the distribution analytic Latent Moderated Structural Equations (LMS; Klein & Moosbrugger, 2000) and Quasi-Maximum Likelihood (QML; Klein & Muthen, 2007) estimators. It compares the properties of LMS and QML to those of the product indicator approaches. A small simulation study compares the two approaches and illustra...",0
https://doi.org/10.1007/978-3-642-12079-4_15,Cultural Consensus Theory: Aggregating Continuous Responses in a Finite Interval,"Cultural consensus theory (CCT) consists of cognitive models for aggregating responses of “informants” to test items about some domain of their shared cultural knowledge. This paper develops a CCT model for items requiring bounded numerical responses, e.g. probability estimates, confidence judgments, or similarity judgments. The model assumes that each item generates a latent random representation in each informant, with mean equal to the consensus answer and variance depending jointly on the informant and the location of the consensus answer. The manifest responses may reflect biases of the informants. Markov Chain Monte Carlo (MCMC) methods were used to estimate the model, and simulation studies validated the approach. The model was applied to an existing cross-cultural dataset involving native Japanese and English speakers judging the similarity of emotion terms. The results sharpened earlier studies that showed that both cultures appear to have very similar cognitive representations of emotion terms.",0
https://doi.org/10.1007/s11336-015-9492-7,Generalized Fiducial Inference for Binary Logistic Item Response Models,"Generalized fiducial inference (GFI) has been proposed as an alternative to likelihood-based and Bayesian inference in mainstream statistics. Confidence intervals (CIs) can be constructed from a fiducial distribution on the parameter space in a fashion similar to those used with a Bayesian posterior distribution. However, no prior distribution needs to be specified, which renders GFI more suitable when no a priori information about model parameters is available. In the current paper, we apply GFI to a family of binary logistic item response theory models, which includes the two-parameter logistic (2PL), bifactor and exploratory item factor models as special cases. Asymptotic properties of the resulting fiducial distribution are discussed. Random draws from the fiducial distribution can be obtained by the proposed Markov chain Monte Carlo sampling algorithm. We investigate the finite-sample performance of our fiducial percentile CI and two commonly used Wald-type CIs associated with maximum likelihood (ML) estimation via Monte Carlo simulation. The use of GFI in high-dimensional exploratory item factor analysis was illustrated by the analysis of a set of the Eysenck Personality Questionnaire data. © 2016, The Psychometric Society.",0
https://doi.org/10.3402/ejpt.v6.27516,Using Bayesian statistics for modeling PTSD through Latent Growth Mixture Modeling: implementation and discussion,"After traumatic events, such as disaster, war trauma, and injuries including burns (which is the focus here), the risk to develop posttraumatic stress disorder (PTSD) is approximately 10% (Breslau & Davis, 1992). Latent Growth Mixture Modeling can be used to classify individuals into distinct groups exhibiting different patterns of PTSD (Galatzer-Levy, 2015). Currently, empirical evidence points to four distinct trajectories of PTSD patterns in those who have experienced burn trauma. These trajectories are labeled as: resilient, recovery, chronic, and delayed onset trajectories (e.g., Bonanno, 2004; Bonanno, Brewin, Kaniasty, & Greca, 2010; Maercker, Gäbler, O'Neil, Schützwohl, & Müller, 2013; Pietrzak et al., 2013). The delayed onset trajectory affects only a small group of individuals, that is, about 4-5% (O'Donnell, Elliott, Lau, & Creamer, 2007). In addition to its low frequency, the later onset of this trajectory may contribute to the fact that these individuals can be easily overlooked by professionals. In this special symposium on Estimating PTSD trajectories (Van de Schoot, 2015a), we illustrate how to properly identify this small group of individuals through the Bayesian estimation framework using previous knowledge through priors (see, e.g., Depaoli & Boyajian, 2014; Van de Schoot, Broere, Perryck, Zondervan-Zwijnenburg, & Van Loey, 2015).We used latent growth mixture modeling (LGMM) (Van de Schoot, 2015b) to estimate PTSD trajectories across 4 years that followed a traumatic burn. We demonstrate and compare results from traditional (maximum likelihood) and Bayesian estimation using priors (see, Depaoli, 2012, 2013). Further, we discuss where priors come from and how to define them in the estimation process.We demonstrate that only the Bayesian approach results in the desired theory-driven solution of PTSD trajectories. Since the priors are chosen subjectively, we also present a sensitivity analysis of the Bayesian results to illustrate how to check the impact of the prior knowledge integrated into the model.We conclude with recommendations and guidelines for researchers looking to implement theory-driven LGMM, and we tailor this discussion to the context of PTSD research.",0
https://doi.org/10.1007/s11336-010-9163-7,Nested Logit Models for Multiple-Choice Item Response Data,"Nested logit item response models for multiple-choice data are presented. Relative to previous models, the new models are suggested to provide a better approximation to multiple-choice items where the application of a solution strategy precedes consideration of response options. In practice, the models also accommodate collapsibility across all distractor categories, making it easier to allow decisions about including distractor information to occur on an item-by-item or application-by-application basis without altering the statistical form of the correct response curves. Marginal maximum likelihood estimation algorithms for the models are presented along with simulation and real data analyses. Â© 2010 The Psychometric Society.",0
https://doi.org/10.1214/088342304000000080,Incorporating Bayesian Ideas into Health-Care Evaluation,"We argue that the Bayesian approach is best seen as providing additional tools for those carrying out health-care evaluations, rather than replacing their traditional methods. A distinction is made between those features that arise from the basic Bayesian philosophy and those that come from the modern ability to make inferences using very complex models. Selected examples of the former include explicit recognition of the wide cast of stakeholders in any evaluation, simple use of Bayes theorem and use of a community of prior distributions. In the context of complex models, we selectively focus on the possible role of simple Monte Carlo methods, alternative structural models for incorporating historical data and making inferences on complex functions of indirectly estimated parameters. These selected issues are illustrated by two worked examples presented in a standardized format. The emphasis throughout is on inference rather than decision-making.",0
,Discrete and Continuous Representations of Unobserved Heterogeneity in Choice Modeling,"We attempt to provide insights into how heterogeneity has been and can be addressed in choice modeling. In doing so, we deal with three topics: Models of heterogeneity. Methods of estimation and Substantive issues. In describing models we focus on discrete versus continuous representations of heterogeneity. With respect to estimation we contrast Markov Chain Monte Carlo methods and (simulated) likelihood methods. The substantive issues discussed deal with empirical tests of heterogeneity assumptions, the formation of empirical generalisations, the confounding of heterogeneity with state dependence and consideration sets, and normativesegmentation.",0
https://doi.org/10.1348/000711003770480075,A general class of latent variable models for ordinal manifest variables with covariate effects on the manifest and latent variables,Previous work on a general class of multidimensional latent variable models for analysing ordinal manifest variables is extended here to allow for direct covariate effects on the manifest ordinal variables and covariate effects on the latent variables. A full maximum likelihood estimation method is used to estimate all the model parameters simultaneously. Goodness-of-fit statistics and standard errors are discussed. Two examples from the 1996 British Social Attitudes Survey are used to illustrate the methodology.,0
https://doi.org/10.1111/j.2517-6161.1991.tb01849.x,On the Relationship between Bayesian and Non-Bayesian Interval Estimates,"SUMMARY Let Y1,..., Y, denote independent observations each distributed according to a density depending on a scalar parameter 0. Suppose that we are interested in constructing an interval estimate for 0. One approach is to construct a confidence region with a specified coverage probability based on the likelihood ratio test statistic. Another approach is to construct a highest posterior density region with a specified posterior probability. The goal of this paper is to study the relationship between these two approaches. In particular, we derive asymptotic expansions for the posterior probability of confidence regions based on the likelihood ratio test statistic and for the coverage probability of highest posterior density regions. Conditions under which the two methods lead to identical regions, at least approximately, are also given.",0
https://doi.org/10.1007/s10071-011-0465-7,The psychophysics of uneconomical choice: non-linear reward evaluation by a nectar feeder,"Uneconomical choices by humans or animals that evaluate reward options challenge the expectation that decision-makers always maximize the return currency. One possible explanation for such deviations from optimality is that the ability to sense differences in physical value between available alternatives is constrained by the sensory and cognitive processes for encoding profitability. In this study, we investigated the capacity of a nectarivorous bat species (Glossophaga commissarisi) to discriminate between sugar solutions with different concentrations. We conducted a two-alternative free-choice experiment on a population of wild electronically tagged bats foraging at an array of computer-automated artificial flowers that recorded individual choices. We used a Bayesian approach to fit individual psychometric functions, relating the strength of preferring the higher concentration option to the intensity of the presented stimulus. Psychometric analysis revealed that discrimination ability increases non-linearly with respect to intensity. We combined this result with a previous psychometric analysis of volume perception. Our theoretical analysis of choice for rewards that vary in two quality dimensions revealed regions of parameter combinations where uneconomic choice is expected. Discrimination ability may be constrained by non-linear perceptual and cognitive encoding processes that result in uneconomical choice.",0
https://doi.org/10.1016/j.neuroimage.2017.02.083,Bayesian switching factor analysis for estimating time-varying functional connectivity in fMRI,"There is growing interest in understanding the dynamical properties of functional interactions between distributed brain regions. However, robust estimation of temporal dynamics from functional magnetic resonance imaging (fMRI) data remains challenging due to limitations in extant multivariate methods for modeling time-varying functional interactions between multiple brain areas. Here, we develop a Bayesian generative model for fMRI time-series within the framework of hidden Markov models (HMMs). The model is a dynamic variant of the static factor analysis model (Ghahramani and Beal, 2000). We refer to this model as Bayesian switching factor analysis (BSFA) as it integrates factor analysis into a generative HMM in a unified Bayesian framework. In BSFA, brain dynamic functional networks are represented by latent states which are learnt from the data. Crucially, BSFA is a generative model which estimates the temporal evolution of brain states and transition probabilities between states as a function of time. An attractive feature of BSFA is the automatic determination of the number of latent states via Bayesian model selection arising from penalization of excessively complex models. Key features of BSFA are validated using extensive simulations on carefully designed synthetic data. We further validate BSFA using fingerprint analysis of multisession resting-state fMRI data from the Human Connectome Project (HCP). Our results show that modeling temporal dependencies in the generative model of BSFA results in improved fingerprinting of individual participants. Finally, we apply BSFA to elucidate the dynamic functional organization of the salience, central-executive, and default mode networks-three core neurocognitive systems with central role in cognitive and affective information processing (Menon, 2011). Across two HCP sessions, we demonstrate a high level of dynamic interactions between these networks and determine that the salience network has the highest temporal flexibility among the three networks. Our proposed methods provide a novel and powerful generative model for investigating dynamic brain connectivity.",0
https://doi.org/10.1198/jasa.2010.ap09237,Bayesian Spatial Quantile Regression,"Tropospheric ozone is one of the six criteria pollutants regulated by the United States Environmental Protection Agency under the Clean Air Act and has been linked with several adverse health effects, including mortality. Due to the strong dependence on weather conditions, ozone may be sensitive to climate change and there is great interest in studying the potential effect of climate change on ozone, and how this change may affect public health. In this paper we develop a Bayesian spatial model to predict ozone under different meteorological conditions, and use this model to study spatial and temporal trends and to forecast ozone concentrations under different climate scenarios. We develop a spatial quantile regression model that does not assume normality and allows the covariates to affect the entire conditional distribution, rather than just the mean. The conditional distribution is allowed to vary from site-to-site and is smoothed with a spatial prior. For extremely large datasets our model is computationally infeasible, and we develop an approximate method. We apply the approximate version of our model to summer ozone from 1997-2005 in the Eastern U.S., and use deterministic climate models to project ozone under future climate conditions. Our analysis suggests that holding all other factors fixed, an increase in daily average temperature will lead to the largest increase in ozone in the Industrial Midwest and Northeast.",0
https://doi.org/10.1037/1082-989x.8.2.164,How many IRT parameters does it take to model psychopathology items?,"The authors compared the fit of the 2- and 3-parameter logistic models (2PLM; 3PLM) on 15 unidimensional factor scales derived from the Minnesota Multiphasic Personality Inventory--Adolescent item pool. Log-likelihood chi-square deviance tests indicated that a 3PLM provided an improved fit. However, residual statistics indicated that the difference in fit between the 2 models was negligible. An unexpected finding was that from 10% to 30% of the items had substantial lower asymptote parameters (c > or = .10) when the scales were scored in the pathology or nonpathology directions. The authors argue that the large lower asymptote parameters are attributable to item-content ambiguity possibly caused by item-level multidimensionality. These findings suggest that the direction of scoring can critically affect an item response theory analysis.",0
https://doi.org/10.1177/0049124113494575,Under What Assumptions Do Site-by-Treatment Instruments Identify Average Causal Effects?,"The increasing availability of data from multisite randomized trials provides a potential opportunity to use instrumental variables (IV) methods to study the effects of multiple hypothesized mediators of the effect of a treatment. We derive nine assumptions needed to identify the effects of multiple mediators when using site-by-treatment interactions to generate multiple instruments. Three of these assumptions are unique to the multiple-site, multiple-mediator case: (1) the assumption that the mediators act in parallel (no mediator affects another mediator); (2) the assumption that the site-average effect of the treatment on each mediator is independent of the site-average effect of each mediator on the outcome; and (3) the assumption that the site-by-compliance matrix has sufficient rank. The first two of these assumptions are nontrivial and cannot be empirically verified, suggesting that multiple-site, multiple-mediator IV models must be justified by strong theory.",0
https://doi.org/10.1007/s10651-009-0119-y,Addressing among-group variation in covariate effects using multilevel models,"Multilevel models are used to model processes associated with hierarchical data structures. Despite infrequent use in the biological and environmental sciences, the use of these models with hierarchically-structured data conveys multiple advantages. These include the assessment of whether covariate effects differ among groups or clusters, and separate estimation of covariate effects by hierarchical level (thereby addressing atomistic and aggregation fallacy concerns). We illustrate these advantages using larval mayfly count data derived from annual surveys on the Mississippi River and a continuous covariate (water depth). Ã‚Â© 2009 Springer Science+Business Media, LLC.",0
https://doi.org/10.1046/j.0039-0402.2003.00255.x,"Testing homogeneity in a random intercept model using asymptotic, posterior predictive and plug-in p-values","In this paper three statistics and three discrepancy measures with which homogeneity in the random intercept model can be investigated will be evaluated. The first two can be used to test the homogeneity of level one residual variances across level two units and the third can be used to test whether effects should be fixed or random. Each statistic and discrepancy measure will be evaluated using asymptotic (if available), posterior predictive and plug in p-values. A simulation study will be used to investigate the frequency properties of these p-values. In the discussion it will be indicated how the results obtained for the random intercept model with one explanatory variable can be useful during the construction of general two level models.",0
https://doi.org/10.1111/1467-9868.00230,Standard errors for EM estimation,"The EM algorithm is a popular method for computing maximum likelihood estimates. One of its drawbacks is that it does not produce standard errors as a by-product. We consider obtaining standard errors by numerical differentiation. Two approaches are considered. The first differentiates the Fisher score vector to yield the Hessian of the log-likelihood. The second differentiates the EM operator and uses an identity that relates its derivative to the Hessian of the log-likelihood. The well-known SEM algorithm uses the second approach. We consider three additional algorithms: one that uses the first approach and two that use the second. We evaluate the complexity and precision of these three and the SEM in algorithm seven examples. The first is a single-parameter example used to give insight. The others are three examples in each of two areas of EM application: Poisson mixture models and the estimation of covariance from incomplete data. The examples show that there are algorithms that are much simpler and more accurate than the SEM algorithm. Hopefully their simplicity will increase the availability of standard error estimates in EM applications. It is shown that, as previously conjectured, a symmetry diagnostic can accurately estimate errors arising from numerical differentiation. Some issues related to the speed of the EM algorithm and algorithms that differentiate the EM operator are identified.",0
https://doi.org/10.1002/(sici)1097-0258(19980615)17:11<1261::aid-sim846>3.0.co;2-z,Extending the simple linear regression model to account for correlated responses: An introduction to generalized estimating equations and multi-level mixed modelling,"Much of the research in epidemiology and clinical science is based upon longitudinal designs which involve repeated measurements of a variable of interest in each of a series of individuals. Such designs can be very powerful, both statistically and scientifically, because they enable one to study changes within individual subjects over time or under varied conditions. However, this power arises because the repeated measurements tend to be correlated with one another, and this must be taken into proper account at the time of analysis or misleading conclusions may result. Recent advances in statistical theory and in software development mean that studies based upon such designs can now be analysed more easily, in a valid yet flexible manner, using a variety of approaches which include the use of generalized estimating equations, and mixed models which incorporate random effects. This paper provides a particularly simple illustration of the use of these two approaches, taking as a practical example the analysis of a study which examined the response of portable peak expiratory flow meters to changes in true peak expiratory flow in 12 children with asthma. The paper takes the reader through the relevant practicalities of model fitting, interpretation and criticism and demonstrates that, in a simple case such as this, analyses based upon these model-based approaches produce reassuringly similar inferences to standard analyses based upon more conventional methods.",0
https://doi.org/10.1002/1097-0258(20010215)20:3<435::aid-sim804>3.0.co;2-e,Bayesian methods for cluster randomized trials with continuous responses,"Bayesian methods for cluster randomized trials extend the random-effects formulation by allowing both the use of external evidence on parameters and straightforward relaxation of the standard normality and constant variance assumptions. Care is required in specifying prior distributions on variance components, and a number of different options are explored with implied prior distributions for other parameters given in closed form. Markov chain Monte Carlo (MCMC) methods permit the fitting of very general models and the introduction of parameter uncertainty into power calculations. We illustrate these ideas using a published example in which general practices were randomized to intervention or control, and show that different choices of supposedly 'non-informative' prior distributions can have substantial influence on conclusions. We also illustrate the use of forward simulation methods in power calculations with uncertainty on multiple inputs. Bayesian methods have the potential to be very useful but guidance is required as to appropriate strategies for robust analysis. Our current experience leads us to recommend a standard 'non-informative' prior distribution for the within-cluster sampling variance, and an independent prior on the intraclass correlation coefficient (ICC). The latter may exploit background evidence or, as a reference analysis, be a uniform ICC or a 'uniform shrinkage' prior.",0
https://doi.org/10.1177/014662169001400104,Bias and the Effect of Priors in Bayesian Estimation of Parameters of Item Response Models,"The effectiveness of a Bayesian approach to the es timation problem in item response models has been sufficiently documented in recent years. Although re search has indicated that Bayesian estimates, in gen eral, are more accurate than joint maximum likelihood (JML) estimates, the effect of choice of priors on the Bayesian estimates is not well known. Moreover, the extent to which the Bayesian estimates are biased in comparison with JML estimates is not known. The ef fect of priors and the amount of bias in Bayesian esti mates is examined in this paper through simulation studies. It is shown that different specifications of prior information have relatively modest effects on the Bayesian estimates. For small samples, it is shown that the Bayesian estimates are less biased than their JML counterparts. Index terms: accuracy, Bayesian estimates, bias, item response models, joint maximum likelihood estimates, priors.",0
https://doi.org/10.15288/jsa.2004.65.105,Adolescents' alcohol and drug use trajectories in the year following treatment.,"Beyond the initial relapse episode, little is known about short-term patterns of alcohol and other drug use in treated adolescents. This study characterized treated teens' short-term alcohol and other drug use trajectories over 1-year follow-up.Adolescents (N = 110, ages 12-18, 65% male, 94% white) recruited from addictions treatment, with a current DSM-IV alcohol diagnosis, reported on daily alcohol and other drug use in monthly telephone contacts over 1-year follow-up using the Timeline Follow-Back procedure. Latent class mixture modeling identified trajectories based on maximum number of consecutive abstinent days per month, separately for alcohol and other drugs.Four alcohol trajectories were identified: high abstinence (53%), decreasing abstinence (10%), increasing abstinence (16%) and low abstinence (21%). The alcohol trajectories were distinguished by gender, age, readiness to change substance use and alcohol-related coping. To characterize changes in alcohol abstinence in relation to abstinence from other drugs, four other drug trajectories were identified: high (59%), decreasing (12%), increasing (14%) and low (15%) abstinence. Cross-classification of the alcohol and drug trajectories indicated a moderate level of concordance (kappa = 0.49).Multiple pathways of short-term change in alcohol and other drug use were identified. Although changes in abstinence from alcohol and other drug use tended to co-occur, exceptions were observed. Differences between alcohol trajectories in readiness to change substance use and use of substance coping suggest the potentially positive impact of targeted and effectively timed interventions that focus on motivational enhancement and on improving substance coping for certain adolescent subgroups.",0
https://doi.org/10.1016/j.ssresearch.2014.08.011,Instrumental variables estimates of peer effects in social networks,"• Instrumental variables play a crucial role in estimating causal peer effects in social networks. • Measurement error in smoking can lead to both under- and imprecise estimates of peer effects. • The analysis shows consistent and robust evidence for peer effects on smoking. • Sharing cigarettes is a plausible mechanism for peer effects on smoking. Estimating peer effects with observational data is very difficult because of contextual confounding, peer selection, simultaneity bias, and measurement error, etc. In this paper, I show that instrumental variables (IVs) can help to address these problems in order to provide causal estimates of peer effects. Based on data collected from over 4000 students in six middle schools in China, I use the IV methods to estimate peer effects on smoking. My design-based IV approach differs from previous ones in that it helps to construct potentially strong IVs and to directly test possible violation of exogeneity of the IVs. I show that measurement error in smoking can lead to both under- and imprecise estimations of peer effects. Based on a refined measure of smoking, I find consistent evidence for peer effects on smoking. If a student’s best friend smoked within the past 30 days, the student was about one fifth (as indicated by the OLS estimate) or 40 percentage points (as indicated by the IV estimate) more likely to smoke in the same time period. The findings are robust to a variety of robustness checks. I also show that sharing cigarettes may be a mechanism for peer effects on smoking. A 10% increase in the number of cigarettes smoked by a student’s best friend is associated with about 4% increase in the number of cigarettes smoked by the student in the same time period.",0
https://doi.org/10.1080/10508422.2014.951720,Mechanisms of Moral Disengagement in the Endorsement of Asylum Seeker Policies in Australia,"Moral disengagement is a process whereby the self-regulatory mechanisms that would otherwise sanction unethical conduct can be selectively disabled. The present research proposed that moral disengagement might be adopted in the endorsement of asylum seeker policies in Australia, and in order to test this, a scale was developed and was validated in two studies. Factor analysis demonstrated that a 2-factor, 16-item structure had the best fit, and the construct validity of the scale was supported. Results provide evidence for the use of moral disengagement in the context of asylum seekers as a means of rationalizing conduct that may otherwise be sanctioned.",0
https://doi.org/10.1080/00273171.2010.498292,Assessing Mediational Models: Testing and Interval Estimation for Indirect Effects,"Theoretical models specifying indirect or mediated effects are common in the social sciences. An indirect effect exists when an independent variable's influence on the dependent variable is mediated through an intervening variable. Classic approaches to assessing such mediational hypotheses ( Baron & Kenny, 1986 ; Sobel, 1982 ) have in recent years been supplemented by computationally intensive methods such as bootstrapping, the distribution of the product methods, and hierarchical Bayesian Markov chain Monte Carlo (MCMC) methods. These different approaches for assessing mediation are illustrated using data from Dunn, Biesanz, Human, and Finn (2007). However, little is known about how these methods perform relative to each other, particularly in more challenging situations, such as with data that are incomplete and/or nonnormal. This article presents an extensive Monte Carlo simulation evaluating a host of approaches for assessing mediation. We examine Type I error rates, power, and coverage. We study normal and nonnormal data as well as complete and incomplete data. In addition, we adapt a method, recently proposed in statistical literature, that does not rely on confidence intervals (CIs) to test the null hypothesis of no indirect effect. The results suggest that the new inferential method-the partial posterior p value-slightly outperforms existing ones in terms of maintaining Type I error rates while maximizing power, especially with incomplete data. Among confidence interval approaches, the bias-corrected accelerated (BC a ) bootstrapping approach often has inflated Type I error rates and inconsistent coverage and is not recommended; In contrast, the bootstrapped percentile confidence interval and the hierarchical Bayesian MCMC method perform best overall, maintaining Type I error rates, exhibiting reasonable power, and producing stable and accurate coverage rates.",0
https://doi.org/10.1177/2158244015605160,Do Men and Women Exhibit Different Preferences for Mates? A Replication of Eastwick and Finkel (2008),"Evolutionary theory predicts that men will prefer physically attractive romantic partners, and women will prefer wealthy, high-status partners. This theory is well-supported when examining ideal hypothetical partner preferences, but less support has been found when people interact face-to-face. The present study served as a direct replication of results reported in Eastwick and Finkel (2008). We recruited 307 participants and utilized a speed-dating methodology to allow in-person interactions, then administered follow-up surveys to measure romantic interest over 30 days. Data were analyzed using multilevel modeling and were aggregated using meta-analysis. Consistent with previous findings, our results showed that participants were more romantically interested in potential partners if they were viewed as attractive and good potential earners, and these associations were not moderated by gender. Results suggest that gender differences predicted by evolutionary theory may not hold when people interact with potential romantic partners face-to-face. However, we discuss these results in light of some general methodological limitations and evidence from other lines of research.",0
https://doi.org/10.1002/sim.1470,Modelling the random effects covariance matrix in longitudinal data,"A common class of models for longitudinal data are random effects (mixed) models. In these models, the random effects covariance matrix is typically assumed constant across subject. However, in many situations this matrix may differ by measured covariates. In this paper, we propose an approach to model the random effects covariance matrix by using a special Cholesky decomposition of the matrix. In particular, we will allow the parameters that result from this decomposition to depend on subject-specific covariates and also explore ways to parsimoniously model these parameters. An advantage of this parameterization is that there is no concern about the positive definiteness of the resulting estimator of the covariance matrix. In addition, the parameters resulting from this decomposition have a sensible interpretation. We propose fully Bayesian modelling for which a simple Gibbs sampler can be implemented to sample from the posterior distribution of the parameters. We illustrate these models on data from depression studies and examine the impact of heterogeneity in the covariance matrix on estimation of both fixed and random effects.",0
https://doi.org/10.1007/s11336-013-9381-x,Hierarchical Approximate Bayesian Computation,"Approximate Bayesian computation (ABC) is a powerful technique for estimating the posterior distribution of a model's parameters. It is especially important when the model to be fit has no explicit likelihood function, which happens for computational (or simulation-based) models such as those that are popular in cognitive neuroscience and other areas in psychology. However, ABC is usually applied only to models with few parameters. Extending ABC to hierarchical models has been difficult because high-dimensional hierarchical models add computational complexity that conventional ABC cannot accommodate. In this paper, we summarize some current approaches for performing hierarchical ABC and introduce a new algorithm called Gibbs ABC. This new algorithm incorporates well-known Bayesian techniques to improve the accuracy and efficiency of the ABC approach for estimation of hierarchical models. We then use the Gibbs ABC algorithm to estimate the parameters of two models of signal detection, one with and one without a tractable likelihood function.",0
https://doi.org/10.1007/bf00225903,Variance component estimation techniques compared for two mating designs with forest genetic architecture through computer simulation,"Computer simulation was used to compare minimum variance quadratic estimation (MIVQUE), minimum norm quadratic unbiased estimation (MINQUE), restricted maximum likelihood (REML), maximum likelihood (ML), and Henderson's Method 3 (HM3) on the basis of variance among estimates, mean square error (MSE), bias and probability of nearness for estimation of both individual variance components and three ratios of variance components. The investigation also compared three procedures for dealing with negative estimates and included the use of both individual observations and plot means as the experimental unit of the analysis. The structure of data simulated (field design, mating designs, genetic architecture and imbalance) represented typical analysis problems in quantitative forest genetics. Results of comparing the estimation techniques demonstrated that: estimates of probability of nearness did not discriminate among techniques; bias was discriminatory among procedures for dealing with negative estimates but not among estimation techniques (except ML); sampling variance among estimates was discriminatory among procedures for dealing with negative estimates, estimation techniques and unit of observation; and MSE provided no additional information to variance of the estimates. HM3 and REML were the closest competitors under these criteria; however, REML demonstrated greater robustness to imbalance. Of the three negative estimate procedures, two are of practical significance and guidelines for their application are presented. Estimates from individual observations were always preferable to those from plot means over the experimental levels of this study. Ã‚Â© 1994 Springer-Verlag.",0
https://doi.org/10.1037/0021-9010.62.5.529,Development of a general solution to the problem of validity generalization.,"U. S. Civil Service Commission andGeorge Washington UniversityJohn E. HunterMichigan State UniversityPersonnel psychologists have traditionally believed that employment test valid-ities are situation specific. This study presents a Bayesian statistical modelwhich allows one to explore the alternate hypothesis that variation in validityoutcomes from study to study for similar jobs and tests is artifactual in nature.Certain outcomes using this model permit validity generalization to new settingswithout carrying out a validation study of any kind. Where such generalizationis not justified, the procedure provides an improved method of data analysisand decision making for the necessary situational validity study. Application tofour distributions of empirical validity coefficients demonstrated the power ofthe model.A recent study (Schmidt, Hunter, & Urry,1976) addressed the belief, dominant in per-sonnel psychology, that meaningful empiricalvalidation studies are possible for most, if notall, jobs in most organizations. This studyshowed that, because of range restriction andless than perfect criterion reliability, thesample sizes necessary to provide adequatestatistical power are usually much larger thanhas typically been assumed, This finding leadsto the conclusion that empirical validitystudies are technically feasible much lessfrequently than the profession has assumed.The present study is addressed to another ofthe orthodoxies of personnel psychology: thebelief that test validity is generally highlysituation specific. Considerable variabilityfrom study to study is observed in raw valida-tion results even when jobs and tests appearto be similar or essentially identical (Ghiselli,1966). The explanation usually advanced forthis phenomenon is that the factor structureof job performance is different from job tojob and that the human observer or job ana-lyst is simply too poor an information receiver",0
,Methods for Meta-Analysis in Medical Research,PART A: META-ANALYSIS METHODOLOGY: THE BASICS Introduction: Meta-analysis: Its Development and Uses Defining Outcome Measures used for Combining via Meta-analysis Random Effects Models for Combining Study Estimates Exploring Between Study Heterogeneity Publication Bias Study Quality Sensitivity Analysis Reporting the Results of a Meta-analysis Fixed Effects Methods for Combining Study Estimates PART B: ADVANCED AND SPECIALIZED META-ANALYSIS TOPICS Bayesian Methods in Meta-analysis Meta Regression Meta-analysis of Different Types of Data Incorporating Study Quality into a Meta-analysis Meta-analysis of Multiple and Correlated Outcome Measures Meta-analysis of Epidemiological and other Observational Studies Generalised Synthesis of Evidence - Combining Different Sources of Evidence Meta-analysis of Survival Data Cumulative Meta-analysis Miscellaneous and Developing Areas of Applications in Meta-Analysis Appendix I: Software Used for the Examples in this Book,0
https://doi.org/10.1207/s15327906mbr4104_7,Fitting Partially Nonlinear Random Coefficient Models as SEMs,"The nonlinear random coefficient model has become increasingly popular as a method for describing individual differences in longitudinal research. Although promising, the nonlinear model it is not utilized as often as it might be because software options are still somewhat limited. In this article we show that a specialized version of the model can be fit to data using SEM software. The specialization is to a model in which the parameters that enter the function in a linear manner are random, whereas those that are nonlinear are common to all individuals. Although this kind of function is not as general as is the fully nonlinear model, it still is applicable to many different data sets. Two examples are presented to show how the models can be estimated using popular SEM computer programs.",0
https://doi.org/10.1007/s11336-017-9587-4,Bayesian Sensitivity Analysis of a Nonlinear Dynamic Factor Analysis Model with Nonparametric Prior and Possible Nonignorable Missingness,"Many psychological concepts are unobserved and usually represented as latent factors apprehended through multiple observed indicators. When multiple-subject multivariate time series data are available, dynamic factor analysis models with random effects offer one way of modeling patterns of within- and between-person variations by combining factor analysis and time series analysis at the factor level. Using the Dirichlet process (DP) as a nonparametric prior for individual-specific time series parameters further allows the distributional forms of these parameters to deviate from commonly imposed (e.g., normal or other symmetric) functional forms, arising as a result of these parameters' restricted ranges. Given the complexity of such models, a thorough sensitivity analysis is critical but computationally prohibitive. We propose a Bayesian local influence method that allows for simultaneous sensitivity analysis of multiple modeling components within a single fitting of the model of choice. Five illustrations and an empirical example are provided to demonstrate the utility of the proposed approach in facilitating the detection of outlying cases and common sources of misspecification in dynamic factor analysis models, as well as identification of modeling components that are sensitive to changes in the DP prior specification.",0
https://doi.org/10.4324/9780203864746-20,Developing a Random Coefficient Model for Nonlinear Repeated Measures Data,,0
https://doi.org/10.1037/0021-9010.91.1.1,"Statistical power and parameter stability when subjects are few and tests are many: Comment on Peterson, Smith, Martorana, and Owens (2003).","Comments on the original article ""The impact of chief executive officer personality on top management team dynamics: One mechanism by which leadership affects organizational performance"", by R. S. Peterson et al.. This comment illustrates how small sample sizes, when combined with many statistical tests, can generate unstable parameter estimates and invalid inferences. Although statistical power for 1 test in a small-sample context is too low, the experimentwise power is often high when many tests are conducted, thus leading to Type I errors that will not replicate when retested. This comment's results show how radically the specific conclusions and inferences in R. S. Peterson, D. B. Smith, P. V. Martorana, and P. D. Owens's (2003) study changed with the inclusion or exclusion of 1 data point. When a more appropriate experimentwise statistical test was applied, the instability in the inferences was eliminated, but all the inferences become nonsignificant, thus changing the positive conclusions.",0
https://doi.org/10.1097/mlr.0b013e3180546867,"Comparison of Meta-Analytic Results of Indirect, Direct, and Combined Comparisons of Drugs for Chronic Insomnia in Adults: A Case Study","Our Center recently conducted a systematic review of the manifestations and management of chronic insomnia in adults. The efficacy and safety of benzodiazepines and nonbenzodiazepines, relative to placebo, were compared indirectly.Determine how the results of indirect comparisons made in the review compare with the results of direct comparisons, as well as with estimates derived from Bayesian mixed treatment comparisons. Establish general appropriateness of the use of results of indirect or mixed treatment comparisons.Treatments were compared using frequentist direct, indirect, and combined methods, as well as Bayesian direct and mixed methods.Estimates for comparisons tended to be clinically and statistically similar across methods. Estimates obtained through indirect comparisons were not biased and were similar to those obtained through direct analysis.Results of indirect comparisons made in the review, accurately reflected the current evidence. Frequentist and Bayesian methods of analysis of indirect comparisons should be considered when performing meta-analyses.",0
https://doi.org/10.1207/s15328007sem0704_3,"The Performance of ML, GLS, and WLS Estimation in Structural Equation Modeling Under Conditions of Misspecification and Nonnormality","This simulation study demonstrates how the choice of estimation method affects indexes of fit and parameter bias for different sample sizes when nested models vary in terms of specification error and the data demonstrate different levels of kurtosis. Using a fully crossed design, data were generated for 11 conditions of peakedness, 3 conditions of misspecification, and 5 different sample sizes. Three estimation methods (maximum likelihood [ML], generalized least squares [GLS], and weighted least squares [WLS]) were compared in terms of overall fit and the discrepancy between estimated parameter values and the true parameter values used to generate the data. Consistent with earlier findings, the results show that ML compared to GLS under conditions of misspecification provides more realistic indexes of overall fit and less biased parameter values for paths that overlap with the true model. However, despite recommendations found in the literature that WLS should be used when data are not normally distribute...",0
https://doi.org/10.1111/j.1467-9868.2004.05304.x,Probabilistic sensitivity analysis of complex models: a Bayesian approach,"Summary.  In many areas of science and technology, mathematical models are built to simulate complex real world phenomena. Such models are typically implemented in large computer programs and are also very complex, such that the way that the model responds to changes in its inputs is not transparent. Sensitivity analysis is concerned with understanding how changes in the model inputs influence the outputs. This may be motivated simply by a wish to understand the implications of a complex model but often arises because there is uncertainty about the true values of the inputs that should be used for a particular application. A broad range of measures have been advocated in the literature to quantify and describe the sensitivity of a model's output to variation in its inputs. In practice the most commonly used measures are those that are based on formulating uncertainty in the model inputs by a joint probability distribution and then analysing the induced uncertainty in outputs, an approach which is known as probabilistic sensitivity analysis. We present a Bayesian framework which unifies the various tools of prob- abilistic sensitivity analysis. The Bayesian approach is computationally highly efficient. It allows effective sensitivity analysis to be achieved by using far smaller numbers of model runs than standard Monte Carlo methods. Furthermore, all measures of interest may be computed from a single set of runs.",0
https://doi.org/10.1037/e576972011-002,Measuring Prestige of Journals in Industrial-Organizational Psychology,,0
https://doi.org/10.1111/j.2517-6161.1994.tb01985.x,Estimation of Finite Mixture Distributions Through Bayesian Sampling,"SUMMARY A formal Bayesian analysis of a mixture model usually leads to intractable calculations, since the posterior distribution takes into account all the partitions of the sample. We present approximation methods which evaluate the posterior distribution and Bayes estimators by Gibbs sampling, relying on the missing data structure of the mixture model. The data augmentation method is shown to converge geometrically, since a duality principle transfers properties from the discrete missing data chain to the parameters. The fully conditional Gibbs alternative is shown to be ergodic and geometric convergence is established in the normal case. We also consider non-informative approximations associated with improper priors, assuming that the sample corresponds exactly to a k-component mixture.",0
https://doi.org/10.1076/1380-3611(200003)6:1;1-i;ft083,Evaluating Educational Interventions Using Multilevel Growth Curves: The Case of Reading Recovery,"Educational interventions are notoriously difficult to evaluate. This article describes an application of multilevel modelling to repeated measures as part of an evaluation of Reading Recovery in England. Growth curve and multivariate models are applied to the within and between school aspects of the design, which also included a phonological intervention. Reading Recovery is shown to be effective in the short term but not in the long term. A measure of social disadvantage is a powerful predictor of reading growth in the long term.",0
https://doi.org/10.2307/2531905,Estimation and Comparison of Changes in the Presence of Informative Right Censoring by Modeling the Censoring Process,"In the estimation and comparison of the rates of change of a continuous variable between two groups, the unweighted averages of individual simple least squares estimates from each group are often used. Under a linear random effects model, when all individuals have complete observations at identical time points, these statistics are maximum likelihood estimates for the expected rates of change. However, with censored or missing data, these estimates are no longer efficient when compared to generalized least squares estimates. When, in addition, the right-censoring process is dependent on the individual rates of change (i.e., informative right censoring), the generalized least squares estimates will be biased. Likelihood-ratio tests for informativeness of the censoring process and maximum likelihood estimates for the expected rates of change and the parameters of the right-censoring process are developed under a linear random effects model with a probit model for the right-censoring process. In realistic situations, we illustrate that the bias in estimating group rate of change and the reduction of power in comparing group differences could be substantial when strong dependency of the right-censoring process on individual rates of change is ignored.",0
https://doi.org/10.1093/ser/mwv017,Can informal economic activities be explained by social and institutional factors? A comparative analysis,"Empirical literature on informal activities often builds on macro-economic country estimates, which impedes testing behavioural hypotheses. The European Social Survey (ESS), documenting self-reported tax evasion in 26 countries, allows us to test individual and institutional factors simultaneously. We model the effect of institutional and social capital factors affecting informal transactions. We predict that informality is fostered by social relations and trust, and curbed by institutional trust. Regulation and taxation fuel informal transactions, while effective enforcement inhibits them. These predictions are simultaneously tested with individual-level data from the ESS, complemented with country-level data on regulation, taxation levels and enforcement. Multilevel binary and multinomial logit, fixed effects, Markov chain Monte Carlo method and adaptive Gaussian quadrature regressions confirm the predictions regarding social capital, trust and tax burden. Contrary to much prior research, we find weak and inconsistent effects of regulation and enforcement, which may also be due to the limited variation of our country sample. Ã‚Â© The Author 2015.",0
https://doi.org/10.3758/brm.41.2.372,Making treatment effect inferences from multiple-baseline data: The utility of multilevel modeling approaches,"Multiple-baseline studies are prevalent in behavioral research, but questions remain about how to best analyze the resulting data. Monte Carlo methods were used to examine the utility of multilevel models for multiple-baseline data under conditions that varied in the number of participants, number of repeated observations per participant, variance in baseline levels, variance in treatment effects, and amount of autocorrelation in the Level 1 errors. Interval estimates of the average treatment effect were examined for two specifications of the Level 1 error structure (sigma(2)I and first-order autoregressive) and for five different methods of estimating the degrees of freedom (containment, residual, between-within, Satterthwaite, and Kenward-Roger). When the Satterthwaite or Kenward-Roger method was used and an autoregressive Level 1 error structure was specified, the interval estimates of the average treatment effect were relatively accurate. Conversely, the interval estimates of the treatment effect variance were inaccurate, and the corresponding point estimates were biased.",0
https://doi.org/10.1007/978-0-387-73186-5_12,Multilevel Structural Equation Modeling,,0
https://doi.org/10.1007/s10182-007-0035-0,On the inefficiency of propensity score matching,"Propensity score matching is now widely used in empirical applications for estimating treatment effects. Propensity score matching (PSM) is preferred to matching on X because of the lower dimension of the estimation problem. In this note, however, it is shown that PSM is inefficient compared to matching on X. Hence, matching on X should be considered as a serious alternative.",0
https://doi.org/10.1207/s15328007sem1301_7,On Multilevel Model Reliability Estimation From the Perspective of Structural Equation Modeling,A covariance structure modeling perspective on reliability estimation can be used to construct a formal approach to estimation of reliability in multilevel models. This article presents a didactic discussion of the relation between a structural modeling procedure for scale reliability estimation and the notion of reliability of observed means in unconditional multilevel models. The utility of the method is demonstrated as an alternative means for conceptualizing multilevel modeling reliability. The relationship between the underlying two reliability notions is illustrated on a set of hierarchical data.,0
https://doi.org/10.1037/a0018719,The importance of covariate selection in controlling for selection bias in observational studies.,"The assumption of strongly ignorable treatment assignment is required for eliminating selection bias in observational studies. To meet this assumption, researchers often rely on a strategy of selecting covariates that they think will control for selection bias. Theory indicates that the most important covariates are those highly correlated with both the real selection process and the potential outcomes. However, when planning a study, it is rarely possible to identify such covariates with certainty. In this article, we report on an extensive reanalysis of a within-study comparison that contrasts a randomized experiment and a quasi-experiment. Various covariate sets were used to adjust for initial group differences in the quasi-experiment that was characterized by self-selection into treatment. The adjusted effect sizes were then compared with the experimental ones to identify which individual covariates, and which conceptually grouped sets of covariates, were responsible for the high degree of bias reduction achieved in the adjusted quasi-experiment. Such results provide strong clues about preferred strategies for identifying the covariates most likely to reduce bias when planning a study and when the true selection process is not known.",0
https://doi.org/10.1007/s11577-013-0233-6,Religiöse Diversität und Sozialintegration im internationalen Vergleich,"Following Robert D. Putnams (2007) thesis that ethnic diversity weakens social cohesion, this study addresses the social consequences of religious diversity. Instead of contrasting social-psychological mechanisms it takes a macro-sociological perspective that focuses on different structural forms of religious diversity and relates them to the trust relations within a population. The empirical results based on a cross-national comparison of 41 European and Non- European societies show that religious macro-structures are indeed related to social trust. But contrary to Putnam's ""hunkering down"" thesis religious diversity does not lead to social anomia and isolation but has different effects on social trust toward religious in- and out-groups. Key is the result that different macro-structural forms of religious diversity have different consequences for social cohesion. The question, whether religious diversity poses a threat or opportunity for the social integration of society, therefore crucially depends on its concrete form. Ã‚Â© 2013 Springer Fachmedien Wiesbaden.",0
https://doi.org/10.1002/cjs.10096,Likelihood-based inference for correlated diffusions,"The authors address the problem of likelihood based inference for correlated diffusions. Such a task presents two issues; the positive definite constraints of the diffusion matrix and the likelihood intractability. The first issue is handled by using the Cholesky factorisation on the diffusion matrix. To deal with the likelihood unavailability, a generalisation of the data augmentation framework of Roberts and Stramer (2001 Biometrika 88(3), 603-621) to d-dimensional correlated diffusions, including multivariate stochastic volatility models, is given. The methodology is illustrated through simulated and real datasets.",0
https://doi.org/10.3758/app.71.7.1664,Applications of nonparametric adaptive methods for simple reaction time experiments,"Adaptive methods are commonly used in psychophysical research for detection and discrimination (see Leek, 2001; Treutwein, 1995, for reviews). In recent years, researchers have investigated via simulations some asymptotic and small-sample properties of two nonparametric adaptive methods—namely, the fixed-step-size up-down (García-Pérez, 1998, 2001) and the (accelerated) stochastic approximation (Faes et al., 2007). In the present article, we extend both methods to the simple reaction time (RT) situation for the measure of signal intensities that elicit certain (fixed) RT percentiles. We focus on extending the following four methods: the stochastic approximation of Robbins and Monro (1951), its accelerated version of Kesten (1958), the transformed up-down of Wetherill (1963), and the “biased coin design” of Durham and Flournoy (1994, 1995). In all simulations, we assume that the RT is Weibull distributed and that there is a linear relationship between the mean RT and its standard deviation. The convergences of the asymptotic and small-sample properties for different starting values, step sizes, and response criteria are systematically investigated.",0
https://doi.org/10.1186/s12879-015-0855-6,Systematic review and mixed treatment comparison meta-analysis of randomized clinical trials of primary oral antifungal prophylaxis in allogeneic hematopoietic cell transplant recipients,"BackgroundAntifungal prophylaxis is a promising strategy for reducing invasive fungal infections (IFIs) in allogeneic hematopoietic cell transplant (alloHCT) recipients, but the optimum prophylactic agent is unknown. We used mixed treatment comparison (MTC) meta-analysis to compare clinical trials examining the use of oral antifungals for prophylaxis in alloHCT recipients, with the goal of informing medical decision-making.MethodsRandomized controlled trials (RCTs) of fluconazole, itraconazole, posaconazole, and voriconazole for primary antifungal prophylaxis were identified through a systematic literature review. Outcomes of interest (incidence of IFI/invasive aspergillosis/invasive candidiasis, all-cause mortality, and use of other antifungals) were extracted from eligible RCTs and incorporated into a Bayesian hierarchical random-effects MTC.ResultsFive eligible RCTs, randomizing 2147 patients in total, were included. Relative to fluconazole, prophylaxis with itraconazole (odds ratio [OR]: 0.52; interquartile range [IQR]: 0.35–0.76), posaconazole (OR: 0.56; IQR: 0.32–0.99), and voriconazole (OR: 0.46; IQR: 0.28–0.73) reduced incidence of overall proven/probable IFI. Posaconazole (OR: 0.31; IQR: 0.17–0.58) and voriconazole (OR: 0.33; IQR: 0.17–0.58) prophylaxis reduced proven/probable invasive aspergillosis more than itraconazole (OR: 0.68; IQR: 0.42–1.12). All-cause mortality was similar across all mould-active agents.ConclusionAs expected, mould-active azoles prevented IFIs, particularly invasive aspergillosis, more effectively than fluconazole in alloHCT recipients. The paucity of comparative efficacy data suggests that other factors such as long-term tolerability, availability of intravenous formulations, local IFI epidemiology, and drug costs may need to form the basis for selection among the mould-active azoles.",0
https://doi.org/10.1108/s0147-912120140000039004,How Do Exit Rates from Social Assistance Benefit in Belgium Vary with Individual and Local Agency Characteristics?,"Abstract The administration of social assistance benefits is devolved to local agencies in Belgium, which raises questions about how much variation in spell lengths of benefit receipt is associated with differences across agencies. We address this issue by analysing the monthly hazard of benefit exit using administrative record data for 14,270 individuals in 574 welfare agencies. Our random-effects model allows for differences in both the observed and unobserved characteristics of beneficiaries and of local agencies. There are large differences in median benefit duration for individuals serviced by different welfare agencies: the range is from two months to more than 24 months. We find strong associations between beneficiary characteristics (sex, age, foreign nationality, citizenship acquisition, work history and being a student) and spell length. The estimates show higher odds of exiting social assistance receipt in bigger municipalities and in agencies which provide more generous supplementary assistance, and also strong evidence of shorter episodes in agencies where active labour market programme participation rates are higher.",0
https://doi.org/10.1002/sim.5821,Avoiding zero between-study variance estimates in random-effects meta-analysis,"Fixed-effects meta-analysis has been criticized because the assumption of homogeneity is often unrealistic and can result in underestimation of parameter uncertainty. Random-effects meta-analysis and meta-regression are therefore typically used to accommodate explained and unexplained between-study variability. However, it is not unusual to obtain a boundary estimate of zero for the (residual) between-study standard deviation, resulting in fixed-effects estimates of the other parameters and their standard errors. To avoid such boundary estimates, we suggest using Bayes modal (BM) estimation with a gamma prior on the between-study standard deviation. When no prior information is available regarding the magnitude of the between-study standard deviation, a weakly informative default prior can be used (with shape parameter 2 and rate parameter close to 0) that produces positive estimates but does not overrule the data, leading to only a small decrease in the log likelihood from its maximum. We review the most commonly used estimation methods for meta-analysis and meta-regression including classical and Bayesian methods and apply these methods, as well as our BM estimator, to real datasets. We then perform simulations to compare BM estimation with the other methods and find that BM estimation performs well by (i) avoiding boundary estimates; (ii) having smaller root mean squared error for the between-study standard deviation; and (iii) better coverage for the overall effects than the other methods when the true model has at least a small or moderate amount of unexplained heterogeneity.",0
https://doi.org/10.3168/jds.2007-0107,"Cow, Farm, and Management Factors During the Dry Period that Determine the Rate of Clinical Mastitis After Calving","The purpose of the research was to investigate cow characteristics, farm facilities, and herd management strategies during the dry period to examine their joint influence on the rate of clinical mastitis after calving. Data were collected over a 2-yr period from 52 commercial dairy farms throughout England and Wales. Cows were separated for analysis into those housed for the dry period (8,710 cow-dry periods) and those at pasture (9,964 cow-dry periods). Multilevel models were used within a Bayesian framework with 2 response variables, the occurrence of a first case of clinical mastitis within the first 30 d of lactation and time to the first case of clinical mastitis during lactation. A variety of cow and herd management factors were identified as being associated with an increased rate of clinical mastitis and these were found to occur throughout the dry period. Significant cow factors were increased parity and at least one somatic cell count > or = 200,000 cells/mL in the 90 d before drying off. A number of management factors related to hygiene were significantly associated with an increased rate of clinical mastitis. These included measures linked to the administration of dry-cow treatments and management of the early and late dry-period accommodation and calving areas. Other farm factors associated with a reduced rate of clinical mastitis were vaccination with a leptospirosis vaccine, selection of dry-cow treatments for individual cows within a herd rather than for the herd as a whole, routine body condition scoring of cows at drying off, and a pasture rotation policy of grazing dry cows for a maximum of 2 wk before allowing the pasture to remain nongrazed for a period of 4 wk. Models demonstrated a good ability to predict the farm incidence rate of clinical mastitis in a given year, with model predictions explaining over 85% of the variability in the observed data. The research indicates that specific dry-period management strategies have an important influence on the rate of clinical mastitis during the next lactation.",0
https://doi.org/10.1111/aphw.12011,Predicting Psychological Needs and Well-Being of Individuals Engaging in Weight Management: The Role of Important Others,"Background: Using the self-determination theory (SDT) framework, we examined how significant others might support or thwart psychological needs of people with weight management goals, and in turn might affect their psychological well-being and weight control behaviors. Design: Longitudinal design with three sets of questionnaires administered over a 6-month period. Methods: One hundred and fifty-six eligible participants (age = 31.01 ± 13.21 years) were asked to complete questionnaires of SDT-based constructs, weight management behaviors, and psychological well-being. Hypotheses were tested using Bayesian path analysis. Results: Perceived autonomy support from significant others was related to psychological need satisfaction, while controlling behaviors by others were associated with need thwarting. In turn, need satisfaction was associated with some beneficial outcomes such as life satisfaction, and need thwarting was related to some maladaptive outcomes such as higher levels of depressive symptoms and increases in unhealthy diet behaviors. Conclusions: Our findings indicate that the quality of interactions between individuals engaged in weight management and their significant others matters in terms of predicting the psychological needs and well-being of the former.",0
https://doi.org/10.1177/0047287515588593,Unobserved Heterogeneity in Hospitality and Tourism Research,"Despite the growing complexity of structural equation model (SEM) applications in tourism, it is surprising that most applications have estimated these models without accounting for unobserved heterogeneity. In this article, we aim to discuss the concept of unobserved heterogeneity in more detail, highlighting its serious threats to the validity and reliability of SEMs. We describe a Bayesian finite mixture modeling framework for estimating SEMs while accounting for unobserved heterogeneity. We provide a comprehensive description of this model, and provide guidance on its estimation using the WinBUGS software. We illustrate the importance of unobserved heterogeneity and the finite mixture modeling framework using a didactic application on brand equity where heterogeneity is likely to play an important role because of the differences in how consumers perceive the different dimensions of brand equity. We compare between various models and illustrate the differences between the standard and heterogeneous SEM and discuss the implications for research and practice.",0
https://doi.org/10.1287/deca.1080.0126,A Comparison of Two Probability Encoding Methods: Fixed Probability vs. Fixed Variable Values,"We present the results of an experiment comparing two popular methods for encoding probability distributions of continuous variables in decision analysis: eliciting values of a variable, X, through comparisons with a fixed probability wheel and eliciting the percentiles of the cumulative distribution, F(X), through comparisons with fixed values of the variable. We show slight but consistent superiority for the fixed variable method along several dimensions such as monotonicity, accuracy, and precision of the estimated fractiles. The fixed variable elicitation method was also slightly faster and preferred by most participants. We discuss the reasons for its superiority and conclude with several recommendations for the practice of probability assessment.",0
https://doi.org/10.1016/j.jmp.2009.04.002,"Stochastic dynamics of stimulus encoding in schizophrenia: Theory, testing, and application","Abstract Cognitive processing among schizophrenia participants, entailing encoding of presenting stimulation into a format facilitating collateral processes (e.g., memory search), is examined in light of stochastic mathematical models of performance. Results implicate additional encoding operations (encoding subprocesses) as the source of schizophrenia encoding-process elongation. Convergent evidence for this inference, including that from auxiliary neuro-connectionist simulations, are brought forth. Developments from initial, fixed-parameter accounts include random-parameter mixtures, and their Bayesian extensions, formally mediating group-level results to assessment of individual performance. Outgrowths bear on model-selection methodology, according to coherence of group-level and individual-level model functioning (in part addressing the issue of “small-trial-sample model testing”); longitudinal monitoring of encoding-specific treatment response; evaluation of treatment-regimen efficacy with respect to encoding efficiency; and specification of times of measurement interest, in fMRI. The symptom significance of encoding elongation, strongly hinted at by model developments, along with a model-endowed window on exacerbating effects of stress, are drawn out.",0
https://doi.org/10.1080/10705511.2014.919828,Fitting Nonlinear Latent Growth Curve Models With Individually Varying Time Points,"Individual growth trajectories of psychological phenomena are often theorized to be nonlinear. Additionally, individuals’ measurement schedules might be unique. In a structural equation framework, latent growth curve model (LGM) applications typically have either (a) modeled nonlinearity assuming some degree of balance in measurement schedules, or (b) accommodated truly individually varying time points, assuming linear growth. This article describes how to fit 4 popular nonlinear LGMs (polynomial, shape-factor, piecewise, and structured latent curve) with truly individually varying time points, via a definition variable approach. The extension is straightforward for certain nonlinear LGMs (e.g., polynomial and structured latent curve) but in the case of shape-factor LGMs requires a reexpression of the model, and in the case of piecewise LGMs requires introduction of a general framework for imparting piecewise structure, along with tools for its automation. All 4 nonlinear LGMs with individually varying ti...",0
https://doi.org/10.1016/j.brainres.2010.07.064,Influences of intra- and crossmodal grouping on visual and tactile Ternus apparent motion,"Previous studies of dynamic crossmodal integration have revealed that the direction of apparent motion in a target modality can be influenced by a spatially incongruent motion stream in another, distractor modality. Yet, it remains to be examined whether non-motion intra- and crossmodal perceptual grouping can affect apparent motion in a given target modality. To address this question, we employed Ternus apparent-motion displays, which consist of three horizontal aligned visual (or tactile) stimuli that can alternately be seen as either 'element motion' or 'group motion'. We manipulated intra- and crossmodal grouping by cueing the middle stimulus with different cue-target onset asynchronies (CTOAs). In unimodal conditions, we found Ternus apparent motion to be readily biased towards 'element motion' by precues with short or intermediate CTOAs in the visual modality and by precues with short CTOAs in the tactile modality. By contrast, crossmodal precues with short or intermediate CTOAs had no influence on Ternus apparent motion. However, crossmodal synchronous tactile cues led to dominant 'group motion' percepts. And for unimodal visual apparent motion, precues with long CTOAs shifted apparent motion towards 'group motion' in general. The results suggest that intra- and crossmodal interactions on visual and tactile apparent motion take place in different temporal ranges, but both are subject to attentional modulations at long CTOAs.",0
https://doi.org/10.1214/11-ba609,Sensitivity analysis in Bayesian generalized linear mixed models for binary data,"Generalized linear mixed models (GLMMs) enjoy increasing popularity because of their ability to model correlated observations. Integrated nested Laplace approximations (INLAs) provide a fast implementation of the Bayesian approach to GLMMs. However, sensitivity to prior assumptions on the random effects precision parameters is a potential problem. To quantify the sensitivity to prior assumptions, we develop a general sensitivity measure based on the Hellinger distance to assess sensitivity of the posterior distributions with respect to changes in the prior distributions for the precision parameters. In addition, for model selection we suggest several cross-validatory techniques for Bayesian GLMMs with a dichotomous outcome. Although the proposed methodology holds in greater generality, we make use of the developed methods in the particular context of the well-known salamander mating data. We arrive at various new findings with respect to the best fitting model and the sensitivity of the estimates of the model components.",0
https://doi.org/10.1509/jmkr.39.4.479.19124,An Empirical Comparison of Logit Choice Models with Discrete versus Continuous Representations of Heterogeneity,"Currently, there is an important debate about the relative merits of models with discrete and continuous representations of consumer heterogeneity. In a recent JMR study, Andrews, Ansari, and Currim (2002 ; hereafter AAC) compared metric conjoint analysis models with discrete and continuous representations of heterogeneity and found no differences between the two models with respect to parameter recovery and prediction of ratings for holdout profiles. Models with continuous representations of heterogeneity fit the data better than models with discrete representations of heterogeneity. The goal of the current study is to compare the relative performance of logit choice models with discrete versus continuous representations of heterogeneity in terms of the accuracy of household-level parameters, fit, and forecasting accuracy. To accomplish this goal, the authors conduct an extensive simulation experiment with logit models in a scanner data context, using an experimental design based on AAC and other recent simulation studies. One of the main findings is that models with continuous and discrete representations of heterogeneity recover household-level parameter estimates and predict holdout choices about equally well except when the number of purchases per household is small, in which case the models with continuous representations perform very poorly. As in the AAC study, models with continuous representations of heterogeneity fit the data better.",0
https://doi.org/10.1016/j.joi.2008.09.003,How to detect indications of potential sources of bias in peer review: A generalized latent variable modeling approach exemplified by a gender study,"Abstract The universalism norm of the ethos of science requires that contributions to science are not excluded because of the contributors’ gender, nationality, social status, or other irrelevant criteria. Here, a generalized latent variable modeling approach is presented that grant program managers at a funding organization can use in order to obtain indications of potential sources of bias in their peer review process (such as the applicants’ gender). To implement the method, the data required are the number of approved and number of rejected applicants for grants among different groups (for example, women and men or natural and social scientists). Using the generalized latent variable modeling approach indications of potential sources of bias can be examined not only for grant peer review but also for journal peer review.",0
https://doi.org/10.3758/bf03193560,Comparing and unifying slope estimates across psychometric function models,"The psychometric function relating stimulus intensity to response probability generally presents itself as a monotonically increasing sigmoid profile. Two summary parameters of the function are particularly important as measures of perceptual performance: the threshold parameter, which defines the location of the function over the stimulus axis (abscissa), and the slope parameter, which defines the (local) rate at which response probability increases with increasing stimulus intensity. In practice, the psychometric function may be modeled by a variety of mathematical structures, and the resulting algebraic expression describing the slope parameter may vary considerably between different functions fitted to the same experimental data. This variation often restricts comparisons between studies that select different functions and compromises the general interpretation of slope values. This article reviews the general characteristics of psychometric function models, discusses three strategies for resolving the issue of slope value differences, and presents mathematical expressions for implementing each strategy.",0
https://doi.org/10.1207/s15328007sem1203_2,"The Relation Among Fit Indexes, Power, and Sample Size in Structural Equation Modeling","The relation among fit indexes, power, and sample size in structural equation modeling is examined. The noncentrality parameter is required to compute power. The 2 existing methods of computing power have estimated the noncentrality parameter by specifying an alternative hypothesis or alternative fit. These methods cannot be implemented easily and reliably. In this study, 4 fit indexes (RMSEA, CFI, McDonald's Fit Index, and Steiger's gamma) were used to compute the noncentrality parameter and sample size to achieve certain level of power. The resulting power and sample size varied as a function of (a) choice of fit index, (b) number of variables/degrees of freedom, (c) relation among the variables, and (d) value of the fit index. However, if the level of misspecification were held constant, then the resulting power and sample size would be identical.",0
https://doi.org/10.1080/00273171.2014.1003770,Bayesian Causal Mediation Analysis for Group Randomized Designs with Homogeneous and Heterogeneous Effects: Simulation and Case Study,"A fully Bayesian approach to causal mediation analysis for group-randomized designs is presented. A unique contribution of this article is the combination of Bayesian inferential methods with G-computation to address the problem of heterogeneous treatment or mediator effects. A detailed simulation study shows that this approach has excellent frequentist properties, particularly in the case of small sample sizes with accurate informative priors. The simulation study also demonstrates that the proposed approach can take into account heterogeneous treatment or mediator effects without bias. A case study using data from a school-based randomized intervention designed to increase parent social capital leading to improved behavioral and academic outcomes in children is offered to illustrate the Bayesian approach to causal mediation in group-randomized designs.",0
https://doi.org/10.1016/s0047-2727(01)00167-0,Does 401(k) eligibility increase saving?,"By comparing 401(k) eligible and ineligible households’ wealth, this paper estimates that, on average, about one half of 401(k) balances represent new private savings, and about one quarter of 401(k) balances represent new national savings. Responses to eligibility vary considerably, however, with households who normally save the most largely contributing funds they would have saved anyway. This paper improves on previous research by: (1) employing propensity score subclassification to control more completely for observed household characteristics, (2) controlling for more household characteristics, including several correlated with unobserved savings preferences, and (3) adjusting the observed measure of households’ wealth to reduce measurement error.",0
https://doi.org/10.1007/bf02294846,Analysis of structural equation model with ignorable missing continuous and polytomous data,"The main purpose of this article is to develop a Bayesian approach for structural equation models with ignorable missing continuous and polytomous data. Joint Bayesian estimates of thresholds, structural parameters and latent factor scores are obtained simultaneously. The idea of data augmentation is used to solve the computational difficulties involved. In the posterior analysis, in addition to the real missing data, latent variables and latent continuous measurements underlying the polytomous data are treated as hypothetical missing data. An algorithm that embeds the Metropolis-Hastings algorithm within the Gibbs sampler is implemented to produce the Bayesian estimates. A goodness-of-fit statistic for testing the posited model is presented. It is shown that the proposed approach is not sensitive to prior distributions and can handle situations with a large number of missing patterns whose underlying sample sizes may be small. Computational efficiency of the proposed procedure is illustrated by simulation studies and a real example.",0
https://doi.org/10.1534/genetics.115.186114,Simple Penalties on Maximum-Likelihood Estimates of Genetic Parameters to Reduce Sampling Variation,"Abstract Multivariate estimates of genetic parameters are subject to substantial sampling variation, especially for smaller data sets and more than a few traits. A simple modification of standard, maximum-likelihood procedures for multivariate analyses to estimate genetic covariances is described, which can improve estimates by substantially reducing their sampling variances. This is achieved by maximizing the likelihood subject to a penalty. Borrowing from Bayesian principles, we propose a mild, default penalty—derived assuming a Beta distribution of scale-free functions of the covariance components to be estimated—rather than laboriously attempting to determine the stringency of penalization from the data. An extensive simulation study is presented, demonstrating that such penalties can yield very worthwhile reductions in loss, i.e., the difference from population values, for a wide range of scenarios and without distorting estimates of phenotypic covariances. Moreover, mild default penalties tend not to increase loss in difficult cases and, on average, achieve reductions in loss of similar magnitude to computationally demanding schemes to optimize the degree of penalization. Pertinent details required for the adaptation of standard algorithms to locate the maximum of the likelihood function are outlined.",0
https://doi.org/10.1177/0013164413496812,Piecewise Linear–Linear Latent Growth Mixture Models With Unknown Knots,"Latent growth curve models with piecewise functions are flexible and useful analytic models for investigating individual behaviors that exhibit distinct phases of development in observed variables. As an extension of this framework, this study considers a piecewise linear–linear latent growth mixture model (LGMM) for describing segmented change of individual behavior over time where the data come from a mixture of two or more unobserved subpopulations (i.e., latent classes). Thus, the focus of this article is to illustrate the practical utility of piecewise linear–linear LGMM and then to demonstrate how this model could be fit as one of many alternatives—including the more conventional LGMMs with functions such as linear and quadratic. To carry out this study, data ( N = 214) obtained from a procedural learning task research were used to fit the three alternative LGMMs: (a) a two-class LGMM using a linear function, (b) a two-class LGMM using a quadratic function, and (c) a two-class LGMM using a piecewise linear–linear function, where the time of transition from one phase to another (i.e., knot) is not known a priori, and thus is a parameter to be estimated.",0
https://doi.org/10.1111/j.2517-6161.1993.tb01466.x,Bayesian Computation Via the Gibbs Sampler and Related Markov Chain Monte Carlo Methods,"The use of the Gibbs sampler for Bayesian computation is reviewed and illustrated in the context of some canonical examples. Other Markov chain Monte Carlo simulation methods are also briefly described, and comments are made on the advantages of sample-based approaches for Bayesian inference summaries",0
https://doi.org/10.1177/1754073915590619,Modeling Affect Dynamics: State of the Art and Future Challenges,"The current article aims to provide an up-to-date synopsis of available techniques to study affect dynamics using intensive longitudinal data (ILD). We do so by introducing the following eight dichotomies that help elucidate what kind of data one has, what process aspects are of interest, and what research questions are being considered: (1) single- versus multiple-person data; (2) univariate versus multivariate models; (3) stationary versus nonstationary models; (4) linear versus nonlinear models; (5) discrete time versus continuous time models; (6) discrete versus continuous variables; (7) time versus frequency domain; and (8) modeling the process versus computing descriptives. In addition, we discuss what we believe to be the most urging future challenges regarding the modeling of affect dynamics.",0
https://doi.org/10.1111/j.1541-0420.2007.00924.x,An Application of a Mixed-Effects Location Scale Model for Analysis of Ecological Momentary Assessment (EMA) Data,"For longitudinal data, mixed models include random subject effects to indicate how subjects influence their responses over repeated assessments. The error variance and the variance of the random effects are usually considered to be homogeneous. These variance terms characterize the within-subjects (i.e., error variance) and between-subjects (i.e., random-effects variance) variation in the data. In studies using ecological momentary assessment (EMA), up to 30 or 40 observations are often obtained for each subject, and interest frequently centers around changes in the variances, both within and between subjects. In this article, we focus on an adolescent smoking study using EMA where interest is on characterizing changes in mood variation. We describe how covariates can influence the mood variances, and also extend the standard mixed model by adding a subject-level random effect to the within-subject variance specification. This permits subjects to have influence on the mean, or location, and variability, or (square of the) scale, of their mood responses. Additionally, we allow the location and scale random effects to be correlated. These mixed-effects location scale models have useful applications in many research areas where interest centers on the joint modeling of the mean and variance structure.",0
https://doi.org/10.1111/1467-9248.12176,Calling European Union Treaty Referendums: Electoral and Institutional Politics,"Many European integration treaties – most notably the failed Constitutional Treaty – have faced ratification by referendum in various member states. Although the literature on voting behaviour in these referendums in now well established, the reasons why these referendums were held in the first place is under-scrutinised. This article examines the reasons EU member states decide to call referendums in order to ratify EU treaties and argues that they do so primarily as a result of domestic political pressure arising from three sources: electoral pressure when the EU is unpopular and elections are close, rules governing the use of referendums, and domestic institutional veto players. This theory is tested using a combination of single and multi-level logistic regression analysis, which finds support for the hypotheses developed here.",0
https://doi.org/10.1207/s15328007sem0903_2,The Impact of Categorization With Confirmatory Factor Analysis,"This study investigated the impact of categorization on confirmatory factor analysis (CFA) parameter estimates, standard errors, and 5 ad hoc fit indexes. Models were generated that represented empirical research situations in terms of model size, sample sizes, and loading values. CFA results obtained from analysis of normally distributed, continuous data were compared to results obtained from 5-category Likert-type data with normal distributions. The ordered categorical data were analyzed using the estimators: Weighted Least Squares (WLS; with polychoric correlation [PC] input) and Maximum Likelihood (ML; with Pearson Product-Moment [PPM] input). ML-PPM-based parameter estimates reported moderate levels of negative bias for all conditions, WLS-PC-based standard errors showed high amounts of bias, especially with a small sample size and moderate loading values. With nonnormally distributed, ordered categorical data, ML-PPM-based parameter estimates, standard errors, and factor intercorrelation showed high...",0
https://doi.org/10.1111/1467-9868.00353,Bayesian measures of model complexity and fit,"Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the ‘hat’ matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.",0
https://doi.org/10.1002/sim.5380,Estimation of mediation effects for zero-inflated regression models,"The goal of mediation analysis is to identify and explicate the mechanism that underlies a relationship between a risk factor and an outcome via an intermediate variable (mediator). In this paper, we consider the estimation of mediation effects in zero-inflated (ZI) models intended to accommodate 'extra' zeros in count data. Focusing on the ZI negative binomial models, we provide a mediation formula approach to estimate the (overall) mediation effect in the standard two-stage mediation framework under a key sequential ignorability assumption. We also consider a novel decomposition of the overall mediation effect for the ZI context using a three-stage mediation model. Estimation of the components of the overall mediation effect requires an assumption involving the joint distribution of two counterfactuals. Simulation study results demonstrate low bias of mediation effect estimators and close-to-nominal coverage probability of confidence intervals. We also modify the mediation formula method by replacing 'exact' integration with a Monte Carlo integration method. The method is applied to a cohort study of dental caries in very low birth weight adolescents. For overall mediation effect estimation, sensitivity analysis was conducted to quantify the degree to which key assumption must be violated to reverse the original conclusion.",0
https://doi.org/10.2307/2669750,Asymptotic Distribution of P Values in Composite Null Models,"We investigate the compatibility of a null model H0 with the data by calculating a p value; that is, the probability, under H0, that a given test statistic T exceeds its observed value. When the null model consists of a single distribution, the p value is readily obtained, and it has a uniform distribution under H0. On the other hand, when the null model depends on an unknown nuisance parameter Î¸, one must somehow get rid of Î¸, (e.g., by estimating it) to calculate a p value. Various proposals have been suggested to ""remove"" Î¸, each yielding a different candidate p value. But unlike the simple case, these p values typically are not uniformly distributed under the null model. In this article we investigate their asymptotic distribution under H0. We show that when the asymptotic mean of the test statistic T depends on Î¸, the posterior predictive p value of Guttman and Rubin, and the plug-in p value are conservative (i.e., their asymptotic distributions are more concentrated around 1/2 than a uniform), with the posterior predictive p value being the more conservative. In contrast, the partial posterior predictive and conditional predictive p values of Bayarri and Berger are asymptotically uniform. Furthermore, we show that the discrepancy p value of Meng and Gelman and colleagues can be conservative, even when the discrepancy measure has mean 0 under the null model. We also describe ways to modify the conservative p values to make their distributions asymptotically uniform.",0
https://doi.org/10.1016/j.csda.2008.03.029,On Bayesian estimation and model comparison of an integrated structural equation model,"In this paper, we introduce a Bayesian approach to the estimation and model comparison of an integrated two-level nonlinear structural equation model with mixed continuous, dichotomous, and ordered categorical data that may be missing at random. This general model can accommodate nonlinearities of latent variables and the effects of fixed covariates on measurement and structural equations in within-groups and between-groups models. A sampling-based algorithm that combines the Gibbs sampler and the Metropolis-Hastings algorithm is proposed for posterior simulation. A procedure that utilizes path sampling is implemented to compute the Bayes factor for model comparison under the framework of the proposed integrated model. Empirical performances of Bayesian methodologies are illustrated via analysis of a real example.",0
https://doi.org/10.1207/s15327906mbr3502_3,Bayesian Interval Estimation of Multiple Correlations with Missing Data: A Gibbs Sampling Approach.,"A Bayesian method for obtaining an interval estimate of the population squared multiple correlation from an incomplete multivariate normal data set is described. The method is applicable to data sets where values are missing on any combination of the dependent and independent variables. Further, the missing data need not be missing in a completely random fashion. The estimates are constructed using a Markov Chain Monte Carlo procedure known as Gibbs Sampling. The important issues of the convergence properties of the Gibbs sampler, the effect of the choice of a reference prior, and the empirical coverage probabilities of the estimates are considered in detail. Investigations using simulated data suggest that the proposed method can yield accurate interval estimates of the population squared multiple correlation.",0
https://doi.org/10.1080/10705511.2011.607714,General Growth Mixture Analysis of Adolescents' Developmental Trajectories of Anxiety: The Impact of Untested Invariance Assumptions on Substantive Interpretations,"Substantively, this study investigates potential heterogeneity in the developmental trajectories of anxiety in adolescence. Methodologically, this study demonstrates the usefulness of general growth mixture analysis (GGMA) in addressing these issues and illustrates the impact of untested invariance assumptions on substantive interpretations. This study relied on data from the Montreal Adolescent Depression Development Project (MADDP), a 4-year follow-up of more than 1,000 adolescents who completed the Beck Anxiety Inventory each year. GGMA models relying on different invariance assumptions were empirically compared. Each of these models converged on a 5-class solution, but yielded different substantive results. The model with class-varying variance–covariance matrices was retained as providing a better fit to the data. These results showed that although elevated levels of anxiety might fluctuate over time, they clearly do not represent a transient phenomenon. This model was then validated in relation to m...",0
https://doi.org/10.1111/j.2044-8295.1990.tb02372.x,A ‘sheep-goat effect’ in repetition avoidance: Extra-sensory perception as an effect of subjective probability?,"Performance on extra-sensory perception (ESP) and on subjective random generation (SRG) has been shown independently to differ along a number of personality dimensions. One of the variables known consistently to influence ESP performance but not studied yet in SRG experiments is belief in the paranormal. We report three experiments on subjective randomness as a function of belief in ESP: (1) a retrospective analysis of randomness in multiple choice answers of a telepathy experiment revealed that believers in ESP (‘sheep’) avoided repetitive responses significantly more than non-believers (‘goats’); (2) in an experiment where subjects had to produce random strings of the digits 1−6, sheep avoided repetitions significantly more than goats; (3) in an experiment where subjects had to compare equiprobable short random sequences, sheep underestimated the number of mathematically expected repetitions significantly more than goats. In all three experiments a stronger bias against the incidence of direct repetitions was found for subjects believing in ESP than for those denying its possibility. This may indicate that believers are more prone to an illusion of causality in the face of everyday coincidences. We suggest that effects of subjective probability and, particularly, of subjective randomness should be considered in future studies concerned with individual differences in ESP scoring.",0
https://doi.org/10.1002/sim.1264,Multilevel modelling of medical data,This tutorial presents an overview of multilevel or hierarchical data modelling and its applications in medicine. A description of the basic model for nested data is given and it is shown how this can be extended to fit flexible models for repeated measures data and more complex structures involving cross-classifications and multiple membership patterns within the software package MLwiN. A variety of response types are covered and both frequentist and Bayesian estimation methods are described.,0
https://doi.org/10.1002/bimj.200390021,Small Sample Correction for the Variance of GEE Estimators,"When clustered multinomial responses are fit using the generalized logistic link, Morel (1989) introduced a small sample correction in the Taylor series based estimator of the covariance matrix of the parameter estimates. The correction reduces the bias of the Type I error rates in small samples and guarantees positive definiteness of the estimated variance-covariance matrix. It is well known that small sample bias in the use of the Delta method persists in any application of the Generalized Estimating Equations (GEE) methodology. In this article, we extend the correction originally suggested for the generalized logistic link, to other link functions and distributions, when parameters are estimated by GEE. In a Monte Carlo study with correlated data generated under different sampling schemes, the small sample correction has been shown to be effective in reducing the Type I error rates when the number of clusters is relatively small.",0
https://doi.org/10.1007/s11336-014-9417-x,Get Over It! A Multilevel Threshold Autoregressive Model for State-Dependent Affect Regulation,"Intensive longitudinal data provide rich information, which is best captured when specialized models are used in the analysis. One of these models is the multilevel autoregressive model, which psychologists have applied successfully to study affect regulation as well as alcohol use. A limitation of this model is that the autoregressive parameter is treated as a fixed, trait-like property of a person. We argue that the autoregressive parameter may be state-dependent, for example, if the strength of affect regulation depends on the intensity of affect experienced. To allow such intra-individual variation, we propose a multilevel threshold autoregressive model. Using simulations, we show that this model can be used to detect state-dependent regulation with adequate power and Type I error. The potential of the new modeling approach is illustrated with two empirical applications that extend the basic model to address additional substantive research questions.",0
https://doi.org/10.1139/cjfr-2012-0268,Population-averaged predictions with generalized linear mixed-effects models in forestry: an estimator based on Gauss−Hermite quadrature,"Data in forestry are often spatially and (or) serially correlated. In the last two decades, mixed models have become increasingly popular for the analysis of such data because they can relax the assumption of independent observations. However, when the relationship between the response variable and the covariates is nonlinear, as is the case in generalized linear mixed models (GLMMs), population-averaged predictions cannot be obtained from the fixed effects alone. This study proposes an estimator, which is based on a five-point Gauss−Hermite quadrature, for population-averaged predictions in the context of GLMM. The estimator was tested through Monte Carlo simulation and compared with a regular generalized linear model (GLM). The estimator was also applied to a real-world case study, a harvest model. The results showed that GLM predictions were unbiased but that their confidence intervals did not achieve their nominal coverage. On the other hand, the proposed estimator yielded unbiased predictions with reliable confidence intervals. The predictions based on the fixed effects of a GLMM exhibited the largest biases. If statistical inferences are needed, the proposed estimator should be used. It is easily implemented as long as the random effect specification does not contain multiple random effects for the same hierarchical level.",0
https://doi.org/10.3102/1076998614546494,Modeling Heterogeneous Variance–Covariance Components in Two-Level Models,"Applications of multilevel models to continuous outcomes nearly always assume constant residual variance and constant random effects variances and covariances. However, modeling heterogeneity of variance can prove a useful indicator of model misspecification, and in some educational and behavioral studies, it may even be of direct substantive interest. The purpose of this article is to review, describe, and illustrate a set of recent extensions to two-level models that allow the residual and random effects variance–covariance components to be specified as functions of predictors. These predictors can then be entered with random coefficients to allow the Level-1 heteroscedastic relationships to vary across Level-2 units. We demonstrate by simulation that ignoring Level-2 variability in residual variances leads the Level-1 variance function regression coefficients to be estimated with spurious precision. We discuss software options for fitting these extensions, and we illustrate them by reanalyzing the classic High School and Beyond data and two-level school effects models presented by Raudenbush and Bryk.",0
https://doi.org/10.1111/j.1365-2729.2010.00368.x,Adaptive item-based learning environments based on the item response theory: possibilities and challenges,"The popularity of intelligent tutoring systems (ITSs) is increasing rapidly. In order to make learning environments more efficient, researchers have been exploring the possibility of an automatic adaptation of the learning environment to the learner or the context. One of the possible adaptation techniques is adaptive item sequencing by matching the difficulty of the items to the learner's knowledge level. This is already accomplished to a certain extent in adaptive testing environments, where the test is tailored to the person's ability level by means of the item response theory (IRT). Even though IRT has been a prevalent computerized adaptive test (CAT) approach for decades and applying IRT in item-based ITSs could lead to similar advantages as in CAT (e.g. higher motivation and more efficient learning), research on the application of IRT in such learning environments is highly restricted or absent. The purpose of this paper was to explore the feasibility of applying IRT in adaptive item-based ITSs. Therefore, we discussed the two main challenges associated with IRT application in such learning environments: the challenge of the data set and the challenge of the algorithm. We concluded that applying IRT seems to be a viable solution for adaptive item selection in item-based ITSs provided that some modifications are implemented. Further research should shed more light on the adequacy of the proposed solutions.",0
https://doi.org/10.1159/000351642,Statistical Power of Multilevel Modelling in Dental Caries Clinical Trials: A Simulation Study,"Outcome data from dental caries clinical trials have a naturally hierarchical structure, with surfaces clustered within teeth, clustered within individuals. Data are often aggregated into the DMF index for each individual, losing tooth- and surface-specific information. If these data are to be analysed by tooth or surface, allowing exploration of effects of interventions on different teeth and surfaces, appropriate methods must be used to adjust for the clustered nature of the data. Multilevel modelling allows analysis of clustered data using individual observations without aggregating data, and has been little used in the field of dental caries. A simulation study was conducted to investigate the performance of multilevel modelling methods and standard caries increment analysis. Data sets were simulated from a three-level binomial distribution based on analysis of a caries clinical trial in Scottish adolescents, with varying sample sizes, treatment effects and random tooth level effects based on trials reported in Cochrane reviews of topical fluoride, and analysed to compare the power of multilevel models and traditional analysis. 40,500 data sets were simulated. Analysis showed that estimated power for the traditional caries increment method was similar to that for multilevel modelling, with more variation in smaller data sets. Multilevel modelling may not allow significant reductions in the number of participants required in a caries clinical trial, compared to the use of traditional analyses, but investigators interested in exploring the effect of their intervention in more detail may wish to consider the application of multilevel modelling to their clinical trial data.",0
,Back to the Future: Effects of Olfaction induced Episodic Memories on Consumer Creativity and Innovation Adoption,,0
https://doi.org/10.1027/1015-5759.23.4.258,Assessing Mood in Daily Life,"Abstract. The repeated measurement of moods in everyday life, as is common in ambulatory monitoring, requires parsimonious scales, which may challenge the reliability of the measures. The current paper evaluates the factor structure, the reliability, and the sensitivity to change of a six-item mood scale designed for momentary assessment in daily life. We analyzed data from 187 participants who reported their current mood four times per day during seven consecutive days using a multilevel approach. The results suggest that the proposed three factors Calmness, Valence, and Energetic arousal are appropriate to assess fluctuations within persons over time. However, calmness and valence are not distinguishable at the between-person level. Furthermore, the analyses showed that two-item scales provide measures that are reliable at the different levels and highly sensitive to change.",0
https://doi.org/10.1002/sim.2934,Recent developments in meta-analysis,"The art and science of meta-analysis, the combination of results from multiple independent studies, is now more than a century old. In the last 30 years, however, as the need for medical research and clinical practice to be based on the totality of relevant and sound evidence has been increasingly recognized, the impact of meta-analysis has grown enormously. In this paper, we review highlights of recent developments in meta-analysis in medical research. We outline in particular how emphasis has been placed on (i) heterogeneity and random-effects analyses; (ii) special consideration in different areas of application; (iii) assessing bias within and across studies; and (iv) extension of ideas to complex evidence synthesis. We conclude the paper with some remarks on ongoing challenges and possible directions for the future.",0
https://doi.org/10.1007/bf02294363,On the relationship between item response theory and factor analysis of discretized variables,"Equivalence of marginal likelihood of the two-parameter normal ogive model in item response theory (IRT) and factor analysis of dichotomized variables (FA) was formally proved. The basic result on the dichotomous variables was extended to multicategory cases, both ordered and unordered categorical data. Pair comparison data arising from multiple-judgment sampling were discussed as a special case of the unordered categorical data. A taxonomy of data for the IRT and FA models was also attempted. Ã‚Â© 1987 The Psychometric Society.",0
https://doi.org/10.1162/089892999563571,Aging and Recognition Memory: Changes in Regional Cerebral Blood Flow Associated with Components of Reaction Time Distributions,"Abstract We used H2 15O positron emission tomography (PET) to measure age-related changes in regional cerebral blood flow (rCBF) during a verbal recognition memory task. Twelve young adults (20 to 29 years) and 12 older adults (62 to 79 years) participated. Separate PET scans were conducted during Encoding, Baseline, and Retrieval conditions. Each of the conditions involved viewing a series of 64 words and making a two-choice response manually. The complete reaction time (RT) distributions in each task condition were characterized in terms of an ex-Gaussian model (convolution of exponential and Gaussian functions). Parameter estimates were obtained for the mean of the exponential component (τ), representing a task-specific decision process and the mean of the Gaussian component (μ), representing residual sensory coding and response processes. Independently of age group, both μ and τ were higher in the Encoding and Retrieval conditions than in the Baseline condition, and τ was higher during Retrieval than during Encoding. Age-related slowing in task performance was evident primarily in μ. For young adults, rCBF activation in the right prefrontal cortex, in the Retrieval condition, was correlated positively with μ but not with τ. For older adults, rCBF changes (both increases and decreases) in several cortical regions were correlated with both μ and τ. The data suggest that the attentional demands of this task are relatively greater for older adults and consequently lead to the recruitment of additional neural systems during task performance.",0
https://doi.org/10.1111/1365-2745.12421,Fuels and fires influence vegetation via above- and belowground pathways in a high-diversity plant community,"Summary Fire strongly influences plant populations and communities around the world, making it an important agent of plant evolution. Fire influences vegetation through multiple pathways, both above- and belowground. Few studies have yet attempted to tie these pathways together in a mechanistic way through soil heating even though the importance of soil heating for plants in fire-prone ecosystems is increasingly recognized. Here we combine an experimental approach with structural equation modelling (SEM) to simultaneously examine multiple pathways through which fire might influence herbaceous vegetation. In a high-diversity longleaf pine groundcover community in Louisiana, USA, we manipulated fine-fuel biomass and monitored the resulting fires with high-resolution thermocouples placed in vertical profile above- and belowground. We predicted that vegetation response to burning would be inversely related to fuel load owing to relationships among fuels, fire temperature, duration and soil heating. We found that fuel manipulations altered fire properties and vegetation responses, of which soil heating proved to be a highly accurate predictor. Fire duration acting through soil heating was important for vegetation response in our SEMs, whereas fire temperature was not. Our results indicate that in this herbaceous plant community, fire duration is a good predictor of soil heating and therefore of vegetation response to fire. Soil heating may be the key determinant of vegetation response to fire in ecosystems wherein plants persist by resprouting or reseeding from soil-stored propagules. Synthesis. Our SEMs demonstrate how the complex pathways through which fires influence plant community structure and dynamics can be examined simultaneously. Comparative studies of these pathways across different communities will provide important insights into the ecology, evolution and conservation of fire-prone ecosystems.",0
https://doi.org/10.1016/j.csda.2010.03.008,Default Bayesian model determination methods for generalised linear mixed models,"A default strategy for fully Bayesian model determination for generalised linear mixed models (GLMMs) is considered which addresses the two key issues of default prior specification and computation. In particular, the concept of unit-information priors is extended to the parameters of a GLMM. A combination of Markov chain Monte Carlo (MCMC) and Laplace approximations is used to compute approximations to the posterior model probabilities to find a subset of models with high posterior model probability. Bridge sampling is then used on the models in this subset to approximate the posterior model probabilities more accurately. The strategy is applied to four examples.",0
https://doi.org/10.1037/1082-989x.8.3.364,Mixture or Homogeneous? Comment on Bauer and Curran (2003).,"D. J. Bauer and P. J. Curran (2003) raised some interesting issues with respect to mixture models of growth curves. Many useful lessons can be learned from their work, and more can be learned by extending the inquiry in related directions. These lessons involve the following issues: (a) what a mixture distribution looks like, (b) the meaning of the term homogeneous distribution, (c) the importance of model checking, (d) advantages and disadvantages of using mixtures and similar procedures to approximate complicated distributions, and (e) intrinsic versus nonintrinsic transformability.",0
https://doi.org/10.1037/0022-006x.74.3.500,A comparison of girls' and boys' aggressive-disruptive behavior trajectories across elementary school: Prediction to young adult antisocial outcomes.,"Multiple group analysis and general growth mixture modeling was used to determine whether aggressive- disruptive behavior trajectories during elementary school, and their association with young adulthood antisocial outcomes, vary by gender. Participants were assessed longitudinally beginning at age 6 as part of an evaluation of 2 school-based preventive programs. Two analogous trajectories were found for girls and boys: chronic high aggression- disruption (CHAD) and stable low aggression- disruption (LAD). A 3rd class of low moderate aggression- disruption (LMAD) for girls and increasing aggression- disruption (IAD) for boys also was found. Girls and boys in analogous CHAD classes did not differ in trajectory level and course, but girls in the CHAD and LAD classes had lower rates of antisocial outcomes than boys. Girls with the LMAD trajectory differed from boys with the IAD trajectory.",0
https://doi.org/10.1016/j.jenvrad.2015.05.007,Foliar interception of radionuclides in dry conditions: a meta-analysis using a Bayesian modeling approach,"Uncertainty on the parameters that describe the transfer of radioactive materials into the (terrestrial) environment may be characterized thanks to datasets such as those compiled within International Atomic Energy Agency (IAEA) documents. Nevertheless, the information included in these documents is too poor to derive a relevant and informative uncertainty distribution regarding dry interception of radionuclides by the pasture grass and the leaves of vegetables. In this paper, 145 sets of dry interception measurements by the aboveground biomass of specific plants were collected from published scientific papers. A Bayesian meta-analysis was performed to derive the posterior probability distributions of the parameters that reflect their uncertainty given the collected data. Four competing models were compared in terms of both fitting performances and predictive abilities to reproduce plausible dry interception data. The asymptotic interception factor, applicable whatever the species and radionuclide to the highest aboveground biomass values (e.g. mature leafy vegetables), was estimated with the best model, to be 0.87 with a 95% credible interval (0.85, 0.89).",0
https://doi.org/10.1207/s15328007sem1302_2,On the Performance of Maximum Likelihood Versus Means and Variance Adjusted Weighted Least Squares Estimation in CFA,"This simulation study compared maximum likelihood (ML) estimation with weighted least squares means and variance adjusted (WLSMV) estimation. The study was based on confirmatory factor analyses with 1, 2, 4, and 8 factors, based on 250, 500, 750, and 1,000 cases, and on 5, 10, 20, and 40 variables with 2, 3, 4, 5, and 6 categories. There was no model misspecification. The most important results were that with 2 and 3 categories the rejection rates of the WLSMV chi-square test corresponded much more to the expected rejection rates according to an alpha level of. 05 than the rejection rates of the ML chi-square test. The magnitude of the loadings was more precisely estimated by means of WLSMV when the variables had only 2 or 3 categories. The sample size for WLSMV estimation needed not to be larger than the sample size for ML estimation.",0
https://doi.org/10.1177/1065912913491464,Traits versus Issues,"As female candidates may face greater challenges in establishing their “qualifications” for office, coverage of their personal traits may be pernicious, because it tends to de-emphasize substantive qualifications. This study focuses on relative amounts of trait and issue coverage of contests with and without women candidates. We find that races with female candidates yield more coverage of traits than male versus male contests and races with female candidates are less likely to generate issue coverage than trait coverage. Candidate gender and office interact; female gubernatorial candidates are most likely to garner trait coverage and least likely to engender issue coverage.",0
https://doi.org/10.1124/jpet.114.222315,Selective Monoacylglycerol Lipase Inhibitors: Antinociceptive versus Cannabimimetic Effects in Mice,"The endogenous cannabinoid 2-arachidonoylglycerol (2-AG) plays an important role in a variety of physiologic processes, but its rapid breakdown by monoacylglycerol lipase (MAGL) results in short-lived actions. Initial MAGL inhibitors were limited by poor selectivity and low potency. In this study, we tested JZL184 [4-nitrophenyl 4-[bis(2H-1,3-benzodioxol-5-yl)(hydroxy)methyl]piperidine-1-carboxylate] and MJN110 [2,5-dioxopyrrolidin-1-yl 4-(bis(4-chlorophenyl)methyl)piperazine-1-carboxylate], MAGL inhibitors that possess increased selectivity and potency, in mouse behavioral assays of neuropathic pain [chronic constriction injury (CCI) of the sciatic nerve], interoceptive cannabimimetic effects (drug-discrimination paradigm), and locomotor activity in an open field test. MJN110 (1.25 and 2.5 mg/kg) and JZL184 (16 and 40 mg/kg) significantly elevated 2-AG and decreased arachidonic acid but did not affect anandamide in whole brains. Both MAGL inhibitors significantly reduced CCI-induced mechanical allodynia with the following potencies [ED50 (95% confidence limit [CL]) values in mg/kg: MJN110 (0.43 [0.30-0.63]) > JZL184 (17.8 [11.6-27.4])] and also substituted for the potent cannabinoid receptor agonist CP55,940 [2-[(1R,2R,5R)-5-hydroxy-2-(3-hydroxypropyl)cyclohexyl]-5-(2-methyloctan-2-yl)phenol] in the drug-discrimination paradigm [ED50 (95% CL) values in mg/kg: MJN110 (0.84 [0.69-1.02]) > JZL184 (24.9 [14.6-42.5])]; however, these compounds elicited differential effects on locomotor behavior. Similar to cannabinoid 1 (CB1) receptor agonists, JZL184 produced hypomotility, whereas MJN110 increased locomotor behavior and did not produce catalepsy or hypothermia. Although both drugs substituted for CP55,940 in the drug discrimination assay, MJN110 was more potent in reversing allodynia in the CCI model than in producing CP55,940-like effects. Overall, these results suggest that MAGL inhibition may alleviate neuropathic pain, while displaying limited cannabimimetic effects compared with direct CB1 receptor agonists.",0
https://doi.org/10.1016/0030-5073(76)90022-2,Task complexity and contingent processing in decision making: An information search and protocol analysis,"Abstract Two process tracing techniques, explicit information search and verbal protocols, were used to examine the information processing strategies subjects use in reaching a decision. Subjects indicated preferences among apartments. The number of alternatives available and number of dimensions of information available was varied across sets of apartments. When faced with a two alternative situation, the subjects employed search strategies consistent with a compensatory decision process. In contrast, when faced with a more complex (multialternative) decision task, the subjects employed decision strategies designed to eliminate some of the available alternatives as quickly as possible and on the basis of a limited amount of information search and evaluation. The results demonstrate that the information processing leading to choice will vary as a function of task complexity. An integration of research in decision behavior with the methodology and theory of more established areas of cognitive psychology, such as human problem solving, is advocated.",0
https://doi.org/10.1007/bf00994135,Latent class metric conjoint analysis,"A latent class methodology for conjoint analysis is proposed, which simultaneously estimates market segment membership and part-worth utilities for each derived market segment using mixtures of multivariate conditional normal distributions. An E-M algorithm to estimate the parameters of these mixtures is briefly discussed. Finally, an application of the methodology to a commercial study (pretest) examining the design of a remote automobile entry device is presented.",0
https://doi.org/10.1109/whc.2013.6548417,Tactile flash lag effect: Taps with changing intensities lead briefly flashed taps,"A brief flash, presented in spatial alignment with a moving object, appears to lag behind the moving object. This illusion has been known as the visual flash lag effect. Sheth et al. (2000) demonstrated that when observers compared the colors of a linear array of flashing circular disks with that of a concurrently-flashed disk of the same color, the former was perceived to lead the latter. An analogous tactile flash-lag illusion is reported here. A sequence of tactile taps with increasing (or decreasing) intensity was presented on one finger. The temporally-middle tap in the sequence was perceived to be weaker in both cases than a synchronous single tap on another finger. This illusion could be accounted by a mechanism of temporal compensation and tactile masking.",0
https://doi.org/10.1348/000711000159303,Statistical analysis of nonlinear structural equation models with continuous and polytomous data,"A general nonlinear structural equation model with mixed continuous and polytomous variables is analysed. A Bayesian approach is proposed to estimate simultaneously the thresholds, the structural parameters and the latent variables. To solve the computational difficulties involved in the posterior analysis, a hybrid Markov chain Monte Carlo method that combines the Gibbs sampler and the Metropolis-Hasting algorithm is implemented to produce the Bayesian solution. Statistical inferences, which involve estimation of parameters and their standard errors, residuals and outliers analyses, and goodness-of-fit statistics for testing the posited model, are discussed. The proposed procedure is illustrated by a simulation study and a real example.",0
https://doi.org/10.1111/j.1467-8721.2009.01619.x,The New Person-Specific Paradigm in Psychology,"Most research methodology in the behavioral sciences employs interindividual analyses, which provide information about the state of affairs of the population. However, as shown by classical mathematical-statistical theorems (the ergodic theorems), such analyses do not provide information for, and cannot be applied at, the level of the individual, except on rare occasions when the processes of interest meet certain stringent conditions. When psychological processes violate these conditions, the interindividual analyses that are now standardly applied have to be replaced by analysis of intraindividual variation in order to obtain valid results. Two illustrations involving analysis of intraindividual variation of personality and emotional processes are given.",0
https://doi.org/10.1108/ijebr-06-2015-0142,The impact of transitions into wage-employment for satisfied and unsatisfied entrepreneurs,"Purpose – Should unsatisfied/satisfied entrepreneurs transition into wage-employment? The purpose of this paper is to investigate the financial, physical, social and emotional consequences of the decision. Design/methodology/approach – The paper uses an Australian, nationally representative panel for two Bayesian multivariate regressions. Findings – Unsatisfied entrepreneurs that transition from self- to wage-employment improve their income, life and job satisfaction. For satisfied entrepreneurs, continuing or transitioning makes little difference: job and life satisfaction develop similarly. The health of continuing entrepreneurs suffers regardless of whether they are satisfied or unsatisfied. Research limitations/implications – Unobserved heterogeneity is only addressed within cohorts, not across cohorts. It is possible, that transitioning entrepreneurs are inherently different from continuing entrepreneurs. Further research could include a more fine-grained study of entrepreneurship’s negative health implications or include work-family balance as return to self-employment. Practical implications – The findings offer clear advice to entrepreneurs that are unsatisfied with their venture: they will likely benefit from transitioning to wage-employment. In addition, it offers a warning to individuals with existing health issues who are considering self-employment. Originality/value – Academic interest in entrepreneurship exit is growing. This paper is the first to study the financial, physical, social and emotional life consequences of both satisfied and unsatisfied entrepreneurs. It contributes to the discussion of what motivates entrepreneurs to become and remain self-employed.",0
https://doi.org/10.1080/15374416.2013.777917,Effects of Video Feedback on Early Coercive Parent–Child Interactions: The Intervening Role of Caregivers’ Relational Schemas,"We examined the effect of adding a video feedback intervention component to the assessment feedback session of the Family Check-Up (FCU) intervention (Dishion & Stormshak, 2007). We hypothesized that the addition of video feedback procedures during the FCU feedback at child age 2 would have a positive effect on caregivers' negative relational schemas of their child, which in turn would mediate reductions in observed coercive caregiver-child interactions assessed at age 5. We observed the caregiver-child interaction videotapes of 79 high-risk families with toddlers exhibiting clinically significant problem behaviors. A quasi-random sample of families was provided with direct feedback on their interactions during the feedback session of the FCU protocol. Path analysis indicated that reviewing and engaging in feedback about videotaped age 2 assessment predicted reduced caregivers' negative relational schemas of the child at age 3, which acted as an intervening variable on the reduction of observed parent-child coercive interactions recorded at age 5. Video feedback predicted improved family functioning over and above level of engagement in the FCU in subsequent years, indicating the important incremental contribution of using video feedback procedures in early family-based preventive interventions for problem behaviors. Supportive video feedback on coercive family dynamics is an important strategy for promoting caregiver motivation to reduce negative attributions toward the child, which fuel coercive interactions. Our study also contributes to the clinical and research literature concerning coercion theory and effective intervention strategies by identifying a potential mechanism of change.",0
https://doi.org/10.3758/s13428-013-0361-y,Fixed- and random-effects meta-analytic structural equation modeling: Examples and analyses in R,"Meta-analytic structural equation modeling (MASEM) combines the ideas of meta-analysis and structural equation modeling for the purpose of synthesizing correlation or covariance matrices and fitting structural equation models on the pooled correlation or covariance matrix. Cheung and Chan (Psychological Methods 10:40-64, 2005b, Structural Equation Modeling 16:28-53, 2009) proposed a two-stage structural equation modeling (TSSEM) approach to conducting MASEM that was based on a fixed-effects model by assuming that all studies have the same population correlation or covariance matrices. The main objective of this article is to extend the TSSEM approach to a random-effects model by the inclusion of study-specific random effects. Another objective is to demonstrate the procedures with two examples using the metaSEM package implemented in the R statistical environment. Issues related to and future directions for MASEM are discussed. Â© 2013 Psychonomic Society, Inc.",0
https://doi.org/10.1348/000711004849259,Bayesian analysis of two-level nonlinear structural equation models with continuous and polytomous data,"Two-level structural equation models with mixed continuous and polytomous data and nonlinear structural equations at both the between-groups and within-groups levels are important but difficult to deal with. A Bayesian approach is developed for analysing this kind of model. A Markov chain Monte Carlo procedure based on the Gibbs sampler and the Metropolis-Hasting algorithm is proposed for producing joint Bayesian estimates of the thresholds, structural parameters and latent variables at both levels. Standard errors and highest posterior density intervals are also computed. A procedure for computing Bayes factor, based on the key idea of path sampling, is established for model comparison.",0
https://doi.org/10.1007/978-1-4612-0919-5_30,Estimation with Quadratic Loss,"It has long been customary to measure the adequacy of an estimator by the smallness of its mean squared error. The least squares estimators were studied by Gauss and by other authors later in the nineteenth century. A proof that the best unbiased estimator of a linear function of the means of a set of observed random variables is the least squares estimator was given by Markov [12], a modified version of whose proof is given by David and Neyman [4]. A slightly more general theorem is given by Aitken [1]. Fisher [5] indicated that for large samples the maximum likelihood estimator approximately minimizes the mean squared error when compared with other reasonable estimators. This paper will be concerned with optimum properties or failure of optimum properties of the natural estimator in certain special problems with the risk usually measured by the mean squared error or, in the case of several parameters, by a quadratic function of the estimators. We shall first mention some recent papers on this subject and then give some results, mostly unpublished, in greater detail.",0
,Asymptotic accuracy of distribution-based estimation of latent variables,"Hierarchical statistical models are widely employed in information science and data engineering. The models consist of two types of variables: observable variables that represent the given data and latent variables for the unobservable labels. An asymptotic analysis of the models plays an important role in evaluating the learning process; the result of the analysis is applied not only to theoretical but also to practical situations, such as optimal model selection and active learning. There are many studies of generalization errors, which measure the prediction accuracy of the observable variables. However, the accuracy of estimating the latent variables has not yet been elucidated. For a quantitative evaluation of this, the present paper formulates distribution-based functions for the errors in the estimation of the latent variables. The asymptotic behavior is analyzed for both the maximum likelihood and the Bayes methods.",0
https://doi.org/10.1111/j.2044-8317.2011.02036.x,Parameter estimation of multiple item response profile model,"Multiple item response profile (MIRP) models are models with crossed fixed and random effects. At least one between-person factor is crossed with at least one within-person factor, and the persons nested within the levels of the between-person factor are crossed with the items within levels of the within-person factor. Maximum likelihood estimation (MLE) of models for binary data with crossed random effects is challenging. This is because the marginal likelihood does not have a closed form, so that MLE requires numerical or Monte Carlo integration. In addition, the multidimensional structure of MIRPs makes the estimation complex. In this paper, three different estimation methods to meet these challenges are described: the Laplace approximation to the integrand; hierarchical Bayesian analysis, a simulation-based method; and an alternating imputation posterior with adaptive quadrature as the approximation to the integral. In addition, this paper discusses the advantages and disadvantages of these three estimation methods for MIRPs. The three algorithms are compared in a real data application and a simulation study was also done to compare their behaviour.",0
https://doi.org/10.1002/sim.4073,The analysis of very small samples of repeated measurements I: An adjusted sandwich estimator,"The statistical analysis of repeated measures or longitudinal data always requires the accommodation of the covariance structure of the repeated measurements at some stage in the analysis. The general linear mixed model is often used for such analyses, and allows for the specification of both a mean model and a covariance structure. Often the covariance structure itself is not of direct interest, but only a means to producing valid inferences about the response. Existing methods of analysis are often inadequate where the sample size is small. More precisely, statistical measures of goodness of fit are not necessarily the right measure of the appropriateness of a covariance structure and inferences based on conventional Wald-type procedures do not approximate sufficiently well their nominal properties when data are unbalanced or incomplete. This is shown to be the case when adopting the Kenward-Roger adjustment where the sample size is very small. A generalization of an approach to Wald tests using a bias-adjusted empirical sandwich estimator for the covariance matrix of the fixed effects parameters from generalized estimating equations is developed for Gaussian repeated measurements. This is shown to attain the correct test size but has very low power. Copyright © 2010 John Wiley & Sons, Ltd.",0
https://doi.org/10.1080/10361149651274,"National Uniformity, and State and Local Effects on Australian Voting: A Multilevel Approach","The existence and extent of influences arising within spatial contexts is an important issue in the study of voting behaviour. This paper extends previous Australian research by using the relatively new technique of multilevel analysis to draw together individual survey data from the 1993 Australian Election Study and ecological census data to investigate the question. The results show that, once individual voter characteristics are taken into account, influences on first preference voting for the ALP at the 1993 election were quite uniform nationally, with relatively small spatial variations. Moreover, those spatial variations which were present were at the divisional, not the state, level and can be almost completely explained by a very small number of sociotropic factors, especially a local economic prosperity influence and the well-known rural-urban cleavage. As far as influences on voting at the 1993 election at the level of individual voters are concerned, these multilevel analyses provide some new ...",0
https://doi.org/10.1207/s15328007sem1103_7,"To Bayes or Not to Bayes, From Whether to When: Applications of Bayesian Methodology to Modeling","This article presents relevant research on Bayesian methods and their major applications to modeling in an effort to lay out differences between the frequentist and Bayesian paradigms and to look at the practical implications of these differences. Before research is reviewed, basic tenets and methods of the Bayesian approach to modeling are presented and contrasted with basic estimation results from a frequentist perspective. It is argued that Bayesian methods have become a viable alternative to traditional maximum likelihood-based estimation techniques and may be the only solution for more complex psychometric data structures. Hence, neither the applied nor the theoretical measurement community can afford to neglect the exciting new possibilities that have opened up on the psychometric horizon.",0
https://doi.org/10.1006/ssre.1999.0653,"Collinearity, Bias, and Effect Size: Modeling “the” Effect of Track on Achievement","Abstract The estimated effect of any factor can be highly dependent on both the model and the data used for the analyses. This article presents an example of the estimated effect of one factor in two different data sets under three different forms of the standard linear model using the effect of track placement on achievement as an example. Some relative advantages and disadvantages of each model are considered. The analyses demonstrate that, given collinearity among the predictor variables, a model with a poorer statistical fit may be useful for some interpretive purposes.",0
https://doi.org/10.1080/03610929208830774,Analysis of structural equation models with Incomplete Polytomous Data,"A two-stage estimation procedure is developed to analyze structural equation models of polytomous variables based on incomplete data. At the first stage, the partition maximum likelihood approach is used to obtain the estimates of the elements in the correlation matrix. It will be shown that the asymptotic distribution of these estimates is jointly multivariate normal. The second stage estimates the structural parameters in the correlation matrix by the generalized least squared approach with a correctly specified weight matrix. Asymptotic properties of the second stage estimates are also provided. Extension of the theory to multisample models, and some illustrative examples are also included.",0
https://doi.org/10.1214/15-ba953,A Two-Component G-Prior for Variable Selection,"We present a Bayesian variable selection method based on an extension of the Zellner’s g-prior in linear models. More specifically, we propose a two-component G-prior, wherein a tuning parameter, calibrated by use of pseudo-variables, is introduced to adjust the distance between the two components. We show that implementing the proposed prior in variable selection is more efficient than using the Zellner’s g-prior. Simulation results also indicate that models selected using the method with the two-component G-prior are generally more favorable with smaller losses compared to other methods considered in our work. The proposed method is further demonstrated using our motivating gene expression data from a lung disease study, and ozone data analyzed in earlier studies.",0
https://doi.org/10.1037/0003-066x.40.1.73,Rationality in psychological research: The good-enough principle.," This article reexamines a number of methodological and procedural issues raised by Meehl (1967, 1978) seem to question rationality of psychological inquiry. The first issue concerns asymmetry in theory testing between psychology and physics and resulting paradox that, because psychological null hypothesis is always false, increases in precision in psychology always lead to weaker tests of a theory, whereas converse is true in physics. The second issue, related to first, regards slow progress observed in psychological research and seeming unwillingness of social scientists to take seriously Popperian requirements for intellectual honesty. We propose a principle to resolve Meehl's methodological paradox and appeal to a more powerful reconstruction of science developed by Lakatos (1978a, 1978b) to account for actual practice of psychological researchers. From time to time every research discipline must reevaluate its method for generating and certifying knowledge. The actual practice of working scientists in a discipline must continually be subjected to severe criticism and be held accountable to standards of intellectual honesty, standards are themselves revised in light of critical appraisal (Lakatos, 1978a). If, on a metatheoretical level, scientific methodology cannot be defended on rational grounds, then metatheory must be reconstructed so as to make science rationally justifiable. The history of science is replete with numerous such reconstructions, from portrayal of science as being inductive and justificationist, to more recent reconstructions favored by (naive and sophisticated) methodological falsificationists, such as Popper (1959), Lakatos (1978a), and Zahar (1973). In last two decades psychology, too, has been subjected to criticism for its research methodology. Of increasing concern is empirical psychology's use of inferential hypothesis-testing techniques and way in which information derived from these procedures is used to help us make decisions about theories under test (e.g., Bakan, 1966; Lykken, 1968; Rozeboom, 1960). In two penetrating essays, Meehl (1967, 1978) has cogently and effectively faulted use of traditional null-hypothesis significance test in psychological research. According to Meehl (1978, p. 817), the almost universal reliance on merely refuting null hypothesis as standard method for corroborating substantive theories [in psychology] is a terrible mistake, is basically unsound, poor scientific strategy, and one of worst things ever happened in history of He maintained it leads to a methodological paradox when compared to theory testing in physics. In addition, Meehl (1978) pointed to apparently slow progress in psychological research and deleterious effect null-hypothesis testing has had on detection of progress in accumulation of psychological knowledge. The cumulative effect of this criticism is to do nothing less than call into question rational character of our empirical inquiries. As yet there has been no attempt to deal with problems raised by Meehl by reconstructing actual practice of psychologists into a logically defensible form. This is purpose of present article. The two articles by Meehl seem to deal with two disparate issues--null-hypothesis testing and slow progress. Both issues, however, are linked in methodological falsificationist reconstruction of science to necessity for scientists to agree on what experimental outcomes are to be considered as disconfirming instances. We will argue methodological paradox can be ameliorated with help of a good-enough principle, to be proposed here, so hypothesis testing in psychology is not rationally disadvantaged when compared to physics. We will also account for apparent slow progress in psychological research, and we will take issue with certain (though not all) claims made by Meehl (1978) in this regard. Both methodological and progress issues will be resolved by an appeal to (sophisticated) methodological falsificationist reconstruction of science developed by Lakatos (1978a), an approach with which Meehl is familiar but one he did not apply to psychology in his articles. January 1985 • American Psychologist Copyright 1985 by American Psychological Association, Inc. 0003-066X/85/$00.75 Vol. 40, No. 1, 73-83 73 Meehl's Asymmetry Argument Let us develop Meehl's argument. It is his contention improved measurement precision has widely different effects in psychology and physics on success of a theory in overcoming an observational hurdle. Perfect precision in behavioral provides an easier hurdle for theories, whereas such accuracy in physics makes it much more difficult for a theory to survive. According to Popperian reconstruction of science (Popper, 1959), scientific theories must be continually subjected to severe tests. But if social are immanently incapable of generating such tests, if they cannot expose their theories to strongest possible threat of refutation, even with ever-increasing measurement precision, then their claim to scientific status might reasonably be questioned. Further, according to this view of research in social sciences, there can be no question of scientific progress based on rational consideration of experimental outcomes. Instead, progress is more a matter of psychological conversion (Kuhn, 1962). Let us look more closely at standard practice in psychology. On basis of some theory T we derive conclusion a parameter 6 will differ for two populations. In order to examine this conclusion, we can set up a point-null hypothesis, Ho: = 0, and test this hypothesis against predicted outcome, H~: 6 4: 0. However, it has also been recognized (Kaiser, 1960; Kimmel, 1957) another question of interest is whether difference is in a certain direction, and so we could instead test directional null hypothesis, I-I~: 6 ~ 0, against directional alternative, H*: ~ > 0. In such tests, we can make two types of errors. The Type I error would lead to rejecting Ho or H~ when they are indeed true, whereas Type II error involves not rejecting Ho or HJ when they are false. The conventional methodology sets Type I (or alpha) error rate at 5% and seeks to reduce frequency of Type II errors. Such a reduction in Type II error rate can be achieved by improving logical structure of experiment, reducing measurement errors, or increasing sample size. Meehl pointed out in behavioral sciences, because of large number of factors affecting variables, we would never expect two populations to have literally equal means. Hence, he concluded An earlier version of this article was read at 1983 meeting of American Educational Research Association. The authors are grateful to Robbie Case, Joel R. Lcvin, and Leonard Marascuilo for reading earlier drafts, and to Crescent L. Kringle for her help with manuscript. Requests for reprints should be sent to Ronald C. Serlin, Department of Educational Psychology, University of Wisconsin, Madison, Wisconsin 53706. point-null hypothesis is always false. With infinite precision, we would always reject Ho. This is perhaps one reason to prefer directional null hypothesis H~. But Meehl then conducted a thought experiment in which direction predicted by T was assigned at random. In such an experiment, T provides no logical connection to predicted direction and so is totally without merit. Because H0 is always false, two populations will always differ, but because direction in H~ is assigned at random, with infinite precision we will reject HJ half of time. Hence, Meehl concluded that effect of increased p r e c i s i o n . . , is to yield a probability approaching 1/2 of corroborating our substantive theory by a significance test, even i f theory is totally without merit (Meehl, 1967, p. 111, emphasis in original). Meehl contrasted this state of affairs with in physics, wherein usual situation involves prediction of a point value. That which corresponds to point-null hypothesis is value flowing as a consequence of a substantive theory T. An increase in statistical power in physics has effect of stiffening experimental hurdle by 'decreasing prior probability of a successful experimental outcome if theory lacks verisimilitude, is, precisely reverse of situation obtaining in social sciences (Meehl, 1967, p. 113). With infinite precision, and if theory has no merit, logical probability of it surviving such a test in physics is negligible; in social sciences, this logical probability for H~ is one half. Perhaps another way of describing asymmetry in hypothesis testing between psychology and physics is to note that, in psychology, point-null hypothesis is not what is derived from a substantive theory. Rather, it is a straw-man competitor whose rejection we interpret as increasing plausibility of T. In physics, on other hand, theories entail point-null statistical hypotheses are very ones physicists take seriously and hope to confirm. If 0 is a predicted outcome of interest, and 0 is its logical complement, then depiction of null and alternative statistical hypotheses in two disciplines can be written as follows:",0
https://doi.org/10.1016/j.jbusres.2013.11.012,Salesperson CLV orientation's effect on performance,"Abstract Previous studies show how strategies based on the customer lifetime value (CLV) can lead to an increase of profitability for a firm. In this context, marketing serves the purpose of maximizing CLV and customer equity (the CLV of current and future customers). For most types of service firms, salespeople are direct participants in implementing the CLV concept. However, prior research does not answer the question of whether or how salesperson CLV orientation can enhance profits. Using data on salespeople in a large Chilean retail bank, this study shows that the effect of salesperson CLV orientation on salesperson performance follows an S-shaped function (which is first convex and then concave). Additionally, data does not support the idea that the optimum level of CLV orientation depends on salesperson customer orientation, salesperson adaptive selling behavior, or salesperson experience (i.e., CLV-oriented behaviors could be effective across a wide range of salespeople). As such, this study addresses an important concern among researchers and managers that is related to how to increase the salesperson performance. The findings of this study suggest that firms need to monitor individual salesperson CLV orientation more closely.",0
https://doi.org/10.1371/journal.pone.0038306,Bayesian Cohort and Cross-Sectional Analyses of the PINCER Trial: A Pharmacist-Led Intervention to Reduce Medication Errors in Primary Care,"Medication errors are an important source of potentially preventable morbidity and mortality. The PINCER study, a cluster randomised controlled trial, is one of the world's first experimental studies aiming to reduce the risk of such medication related potential for harm in general practice. Bayesian analyses can improve the clinical interpretability of trial findings.Experts were asked to complete a questionnaire to elicit opinions of the likely effectiveness of the intervention for the key outcomes of interest--three important primary care medication errors. These were averaged to generate collective prior distributions, which were then combined with trial data to generate bayesian posterior distributions. The trial data were analysed in two ways: firstly replicating the trial reported cohort analysis acknowledging pairing of observations, but excluding non-paired observations; and secondly as cross-sectional data, with no exclusions, but without acknowledgement of the pairing. Frequentist and bayesian analyses were compared.Bayesian evaluations suggest that the intervention is able to reduce the likelihood of one of the medication errors by about 50 (estimated to be between 20% and 70%). However, for the other two main outcomes considered, the evidence that the intervention is able to reduce the likelihood of prescription errors is less conclusive.Clinicians are interested in what trial results mean to them, as opposed to what trial results suggest for future experiments. This analysis suggests that the PINCER intervention is strongly effective in reducing the likelihood of one of the important errors; not necessarily effective in reducing the other errors. Depending on the clinical importance of the respective errors, careful consideration should be given before implementation, and refinement targeted at the other errors may be something to consider.",0
https://doi.org/10.1016/j.lisr.2011.07.004,Factorial invariance of LibQUAL+® as a measure of library service quality over time,"Abstract LibQUAL+® is an instrument purported to measure three dimensions of library service quality: service affect, library as a place, and information control. After changes were made to the instrument in 2003, however, no confirmatory factor analyses have been published in peer-reviewed journals affirming the three-factor structure of LibQUAL+®. These deficiencies were addressed by testing the hypothesized three-factor structure and the stability of that structure over time. Specifically, data from three samples (n = 550; n = 3261; n = 2103) were collected over a five-year period and analyzed using a multi-group confirmatory factor analysis. Results suggest that the theoretical model fit the data across the three samples and demonstrates factorial invariance over time. Multicollinearity between affect of service and information control, however, indicate that service quality may be measured as two dimensions rather than three, providing a more parsimonious explanation of service quality.",0
https://doi.org/10.1037/1082-989x.8.3.338,Distributional Assumptions of Growth Mixture Models: Implications for Overextraction of Latent Trajectory Classes.,"Growth mixture models are often used to determine if subgroups exist within the population that follow qualitatively distinct developmental trajectories. However, statistical theory developed for finite normal mixture models suggests that latent trajectory classes can be estimated even in the absence of population heterogeneity if the distribution of the repeated measures is nonnormal. By drawing on this theory, this article demonstrates that multiple trajectory classes can be estimated and appear optimal for nonnormal data even when only 1 group exists in the population. Further, the within-class parameter estimates obtained from these models are largely uninterpretable. Significant predictive relationships may be obscured or spurious relationships identified. The implications of these results for applied research are highlighted, and future directions for quantitative developments are suggested.",0
https://doi.org/10.1523/jneurosci.4741-13.2014,Oscillatory Brain State Predicts Variability in Working Memory,"Our capacity to remember and manipulate objects in working memory (WM) is severely limited. However, this capacity limitation is unlikely to be fixed because behavioral models indicate variability from trial to trial. We investigated whether fluctuations in neural excitability at stimulus encoding, as indexed by low-frequency oscillations (in the alpha band, 8-14 Hz), contribute to this variability. Specifically, we hypothesized that the spontaneous state of alpha band activity would correlate with trial-by-trial fluctuations in visual WM. Electroencephalography recorded from human observers during a visual WM task revealed that the prestimulus desynchronization of alpha oscillations predicts the accuracy of memory recall on a trial-by-trial basis. A model-based analysis indicated that this effect arises from a modulation in the precision of memorized items, but not the likelihood of remembering them (the recall rate). The phase of posterior alpha oscillations preceding the memorized item also predicted memory accuracy. Based on correlations between prestimulus alpha levels and stimulus-related visual evoked responses, we speculate that the prestimulus state of the visual system prefigures a cascade of state-dependent processes, ultimately affecting WM-guided behavior. Overall, our results indicate that spontaneous changes in cortical excitability can have profound consequences for higher visual cognition.",0
https://doi.org/10.1037/a0023028,Investigating spousal influence using moment-to-moment affect data from marital conflict.,"Gottman and colleagues proposed using a dynamical systems model to study dyadic interaction in marriage. In this model, each spouse's affect in each 6-s window is described as a function of an uninfluenced linear steady state and a nonlinear influence function of the partner's affect in the previous window. Recently, an alternative parameter estimation procedure for the equations of marriage was introduced, which is based on threshold autoregressive models. We apply this estimation procedure to data from a study of couples (N = 124) and newlyweds (N = 130) to compare different forms of spousal influence using the Bayesian information criterion. Although results show some statistically significant evidence for influence, this is only slightly greater than what would be expected by random association. One model of influence does not fit all couples. This suggests that for many people initial state and emotional inertia dictate the outcome of the conflict discussion far more than the moment-to-moment affect of the spouse. This latter finding is in conflict with most models of couples' interaction, which suggest that the outcome of conflict discussions are determined by the nature of the couples' mutual influence processes.",0
https://doi.org/10.1214/ss/1177010123,Bayesian Computation and Stochastic Systems,"Markov chain Monte Carlo (MCMC) methods have been used extensively in statistical physics over the last 40 years, in spatial statistics for the past 20 and in Bayesian image analysis over the last decade. In the last five years, MCMC has been introduced into significance testing, general Bayesian inference and maximum likelihood estimation. This paper presents basic methodology of MCMC, emphasizing the Bayesian paradigm, conditional probability and the intimate relationship with Markov random fields in spatial statistics. Hastings algorithms are discussed, including Gibbs, Metropolis and some other variations. Pairwise difference priors are described and are used subsequently in three Bayesian applications, in each of which there is a pronounced spatial or temporal aspect to the modeling. The examples involve logistic regression in the presence of unobserved covariates and ordinal factors; the analysis of agricultural field experiments, with adjustment for fertility gradients; and processing of low-resolution medical images obtained by a gamma camera. Additional methodological issues arise in each of these applications and in the Appendices. The paper lays particular emphasis on the calculation of posterior probabilities and concurs with others in its view that MCMC facilitates a fundamental breakthrough in applied Bayesian modeling.",0
https://doi.org/10.1080/00273171.2014.933762,Ignoring Clustering in Confirmatory Factor Analysis: Some Consequences for Model Fit and Standardized Parameter Estimates,"In many situations, researchers collect multilevel (clustered or nested) data yet analyze the data either ignoring the clustering (disaggregation) or averaging the micro-level units within each cluster and analyzing the aggregated data at the macro level (aggregation). In this study we investigate the effects of ignoring the nested nature of data in confirmatory factor analysis (CFA). The bias incurred by ignoring clustering is examined in terms of model fit and standardized parameter estimates, which are usually of interest to researchers who use CFA. We find that the disaggregation approach increases model misfit, especially when the intraclass correlation (ICC) is high, whereas the aggregation approach results in accurate detection of model misfit in the macro level. Standardized parameter estimates from the disaggregation and aggregation approaches are deviated toward the values of the macro- and micro-level standardized parameter estimates, respectively. The degree of deviation depends on ICC and cluster size, particularly for the aggregation method. The standard errors of standardized parameter estimates from the disaggregation approach depend on the macro-level item communalities. Those from the aggregation approach underestimate the standard errors in multilevel CFA (MCFA), especially when ICC is low. Thus, we conclude that MCFA or an alternative approach should be used if possible.",0
https://doi.org/10.1016/s0169-7161(05)25018-6,Variable Selection and Covariance Selection in Multivariate Regression Models,This article provides a general framework for Bayesian variable selection and covariance selection in a multivariate regression model with Gaussian errors. By variable selection we mean allowing certain regression coefficients to be zero. By covariance selection we mean allowing certain elements of the inverse covariance matrix to be zero. We estimate all the model parameters by model averaging using a Markov chain Monte Carlo simulation method. The methodology is illustrated by applying it to four real data sets. The effectiveness of variable selection and covariance selection in estimating the multivariate regression model is assessed by using four loss functions and four simulated data sets. Each of the simulated data sets is based on parameter estimates obtained from a corresponding real data set.,0
https://doi.org/10.1080/00401706.1975.10489269,The Nelder-Mead Simplex Procedure for Function Minimization,"The Nelder-Mead simplex method for function minimization is a “direct” method requiring no derivatives. The objective function is evaluated at the vertices of a simplex, and movement is away from the poorest value. The process is adaptive, causing the simplexes to be continually revised to best conform to the nature of the response surface. The generality of the method is illust'rated by using it. to solve six problems appearing in the May 1973 issue of Technometrics.",0
https://doi.org/10.1016/j.jmp.2013.01.004,Cultural consensus theory for multiple consensus truths,"Abstract Cultural Consensus Theory (CCT) is a popular information pooling methodology used in the social and behavioral sciences. CCT consists of cognitive models designed to determine a consensus truth shared by a group of informants (respondents), and to better understand the cognitive characteristics of the informants (e.g. level knowledge, response biases). However prior to this paper, no CCT models have been developed that allow the possibility of the informant responses to come from a mixture of two or more consensus answer patterns. The major advance in the current paper is to endow one of the popular CCT models, the General Condorcet Model (GCM) for dichotomous responses, with the possibility of having several latent consensus answer patterns, each corresponding to a different, latent subgroup of informants. In addition, we augment the model to allow the possibility of questions having differential difficulty (cultural saliency). This is the first CCT finite-mixture model, and it is named the Multi-Culture GCM (MC-GCM). The model is developed axiomatically and a notable property is derived that can suggest the appropriate number of mixtures for a given data set. The model is extended in a hierarchical Bayesian framework and its application is successfully demonstrated on both simulated and real data, including a new experimental data set on political viewpoints.",0
https://doi.org/10.1207/s15328007sem1204_5,Applications of Multilevel Structural Equation Modeling to Cross-Cultural Research,"Multilevel structural equation modeling (MSEM) has been proposed as an extension to structural equation modeling for analyzing data with nested structure. We have begun to see a few applications in cross-cultural research in which MSEM fits well as the statistical model. However, given that cross-cultural studies can only afford collecting data from a relatively small number of countries, the appropriateness of MSEM has been questioned. Using the data from the International Social Survey Program (1997; N = 15,244 from 27 countries), we first showed how Muth�n's MSEM procedure could be applied to a real data set on cross-cultural research. Given a small country-level sample size (27 countries) we then demonstrated that results on the individual level were quite stable even when using small individual-level sample sizes, whereas the group-level parameter estimates and their standard errors were affected unsystematically by varying individual-level sample sizes. Use of the findings for cross-cultural researc...",0
https://doi.org/10.3168/jds.2012-6115,Association between somatic cell count early in the first lactation and the longevity of Irish dairy cows,"Reduced longevity of cows is an important component of mastitis costs, and increased somatic cell count (SCC) early in the first lactation has been reported to increase culling risk throughout the first lactation. Generally, cows must survive beyond the first lactation to break even on their rearing costs. The aim of this research was to assess the association between SCC of primiparous cows at 5 to 30 days in milk (SCC1), and survival over a 5-y period for cows in Irish dairy herds. The data set used for model development was based on 147,458 test day records from 7,537 cows in 812 herds. Cows were censored at their last recording if identified at a later date in other herds or if recorded at the last available test date for their herd, otherwise, date of disposal was taken to be at the last test date for each cow. Survival time was calculated as the number of days between the dates of first calving and the last recording, which was split into 50-d intervals. Data were analyzed in discrete time logistic survival models that accounted for clustering of 50-d intervals within cows, and cows within herds. Models were fitted in a Bayesian framework using Markov chain Monte Carlo simulations. Model fit was assessed by comparison of posterior predictions to the observed disposal risk for cows aggregated by parameters in the model. Model usefulness was assessed by cross validation in a separate data set, which contained 144,113 records from 7,353 cows in 808 herds, and posterior predictions were compared with the observed disposal risk for cows aggregated by parameters of biological importance. Disposal odds increased by a factor of 5% per unit increase in ln SCC1. Despite this, posterior predictive distributions revealed that the probability of reducing replacement costs by >€10 per heifer calved, through decreasing the herd level prevalence of cows with SCC1 ≥ 400,000 cells/mL (from an initial prevalence of ≥ 20 to <10%) only exceeded 50% for less than 1 in 5 Irish herds. These results indicate that the effect of a reduction in the prevalence of cows with SCC1 ≥ 400,000 cells/mL on replacement costs alone for most Irish dairy herds is small, and future research should investigate other potential losses, such as the effect of SCC1 on lifetime milk yield.",0
https://doi.org/10.3141/2076-20,Influence of Transportation Access and Market Dynamics on Property Values,"This paper presents housing price models by using multilevel modeling techniques. The key motivation of using the multilevel modeling technique is that it clearly identifies and differentiates between-cluster heterogeneity (i.e., intrinsic differences across aggregated units) and heterogeneity between units of analysis that are nested within aggregated clusters. Two different specifications are tested: two-level spatial and mixed two-level spatiotemporal random effects models. Whereas the first specification assumes that dwelling units are nested within spatial clusters (i.e., neighborhoods), the second specification hypothesizes that dwelling units are nested within spatiotemporal clusters (neighborhoods in a given time period). The unique contribution of this paper is that it accounts for temporal heterogeneity simultaneously with spatial heterogeneity in the housing price models. The study uses an extensive sample of more than 250,000 housing property transactions in 1987–1995 in the Greater Toronto Area of Canada. The paper examines the functional form of the hedonic price model and chooses a semilogarithmic model for subsequent multilevel housing price modeling. The results suggest that the spatiotemporal model performs better in terms of explanatory power and parameter estimates.",0
https://doi.org/10.1111/j.1745-3984.1987.tb00274.x,Item Clusters and Computerized Adaptive Testing: A Case for Testlets,"It is observed that many sorts of difficulties may preclude the uneventful construction of tests by a computerized algorithm, such as those currently in favor in Computerized Adaptive Testing (CAT). In this essay we discuss a number of these problems, as well as some possible avenues of solution. We conclude with the development of the “testlet,” a bundle of items that can be arranged either hierarchically or linearly, thus maintaining the efficiency of an adaptive test while keeping the quality control of test construction that is possible currently only with careful expert scrutiny. Performance on the separate testlets is aggregated to yield ability estimates.",0
https://doi.org/10.1037/0021-9010.88.2.234,Effectiveness of training in organizations: A meta-analysis of design and evaluation features.,"The authors used meta-analytic procedures to examine the relationship between specified training design and evaluation features and the effectiveness of training in organizations. Results of the meta-analysis revealed training effectiveness sample-weighted mean ds of 0.60 (k = 15, N = 936) for reaction criteria, 0.63 (k = 234, N = 15,014) for learning criteria, 0.62 (k = 122, N = 15,627) for behavioral criteria, and 0.62 (k = 26, N = 1,748) for results criteria. These results suggest a medium to large effect size for organizational training. In addition, the training method used, the skill or task characteristic trained, and the choice of evaluation criteria were related to the effectiveness of training programs. Limitations of the study along with suggestions for future research are discussed.",0
https://doi.org/10.1016/j.jmp.2007.11.001,The parameters in the near-miss-to-Weber's law,"Abstract Many empirical data support the hypothesis that the sensitivity function grows as a power function of the stimulus intensity. This is usually referred to as the near-miss-to-Weber's law. The aim of the paper is to examine the near-miss-to-Weber's law in the context of psychometric models of discrimination. We study two types of psychometric functions, characterized by the representations P a ( x ) = F ( ρ ( a ) x γ ( a ) ) (type A), and P a ( x ) = F ( γ ( a ) + ρ ( a ) x ) (type B). A central result shows that both types of psychometric functions are compatible with the near-miss-to-Weber's law. If a representation of type B exists, then the exponent in the near-miss is necessarily a constant function, that is, does not depend on the criterion value used to define “just noticeably different”. If, on the other hand, a representation of type A exists, then the exponent in the near-miss-to-Weber's law can vary with the criterion value. In that case, the parameters in the near-miss co-vary systematically.",0
https://doi.org/10.1016/s0042-6989(96)00310-0,Principles of an Adaptive Method for Measuring the Slope of the Psychometric Function,"Recent developments in the efficient estimation of threshold are here extended to the problem of how best to estimate the slope of the psychometric function. An adaptive method is described for selecting stimulus intensities that are optimal for slope estimation. A two-dimensional array of probabilities of different thresholds and slopes is used to calculate the stimulus intensity for the next trial; this array is updated after the trial, using Bayes' theorem to incorporate information from the subject's response. The practical implementation and efficiency of the method are demonstrated and discussed. © 1997 Elsevier Science Ltd.",0
https://doi.org/10.4135/9781483349428,Computational Modeling in Cognition: Principles and Practice,Preface 1. Introduction 1.1 Models and Theories in Science 1.2 Why Quantitative Modeling? 1.3 Quantitative Modeling in Cognition 1.4 The Ideas Underlying Modeling and Its Distinct Applications 1.5 What Can We Expect From Models? 1.6 Potential Problems 2. From Words to Models: Building a Toolkit 2.1 Working Memory 2.2 The Phonological Loop: 144 Models of Working Memory 2.3 Building a Simulation 2.4 What Can We Learn From These Simulations? 2.5 The Basic Toolkit 2.6 Models and Data: Sufficiency and Explanation 3. Basic Parameter Estimation Techniques 3.1 Fitting Models to Data: Parameter Estimation 3.2 Considering the Data: What Level of Analysis? 4. Maximum Likelihood Estimation 4.1 Basics of Probabilities 4.2 What Is a Likelihood? 4.3 Defining a Probability Function 4.4 Finding the Maximum Likelihood 4.5 Maximum Likelihood Estimation for Multiple Participants 4.6 Properties of Maximum Likelihood Estimators 5. Parameter Uncertainty and Model Comparison 5.1 Error on Maximum Likelihood Estimates 5.2 Introduction to Model Selection 5.3 The Likelihood Ratio Test 5.4 Information Criteria and Model Comparison 5.5 Conclusion 6. Not Everything That Fits Is Gold: Interpreting the Modeling 6.1 Psychological Data and The Very Bad Good Fit 6.2 Parameter Identifiability and Model Testability 6.3 Drawing Lessons and Conclusions From Modeling 7. Drawing It All Together: Two Examples 7.1 WITNESS: Simulating Eyewitness Identification 7.2 Exemplar Versus Boundary Models: Choosing Between Candidates 7.3 Conclusion 8. Modeling in a Broader Context 8.1 Bayesian Theories of Cognition 8.2 Neural Networks 8.3 Neuroscientific Modeling 8.4 Cognitive Architectures 8.5 Conclusion References Author Index Subject Index About the Authors,0
https://doi.org/10.1177/0956797613480187,The Relative Trustworthiness of Inferential Tests of the Indirect Effect in Statistical Mediation Analysis,"A content analysis of 2 years of Psychological Science articles reveals inconsistencies in how researchers make inferences about indirect effects when conducting a statistical mediation analysis. In this study, we examined the frequency with which popularly used tests disagree, whether the method an investigator uses makes a difference in the conclusion he or she will reach, and whether there is a most trustworthy test that can be recommended to balance practical and performance considerations. We found that tests agree much more frequently than they disagree, but disagreements are more common when an indirect effect exists than when it does not. We recommend the bias-corrected bootstrap confidence interval as the most trustworthy test if power is of utmost concern, although it can be slightly liberal in some circumstances. Investigators concerned about Type I errors should choose the Monte Carlo confidence interval or the distribution-of-the-product approach, which rarely disagree. The percentile bootstrap confidence interval is a good compromise test.",0
https://doi.org/10.4324/9780203848852.ch8,Bayesian Estimation of Multilevel Models,,0
https://doi.org/10.1037/0012-1649.44.4.1148,Life satisfaction shows terminal decline in old age: Longitudinal evidence from the German Socio-Economic Panel Study (SOEP).,"Longitudinal data spanning 22 years, obtained from deceased participants of the German Socio-Economic Panel Study (SOEP; N = 1,637; 70- to 100-year-olds), were used to examine if and how life satisfaction exhibits terminal decline at the end of life. Changes in life satisfaction were more strongly associated with distance to death than with distance from birth (chronological age). Multiphase growth models were used to identify a transition point about 4 years prior to death where the prototypical rate of decline in life satisfaction tripled from -0.64 to -1.94 T-score units per year. Further individual-level analyses suggest that individuals dying at older ages spend more years in the terminal periods of life satisfaction decline than individuals dying at earlier ages. Overall, the evidence suggests that late-life changes in aspects of well-being are driven by mortality-related mechanisms and characterized by terminal decline.",0
https://doi.org/10.2307/2532087,Nonlinear Mixed Effects Models for Repeated Measures Data,"We propose a general, nonlinear mixed effects model for repeated measures data and define estimators for its parameters. The proposed estimators are a natural combination of least squares estimators for nonlinear fixed effects models and maximum likelihood (or restricted maximum likelihood) estimators for linear mixed effects models. We implement Newton-Raphson estimation using previously developed computational methods for nonlinear fixed effects models and for linear mixed effects models. Two examples are presented and the connections between this work and recent work on generalized linear mixed effects models are discussed.",0
https://doi.org/10.1207/s15327906mbr1802_2,Cross-Validation Of Covariance Structures,"This paper examines methods for comparing the suitability of alternative models for covariance matrices. A cross-validation procedure is suggested and its properties are examined. To motivate the discussion, a series of examples is presented using longitudinal data.",0
https://doi.org/10.1177/014662169301700105,EQUATE 2.0: A Computer Program for the Characteristic Curve Method of IRT Equating,,0
https://doi.org/10.1177/014662169602000201,Monte Carlo Studies in Item Response Theory,"Monte carlo studies are being used in item response theory (IRT) to provide information about how validly these methods can be applied to realistic datasets (e.g., small numbers of examinees and multidimensional data). This paper describes the conditions under which monte carlo studies are appropriate in IRT-based re search, the kinds of problems these techniques have been applied to, available computer programs for gen erating item responses and estimating item and exam inee parameters, and the importance of conceptualizing these studies as statistical sampling experiments that should be subject to the same principles of experimen tal design and data analysis that pertain to empirical studies. The number of replications that should be used in these studies is also addressed.",0
https://doi.org/10.1287/mnsc.48.10.1350.272,Augmenting Conjoint Analysis to Estimate Consumer Reservation Price,"Consumer reservation price is a key concept in marketing and economics. Theoretically, this concept has been instrumental in studying consumer purchase decisions,competitive pricing strategies,and welfare economics. Managerially,knowledge of consumer reservation prices is critical for implementing many pricing tactics such as bundling,tar get promotions,nonlinear pricing,and one-to-one pricing,and for assessing the impact of marketing strategy on demand. Despite the practical and theoretical importance of this concept, its measurement at the individual level in a practical setting proves elusive. We propose a conjoint-based approach to estimate consumer-level reservation prices. This approach integrates the preference estimation of traditional conjoint with the economic theory of consumer choice. This integration augments the capability of traditional conjoint such that consumers' reservation prices for a product can be derived directly from the individuallevel estimates of conjoint coefficients. With this augmentation,we can model a consumer's decision of not only which product to buy,but also whether to buy at all in a category. Thus, we can simulate simultaneously three effects that a change in price or the introduction of a new product may generate in a market: the customer switching effect,the cannibalization effect,and the market expansion effect. We show in a pilot application how this approach can aid product and pricing decisions. We also demonstrate the predictive validity of our approach using data from a commercial study of automobile batteries.",0
https://doi.org/10.1111/j.1744-6570.2010.01186.x,VALIDATING SYNTHETIC VALIDATION: COMPARING TRADITIONAL AND SYNTHETIC VALIDITY COEFFICIENTS,"We describe a unique application of a synthetic validation technique to a selection system development project in a large organization. Job analysis data were collected from 4,725 job incumbents and 619 supervisors, and were used to identify 11 job families and 27 job components. We developed 12 tests to predict performance on these job components and conducted a concurrent validation study to collect test and job component data for 1,926 incumbents. We created a test composite for each job component and then chose a test battery for each job family based on its relevant job components. Synthetic validity coefficients were computed for each test battery and compared to traditional validity coefficients that were computed within job families with large sample sizes. The synthetic validity coefficient was very close to the within-family validity coefficient for most job families and was within the bounds of sampling error for all job families. Validities tended to be highest when predictors were weighted according to the number of job components to which they were relevant and job component criterion measures were unit weighted.",0
https://doi.org/10.5860/choice.45-1510,Structural equation modeling: a Bayesian approach,About the Author. Preface. Chapter 1. Introduction. Chapter 2. Some Basic Structural Equation Models. Chapter 3. Covariance Structure Analysis. Chapter 4. Bayesian Estimation of Structural Equation Models. Chapter 5. Model Comparison and Model Checking. Chapter 6. Structural Equation Models with Continuous and Ordered Categorical Variables. Chapter 7. Structural Equation Models with Dichotomous Variables. Chapter 8. Nonlinear Structural Equation Models. Chapter 9. Two-level Nonlinear Structural Equation Models. Chapter 10. Multisample Analysis of Structural Equation Models. Chapter 11. Finite Mixtures in Structural Equation Models. Chapter 12. Structural Equation Models with Missing Data. Chapter 13. Structural Equation Models with Exponential Family of Distributions. Chapter 14. Conclusion. Index.,0
https://doi.org/10.1123/jpah.10.4.581,Advancing Science and Policy Through a Coordinated International Study of Physical Activity and Built Environments: IPEN Adult Methods,"Background: National and international strategies to increase physical activity emphasize environmental and policy changes that can have widespread and long-lasting impact. Evidence from multiple countries using comparable methods is required to strengthen the evidence base for such initiatives. Because some environment and policy changes could have generalizable effects and others may depend on each country’s context, only international studies using comparable methods can identify the relevant differences. Methods: Currently 12 countries are participating in the International Physical Activity and the Environment Network (IPEN) study. The IPEN Adult study design involves recruiting adult participants from neighborhoods with wide variations in environmental walkability attributes and socioeconomic status (SES). Results: Eleven of twelve countries are providing accelerometer data and 11 are providing GIS data. Current projections indicate that 14,119 participants will provide survey data on built environments and physical activity and 7145 are likely to provide objective data on both the independent and dependent variables. Though studies are highly comparable, some adaptations are required based on the local context. Conclusions: This study was designed to inform evidence-based international and country-specific physical activity policies and interventions to help prevent obesity and other chronic diseases that are high in developed countries and growing rapidly in developing countries.",0
https://doi.org/10.2307/2944713,Bayesian Inference for Comparative Research,"Regression analysis in comparative research suffers from two distinct problems of statistical inference. First, because the data constitute all the available observations from a population, conventional inference based on the long-run behavior of a repeatable data mechanism is not appropriate. Second, the small and collinear data sets of comparative research yield imprecise estimates of the effects of explanatory variables. We describe a Bayesian approach to statistical inference that provides a unified solution to these two problems. This approach is illustrated in a comparative analysis of unionization.",0
https://doi.org/10.1177/096228029200100203,Structural equation models in medical research,"Structural equation modelling (SEM) is a modern statistical method that allows one to evaluate causal hypotheses on a set of intercorrelated nonexperimental data. The sample variances and covariances, and possibly the means, are compared to those predicted by a theory-based hypothetical model after optimal estimation of the parameters of the model. The goodness-of-fit of the empirical data to the hypothesized model is evaluated statistically. This review describes the underlying statistical theory and rationale of SEM. Both confirmatory factor analysis and latent variable path models are discussed. The applicability of SEM to assessment of reliability and validity is noted. A detailed example is provided, and several examples from the medical literature are briefly reviewed. Cautions regarding the possible misuse or misinterpretation of the technique are also mentioned. Possible future directions for the use of SEM in medical research are suggested. Two appendices provide more technical details.",0
https://doi.org/10.3758/s13414-013-0613-z,An objective measure of auditory stream segregation based on molecular psychophysics,"Auditory stream segregation is an important paradigm in the study of auditory scene analysis. Performance-based measures of auditory stream segregation have received increasing use as a complement to subjective reports of streaming. For example, the sensitivity in discriminating a temporal shift imposed on one B tone in an ABA sequence consisting of A and B tones that differ in frequency is often used to infer the perceptual organization (one stream vs. two streams). Limitations of these measures are discussed here, and an alternative measure based on the combination of decision weights and sensitivity is suggested. In the experiment, for ABA and ABB sequences varying in tempo (fast/slow) and duration (long/short), the sensitivity (dâ€²) in the temporal shift discrimination task did not differ between fast and slow sequences, despite strong differences in perceptual organization. The decision weights assigned to within-stream and between-stream interonset intervals also deviated from the idealized pattern of near-exclusive reliance on between-stream information in the subjectively integrated case, and on within-stream information in the subjectively segregated case. However, an estimate of internal noise computed using a combination of the estimated decision weights and sensitivity differentiated between sequences that were predominantly perceived as integrated or segregated, with significantly higher internal noise estimates for the segregated case. Therefore, the method of using a combination of decision weights and sensitivity provides a measure of auditory stream segregation that overcomes some of the limitations of purely sensitivity-based measures. Â© 2014 Psychonomic Society, Inc.",0
https://doi.org/10.1080/10705511.2015.1062730,Causal Mediation Analysis With a Binary Outcome and Multiple Continuous or Ordinal Mediators: Simulations and Application to an Alcohol Intervention,"We investigate a method to estimate the combined effect of multiple continuous/ordinal mediators on a binary outcome: 1) fit a structural equation model with probit link for the outcome and identity/probit link for continuous/ordinal mediators, 2) predict potential outcome probabilities, and 3) compute natural direct and indirect effects. Step 2 involves rescaling the latent continuous variable underlying the outcome to address residual mediator variance/covariance. We evaluate the estimation of risk-difference- and risk-ratio-based effects (RDs, RRs) using the ML, WLSMV and Bayes estimators in Mplus. Across most variations in path-coefficient and mediator-residual-correlation signs and strengths, and confounding situations investigated, the method performs well with all estimators, but favors ML/WLSMV for RDs with continuous mediators, and Bayes for RRs with ordinal mediators. Bayes outperforms WLSMV/ML regardless of mediator type when estimating RRs with small potential outcome probabilities and in two other special cases. An adolescent alcohol prevention study is used for illustration.",0
https://doi.org/10.1037/ccp0000034,The trajectory of fidelity in a multiyear trial of the family check-up predicts change in child problem behavior.,"Therapist fidelity to evidence-based family interventions has consistently been linked to child and family outcomes. However, few studies have evaluated the potential ebb and flow of fidelity of therapists over time. We examined therapist drift in fidelity over 4 years in the context of a Family Check-Up prevention services in early childhood (ages 2-5 years).At age 2, families engaging in Women, Infants, and Children Nutritional Supplement Program services were randomized and offered annual Family Check-Ups. Seventy-nine families with a child in the clinical range of problem behaviors at age 2 years were included in this analysis.Latent growth modeling revealed a significant linear decline in fidelity over time (M = -0.35, SD = 0.35) and that steeper declines were related to less improvement in caregiver-reported problem behaviors assessed at ages 7.5/8.5 years (b = -.69, p = .003; β = -.95, 95% CI [-2.11, -0.22]).These findings add to the literature concerning the need to continually monitor therapist fidelity to an evidence-based practice over time to optimize family benefits. Limitations and directions for future research are discussed.",0
https://doi.org/10.3758/pbr.15.6.1201,A hierarchical approach for fitting curves to response time measurements,"Understanding how response time (RT) changes with manipulations has been critical in distinguishing among theories in cognition. It is well known that aggregating data distorts functional relationships (e.g., Estes, 1956). Less well appreciated is a second pitfall: Minimizing squared errors (i.e., OLS regression) also distorts estimated functional forms with RT data. We discuss three properties of RT that should be modeled for accurate analysis and, on the basis of these three properties, provide a hierarchical Weibull regression model for regressing RT onto covariates. Hierarchical regression model analysis of lexical decision task data reveals that RT decreases as a power function of word frequency with the scale of RT decreasing 11% for every doubling of word frequency. A detailed discussion of the model and analysis techniques are presented as archived materials and may be downloaded from www.psychonomic.org/archive. Â© 2008 Psychonomic Society, Inc.",0
https://doi.org/10.1177/0010414009332136,The Politicized Participant,"Modern liberal democracies demand high and equal levels of political action. Unequal levels of political action between ideological groups may ultimately lead to biased policy. But to what extent do citizens’ ideological preferences affect their likelihood to participate politically? And does the institutional environment moderate this relationship? From rivaling theories, the authors construct hypotheses regarding the relationship between ideological preferences and participation and those regarding the moderating effect of state institutions. They test them for six modes of political action—voting, contacting, campaigning, cooperating, persuading, and protesting—through multilevel analyses of 27 elections in 20 Western democracies. First, they find that citizens’ ideological preferences are an important determinant political action. Second, they find that majoritarianism outperforms consensualism: In majoritarian systems, political action is more widespread and not less equal across the crucial factor of ideological preferences. The field should therefore reconsider Lijphart’s conclusions about the superiority of consensualism.",0
https://doi.org/10.1093/pan/mps040,Identification and Sensitivity Analysis for Multiple Causal Mechanisms: Revisiting Evidence from Framing Experiments,"Social scientists are often interested in testing multiple causal mechanisms through which a treatment affects outcomes. A predominant approach has been to use linear structural equation models and examine the statistical significance of the corresponding path coefficients. However, this approach implicitly assumes that the multiple mechanisms are causally independent of one another. In this article, we consider a set of alternative assumptions that are sufficient to identify the average causal mediation effects when multiple, causally related mediators exist. We develop a new sensitivity analysis for examining the robustness of empirical findings to the potential violation of a key identification assumption. We apply the proposed methods to three political psychology experiments, which examine alternative causal pathways between media framing and public opinion. Our analysis reveals that the validity of original conclusions is highly reliant on the assumed independence of alternative causal mechanisms, highlighting the importance of proposed sensitivity analysis. All of the proposed methods can be implemented via an open source R package, mediation .",0
https://doi.org/10.3102/10769986026003307,A Rasch Hierarchical Measurement Model,"In this article, a hierarchical measurement model is developed that enables researchers to measure a latent trait variable and model the error variance corresponding to multiple levels. The Rasch hierarchical measurement model (HMM) results when a Rasch IRT model and a one-way ANOVA with random effects are combined ( Bryk &amp; Raudenbush, 1992 ; Goldstein, 1987 ; Rasch, 1960 ). This model is appropriate for modeling dichotomous response strings nested within a contextual level. Examples of this type of structure include responses from students nested within schools and multiple response strings nested within people. Model parameter estimates of the Rasch HMM were obtained using the Bayesian data analysis methods of Gibbs sampling and the Metropolis-Hastings algorithm ( Gelfand, Hills, Racine-Poon, &amp; Smith, 1990 ; Hastings, 1970 ; Metropolis, Rosenbluth, Rosenbluth, Teller, &amp; Teller, 1953 ). The model is illustrated with two simulated data sets and data from the Sloan Study of Youth and Social Development. The results are discussed and parameter estimates for the simulated data sets are compared to parameter estimates obtained using a two-step estimation approach.",0
https://doi.org/10.1016/j.jneumeth.2009.10.006,A novel method for modeling facial allodynia associated with migraine in awake and freely moving rats,"Migraine is a neurovascular disorder that induces debilitating headaches associated with multiple symptoms including facial allodynia, characterized by heightened responsivity to normally innocuous mechanical stimuli. It is now well accepted that immune activation and immune-derived inflammatory mediators enhance pain responsivity, including the trigeminal system. Nociceptive (""pain"" responsive) trigeminal nerves densely innervate the cranial meninges. We have recently proposed that the meninges may serve as a previously unidentified, key interface between the peripheral immune system and the CNS with potential implications for understanding underlying migraine mechanisms. Our focus here is the development of a model for facial allodynia associated with migraine. We developed a model wherein an indwelling catheter is placed between the skull and dura, allowing immunogenic stimuli to be administered over the dura in awake and freely moving rats. Since the catheter does not contact the brain itself, any proinflammatory cytokines induced following manipulation derive from resident or recruited meningeal immune cells. While surgery alone does not alter immune activation markers, TNF or IL6 mRNA and/or protein, it does decrease gene expression and increase protein expression of IL-1 at 4 days after surgery. Using this model we show the induction of facial allodynia in response to supradural administration of either the HIV glycoprotein gp120 or inflammatory soup (bradykinin, histamine, serotonin, and prostaglandin E2), and the induction of hindpaw allodynia in our model after inflammatory soup. This model allows time- and dose-dependent assessment of the relationship between changes in meningeal inflammation and corresponding exaggerated pain behaviors.",0
https://doi.org/10.1080/00273171.2013.787870,Regime-Switching Bivariate Dual Change Score Model,"Mixture structural equation model with regime switching (MSEM-RS) provides one possible way of representing over-time heterogeneities in dynamic processes by allowing a system to manifest qualitatively or quantitatively distinct change processes conditional on the latent ""regime"" the system is in at a particular time point. Unlike standard mixture structural equation models such as growth mixture models, MSEM-RS allows individuals to transition between latent classes over time. This class of models, often referred to as regime-switching models in the time series and econometric applications, can be specified as regime-switching mixture structural equation models when the number of repeated measures involved is not large. We illustrate the empirical utility of such models using one special case-a regime-switching bivariate dual change score model in which two growth processes are allowed to manifest regime-dependent coupling relations with one another. The proposed model is illustrated using a set of longitudinal reading and arithmetic performance data from the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 study (ECLS-K; U.S. Department of Education, National Center for Education Statistics, 2010).",0
https://doi.org/10.1111/j.2517-6161.1972.tb00885.x,Bayes Estimates for the Linear Model,,0
https://doi.org/10.1207/s15328007sem0901_1,Comparison of Methods for Estimating and Testing Latent Variable Interactions,"Structural equation modeling methods for estimating and testing hypotheses about an interaction between continuous variables were investigated. The methods were (a) Bollen's (1996) 2-stage least squares (TSLS) method, Ping's (1996) 2-step maximum likelihood (ML) method, and Jaccard and Wan's (1995) ML method for the Kenny-Judd model (Kenny & Judd, 1984); (b) a 2-step ML procedure and ML estimation of the Joreskog-Yang model (Joreskog & Yang 1996); and (c) ML estimation of a revised Joreskog-Yang model. The TSLS procedure exhibited more bias and lower power than the other methods. Under ML estimation of the Joreskog-Yang model, Type I error rates were not well controlled when robust standard errors were used. Among the remaining procedures, the Jaccard-Wan procedure and ML estimation of the revised Joreskog-Yang procedure were most effective, with the latter having some small advantages over the former.",0
https://doi.org/10.1111/mec.13447,The aggregate site frequency spectrum for comparative population genomic inference,"Understanding how assemblages of species responded to past climate change is a central goal of comparative phylogeography and comparative population genomics, an endeavour that has increasing potential to integrate with community ecology. New sequencing technology now provides the potential to perform complex demographic inference at unprecedented resolution across assemblages of nonmodel species. To this end, we introduce the aggregate site frequency spectrum (aSFS), an expansion of the site frequency spectrum to use single nucleotide polymorphism (SNP) data sets collected from multiple, co-distributed species for assemblage-level demographic inference. We describe how the aSFS is constructed over an arbitrary number of independent population samples and then demonstrate how the aSFS can differentiate various multispecies demographic histories under a wide range of sampling configurations while allowing effective population sizes and expansion magnitudes to vary independently. We subsequently couple the aSFS with a hierarchical approximate Bayesian computation (hABC) framework to estimate degree of temporal synchronicity in expansion times across taxa, including an empirical demonstration with a data set consisting of five populations of the threespine stickleback (Gasterosteus aculeatus). Corroborating what is generally understood about the recent postglacial origins of these populations, the joint aSFS/hABC analysis strongly suggests that the stickleback data are most consistent with synchronous expansion after the Last Glacial Maximum (posterior probability = 0.99). The aSFS will have general application for multilevel statistical frameworks to test models involving assemblages and/or communities, and as large-scale SNP data from nonmodel species become routine, the aSFS expands the potential for powerful next-generation comparative population genomic inference.",0
https://doi.org/10.5641/027013605x13080719841239,Aging and Tennis Playing in a Coincidence-Timing Task With an Accelerating Object: The Role of Visuomotor Delay,"The purpose of the present study was to determine whether playing a specific ball sport, such as tennis, could maintain the coincidence-timing (CT) performance of older adults at a similar level to that of younger ones. To address this question, tennis players and nonplayers of three different age ranges (ages 20-30, 60-70, and 70-80 years) performed a simple CT task consisting of timing their response (pressing a button) to coincide with the arrival of a stimulus at a target. The stimulus moved at either an accelerating, constant, or decelerating velocity. As expected, all participants were affected by the velocity manipulation, which led to late and early responses to accelerating and decelerating stimuli, respectively. Whereas this response bias was increasingly pronounced with advancing age in nonplayers, no difference was found among player groups of different ages. Finally, we showed that the length of the visuomotor delay could explain the effect of nonconstant velocities. Â©2005 by the American Alliance for Health, Physical Education, Recreation and Dance.",0
https://doi.org/10.1017/cbo9780511802461.012,Bayesian multilevel modelling of cosmological populations,"Introduction Surveying the Universe is the ultimate remote sensing problem. Inferring the intrinsic properties of the galaxy population, via analysis of survey-generated catalogues, is a major challenge for twenty-first century cosmology, but this challenge must be met without any prospect of measuring these properties in situ. Thus, for example, our knowledge of the intrinsic luminosity and spatial distribution of galaxies is filtered by imperfect distance information and by observational selection effects, issues which have come to be known generically in the literature as 'Malmquist bias'. Figure 11.1 shows schematically how such effects may distort our inferences about the underlying population since, in general, these must be derived from a noisy, sparse and truncated sample of galaxies. There is a long (and mostly honourable!) tradition in the astronomical literature of attempts to cast such remote surveying problems within a rigorous statistical framework. Indeed, it is interesting to note that seminal examples from the early twentieth century (Eddington 1913, 1940; Malmquist 1920, 1922) display, at least with hindsight, hints of a Bayesian formulation long before the recent renaissance of Bayesian methods in astronomy. Unfortunately, space does not permit us to review in detail that early literature, nor many of the more recent papers which evolved from it. A more thorough discussion of the literature on statistical analysis of survey data can be found in, e.g., Hendry and Simmons (1995), Strauss and Willick (1995), Teerikorpi (1997) and Loredo (2007). Ã‚Â© Cambridge University Press, 2010.",0
https://doi.org/10.1146/annurev.soc.30.012703.110629,Panel Models in Sociological Research: Theory into Practice,"A selection of panel studies appearing in the American Sociological Review and the American Journal of Sociology between 1990 and 2003 shows that sociologists have been slow to capitalize on the advantages of panel data for controlling unobservables that threaten causal inference in observational studies. This review emphasizes regression methods that capitalize on the strengths of panel data for consistently estimating causal parameters in models for metric outcomes when measured explanatory variables are correlated with unit-specific unobservables. Both static and dynamic models are treated. Among the major subjects are fixed versus random effects methods, Hausman tests, Hausman-Taylor models, and instrumental variables methods, including Arrelano-Bond and Anderson-Hsaio estimation for models with lagged endogenous variables.",0
https://doi.org/10.1016/j.brat.2015.11.008,Increasing clinicians' EBT exploration and preparation behavior in youth mental health services by changing organizational culture with ARC,"Clinician EBT exploration and preparation behavior is essential to the ongoing implementation of new EBTs. This study examined the effect of the ARC organizational intervention on clinician EBT exploration and preparation behavior and assessed the mediating role of organizational culture as a linking mechanism.Fourteen community mental health agencies that serve youth in a major Midwestern metropolis along with 475 clinicians who worked in those agencies, were randomly assigned to either the three-year ARC intervention or control condition. Organizational culture was assessed with the Organizational Social Context (OSC) measure at baseline and follow-up. EBT exploration and preparation behavior was measured as clinician participation in nine separate community EBT workshops held over a three-year period.There was 69 percent greater odds (OR = 1.69, p < .003) of clinicians in the ARC condition (versus control) attending each subsequent workshop. Workshop attendance in the control group remained under two percent and declined over three years while attendance in the ARC condition grew from 3.6 percent in the first workshop to 12 percent in the ninth and final workshop. Improvement in proficient organizational culture mediated the positive effect of the ARC intervention on clinicians' workshop attendance (a × b = .21; 95% CI:LL = .05, UL = .40).Organizational interventions that create proficient mental health agency cultures can increase clinician EBT exploration and preparation behavior that is essential to the ongoing implementation of new EBTs in community youth mental health settings.",0
https://doi.org/10.1002/9780470583333.ch6,A Bootstrap Test of Shape Invariance Across Distributions,,0
https://doi.org/10.1016/b978-044452044-9/50004-5,Covariance Structure Models for Maximal Reliability of Unit-Weighted Composites,"Abstract When developing or evaluating scales, the internal consistency reliability of the scale based on its items or parts is always an important issue. The growth of structural modeling has allowed the easy computation of model-based estimates of reliability. These are typically touted as replacements for coefficient alpha, which remains the most widely used measure of internal consistency. Among model-based estimates, coefficients based on a 1-factor model have been most widely recommended. However, when the 1-factor model does not fit the data, the meaning of such a coefficient is unclear. A new identification condition for factor analytic models is proposed that assures the composite can be modeled with only one common factor even if the components are multidimensional. This common factor is maximally correlated with the composite, and the reliability of the composite is the maximal internal consistency coefficient for a unit-weighted composite. The coefficient also describes k-factor reliability, the greatest lower bound to reliability, and reliability for any composite from a latent variable model with additive errors. Reliability coefficients for differentially-weighted composites are also described, and differentially-weighted maximal reliability is contrasted with unit-weighted maximal reliability. Computational methods for these coefficients are described.",0
https://doi.org/10.1177/0013164414565007,Assessing Spurious Interaction Effects in Structural Equation Modeling,"Several studies have stressed the importance of simultaneously estimating interaction and quadratic effects in multiple regression analyses, even if theory only suggests an interaction effect should be present. Specifically, past studies suggested that failing to simultaneously include quadratic effects when testing for interaction effects could result in Type I errors, Type II errors, or misleading interactions. Research investigating this issue has been limited to multiple regression models. Contrarily, structural equation modeling is a more appropriate analysis when hypotheses include latent variables. The current study utilized Monte Carlo simulation to investigate whether quadratic effects should be included in the latent variable interaction model. Consistent with previous research, it was found that including latent variable quadratic effects in the model successfully reduced the frequency of spurious interaction effects but at a cost of low power to detect true interaction effects, inaccurate parameter estimates, inaccurate standard error estimates, and reduced convergence rates. Based on findings from the current study, we recommend that researchers hypothesizing interactions between latent variables should test for these relations using the latent variable interaction model rather than the interaction quadratic model. If researchers are concerned about spurious interactions, then they may want to consider including quadratic effects in the model, provided that they have sample sizes of at least 500 and high indicator reliability. We caution all researchers to base higher order effects models on theory.",0
https://doi.org/10.1214/aos/1176342360,A Bayesian Analysis of Some Nonparametric Problems,"The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let $\mathscr{X}$ be a space and $\mathscr{A}$ a $\sigma$-field of subsets, and let $\alpha$ be a finite non-null measure on $(\mathscr{X}, \mathscr{A})$. Then a stochastic process $P$ indexed by elements $A$ of $\mathscr{A}$, is said to be a Dirichlet process on $(\mathscr{X}, \mathscr{A})$ with parameter $\alpha$ if for any measurable partition $(A_1, \cdots, A_k)$ of $\mathscr{X}$, the random vector $(P(A_1), \cdots, P(A_k))$ has a Dirichlet distribution with parameter $(\alpha(A_1), \cdots, \alpha(A_k)). P$ may be considered a random probability measure on $(\mathscr{X}, \mathscr{A})$, The main theorem states that if $P$ is a Dirichlet process on $(\mathscr{X}, \mathscr{A})$ with parameter $\alpha$, and if $X_1, \cdots, X_n$ is a sample from $P$, then the posterior distribution of $P$ given $X_1, \cdots, X_n$ is also a Dirichlet process on $(\mathscr{X}, \mathscr{A})$ with a parameter $\alpha + \sum^n_1 \delta_{x_i}$, where $\delta_x$ denotes the measure giving mass one to the point $x$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on $(\mathscr{X}, \mathscr{A})$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space $\mathscr{X}$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where $\mathscr{X}$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general $\mathscr{X}$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that $P$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a $P$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis $H_0$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter $\alpha$ itself a uniform measure on [0, 1], and if we are given a sample of size $n \geqq 2$, the only nontrivial nonrandomized Bayes rule is to reject $H_0$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.",0
https://doi.org/10.1007/s10464-013-9570-x,Improving Classroom Quality with The RULER Approach to Social and Emotional Learning: Proximal and Distal Outcomes,"The RULER Approach to Social and Emotional Learning (""RULER"") is designed to improve the quality of classroom interactions through professional development and classroom curricula that infuse emotional literacy instruction into teaching-learning interactions. Its theory of change specifies that RULER first shifts the emotional qualities of classrooms, which are then followed, over time, by improvements in classroom organization and instructional support. A 2-year, cluster randomized controlled trial was conducted to test hypotheses derived from this theory. Sixty-two urban schools either integrated RULER into fifth- and sixth-grade English language arts (ELA) classrooms or served as comparison schools, using their standard ELA curriculum only. Results from multilevel modeling with baseline adjustments and structural equation modeling support RULER's theory of change. Compared to classrooms in comparison schools, classrooms in RULER schools exhibited greater emotional support, better classroom organization, and more instructional support at the end of the second year of program delivery. Improvements in classroom organization and instructional support at the end of Year 2 were partially explained by RULER's impacts on classroom emotional support at the end of Year 1. These findings highlight the important contribution of emotional literacy training and development in creating engaging, empowering, and productive learning environments.",0
https://doi.org/10.1177/0081175014529767,An Extended Cultural Consensus Theory Model to Account for Cognitive Processes in Decision Making in Social Surveys,"In recent decades, cultural consensus theory (CCT) models have been extensively applied across research domains to explore shared cultural knowledge and beliefs. These models are parameterized in terms of person-specific cognitive parameters such as abilities and guessing biases as well as item difficulties. Although psychometric test theory is also formalized in terms of abilities and item difficulties, a quality that clearly sets CCT models apart from other test theory models is their specification to operate on data in which the answer key is latent. In doing so, CCT models specify the answer key as parameters of the model, and also involved with this specification are procedures to verify the integrity of the answer key that is estimated. In this article, the authors develop the following methods to propagate the application of these CCT models in the field of social surveys: (1) by extending the underlying cognitive model to be able to account for uncertainty in decision making (“don’t know” responses), (2) by allowing covariate information to be entered in the analysis, and (3) by deriving statistical inference in the hierarchical Bayesian framework. The proposed model is fit to data describing knowledge on science and on aging to demonstrate the novel findings that can be achieved by the approach.",0
https://doi.org/10.1016/j.cmpb.2010.05.003,A Bayesian multilevel model for fMRI data analysis,"Bayesian approaches have been proposed by several functional magnetic resonance imaging (fMRI) researchers in order to overcome the fundamental limitations of the popular statistical parametric mapping method. However, the difficulties associated with subjective prior elicitation have prevented the widespread adoption of the Bayesian methodology by the neuroimaging community. In this paper, we present a Bayesian multilevel model for the analysis of brain fMRI data. The main idea is to consider that all the estimated group effects (fMRI activation patterns) are exchangeable. This means that all the collected voxel time series are considered manifestations of a few common underlying phenomena. In contradistinction to other Bayesian approaches, we think of the estimated activations as multivariate random draws from the same distribution without imposing specific prior spatial and/or temporal information for the interaction between voxels. Instead, a two-stage empirical Bayes prior approach is used to relate voxel regression equations through correlations between the regression coefficient vectors. The adaptive shrinkage properties of the Bayesian multilevel methodology are exploited to deal with spatial variations, and noise outliers. The characteristics of the proposed model are evaluated by considering its application to two real data sets.",0
https://doi.org/10.3758/bf03194547,Converting between measures of slope of the psychometric function,"The psychometric function's slope provides information about the reliability of psychophysical threshold estimates. Furthermore, knowing the slope allows one to compare, across studies, thresholds that were obtained at different performance criterion levels. Unfortunately, the empirical validation of psychometric function slope estimates is hindered by the bewildering variety of slope measures that are in use. The present article provides conversion formulas for the most popular cases, including the logistic, Weibull, Quick, cumulative normal, and hyperbolic tangent functions as analytic representations, in both linear and log coordinates and to different log bases, the practical decilog unit, the empirically based interquartile range measure of slope, and slope in a d' representation of performance.",0
https://doi.org/10.1111/biom.12299,Bayesian function-on-function regression for multilevel functional data,"Medical and public health research increasingly involves the collection of complex and high dimensional data. In particular, functional data-where the unit of observation is a curve or set of curves that are finely sampled over a grid-is frequently obtained. Moreover, researchers often sample multiple curves per person resulting in repeated functional measures. A common question is how to analyze the relationship between two functional variables. We propose a general function-on-function regression model for repeatedly sampled functional data on a fine grid, presenting a simple model as well as a more extensive mixed model framework, and introducing various functional Bayesian inferential procedures that account for multiple testing. We examine these models via simulation and a data analysis with data from a study that used event-related potentials to examine how the brain processes various types of images.",0
https://doi.org/10.1162/003465304323023705,"Using Matching to Estimate Treatment Effects: Data Requirements, Matching Metrics, and Monte Carlo Evidence","We compare propensity-score matching methods with covariatematching estimators. We first discuss the data requirements of propensity-score matching estimators and covariate matching estimators. Then we propose two new matching metrics incorporating the treatment outcome information and participation indicator information, and discuss the motivations of different metrics. Next we study the small-sample properties of propensity-score matching versus covariate matching estimators, and of different matching metrics, through Monte Carlo experiments. Through a series of simulations, we provide some guidance to practitioners on how to choose among different matching estimators and matching metrics.",0
https://doi.org/10.1371/journal.pone.0075946,"A Structural Equation Model Analysis of Relationships among ENSO, Seasonal Descriptors and Wildfires","Seasonality drives ecological processes through networks of forcings, and the resultant complexity requires creative approaches for modeling to be successful. Recently ecologists and climatologists have developed sophisticated methods for fully describing seasons. However, to date the relationships among the variables produced by these methods have not been analyzed as networks, but rather with simple univariate statistics. In this manuscript we used structural equation modeling (SEM) to analyze a proposed causal network describing seasonality of rainfall for a site in south-central Florida. We also described how this network was influenced by the El Niño-Southern Oscillation (ENSO), and how the network in turn affected the site's wildfire regime. Our models indicated that wet and dry seasons starting later in the year (or ending earlier) were shorter and had less rainfall. El Niño conditions increased dry season rainfall, and via this effect decreased the consistency of that season's drying trend. El Niño conditions also negatively influenced how consistent the moistening trend was during the wet season, but in this case the effect was direct and did not route through rainfall. In modeling wildfires, our models showed that area burned was indirectly influenced by ENSO via its effect on dry season rainfall. Area burned was also indirectly reduced when the wet season had consistent rainfall, as such wet seasons allowed fewer wildfires in subsequent fire seasons. Overall area burned at the study site was estimated with high accuracy (R (2) score = 0.63). In summary, we found that by using SEMs, we were able to clearly describe causal patterns involving seasonal climate, ENSO and wildfire. We propose that similar approaches could be effectively applied to other sites where seasonality exerts strong and complex forcings on ecological processes.",0
https://doi.org/10.1007/s10880-015-9423-x,Parenting Stress Related to Behavioral Problems and Disease Severity in Children with Problematic Severe Asthma,"Our study examined parenting stress and its association with behavioral problems and disease severity in children with problematic severe asthma. Research participants were 93 children (mean age 13.4 ± 2.7 years) and their parents (86 mothers, 59 fathers). As compared to reference groups analyzed in previous research, scores on the Parenting Stress Index in mothers and fathers of the children with problematic severe asthma were low. Higher parenting stress was associated with higher levels of internalizing and externalizing behavioral problems in children (Child Behavior Checklist). Higher parenting stress in mothers was also associated with higher airway inflammation (FeNO). Thus, although parenting stress was suggested to be low in this group, higher parenting stress, especially in the mother, is associated with more airway inflammation and greater child behavioral problems. This indicates the importance of focusing care in this group on all possible sources of problems, i.e., disease exacerbations and behavioral problems in the child as well as parenting stress.",0
https://doi.org/10.1080/03610920903009376,Assessing Differential Area Mortality Trends via Bayesian Random Effects,"Changes in area mortality are important for assessing spatial health inequality. They are likely to be differentiated by age as well as spatially and may vary by demographic strata (e.g., gender, ethnic group). A simple approach assumes linear improvement in log mortality risks, with noninteracting area and age coefficients. By contrast, this article considers parsimonious models for mortality change allowing nonlinear trends and interactions between ages and areas in mortality levels and trends. A case study considers trends in mortality in 32 London boroughs over the 8-year period 1999–2006 for deaths data disaggregated by age, sex, and area.",0
https://doi.org/10.1186/s12939-015-0217-4,"Social inequality in morbidity, framed within the current economic crisis in Spain","Inspired by the 'Fundamental Cause Theory (FCT)' we explore social inequalities in preventable versus relatively less-preventable illnesses in Spain. The focus is on the education-health gradient, as education is one of the most important components of an individual's socioeconomic status (SES). Framed in the context of the recent economic crisis, we investigate the education gradient in depression, diabetes, and myocardial infarction (relatively highly preventable illnesses) and malignant tumors (less preventable), and whether this educational gradient varies across the regional-economic context and changes therein.We use data from three waves of the Spanish National Health Survey (2003-2004, 2006-2007, and 2011-2012), and from the 2009-2010 wave of the European Health Survey in Spain, which results in a repeated cross-sectional design. Logistic multilevel regressions are performed with depression, diabetes, myocardial infarction, and malignant tumors as dependent variables. The multilevel design has three levels (the individual, period-regional, and regional level), which allows us to estimate both longitudinal and cross-sectional macro effects. The regional-economic context and changes therein are assessed using the real GDP growth rate and the low work intensity indicator.Education gradients in more-preventable illness are observed, while this is far less the case in our less-preventable disease group. Regional economic conditions seem to have a direct impact on depression among Spanish men (y-stand. OR = 1.04 [95 % CI: 1.01-1.07]). Diabetes is associated with cross-regional differences in low work intensity among men (y-stand. OR = 1.02 [95 % CI: 1.00-1.05]) and women (y-stand. OR = 1.04 [95 % CI: 1.01-1.06]). Economic contraction increases the likelihood of having diabetes among men (y-stand. OR = 1.04 [95 % CI: 1.01-1.06]), and smaller decreases in the real GDP growth rate are associated with lower likelihood of myocardial infarction among women (y-stand. OR = 0.83 [95 % CI: 0.69-1.00]). Finally, there are interesting associations between the macroeconomic changes across the crisis period and the likelihood of suffering from myocardial infarction among lower educated groups, and the likelihood of having depression and diabetes among less-educated women.Our findings partially support the predictions of the FCT for Spain. The crisis effects on health emerge especially in the case of our more-preventable illnesses and among lower educated groups. Health inequalities in Spain could increase rapidly in the coming years due to the differential effects of recession on socioeconomic groups.",0
https://doi.org/10.1037/a0035666,Differences in within- and between-person factor structure of positive and negative affect: Analysis of two intensive measurement studies using multilevel structural equation modeling.,"The Positive and Negative Affect Schedule (PANAS) is a widely used measure of emotional experience. The factor structure of the PANAS has been examined predominantly with cross-sectional designs, which fails to disaggregate within-person variation from between-person differences. There is still uncertainty as to the factor structure of positive and negative affect and whether they constitute 2 distinct independent factors. The present study examined the within-person and between-person factor structure of the PANAS in 2 independent samples that reported daily affect over 7 and 14 occasions, respectively. Results from multilevel confirmatory factor analyses revealed that a 2-factor structure at both the within-person and between-person levels, with correlated specific factors for overlapping items, provided good model fit. The best-fitting solution was one where within-person factors of positive and negative affect were inversely correlated, but between-person factors were independent. The structure was further validated through multilevel structural equation modeling examining the effects of cognitive interference, daily stress, physical symptoms, and physical activity on positive and negative affect factors.",0
https://doi.org/10.1111/j.1467-9574.2005.00279.x,Bayesian model selection using encompassing priors,"This paper deals with Bayesian selection of models that can be specified using inequality constraints among the model parameters. The concept of encompassing priors is introduced, that is, a prior distribution for an unconstrained model from which the prior distributions of the constrained models can be derived. It is shown that the Bayes factor for the encompassing and a constrained model has a very nice interpretation: it is the ratio of the proportion of the prior and posterior distribution of the encompassing model in agreement with the constrained model. It is also shown that, for a specific class of models, selection based on encompassing priors will render a virtually objective selection procedure. The paper concludes with three illustrative examples: an analysis of variance with ordered means; a contingency table analysis with ordered odds-ratios; and a multilevel model with ordered slopes.",0
https://doi.org/10.3758/bf03202828,Quest: A Bayesian adaptive psychometric method,"An adaptive psychometric procedure that places each trial at the current most probable Bayesian estimate of threshold is described. The procedure takes advantage of the common finding that the human psychometric function is invariant in form when expressed as a function of log intensity. The procedure is simple, fast, and efficient, and may be easily implemented on any computer.",0
https://doi.org/10.1177/0047287507299576,Perceptions-Based Analysis of Tourism Products and Service Providers,"Perceptions-based analysis is a general framework for analyzing tourist choice alternatives involving large sets of perceived attributes. Its development in tourism research was encouraged as tourism is characterized by multifaceted choice alternatives such as destinations and other experiential “products” that the tourists cannot succinctly evaluate with a small number of attributes. PBA differentiates between the generic perceptions of a class of choice alternatives (destinations, product/service providers; e.g., “metropolitan city”; “tour operator”), the perceptual profiles of a specific choice alternative (“San Francisco”; “Thomas Cook”), and the profiles of the choice alternatives preferred or actually selected. It condenses the attribute profiles into distinguished perceptual positions and analyzes their competitive relationships by diagnosing the perceptual strengths and weaknesses of the choice alternatives. Further analysis reveals the number of choice decisions made in favor of a uniquely positioned choice alternative or one sharing its position with others. An empirical example for tour operators illustrates the various data processing steps and discusses their policy implications.",0
https://doi.org/10.1177/0013161x11419653,Toward an Organizational Model of Change in Elementary Schools,"Purpose: This study explored a theoretical model that links teachers’ perceived uncertainty and teachers’ sense of collective efficacy with organizational learning mechanisms (OLMs) in elementary schools. OLMs serve as a mediator construct. Research Design: For testing the primary theoretical model, 801 teachers from 61 elementary schools (33 urban and 28 suburban) in Israel’s largest district responded to the research instruments. The authors used structural equation modeling to determine whether OLMs mediate between teachers’ perceived uncertainty and their sense of collective efficacy. Findings: A significant model, which included direct and indirect effects of teachers’ perceived uncertainty on teachers’ sense of collective efficacy, emerged for the urban school context. Although OLMs (storing, retrieving, and putting to use of information) served as a prominent mediating variable in the urban school context, OLMs did not play a mediating role in the research model for the suburban school context. Conclusions: This study strengthens the feasibility of the OLMs framework, based on information processing, to provide a concrete description of organizational learning processes in schools. The study provides a deeper understanding of how OLMs can serve as a significant link between the dynamic school environment and teachers’ attitudes, which may ultimately improve teachers’ work and students’ learning.",0
https://doi.org/10.1037/a0020957,Evaluating expectations about negative emotional states of aggressive boys using Bayesian model selection.,"Researchers often have expectations about the research outcomes in regard to inequality constraints between, e.g., group means. Consider the example of researchers who investigated the effects of inducing a negative emotional state in aggressive boys. It was expected that highly aggressive boys would, on average, score higher on aggressive responses toward other peers than moderately aggressive boys, who would in turn score higher than nonaggressive boys. In most cases, null hypothesis testing is used to evaluate such hypotheses. We show, however, that hypotheses formulated using inequality constraints between the group means are generally not evaluated properly. The wrong hypotheses are tested, i.e.. the null hypothesis that group means are equal. In this article, we propose an innovative solution to these above-mentioned issues using Bayesian model selection, which we illustrate using a case study.",0
https://doi.org/10.3758/bf03196299,Quantile maximum likelihood estimation of response time distributions,"We introduce and evaluate via a Monte Carlo study a robust new estimation technique that fits distribution functions to grouped response time (RT) data, where the grouping is determined by sample quantiles. The new estimator, quantile maximum likelihood (QML), is more efficient and less biased than the best alternative estimation technique when fitting the commonly used ex-Gaussian distribution. Limitations of the Monte Carlo results are discussed and guidance provided for the practical application of the new technique. Because QML estimation can be computationally costly, we make fast open source code for fitting available that can be easily modified to use QML in the estimation of any distribution function.",0
https://doi.org/10.1097/meg.0000000000000376,Comparative effectiveness of antiviral treatment for hepatitis B,"A wide variety of competing drugs are available to patients for the treatment of chronic hepatitis B. We update a recent meta-analysis to include additional trial evidence with the aim of determining which treatment is the most effective.Twelve monotherapy or combination therapy were evaluated in treatment-naive individuals with hepatitis B e antigen (HBeAg) positive or negative patients. Databases were searched for randomized controlled trials in the first year of therapy. Bayesian random effects network meta-analysis was used to calculate the pairwise odds ratios, 95% credible intervals and ranking of six surrogate outcomes.In total, 22 studies were identified (7508 patients): 12 studies analysed HBeAg-positive patients, six analysed HBeAg-negative patients, and four evaluated both. Tenofovir was most effective at increasing efficacy in HBeAg-positive patients, ranking first for three outcomes and increased odds of undetectable levels of hepatitis B virus (HBV) DNA compared with seven other therapies (such as lamivudine: odds ratio 33.0; 95% credible interval 7.0-292.7). For HBeAg-negative patients, the large network (seven therapies) ranked entecavir alone or in combination with tenofovir highly for reduction in HBV DNA and histologic improvement. In the smaller network (three therapies), tenofovir ranked first for undetectable HBV DNA and histologic improvement. No data existed to directly or indirectly compare these treatments.For HBeAg-positive patients tenofovir is the most effective at increasing efficacy, whereas for HBeAg-negative patients, either tenofovir or entecavir is most effective. Further research should focus on strengthening the network connections, in particular comparing tenofovir and entecavir in HBeAg-negative patients.",0
https://doi.org/10.1093/esr/jcu049,"Gen(d)eralized Trust: Women, Work, and Trust in Strangers","This article deals with the question as to whether gender equality in labour force participation affects generalized trust. Following the seminal work of Rothstein and Uslaner, a first hypothesis m ...",0
https://doi.org/10.1177/01461672012711014,Conscientiousness and the Theory of Planned Behavior: Toward a more Complete Model of the Antecedents of Intentions and Behavior,"Two studies explored the relationship between past behavior, personality traits, intentions, and behavior. Study 1 (N = 181) considered intentions to engage in goal-directed activity (health protection). Cognitions specified by the Theory of Planned Behavior were examined as mediators of the relationship between past behavior, personality, and intentions. The effect of conscientiousness on intention was partially mediated by cognitions, whereas the effect of past behavior was partially mediated by cognitions and conscientiousness. Study 2 (N = 123) examined predictions of intentions and self-reported behavior in relation to both health protection and exercise, a more specific behavior. In both cases, the effect of conscientiousness on intention was totally mediated, whereas the effect on behavior was partially mediated. Similarly, the effects of past behavior on intentions were totally mediated, whereas the effects on behavior were partially mediated by cognitions and conscientiousness. Thus, combining personality traits and cognitions provided a more sufficient account of the determinants of intentions and behavior.",0
https://doi.org/10.4324/9780203775851-19,Pulling the Sobel Test Up By Its Bootstraps,,0
https://doi.org/10.1080/10705511.2016.1185723,Assessing and Controlling Acquiescent Responding When Acquiescence and Content Are Related: A Comprehensive Factor-Analytic Approach,"This article proposes procedures for assessing and controlling acquiescence in personality scales when acquiescence is related to the content that the scale intends to measure. Our proposal is comprehensive in that it can be applied to different item response formats fitted with response models that can be parameterized as factor-analytic models. In the calibration stage, our proposal makes joint use of a balanced scale and a set of markers for acquiescence, and consists of 2 sequential procedures: a direct semirestricted solution, and a restricted solution with minimal identification constraints. In the scoring stage, we discuss how the information given by the acquiescence–content relation can be used to obtain Bayes expected a posteriori scores. The robustness of the direct procedure is assessed both analytically and by simulation. A free, user-friendly program that implements the procedures proposed is made available. Practical issues of use and interpretation are discussed and illustrated with an emp...",0
https://doi.org/10.1080/01621459.1989.10478848,Choosing among Alternative Nonexperimental Methods for Estimating the Impact of Social Programs: The Case of Manpower Training,The recent literature on evaluating manpower training programs demonstrates that alternative nonexperimental estimators of the same program produce a array of estimates of program impact. These findings have led to the call for experiments to be used to perform credible program evaluations. Missing in all of the recent pessimistic analyses of nonexperimental methods is any systematic discussion of how to choose among competing estimators. This paper explores the value of simple specification tests in selecting an appropriate nonexperimental estimator. A reanalysis of the National Supported Work Demonstration Data previously analyzed by proponents of social experiments reveals that a simple testing procedure eliminates the range of nonexperimental estimators that are at variance with the experimental estimates of program impact.,0
https://doi.org/10.1007/s11336-015-9461-1,Profile Likelihood-Based Confidence Intervals and Regions for Structural Equation Models,"Structural equation models (SEM) are widely used for modeling complex multivariate relationships among measured and latent variables. Although several analytical approaches to interval estimation in SEM have been developed, there lacks a comprehensive review of these methods. We review the popular Wald-type and lesser known likelihood-based methods in linear SEM, emphasizing profile likelihood-based confidence intervals (CIs). Existing algorithms for computing profile likelihood-based CIs are described, including two newer algorithms which are extended to construct profile likelihood-based confidence regions (CRs). Finally, we illustrate the use of these CIs and CRs with two empirical examples, and provide practical recommendations on when to use Wald-type CIs and CRs versus profile likelihood-based CIs and CRs. OpenMx example code is provided in an Online Appendix for constructing profile likelihood-based CIs and CRs for SEM. Â© 2015, The Psychometric Society.",0
,Conditional logit analysis of qualitative choice behavior,,0
https://doi.org/10.1198/016214507000001409,Bivariate Binomial Spatial Modeling of <i>Loa loa</i> Prevalence in Tropical Africa,"We present a state-of-the-art application of smoothing for dependent bivariate binomial spatial data to Loa loa prevalence mapping in West Africa. This application starts with the nonspatial calibration of survey instruments, continues with the spatial model building and assessment, and ends with robust, tested software intended for use by field workers for online prevalence map updating. From a statistical perspective, we address several important methodological issues: building spatial models that are sufficiently complex to capture the structure of the data but remain computationally usable, reducing the computational burden in the handling of very large covariate data sets, and devising methods for comparing spatial prediction methods for a given exceedance policy threshold.",0
https://doi.org/10.1007/s10198-013-0524-x,Equity in specialist waiting times by socioeconomic groups: evidence from Spain,"In countries with publicly financed health care systems, waiting time - rather than price - is the rationing mechanism for access to health care services. The normative statement underlying such a rationing device is that patients should wait according to need and irrespective of socioeconomic status or other non-need characteristics. The aim of this paper is to test empirically that waiting times for publicly funded specialist care do not depend on patients' socioeconomic status. Waiting times for specialist care can vary according to the type of medical specialty, type of consultation (review or diagnosis) and the region where patients' reside. In order to take into account such variability, we use Bayesian random parameter models to explain waiting times for specialist care in terms of need and non-need variables. We find that individuals with lower education and income levels wait significantly more time than their counterparts. Ã‚Â© 2013 Springer-Verlag Berlin Heidelberg.",0
https://doi.org/10.1111/j.1467-9868.2007.00612.x,Order-free co-regionalized areal data models with application to multiple-disease mapping,"Summary. With the ready availability of spatial databases and geographical information system software, statisticians are increasingly encountering multivariate modelling settings featuring associations of more than one type: spatial associations between data locations and associations between the variables within the locations. Although flexible modelling of multivariate point-referenced data has recently been addressed by using a linear model of co-regionalization, existing methods for multivariate areal data typically suffer from unnecessary restrictions on the covariance structure or undesirable dependence on the conditioning order of the variables. We propose a class of Bayesian hierarchical models for multivariate areal data that avoids these restrictions, permitting flexible and order-free modelling of correlations both between variables and across areal units. Our framework encompasses a rich class of multivariate conditionally autoregressive models that are computationally feasible via modern Markov chain Monte Carlo methods. We illustrate the strengths of our approach over existing models by using simulation studies and also offer a real data application involving annual lung, larynx and oesophageal cancer death-rates in Minnesota counties between 1990 and 2000.",0
https://doi.org/10.1037/a0014990,Editorial.,"The Journal of Applied Psychology is the oldest and largest top-tier journal publishing theory and research relevant to industrial and organizational psychology, organizational behavior, and human resources management. The primary emphasis of this journal is the publication of original investigations that advance theoretical understanding and create new knowledge for applied psychology within the broad scope of the organizational sciences. We are primarily interested in publishing empirical research and conceptual articles that enhance understanding of psychological phenomena in human and organizational systems. This editorial also covers the expectations and review process that the Journal of Applied Psychology has for manuscripts submitted to the journal. (PsycINFO Database Record (c) 2009 APA, all rights reserved).",0
https://doi.org/10.1080/07350015.2015.1052457,Unobserved Heterogeneity in Income Dynamics: An Empirical Bayes Perspective,"Empirical Bayes methods for Gaussian compound decision problems involving longitudinal data are considered. The new convex optimization formulation of the nonparametric (Kiefer–Wolfowitz) maximum likelihood estimator for mixture models is employed to construct nonparametric Bayes rules for compound decisions. The methods are first illustrated with some simulation examples and then with an application to models of income dynamics. Using panel data, we estimate a simple dynamic model of earnings that incorporates bivariate heterogeneity in intercept and variance of the innovation process. Profile likelihood is employed to estimate an AR(1) parameter controlling the persistence of the innovations. We find that persistence is relatively modest, ρ^≈0.48, when we permit heterogeneity in variances. Evidence of negative dependence between individual intercepts and variances is revealed by the nonparametric estimation of the mixing distribution, and has important consequences for forecasting future income trajecto...",0
https://doi.org/10.1080/00273171.2014.1003772,"A Multilevel AR(1) Model: Allowing for Inter-Individual Differences in Trait-Scores, Inertia, and Innovation Variance.","In this article we consider a multilevel first-order autoregressive [AR(1)] model with random intercepts, random autoregression, and random innovation variance (i.e., the level 1 residual variance). Including random innovation variance is an important extension of the multilevel AR(1) model for two reasons. First, between-person differences in innovation variance are important from a substantive point of view, in that they capture differences in sensitivity and/or exposure to unmeasured internal and external factors that influence the process. Second, using simulation methods we show that modeling the innovation variance as fixed across individuals, when it should be modeled as a random effect, leads to biased parameter estimates. Additionally, we use simulation methods to compare maximum likelihood estimation to Bayesian estimation of the multilevel AR(1) model and investigate the trade-off between the number of individuals and the number of time points. We provide an empirical illustration by applying the extended multilevel AR(1) model to daily positive affect ratings from 89 married women over the course of 42 consecutive days.",0
https://doi.org/10.3758/bf03203007,A quadrature method for Bayesian sequential threshold estimation,"For two-alternative forced-choice (2AFC) sequential threshold estimation, a finite Bayesian method (Emerson, 1986b) has been found to perform better than a closely related maximum-likelihood (ML) method. The comparison was especially apt because the two methods used all the same computations (Shelton, 1983) in maintaining and updating the representation of the log-likelihood function. This representation was in the form of a finite discrete array of about 50 equally spaced numerical elements in computer memory. The updating consisted of a series of offset lookups and additions from either of two precomputed arrays representing the log-likelihoods of correct and incorrect responses on a single trial. The difference between the ML and Bayesian methods was in the way the updated log-likelihood array was used to obtain the new estimate of the threshold after each trial. For the ML method, the array was merely scanned for its maximal element. For the Bayesian method, each element was exponentiated and the results were treated as unnormalized probabilities. These were then summed, with and without a multiplicative factor of the array index, to obtain the Bayesian normalizing constant and expected value of the threshold. These additional computations made the Bayesian method considerably slower than the ML method. Real-time use of that finite Bayesian method requires a compiled version of the program, rather than interpretive BASIC, and one of the faster PC-class computers. Statistically, the Bayesian estimates were generally better than the ML in that (1) there was no general negative bias, (2) biases toward the initial estimate decreased rapidly with increasing trials in the run, (3) variances were generally smaller, and (4) the biases and variances depended much less on the location of the true threshold in the assumed stimulus range.",0
https://doi.org/10.1016/0167-8116(94)90033-7,Commercial use of conjoint analysis in Europe: Results and critical reflections,"Abstract We report the incidence of conjoint analysis applications by European market research suppliers. Based on responses to a survey, we document about 1,000 commercial projects over a five-year period and show breakdowns by product category, study purpose, and other characteristics such as study design, data collection, and data analysis. The results are compared with information collected for an earlier time period in the United States. We also discuss recent conjoint research results, identify issues that warrant further study, and suggest how the current practice of conjoint analysis can be improved.",0
,Evaluation of suspected measles incidence rate trend in Iran and the affecting factors: Negative-Binomial mixed model,"Introduction: The identifying incidence rate trend of disease and its changes lead to update response of surveillance system. Syndromic surveillance system is based on the suspected cases, so it has high speed in detecting outbreaks. This study aimed to evaluate trend of fever and rash incidence rates and detect affecting factors. Materials and Methods: This study was a retrospective cohort study and the data included the suspected measles cases in provinces of Iran in 1977-2012, which extracted from surveillance system of vaccine preventable diseases. We fitted Poisson and Negative-Binomial regression models with random effect. Modeling and inferences were based on a Bayesian algorithm. We used R and OpenBUGS software. The fitted models were compared based on Deviance and Chi-square goodness of fit statistics. Results: Interaction effect between year and immunization campaign was statistically significant (95% CrI:1.083,1.737), after immunization campaign, trend was increasing. The variance of random component in model was statistically significant (95% CrI: 0.219,0.430). On the other hand, province-specific characterizes found affecting factor on suspected incidence rate. Conclusion: In attention to increasing trend of this incidence in Iran, especially in recently years, and affecting of province-specific characterizes on suspected incidence rate, We found that more accurate control and improvement of quality vaccination is essential. Ã‚Â© 2015, Semnan University of Medical Sciences. All rights reserved.",0
https://doi.org/10.1111/j.1745-3984.2006.00015.x,Comparing Methods of Assessing Differential Item Functioning in a Computerized Adaptive Testing Environment,"Mantel-Haenszel and SIBTEST, which have known difficulty in detecting non-unidirectional differential item functioning (DIF), have been adapted with some success for computerized adaptive testing (CAT). This study adapts logistic regression (LR) and the item-response-theory-likelihood-ratio test (IRT-LRT), capable of detecting both unidirectional and non-unidirectional DIF, to the CAT environment in which pretest items are assumed to be seeded in CATs but not used for trait estimation. The proposed adaptation methods were evaluated with simulated data under different sample size ratios and impact conditions in terms of Type I error, power, and specificity in identifying the form of DIF. The adapted LR and IRT-LRT procedures are more powerful than the CAT version of SIBTEST for non-unidirectional DIF detection. The good Type I error control provided by IRT-LRT under extremely unequal sample sizes and large impact is encouraging. Implications of these and other findings are discussed.",0
https://doi.org/10.1186/1471-2288-11-77,Logistic random effects regression models: a comparison of statistical packages for binary and ordinal outcomes,"Abstract Background Logistic random effects models are a popular tool to analyze multilevel also called hierarchical data with a binary or ordinal outcome. Here, we aim to compare different statistical software implementations of these models. Methods We used individual patient data from 8509 patients in 231 centers with moderate and severe Traumatic Brain Injury (TBI) enrolled in eight Randomized Controlled Trials (RCTs) and three observational studies. We fitted logistic random effects regression models with the 5-point Glasgow Outcome Scale (GOS) as outcome, both dichotomized as well as ordinal, with center and/or trial as random effects, and as covariates age, motor score, pupil reactivity or trial. We then compared the implementations of frequentist and Bayesian methods to estimate the fixed and random effects. Frequentist approaches included R (lme4), Stata (GLLAMM), SAS (GLIMMIX and NLMIXED), MLwiN ([R]IGLS) and MIXOR, Bayesian approaches included WinBUGS, MLwiN (MCMC), R package MCMCglmm and SAS experimental procedure MCMC. Three data sets (the full data set and two sub-datasets) were analysed using basically two logistic random effects models with either one random effect for the center or two random effects for center and trial. For the ordinal outcome in the full data set also a proportional odds model with a random center effect was fitted. Results The packages gave similar parameter estimates for both the fixed and random effects and for the binary (and ordinal) models for the main study and when based on a relatively large number of level-1 (patient level) data compared to the number of level-2 (hospital level) data. However, when based on relatively sparse data set, i.e. when the numbers of level-1 and level-2 data units were about the same, the frequentist and Bayesian approaches showed somewhat different results. The software implementations differ considerably in flexibility, computation time, and usability. There are also differences in the availability of additional tools for model evaluation, such as diagnostic plots. The experimental SAS (version 9.2) procedure MCMC appeared to be inefficient. Conclusions On relatively large data sets, the different software implementations of logistic random effects regression models produced similar results. Thus, for a large data set there seems to be no explicit preference (of course if there is no preference from a philosophical point of view) for either a frequentist or Bayesian approach (if based on vague priors). The choice for a particular implementation may largely depend on the desired flexibility, and the usability of the package. For small data sets the random effects variances are difficult to estimate. In the frequentist approaches the MLE of this variance was often estimated zero with a standard error that is either zero or could not be determined, while for Bayesian methods the estimates could depend on the chosen ""non-informative"" prior of the variance parameter. The starting value for the variance parameter may be also critical for the convergence of the Markov chain.",0
https://doi.org/10.17645/pag.v4i1.458,Context Matters: Economic Voting in the 2009 and 2014 European Parliament Elections,"Using the 2009 and 2014 European Election Studies (EES), we explore the effect of the economy on the vote in the 2009 and 2014 European Parliament (EP) elections. The paper demonstrates that the economy did influence voters in both contests. However, its impact was heterogeneous across the two elections and between countries. While assessments of the economy directly motivated voters in 2009 by 2014 economic appraisals were conditioned by how much responsibility voters felt the national government had for the state of the economy, implying a shift in calculus between the two elections. The analysis suggests that voters in 2009 were simply reacting to the economic tsunami that was the Global Financial Crisis, with motivations primarily driven by the unfavourable economic conditions countries faced. But in 2014, evaluations were conditioned by judgments about responsibility for the economy, suggesting a more conscious holding to account of the government. Our paper also reveals cross-country differences in the influence of the economy on vote. Attribution of responsibility and economic evaluations had a more potent impact on support for the government in bailout countries compared to non-bailout countries in 2014. Our findings demonstrate the importance of economy on vote in EP elections but also highlight how its impact on vote can vary based on context.",0
https://doi.org/10.1080/10705511.2012.659612,Diagnostic Procedures for Detecting Nonlinear Relationships Between Latent Variables,"Structural equation models are commonly used to estimate relationships between latent variables. Almost universally, the fitted models specify that these relationships are linear in form. This assumption is rarely checked empirically, largely for lack of appropriate diagnostic techniques. This article presents and evaluates two procedures that can be used to visualize and detect nonlinear relationships between latent variables. The first procedure involves fitting a linear structural equation model and then inspecting plots of factor score estimates for evidence of nonlinearity. The second procedure is to use a mixture of linear structural equation models to approximate the underlying, potentially nonlinear function. Targeted simulations indicate that the first procedure is more efficient, but that the second procedure is less biased. The mixture modeling approach is recommended, particularly with medium to large samples.",0
https://doi.org/10.1198/106186005x63185,Nonparametric Bayesian Modeling for Multivariate Ordinal Data,"This article proposes a probability model for k-dimensional ordinal outcomes, that is, it considers inference for data recorded in k-dimensional contingency tables with ordinal factors. The proposed approach is based on full posterior inference, assuming a flexible underlying prior probability model for the contingency table cell probabilities. We use a variation of the traditional multivariate probit model, with latent scores that determine the observed data. In our model, a mixture of normals prior replaces the usual single multivariate normal model for the latent variables. By augmenting the prior model to a mixture of normals we generalize inference in two important ways. First, we allow for varying local dependence structure across the contingency table. Second, inference in ordinal multivariate probit models is plagued by problems related to the choice and resampling of cutoffs defined for these latent variables. We show how the proposed mixture model approach entirely removes these problems. We ill...",0
https://doi.org/10.1111/j.2517-6161.1995.tb02017.x,Fractional Bayes Factors for Model Comparison,"Bayesian comparison of models is achieved simply by calculation of posterior probabilities of the models themselves. However, there are difficulties with this approach when prior information about the parameters of the various models is weak. Partial Bayes factors offer a resoIution of the problem by setting aside part of the data as a training sampIe. The training sampIe is used to obtain an initiaI informative posterior distribution of the parameters in each model. Model comparison is then based on a Bayes factor calculated from the remaining data. Properties of partial Bayes factors are discussed, particularly in the context of weak prior information, and they are found to have advantages over other proposed methods of model comparison. A new variant of the partial Bayes factor, the fractional Bayes factor, is advocated on grounds of consistency, simplicity, robustness and coherence",0
https://doi.org/10.1002/sim.2849,Estimating and testing interactions in linear regression models when explanatory variables are subject to classical measurement error,"Estimating and testing interactions in a linear regression model when normally distributed explanatory variables are subject to classical measurement error is complex, since the interaction term is a product of two variables and involves errors of more complex structure. Our aim is to develop simple methods, based on the method of moments (MM) and regression calibration (RC) that yield consistent estimators of the regression coefficients and their standard errors when the model includes one or more interactions. In contrast to previous work using structural equations models framework, our methods allow errors that are correlated with each other and can deal with measurements of relatively low reliability. Using simulations, we show that, under the normality assumptions, the RC method yields estimators with negligible bias and is superior to MM in both bias and variance. We also show that the RC method also yields the correct type I error rate of the test of the interaction. However, when the true covariates are not normally distributed, we recommend using MM. We provide an example relating homocysteine to serum folate and B12 levels.",0
https://doi.org/10.1007/bf02293687,Simulating multivariate nonnormal distributions,"A method for generating multivariate nonnormal distributions with specified intercorrelations and marginal means, variances, skews, and kurtoses is proposed. As an example, the method is applied to the generation of simulated scores on three psychological tests administered to a single group of individuals. Â© 1983 The Psychometric Society.",0
,Some latent trait models and their use in inferring an examinee's ability,,0
https://doi.org/10.1007/s10802-011-9516-4,"Prospective Relations Among Fearful Temperament, Protective Parenting, and Social Withdrawal: The Role of Maternal Accuracy in a Moderated Mediation Framework","Early social withdrawal and protective parenting predict a host of negative outcomes, warranting examination of their development. Mothers' accurate anticipation of their toddlers' fearfulness may facilitate transactional relations between toddler fearful temperament and protective parenting, leading to these outcomes. Currently, we followed 93 toddlers (42 female; on average 24.76 months) and their mothers (9% underrepresented racial/ethnic backgrounds) over 3 years. We gathered laboratory observation of fearful temperament, maternal protective behavior, and maternal accuracy during toddlerhood and a multi-method assessment of children's social withdrawal and mothers' self-reported protective behavior at kindergarten entry. When mothers displayed higher accuracy, toddler fearful temperament significantly related to concurrent maternal protective behavior and indirectly predicted kindergarten social withdrawal and maternal protective behavior. These results highlight the important role of maternal accuracy in linking fearful temperament and protective parenting, which predict further social withdrawal and protection, and point to toddlerhood for efforts of prevention of anxiety-spectrum outcomes.",0
https://doi.org/10.1093/biomet/93.1.179,A shrinkage estimator for spectral densities,"SUMMARY We propose a shrinkage estimator for spectral densities based on a multilevel normal hierarchical model. The first level captures the sampling variability via a likelihood con structed using the asymptotic properties of the periodogram. At the second level, the spectral density is shrunk towards a parametric time series model. To avoid selecting a particular parametric model for the second level, a third level is added which induces an estimator that averages over a class of parsimonious time series models. The estimator derived from this model, the model averaged shrinkage estimator, is consistent, is shown to be highly competitive with other spectral density estimators via simulations, and is computationally inexpensive.",0
https://doi.org/10.1016/j.electstud.2014.06.008,Who supports minority rights in popular votes? Empirical evidence from Switzerland,"Recent research shows that well-educated citizens are more supportive of minority rights in direct democratic votes than people with less education. This article however suggests that educational effects on minority rights only emerge under certain conditions. A Bayesian multilevel analysis of 39 referendums and initiatives on minority rights in Switzerland (1981–2009) shows that educational effects are particularly strong when the rights of lesser-known cultural minorities are to be extended. They are entirely absent, however, when referenda address the curtailment of rights for well-known minority groups. • We study educational effects on minority rights in direct democratic votes. • We use individual data (surveys) from 39 referendums and initiatives on minority rights in Switzerland (1981–2009) and conduct a Bayesian multilevel analysis. • We show that the relationship between education and pro-minority voting behaviour is very strong when a ballot proposition aims at expanding the rights of out-group minorities (e.g. foreigners). • Educational effects are absent when referenda address the curtailment of rights for well-known minority groups (e.g. disabled persons).",0
https://doi.org/10.1093/biomet/71.3.545,An analysis of correlation matrices: Equal correlations,"SUMMARY We study the large-sample joint distribution of Z, the -fp(p - 1) Fisher z-transforms of the elements in a p variable correlation matrix. Under the null hypothesis of equal population correlations the variance matrix of Z has just three projector matrices in its spectral decomposition. These define three mutually orthogonal invariant subspaces of sample space, or 'error strata' as they would be called in the analysis of variance. The squared lengths of the projections of the sample vector onto each of these subspaces, when divided by the stratum variance, provide a natural partition for the large-sample chi-squared test for equality of correlations. A linear model is given which provides a statistical interpretation for the error strata, and hence the components in the partition. As well as providing a simple test for equality of correlations, the procedure indicates how familiar techniques in the spirit of analysis of variance can be used to investigate correlation matrices.",0
https://doi.org/10.1016/j.csda.2012.04.016,A Bayesian approach for generalized random coefficient structural equation models for longitudinal data with adjacent time effects,This paper proposes a generalized random coefficient structural equation model for analyzing longitudinal data by incorporating the correlated structure due to adjacent time effects and by allowing structural parameters to vary across individuals. The coregionalization for modeling multivariate spatial data is adopted to formulate the correlated structure between adjacent time points. A Bayesian approach coupled with the Gibbs sampler and the Metropolis-Hastings algorithm is developed to obtain the Bayesian estimates of unknown parameters and latent variables simultaneously. A simulation study and a real example related to an emotion study are presented to illustrate the newly developed methodology.,0
https://doi.org/10.1348/000711008x374126,A Box-Cox normal model for response times,"The log-transform has been a convenient choice in response time modelling on test items. However, motivated by a dataset of the Medical College Admission Test where the lognormal model violated the normality assumption, the possibilities of the broader class of Box-Cox transformations for response time modelling are investigated. After an introduction and an outline of a broader framework for analysing responses and response times simultaneously, the performance of a Box-Cox normal model for describing response times is investigated using simulation studies and a real data example. A transformation-invariant implementation of the deviance information criterium (DIC) is developed that allows for comparing model fit between models with different transformation parameters. Showing an enhanced description of the shape of the response time distributions, its application in an educational measurement context is discussed at length.",0
https://doi.org/10.7326/m14-0511,Pharmacologic Interventions for Painful Diabetic Neuropathy,"Multiple treatments for painful diabetic peripheral neuropathy are available.To evaluate the comparative effectiveness of oral and topical analgesics for diabetic neuropathy.Multiple electronic databases between January 2007 and April 2014, without language restriction.Parallel or crossover randomized, controlled trials that evaluated pharmacologic treatments for adults with painful diabetic peripheral neuropathy.Duplicate extraction of study data and assessment of risk of bias.65 randomized, controlled trials involving 12 632 patients evaluated 27 pharmacologic interventions. Approximately one half of these studies had high or unclear risk of bias. Nine head-to-head trials showed greater pain reduction associated with serotonin-norepinephrine reuptake inhibitors (SNRIs) than anticonvulsants (standardized mean difference [SMD], -0.34 [95% credible interval {CrI}, -0.63 to -0.05]) and with tricyclic antidepressants (TCAs) than topical capsaicin 0.075%. Network meta-analysis showed that SNRIs (SMD, -1.36 [CrI, -1.77 to -0.95]), topical capsaicin (SMD, -0.91 [CrI, -1.18 to -0.08]), TCAs (SMD, -0.78 [CrI, -1.24 to -0.33]), and anticonvulsants (SMD, -0.67 [CrI, -0.97 to -0.37]) were better than placebo for short-term pain control. Specifically, carbamazepine (SMD, -1.57 [CrI, -2.83 to -0.31]), venlafaxine (SMD, -1.53 [CrI, -2.41 to -0.65]), duloxetine (SMD, -1.33 [CrI, -1.82 to -0.86]), and amitriptyline (SMD, -0.72 [CrI, -1.35 to -0.08]) were more effective than placebo. Adverse effects included somnolence and dizziness with TCAs, SNRIs, and anticonvulsants; xerostomia with TCAs; and peripheral edema and burning sensation with pregabalin and capsaicin.Confidence in findings was limited because most evidence came from indirect comparisons of trials with short (≤3 months) follow-up and unclear or high risk of bias.Several medications may be effective for short-term management of painful diabetic neuropathy, although their comparative effectiveness is unclear.Mayo Foundation for Medical Education and Research.",0
https://doi.org/10.1111/j.1744-6570.1982.tb02203.x,SYNTHETIC VALIDITY AND ITS APPLICATION TO THE UNIFORM GUIDELINES VALIDATION REQUIREMENTS,"The Uniform Guidelines for Employee Selection Procedures have served to create an urgent need for efficient validation methods that can be generalized to a class of occupations. The one method currently authorized for such a purpose by the Guidelines is synthetic validation. (The Guidelines erroneously describe the synthetic validity paradigm as construct validity.) Approaches to synthetic validity employed by Lawshe, Guion, McCormick, and Primoff are described. Their extent of conformance to the Guidelines validation requirements is noted. Primoff's J-Coefficient approach is recommended for two reasons; it meets the Guidelines requirements and under certain circumstances it permits the test user to estimate the traditional validity coefficient. An illustrated example of Primoff's method is presented.",0
https://doi.org/10.1177/0956797610397956,Hierarchical Encoding in Visual Working Memory,"Influential models of visual working memory treat each item to be stored as an independent unit and assume that there are no interactions between items. However, real-world displays have structure that provides higher-order constraints on the items to be remembered. Even in the case of a display of simple colored circles, observers can compute statistics, such as mean circle size, to obtain an overall summary of the display. We examined the influence of such an ensemble statistic on visual working memory. We report evidence that the remembered size of each individual item in a display is biased toward the mean size of the set of items in the same color and the mean size of all items in the display. This suggests that visual working memory is constructive, encoding displays at multiple levels of abstraction and integrating across these levels, rather than maintaining a veridical representation of each item independently.",0
https://doi.org/10.1515/jos-2016-0009,Bayesian Predictive Inference of a Proportion Under a Twofold Small-Area Model,"Abstract We extend the twofold small-area model of Stukel and Rao (1997; 1999) to accommodate binary data. An example is the Third International Mathematics and Science Study (TIMSS), in which pass-fail data for mathematics of students from US schools (clusters) are available at the third grade by regions and communities (small areas). We compare the finite population proportions of these small areas. We present a hierarchical Bayesian model in which the firststage binary responses have independent Bernoulli distributions, and each subsequent stage is modeled using a beta distribution, which is parameterized by its mean and a correlation coefficient. This twofold small-area model has an intracluster correlation at the first stage and an intercluster correlation at the second stage. The final-stage mean and all correlations are assumed to be noninformative independent random variables. We show how to infer the finite population proportion of each area. We have applied our models to synthetic TIMSS data to show that the twofold model is preferred over a onefold small-area model that ignores the clustering within areas. We further compare these models using a simulation study, which shows that the intracluster correlation is particularly important.",0
,Sulla determinazione empirica di una legge di distribuzione,,0
https://doi.org/10.1177/1073191111411658,How Much Power and Speed Is Measured in This Test?,"An old issue in psychological assessment is to what extent power and speed each are measured by a given intelligence test. Starting from accuracy and response time data, an approach based on posterior time limits (cut-offs of recorded response time) leads to three kinds of recoded data: time data (whether or not the response precedes the cut-off), time-accuracy data (whether or not a response is correct and precedes the cut-off), and accuracy data (as time-accuracy data, but coded as missing when not preceding the time cut-off). Each type of data can be modeled as binary responses. Speed and power are investigated through the effect of posterior time limits on two main aspects: (a) the latent variable that is measured: whether it is more power-related or more speed-related; (b) how well the latent variable (of whatever kind) is measured through the item(s). As empirical data, we use responses and response times for a verbal analogies test. The main findings are that, independent of the posterior time limit, basically the same latent speed trait was measured through the time data, and basically the same latent power trait was measured through the accuracy data, while for the time-accuracy data the nature of the latent trait moved from power to speed when the posterior time limit was reduced. It was also found that a reduction of the posterior time limit had no negative effect on the reliability of the latent trait measures (of whatever kind).",0
https://doi.org/10.1111/1467-9868.00267,Improved small sample inference in the mixed linear model: Bartlett correction and adjusted likelihood,"Summary. The mixed linear model is a popular method for analysing unbalanced repeated measurement data. The classical statistical tests for parameters in this model are based on asymptotic theory that is unreliable in the small samples that are often encountered in practice. For testing a given fixed effect parameter with a small sample, we develop and investigate refined likelihood ratio (LR) tests. The refinements considered are the Bartlett correction and use of the Cox-Reid adjusted likelihood; these are examined separately and in combination. We illustrate the various LR tests on an actual data set and compare them in two simulation studies. The conventional LR test yields type I error rates that are higher than nominal. The adjusted LR test yields rates that are lower than nominal, with absolute accuracy similar to that of the conventional LR test in the first simulation study and better in the second. The Bartlett correction substantially improves the accuracy of the type I error rates with either the conventional or the adjusted LR test. In many cases, error rates that are very close to nominal are achieved with the refined methods.",0
https://doi.org/10.1016/b978-0-12-742780-5.50011-1,Small N Justifies Rasch Model,The usual Birnbaum item response function requires the determination of three parameters for each item; the Rasch model requires only one. It would be useful to know how large the sample of examinees must be before it is worthwhile to use a two- or three-parameter item response model in preference to the Rasch model. The answer to this question depends on the purpose to be served. This chapter answers this question only for the two-parameter logistic model and only for one very limited situation. It points out the problem to indicate a method of solution and to provide some numerical results indicating the sample size required when there is no guessing.,0
https://doi.org/10.1037/a0038280,Hindrances are not threats: Advancing the multidimensionality of work stress.,"The challenge-hindrance framework has proved useful for explaining inconsistencies in relationships between work stressors and important outcomes. By introducing the distinction between threat and hindrance to this framework, we capture the potential for personal harm or loss (threat) associated with stressors, as distinct from the potential to block goal attainment (hindrance) or promote gain (challenge). In Study 1, survey data were collected from 609 retail workers, 220 of whom responded 6 months later. The results supported a 3-factor threat-hindrance-challenge stressor structure and showed that threat stressors are associated with increased psychological distress and emotional exhaustion, and reduced dedication, whereas hindrance stressors undermine dedication but may not be related to distress or exhaustion with threats included in the model. Study 2 utilized a diary study design, with data collected from 207 workers over 3 workdays. Findings revealed that the threat, hindrance, and challenge appraisals of individual workers are statistically distinct, and associated with stressors and well-being as anticipated: threats with role conflict and anxiety, hindrances with organizational constraints and fatigue, and challenges with skill demands and enthusiasm. Overall, moving to a 3-dimensional challenge-hindrance-threat framework for stressors and stress appraisals will support a more accurate picture regarding the nature, processes, and effects of stressors on individuals and organizations, and ensure prevention efforts are not misguided.",0
https://doi.org/10.1207/s15327906mbr3604_03,Fitting Item Response Theory Models to Two Personality Inventories: Issues and Insights,"The present study compared the fit of several IRT models to two personality assessment instruments. Data from 13,059 individuals responding to the US-English version of the Fifth Edition of the Sixteen Personality Factor Questionnaire (16PF) and 1,770 individuals responding to Goldberg's 50 item Big Five Personality measure were analyzed. Various issues pertaining to the fit of the IRT models to personality data were considered. We examined two of the most popular parametric models designed for dichotomously scored items (i.e., the two- and three-parameter logistic models) and a parametric model for polytomous items (Samejima's graded response model). Also examined were Levine's nonparametric maximum likelihood formula scoring models for dichotomous and polytomous data, which were previously found to provide good fits to several cognitive ability tests (Drasgow, Levine, Tsien, Williams, & Mead, 1995). The two- and three-parameter logistic models fit some scales reasonably well but not others; the graded response model generally did not fit well. The nonparametric formula scoring models provided the best fit of the models considered. Several implications of these findings for personality measurement and personnel selection were described.",0
https://doi.org/10.1007/bf02294761,Specifying optimum examinees for item parameter estimation in item response theory,,0
https://doi.org/10.1109/cvpr.2005.16,A Bayesian Hierarchical Model for Learning Natural Scene Categories,"We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a theme. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.",0
https://doi.org/10.1371/journal.pone.0074144,"Weber’s Law, the Magnitude Effect and Discrimination of Sugar Concentrations in Nectar-Feeding Animals","Weber's law quantifies the perception of difference between stimuli. For instance, it can explain why we are less likely to detect the removal of three nuts from a bowl if the bowl is full than if it is nearly empty. This is an example of the magnitude effect - the phenomenon that the subjective perception of a linear difference between a pair of stimuli progressively diminishes when the average magnitude of the stimuli increases. Although discrimination performances of both human and animal subjects in various sensory modalities exhibit the magnitude effect, results sometimes systematically deviate from the quantitative predictions based on Weber's law. An attempt to reformulate the law to better fit data from acoustic discrimination tasks has been dubbed the ""near-miss to Weber's law"". Here, we tested the gustatory discrimination performance of nectar-feeding bats (Glossophaga soricina), in order to investigate whether the original version of Weber's law accurately predicts choice behavior in a two-alternative forced choice task. As expected, bats either preferred the sweeter of the two options or showed no preference. In 4 out of 6 bats the near-miss to Weber's law provided a better fit and Weber's law underestimated the magnitude effect. In order to test the generality of this observation in nectar-feeders, we reviewed previously published data on bats, hummingbirds, honeybees, and bumblebees. In all groups of animals the near-miss to Weber's law provided better fits than Weber's law. Furthermore, whereas the magnitude effect was stronger than predicted by Weber's law in vertebrates, it was weaker than predicted in insects. Thus nectar-feeding vertebrates and insects seem to differ in how their choice behavior changes as sugar concentration is increased. We discuss the ecological and evolutionary implications of the observed patterns of sugar concentration discrimination.",0
https://doi.org/10.1123/jsep.24.1.3,A Meta-Analytic Review of the Theories of Reasoned Action and Planned Behavior in Physical Activity: Predictive Validity and the Contribution of Additional Variables,"The aim of the present study was to examine relations between behavior, intentions, attitudes, subjective norms, perceived behavioral control, self-efficacy, and past behavior across studies using the Theories of Reasoned Action (TRA) and Planned Behavior (TPB) in a physical activity context. Meta-analytic techniques were used to correct the correlations between the TRA/TPB constructs for statistical artifacts across 72 studies, and path analyses were conducted to examine the pattern of relationships among the variables. Results demonstrated that the TRA and TPB both exhibited good fit with the corrected correlation matrices, but the TPB accounted for more variance in physical activity intentions and behavior. In addition, self-efficacy explained unique variance in intention, and the inclusion of past behavior in the model resulted in the attenuation of the intention-behavior, attitude-intention, self-efficacy-intention, and self-efficacy-behavior relationships. There was some evidence that the study relationships were moderated by attitude-intention strength and age, but there was a lack of homogeneity in the moderator groups. It was concluded that the major relationships of the TRA/TPB were supported in this quantitative integration of the physical activity literature, and the inclusion of self-efficacy and past behavior are important additions to the model.",0
https://doi.org/10.1002/sim.3747,Bayesian methods of analysis for cluster randomized trials with count outcome data,"Bayesian approaches to inference in cluster randomized trials have been investigated for normally distributed and binary outcome measures. However, relatively little attention has been paid to outcome measures which are counts of events. We discuss an extension of previously published Bayesian hierarchical models to count data, which usually can be assumed to be distributed according to a Poisson distribution. We develop two models, one based on the traditional rate ratio, and one based on the rate difference which may often be more intuitively interpreted for clinical trials, and is needed for economic evaluation of interventions. We examine the relationship between the intracluster correlation coefficient (ICC) and the between-cluster variance for each of these two models. In practice, this allows one to use the previously published evidence on ICCs to derive an informative prior distribution which can then be used to increase the precision of the posterior distribution of the ICC. We demonstrate our models using a previously published trial assessing the effectiveness of an educational intervention and a prior distribution previously derived. We assess the robustness of the posterior distribution for effectiveness to departures from a normal distribution of the random effects.",0
https://doi.org/10.1177/014662169101500205,The Measurement of Latent Traits by Proximity Items,"A probabilistic parallelogram model for the mea surement of latent traits by proximity items (the PARELLA model) is introduced. This model assumes that the responses of persons to items result from proximity relations: the smaller the distance between person and item, the larger the probability that the person will agree with the content of the item. The model is unidimensional and assigns locations to items and persons on the latent trait. The parameters of the PARELLA model are estimated by marginal maximum likelihood and expectation maximization. The efficiency of the estimation procedure is illustrated, a diagnostic for the fit of items to the model is presented, and the PARELLA model is used for the analysis of three empirical datasets.",0
https://doi.org/10.1371/journal.pone.0074815,Four Theorems on the Psychometric Function,"In a 2-alternative forced-choice (2AFC) discrimination task, observers choose which of two stimuli has the higher value. The psychometric function for this task gives the probability of a correct response for a given stimulus difference, Δx. This paper proves four theorems about the psychometric function. Assuming the observer applies a transducer and adds noise, Theorem 1 derives a convenient general expression for the psychometric function. Discrimination data are often fitted with a Weibull function. Theorem 2 proves that the Weibull ""slope"" parameter, β, can be approximated by β(Noise) x β(Transducer), where β(Noise) is the β of the Weibull function that fits best to the cumulative noise distribution, and β(Transducer) depends on the transducer. We derive general expressions for β(Noise) and β(Transducer), from which we derive expressions for specific cases. One case that follows naturally from our general analysis is Pelli's finding that, when d' ∝ (Δx)(b), β ≈ β(Noise) x b. We also consider two limiting cases. Theorem 3 proves that, as sensitivity improves, 2AFC performance will usually approach that for a linear transducer, whatever the actual transducer; we show that this does not apply at signal levels where the transducer gradient is zero, which explains why it does not apply to contrast detection. Theorem 4 proves that, when the exponent of a power-function transducer approaches zero, 2AFC performance approaches that of a logarithmic transducer. We show that the power-function exponents of 0.4-0.5 fitted to suprathreshold contrast discrimination data are close enough to zero for the fitted psychometric function to be practically indistinguishable from that of a log transducer. Finally, Weibull β reflects the shape of the noise distribution, and we used our results to assess the recent claim that internal noise has higher kurtosis than a Gaussian. Our analysis of β for contrast discrimination suggests that, if internal noise is stimulus-independent, it has lower kurtosis than a Gaussian.",0
https://doi.org/10.1007/978-3-642-29047-3_7,Cultural Consensus Theory: Aggregating Signed Graphs under a Balance Constraint,"Cultural Consensus Theory (CCT) consists of cognitive models for aggregating the responses of experts to test questions about some domain of their shared cultural knowledge. This paper proposes a new CCT model for a situation where experts judge the ties in a complete signed graph. New to CCT is that the model imposes a side constraint on the aggregation process that requires that the consensus signed graph satisfy the social network property of structural balance. Balanced signed graphs require that the nodes can be partitioned into two sets with positive ties between nodes in the same set and negative ties between nodes in different sets. While the balance constraint is imposed on the consensus aggregation, it is not assumed that each expert's responses satisfy balance because they may be error-prone or biased. The model is presented in terms of signal detection assumptions that allow heterogeneity in expert ability and item difficulty. Bayesian inference of the model is developed using a specially designed Markov Chain Monte Carlo sampler. It is shown that the sampler can recover parameters from simulated data, and then the model is applied to interpret experimental data. Of particular interest is that the model aggregation reveals a single consensus balanced signed graph with a high posteriori probability despite the fact that none of the experts' responses satisfy the balance constraint.",0
https://doi.org/10.1177/0146621602026003007,Recovery of Item Parameters in the Nominal Response Model: A Comparison of Marginal Maximum Likelihood Estimation and Markov Chain Monte Carlo Estimation,"Markov chain Monte Carlo (MCMC) methods, such as Gibbs sampling, present an alternative to marginal maximum likelihood (MML) estimation, which offers some promise for parameter estimation particularly with complex models, in small sample situations, and for other applications where MML algorithms have not been established. MCMC circumvents the problems associated with implementing an estimation algorithm for complex, multidimensional probability distributions by sampling the parameters from each of the one-dimensional conditional posterior distributions at each stage of the Markov chain. In this article, the authors compared the quality of item parameter estimates for MML and MCMC with one type of complex item response theory model, the nominal response model. The quality of item parameter recovery was nearly identical for both MML and MCMC. Both methods tended to produce good estimates, even for short tests and relatively small sample sizes. Parameter recovery was best for items of moderate dif.culty (i.e., items matched to the latent trait distribution); recovery was worst for items that were extremely easy or dif.cult. The quality of item parameter recovery improved as test length increased from 10 to 30 items, but did not change as test length increased from 20 to 30 items. MCMC estimation takes substantially longer but appears to be a good surrogate for MML for those situations for which an MML algorithm has not been developed.",0
https://doi.org/10.1207/s15327906mbr3103_6,Mixed-effects Models in the Study of Individual Differences with Repeated Measures Data,Nonlinear mixed-effects models are used to describe each person's set of scores from a longitudinal design or repeated measures study by a function that includes an overall group effect plus an effect for the individual. The model is ideal for many kinds of behavioral data. Some characteristics of mixed models are reviewed in this article and illustrated by a series of examples.,0
https://doi.org/10.1016/j.jneumeth.2010.08.025,A novel animal model of graded neuropathic pain: Utility to investigate mechanisms of population heterogeneity,"The mechanisms underlying neuropathic pain are not well understood, resulting in unsatisfactory treatment outcomes for many patients. Animal models underpin much of the current understanding of pain mechanisms due to their perceived ability to mimic pain hypersensitivities; however, are limited by their binomial approach (pain vs. control), which does not reflect the clinical heterogeneity in nociceptive hypersensitivity. We modified the chronic constriction injury model by varying the number of sciatic nerve chromic gut sutures. Each Sprague Dawley rat received 4 pieces of chromic gut to control for the inflammatory challenge posed by the gut. Treatment groups were neuronal sutures (N), subcutaneous sutures (S) N0S0, N0S4, N1S3, N2S2 and N4S0. At postoperative (PO) day 29, there was a 'dose-response' relationship between the number of perineural sutures and von Frey threshold (N0S4<N1S3<N2S2<N4S0, P<0.05). This graded model was applied to investigate lumbar dorsal spinal cord glial activation marker expression. Microglial CD11b expression was positively correlated with graded allodynia in the ipsilateral dorsal horn (P<0.05, r(2)>0.9) and associated in the dorsolateral funiculus (DLF; P=0.10, r(2)>0.8) at PO day 14. Astrocyte GFAP expression was positively associated with graded allodynia in the ipsilateral dorsal horn (P=0.18, r(2)>0.6) and ipsilateral DLF (P<0.05, r(2)>0.9). DLF glial activation may represent a contributor to contralateral pain. Our novel graded model has a dynamic range, allowing sensitive detection of interactions and subtle influences on neuropathic pain processing.",0
https://doi.org/10.1016/j.foodqual.2015.05.005,Modeling target group heterogeneity in experimental consumer studies,"Abstract Acceptance of a product by a consumer may result from a convoluted interplay between product attributes and individual characteristics of that consumer. Different methods that systematically combine product properties with consumer groups segmented on such characteristics have provided unprecedented insight, but ignore heterogeneity in acceptance within each consumer group. Although such knowledge is invaluable for targeted marketing, dedicated methods for consumer group heterogeneity are lacking. The authors aim to fill this gap by the Individual Differences (InD) method, which models differences between consumers within the same target group. The method scores the ‘diffusion’ within each group, shows how much each consumer contributes to that, and relates this information to product properties. Thereby also novel groups may be discovered, with attributes not covered in the consumer segmentation. The illustrative consumer study on apple juice reveals how young women differ in their price-consciousness and their acceptance on specific preparation technologies more than older women. Although men exhibit heterogeneity on the same product attributes, their mutual variability is considerably lower and they thereby form more homogeneous target groups.",0
https://doi.org/10.1111/jsbm.12067,Antecedents of Entrepreneurial Intention among Laid-Off Individuals: A Cognitive Appraisal Approach,This study contributes to our understanding of why laid‐off individuals might explore entrepreneurial careers. Findings among 838 laid‐off individuals suggest that financial strain is associated wi...,0
https://doi.org/10.1198/073500103288619449,An Empirical Bayes Procedure for Improving Individual-Level Estimates and Predictions From Finite Mixtures of Multinomial Logit Models,"Unobserved heterogeneity in random utility choice models can be dealt with by specifying either a multinomial or a normal distribution of the coefficients, leading to finite mixture logit and mixed logit models. Focusing on the former, we show that individual-level estimates and predictions of finite mixtures estimated by maximizing the likelihood function can be improved through integration over the estimation error of the hyperparameters, using an empirical Bayes approach. We investigate the conjecture that this approach is more robust against departures of the underlying assumptions of the finite mixture model in two Monte Carlo studies. We show that our approach improves the performance of the finite mixture model in representing individual-level parameters and producing hold-out forecasts. We illustrate with two examples that our approach may offer advantages in empirical applications involving the analysis of heterogeneous choice data.",0
https://doi.org/10.1348/000711007x255336,MCMC estimation for the p2 network regression model with crossed random effects,"The p2 model is a statistical model for the analysis of binary relational data with covariates, as occur in social network studies. It can be characterized as a multinomial regression model with crossed random effects that reflect actor heterogeneity and dependence between the ties from and to the same actor in the network. Three Markov chain Monte Carlo (MCMC) estimation methods for the p2 model are presented to improve iterative generalized least squares (IGLS) estimation developed earlier, two of which use random walk proposals. The third method, an independence chain sampler, and one of the random walk algorithms use normal approximations of the binary network data to generate proposals in the MCMC algorithms. A large-scale simulation study compares MCMC estimates with IGLS estimates for networks with 20 and 40 actors. It was found that the IGLS estimates have a smaller variance but are severely biased, while the MCMC estimates have a larger variance with a small bias. For networks with 20 actors, mean squared errors are generally comparable or smaller for the IGLS estimates. For networks with 40 actors, mean squared errors are the smallest for the MCMC estimates. Coverage rates of confidence intervals are good for the MCMC estimates but not for the IGLS estimates.",0
https://doi.org/10.1002/env.873,Space—time zero-inflated count models of Harbor seals,"Environmental data are spatial, temporal, and often come with many zeros. In this paper, we included space-time random effects in zero-inflated Poisson (ZIP) and ‘hurdle’ models to investigate haulout patterns of harbor seals on glacial ice. The data consisted of counts, for 18 dates on a lattice grid of samples, of harbor seals hauled out on glacial ice in Disenchantment Bay, near Yakutat, Alaska. A hurdle model is similar to a ZIP model except it does not mix zeros from the binary and count processes. Both models can be used for zero-inflated data, and we compared space-time ZIP and hurdle models in a Bayesian hierarchical model. Space-time ZIP and hurdle models were constructed by using spatial conditional autoregressive (CAR) models and temporal first-order autoregressive (AR(1)) models as random effects in ZIP and hurdle regression models. We created maps of smoothed predictions for harbor seal counts based on ice density, other covariates, and spatio-temporal random effects. For both models predictions around the edges appeared to be positively biased. The linex loss function is an asymmetric loss function that penalizes overprediction more than underprediction, and we used it to correct for prediction bias to get the best map for space-time ZIP and hurdle models. Published in 2007 by John Wiley & Sons, Ltd.",0
https://doi.org/10.1016/j.conb.2010.03.005,Discrete capacity limits in visual working memory,"The amount of information we can actively maintain ‘in mind’ is very limited. This capacity limitation, known as working memory (WM) capacity, has been of great interest because of its wide scope influence on the variety of intellectual abilities. Recently, there has been an ongoing debate about how this capacity should be best characterized. One viewpoint argues that WM capacity is allocated in a discrete fashion with an upper limit of three to four representations. An alternative viewpoint argues that the capacity can be allocated in a continuous fashion with no upper limit in the number of representations. In this article, we will review recent neurobiological and behavioral evidence that has helped shape the debate regarding one of the more central mechanisms in cognitive neuroscience.",0
https://doi.org/10.1111/j.1745-3984.2005.00007,A Mixture Model Analysis of Differential Item Functioning,"Once a differential item functioning (DIF) item has been identified, little is known about the examinees for whom the item functions differentially. This is because DIF focuses on manifest group characteristics that are associated with it, but do not ex plain why examinees respond differentially to items. We first analyze item response patterns for gender DIF and then illustrate, through the use of a mixture item re sponse theory (IRT) model, how the manifest characteristic associated with DIF of ten has a very weak relationship with the latent groups actually being advantaged or disadvantaged by the item(s). Next, we propose an alternative approach to DIF as sessment that first uses an exploratory mixture model analysis to define the primary dimension(s) that contribute to DIF, and secondly studies examinee characteristics associated with those dimensions in order to understand the cause(s) of DIF. Com parison of academic characteristics of these examinees across classes reveals some clear differences in manifest characteristics between groups. Differential item functioning (DIF) can be attributed to the presence of nuisance dimensions intruding on the ability intended to be measured (Ackerman, 1992). DIF is typically investigated by examining the relationship between some manifest ex aminee characteristic, such as gender or ethnic group membership, with differential performance on the item. Unfortunately, this strategy is not ideal for understanding the root causes of DIF. One problem with this approach is that it focuses on the examinee characteristic of interest, not on the dimension causing the DIF. Conse quently, when differences are found, it is not so readily apparent who is primarily being advantaged or disadvantaged by the DIF items. We illustrate the limitations of this strategy by first using a standard DIF detection approach to identify DIF items on the basis of a manifest characteristic, in this case, gender. Results of the DIF analysis are used to define a two-group mixture IRT model with groups defined by gender, and only gender DIF items are allowed to assume different parameters across groups. We proceed next by assuming that the actual respondents advantaged or disadvan taged by the items may be better regarded as latent classes in the data. Holding the estimates of the multigroup IRT model as fixed, we classify examinees (irrespective of gender) into the group for which their response patterns are most similar. We then demonstrate that membership in these latent classes may be only slightly associated with gender. In the second study, we illustrate an alternative approach. The strategy first identifies the latent groups of examinees for whom DIF is greatest. We begin by fitting a mixture item response theory (IRT) model to the data so as to identify content classes of examinees for which the test performs most differentially. Then",0
https://doi.org/10.1002/hec.1141,Predicting costs over time using Bayesian Markov chain Monte Carlo methods: an application to early inflammatory polyarthritis,"This article focuses on the modelling and prediction of costs due to disease accrued over time, to inform the planning of future services and budgets. It is well documented that the modelling of cost data is often problematic due to the distribution of such data; for example, strongly right skewed with a significant percentage of zero-cost observations. An additional problem associated with modelling costs over time is that cost observations measured on the same individual at different time points will usually be correlated. In this study we compare the performance of four different multilevel/hierarchical models (which allow for both the within-subject and between-subject variability) for analysing healthcare costs in a cohort of individuals with early inflammatory polyarthritis (IP) who were followed-up annually over a 5-year time period from 1990/1991. The hierarchical models fitted included linear regression models and two-part models with log-transformed costs, and two-part model with gamma regression and a log link. The cohort was split into a learning sample, to fit the different models, and a test sample to assess the predictive ability of these models. To obtain predicted costs on the original cost scale (rather than the log-cost scale) two different retransformation factors were applied. All analyses were carried out using Bayesian Markov chain Monte Carlo (MCMC) simulation methods.",0
https://doi.org/10.1214/aos/1176325622,Posterior Predictive $p$-Values,"Extending work of Rubin, this paper explores a Bayesian counterpart of the classical $p$-value, namely, a tail-area probability of a ""test statistic"" under a null hypothesis. The Bayesian formulation, using posterior predictive replications of the data, allows a ""test statistic"" to depend on both data and unknown (nuisance) parameters and thus permits a direct measure of the discrepancy between sample and population quantities. The tail-area probability for a ""test statistic"" is then found under the joint posterior distribution of replicate data and the (nuisance) parameters, both conditional on the null hypothesis. This posterior predictive $p$-value can also be viewed as the posterior mean of a classical $p$-value, averaging over the posterior distribution of (nuisance) parameters under the null hypothesis, and thus it provides one general method for dealing with nuisance parameters. Two classical examples, including the Behrens-Fisher problem, are used to illustrate the posterior predictive $p$-value and some of its interesting properties, which also reveal a new (Bayesian) interpretation for some classical $p$-values. An application to multiple-imputation inference is also presented. A frequency evaluation shows that, in general, if the replication is defined by new (nuisance) parameters and new data, then the Type I frequentist error of an $\alpha$-level posterior predictive test is often close to but less than $\alpha$ and will never exceed $2\alpha$.",0
https://doi.org/10.2307/2986113,Informative Drop-Out in Longitudinal Data Analysis,"A model is proposed for continuous longitudinal data with non-ignorable or informative drop-out (ID). The model combines a multivariate linear model for the underlying response with a logistic regression model for the drop-out process. The latter incorporates dependence of the probability of drop-out on unobserved, or missing, observations. Parameters in the model are estimated by using maximum likelihood (ML) and inferences drawn through conventional likelihood procedures. In particular, likelihood ratio tests can be used to assess the informativeness of the drop-out process through comparison of the full model with reduced models corresponding to random drop-out (RD) and completely random processes",0
https://doi.org/10.1017/s0003055409990177,"Legislative Involvement in Parliamentary Systems: Opportunities, Conflict, and Institutional Constraints","In parliamentary systems, the need to preserve the political agreement that sustains the executive often motivates legislative involvement in policymaking. Institutional arrangements regulating executive–legislative relations and ministerial autonomy also structure parliamentary participation. However, empirical evidence of these effects remains limited to a few policies and countries. European Union legislation provides the opportunity to test expectations about legislative involvement for different types of measure across various institutional arrangements, across multiple policy areas, and across time. In this article, we investigate legislative involvement in the transposition of 724 directives in 15 member states from 1978 to 2004. Our results confirm that involvement increases as conflict between the responsible minister and her coalition partners intensifies. The discretionary scope embedded in the directive further inflates this effect. Additionally, parliamentary involvement decreases as the government's institutional advantage over the legislature increases, especially if intracoalitional conflict deepens.",0
https://doi.org/10.1080/00273171.2010.498290,Quantifying and Testing Indirect Effects in Simple Mediation Models When the Constituent Paths Are Nonlinear,Most treatments of indirect effects and mediation in the statistical methods literature and the corresponding methods used by behavioral scientists have assumed linear relationships between variables in the causal system. Here we describe and extend a method first introduced by Stolzenberg (1980) for estimating indirect effects in models of mediators and outcomes that are nonlinear functions but linear in their parameters. We introduce the concept of the instantaneous indirect effect of X on Y through M and illustrate its computation and describe a bootstrapping procedure for inference. Mplus code as well as SPSS and SAS macros are provided to facilitate the adoption of this approach and ease the computational burden on the researcher.,0
https://doi.org/10.1177/01466216000241001,A General Item Response Theory Model for Unfolding Unidimensional Polytomous Responses,"The generalized graded unfolding model (GGUM) is developed. This model allows for either binary or graded responses and generalizes previous item response models for unfolding in two useful ways. First, it implements a discrimination parameter that varies across items, which allows items to discriminate among respondents in different ways. Second, the GGUM permits response category threshold parameters to vary across items. Amarginal maximum likelihood algorithm is implemented to estimate GGUM item parameters, whereas person parameters are derived from an expected a posteriori technique. The applicability of the GGUM to common attitude testing situations is illustrated with real data on student attitudes toward abortion.",0
https://doi.org/10.1002/bimj.201100134,The empirical coverage of confidence intervals: Point estimates and confidence intervals for confidence levels,"Many confidence intervals calculated in practice are potentially not exact, either because the requirements for the interval estimator to be exact are known to be violated, or because the (exact) distribution of the data is unknown. If a confidence interval is approximate, the crucial question is how well its true coverage probability approximates its intended coverage probability. In this paper we propose to use the bootstrap to calculate an empirical estimate for the (true) coverage probability of a confidence interval. In the first instance, the empirical coverage can be used to assess whether a given type of confidence interval is adequate for the data at hand. More generally, when planning the statistical analysis of future trials based on existing data pools, the empirical coverage can be used to study the coverage properties of confidence intervals as a function of type of data, sample size, and analysis scale, and thus inform the statistical analysis plan for the future trial. In this sense, the paper proposes an alternative to the problematic pretest of the data for normality, followed by selection of the analysis method based on the results of the pretest. We apply the methodology to a data pool of bioequivalence studies, and in the selection of covariance patterns for repeated measures data.",0
https://doi.org/10.1177/0010414007313113,The Sensitive Left and the Impervious Right,"Recent years have seen increased attention to integrating what we know about individual citizens with what we know about macro-level contexts that vary across countries. This article discusses the growing literature on how people's interpretations, opinions, and actions are shaped by variable contextual parameters and provides a novel substantive application. Using surveys conducted in 20 European democracies, the authors examine the effect of income inequality on people's attitudes about the functioning of the political system and trust in public institutions. They find that citizens in countries with higher levels of income inequality express more negative attitudes toward public institutions. Moreover, they show that the negative effect of inequality on attitudes toward the political system is particularly powerful among individuals on the political left. In contrast, inequality's negative effect on people's faith in the system is muted among those on the right.",0
https://doi.org/10.1037/1082-989x.9.3.275,Structural Equation Models of Latent Interactions: Evaluation of Alternative Estimation Strategies and Indicator Construction.,"Interactions between (multiple indicator) latent variables are rarely used because of implementation complexity and competing strategies. Based on 4 simulation studies, the traditional constrained approach performed more poorly than did 3 new approaches--unconstrained, generalized appended product indicator, and quasi-maximum-likelihood (QML). The authors' new unconstrained approach was easiest to apply. All 4 approaches were relatively unbiased for normally distributed indicators, but the constrained and QML approaches were more biased for nonnormal data; the size and direction of the bias varied with the distribution but not with the sample size. QML had more power, but this advantage was qualified by consistently higher Type I error rates. The authors also compared general strategies for defining product indicators to represent the latent interaction factor.",0
https://doi.org/10.1080/00031305.2014.955211,Are Nonprofit Antipoverty Organizations Located Where They Are Needed? A Spatial Analysis of the Greater Hartford Region,"The geographic distribution of nonprofit antipoverty organizations has important implications for economic development, social services, public health, and policy efforts. With counts of antipoverty nonprofits at the census tract level in Greater Hartford, Connecticut, we examine whether these organizations are located in areas with high levels of poverty with a spatial zero-inflated-Poisson model. Covariates that measure need, resources, urban structure, and demographic characteristics are incorporated into both the zero-inflation component and the Poisson component of the model. Variation not explained by the covariates is captured by the combination of a spatial random effect and an unstructured random effect. Statistical inferences are done within the Bayesian framework. Model comparison with the conditional predictive ordinate suggests that the random effects and the zero-inflation are both important components in fitting the data. All three need measures—proportion of people below the poverty line, ...",0
https://doi.org/10.1080/13691058.2014.989265,Prevalence and correlates of young people's sexual aggression perpetration and victimisation in 10 European countries: a multi-level analysis,"Data are presented on young people's sexual victimisation and perpetration from 10 European countries (Austria, Belgium, Cyprus, Greece, Lithuania, the Netherlands, Poland, Portugal, Slovakia and Spain) using a shared measurement tool (N = 3480 participants, aged between 18 and 27 years). Between 19.7 and 52.2% of female and between 10.1 and 55.8% of male respondents reported having experienced at least one incident of sexual victimisation since the age of consent. In two countries, victimisation rates were significantly higher for men than for women. Between 5.5 and 48.7% of male and 2.6 and 14.8% of female participants reported having engaged in a least one act of sexual aggression perpetration, with higher rates for men than for women in all countries. Victimisation rates correlated negatively with sexual assertiveness and positively with alcohol use in sexual encounters. Perpetration rates correlated positively with attitudes condoning physical dating violence and with alcohol use in men, and negatively with sexual assertiveness in women. At the country level, lower gender equality in economic power and in the work domain was related to higher male perpetration rates. Lower gender equality in political power and higher sexual assertiveness in women relative to men were linked to higher male victimisation rates.",0
https://doi.org/10.1007/bf02221049,Growth curves of deviant behavior in early adolescence: A multilevel analysis,"Multilevel growth curve models provide a means of analyzing individual differences in the growth of deviance, allow a number of theories to be integrated in a single model, and can help to unify research on deviant/delinquent/criminal careers at different stages of the life cycle. Building on the distinction between ""population heterogeneity"" and ""state dependence"" as alternative explanations of persistent individual differences in deviance (Heckman, 1981; Nagin and Paternoster, 1991), we show that models with two levels can be used to represent and analyze a variety of criminological theories. The first level (level 1) uses repeated measurements on individuals to estimate individual-level growth curves. The second level treats the level 1 growth curve parameters (e.g., slope, intercept) as outcome variables and uses time-invariant factors to explain variation in these parameters across individuals. We illustrate this approach by estimating a model of growth in deviance drawn from Gottfredson and Hirschi's deviant propensity theory. An innovative feature is the assumption that adolescents' expected growth curves of deviance follow a classical Pearl-Verhulst logistic growth model (Pearl, 1930). The results suggest that five risk factors - parental psychiatric problems, lack of parental support, living arrangements with zero or one parent in residence, low family income, and male gender - have strongly positive effects on deviant propensity. For example, adolescents with no supportive parents, and no other risk factors, have expected asymptotic levels of deviance (peak levels attained at about age 18) that are about twice as high as those of adolescents with no risk factors. Yet more than two-thirds of the individual-level variability in growth curves is unexplained by the five risk factors. This unobserved heterogeneity would remain hidden in analyses using conventional structural equations models and the same explanatory variables. Ã‚Â© 1997 Plenum Publishing Corporation.",0
https://doi.org/10.1002/wrcr.20445,Accounting for seasonal dependence in hydrological model errors and prediction uncertainty,"[1] Streamflows often vary strongly with season, and this leads to seasonal dependence in hydrological model errors and prediction uncertainty. In this study, we introduce three error models to describe errors from a monthly rainfall-runoff model: a seasonally invariant model, a seasonally variant model, and a hierarchical error model. The seasonally variant model and the hierarchical error model use month-specific parameters to explicitly account for seasonal dependence, while the seasonally invariant model does not. A Bayesian prior is used in the hierarchical error model to account for potential variation and connection among model parameters of different months. The three error models are applied to predicting streamflows for five Australian catchments and are compared by various performance scores and diagnostic plots. The seasonally variant model and the hierarchical model both perform substantially better than the seasonally invariant model. From a cross-validation analysis, the hierarchical error model provides both the most accurate prediction mean and the most reliable prediction uncertainty distribution in most situations. The use of the prior to constrain the model parameters in the hierarchical model produces more robust parameter estimation than the other two models.",0
https://doi.org/10.1002/0470036486,Longitudinal Data Analysis,"Longitudinal data analysis for biomedical and behavioral sciences. This innovative book sets forth and describes methods for the analysis of longitudinaldata, emphasizing applications to problems in the biomedical and behavioral sciences. Reflecting the growing importance and use of longitudinal data across many areas of research, the text is designed to help users of statistics better analyze and understand this type of data. Much of the material from the book grew out of a course taught by Dr. Hedeker on longitudinal data analysis. The material is, therefore, thoroughly classroom tested and includes a number of features designed to help readers better understand and apply the material. Statistical procedures featured within the text include: Repeated measures analysis of variance; Multivariate analysis of variance for repeated measures; Random-effects regression models (RRM); Covariance-pattern models; Generalized-estimating equations (GEE) models; Generalizations of RRM and GEE for categorical outcomes. Practical in their approach, the authors emphasize the applications of the methods, using real-world examples for illustration. Some syntax examples are provided, although the authors do not generally focus on software in this book. Several datasets and computer syntax examples are posted on this title's companion Web site. The authors intend to keep the syntax examples current as new versions of the software programs emerge. This text is designed for both undergraduate and graduate courses in longitudinal data analysis. Instructors can take advantage of overheads and additional course materials available online for adopters. Applied statisticians in biomedicine and the social sciences can also use the book as a convenient reference. Â© 2006 by John Wiley & Sons, Inc. All rights reserved.",0
https://doi.org/10.1111/j.2044-8317.1992.tb00975.x,A comparison of some methodologies for the factor analysis of non-normal Likert variables: A note on the size of the model,"This paper expands on a recent study by Muthen & Kaplan (1985) by examining the impact of non-normal Likert variables on testing and estimation in factor analysis for models of various size. Normal theory GLS and the recently developed ADF estimator are compared for six cases of non-normality, two sample sizes, and four models of increasing size in a Monte Carlo framework with a large number of replications. Results show that GLS and ADF chi-square tests are increasingly sensitive to non-normality when the size of the model increases. No parameter estimate bias was observed for GLS and only slight parameter bias was found for ADF. A downward bias in estimated standard errors was found for GLS which remains constant across model size. For ADF, a downward bias in estimated standard errors was also found which became increasingly worse with the size of the model.",0
https://doi.org/10.1086/224533,Issues in Multiple Regression,"Controlling for variables implies conceptual distinctness between the control and zero-order variables. However, there are different levels of distinctness, some more subtle than others. These levels are determined by the theoretical context of the research. Failure to specify the theoretical context creates ambiguity as to the level of distinctness, and leads to the partialling fallacy, in which one controls for variables that are distinct in terms of appropriate theory. Although this can occur in using any control procedure, it is especially likely to occur in multiple regression, where high-order partial regression coefficients are routinely obtained in order to determine the relative importance of variables. Four major ways in which these regression coefficients can be seriously misleading are discussed. Although warnings concerning multicollinearity are to be found in statistics texts, they are insufficiently informative to prevent the mistakes described here. This is because the problem is essentially one of substantive interpretation rather than one of mathematical statistics per se.",0
https://doi.org/10.1111/biom.12294,Multilevel quantile function modeling with application to birth outcomes,"Infants born preterm or small for gestational age have elevated rates of morbidity and mortality. Using birth certificate records in Texas from 2002 to 2004 and Environmental Protection Agency air pollution estimates, we relate the quantile functions of birth weight and gestational age to ozone exposure and multiple predictors, including parental age, race, and education level. We introduce a semi-parametric Bayesian quantile approach that models the full quantile function rather than just a few quantile levels. Our multilevel quantile function model establishes relationships between birth weight and the predictors separately for each week of gestational age and between gestational age and the predictors separately across Texas Public Health Regions. We permit these relationships to vary nonlinearly across gestational age, spatial domain and quantile level and we unite them in a hierarchical model via a basis expansion on the regression coefficients that preserves interpretability. Very low birth weight is a primary concern, so we leverage extreme value theory to supplement our model in the tail of the distribution. Gestational ages are recorded in completed weeks of gestation (integer-valued), so we present methodology for modeling quantile functions of discrete response data. In a simulation study we show that pooling information across gestational age and quantile level substantially reduces MSE of predictor effects. We find that ozone is negatively associated with the lower tail of gestational age in south Texas and across the distribution of birth weight for high gestational ages. Our methods are available in the R package BSquare.",0
https://doi.org/10.1080/01621459.1990.10474930,A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithms,"Abstract The first part of this article presents the Monte Carlo implementation of the E step of the EM algorithm. Given the current guess to the maximizer of the posterior distribution, latent data patterns are generated from the conditional predictive distribution. The expected value of the augmented log-posterior is then updated as a mixture of augmented log-posteriors, mixed over the generated latent data patterns (multiple imputations). In the M step of the algorithm, this mixture is maximized to obtain the update to the maximizer of the observed posterior. The gradient and Hessian of the observed log posterior are also expressed as mixtures, mixed over the multiple imputations. The relation between the Monte Carlo EM (MCEM) algorithm and the data augmentation algorithm is noted. Two modifications to the MCEM algorithm (the poor man's data augmentation algorithms), which allow for the calculation of the entire posterior, are then presented. These approximations serve as diagnostics for the validity o...",0
https://doi.org/10.1037/met0000162,Prior sensitivity analysis in default Bayesian structural equation modeling.,"Bayesian structural equation modeling (BSEM) has recently gained popularity because it enables researchers to fit complex models and solve some of the issues often encountered in classical maximum likelihood estimation, such as nonconvergence and inadmissible solutions. An important component of any Bayesian analysis is the prior distribution of the unknown model parameters. Often, researchers rely on default priors, which are constructed in an automatic fashion without requiring substantive prior information. However, the prior can have a serious influence on the estimation of the model parameters, which affects the mean squared error, bias, coverage rates, and quantiles of the estimates. In this article, we investigate the performance of three different default priors: noninformative improper priors, vague proper priors, and empirical Bayes priors-with the latter being novel in the BSEM literature. Based on a simulation study, we find that these three default BSEM methods may perform very differently, especially with small samples. A careful prior sensitivity analysis is therefore needed when performing a default BSEM analysis. For this purpose, we provide a practical step-by-step guide for practitioners to conducting a prior sensitivity analysis in default BSEM. Our recommendations are illustrated using a well-known case study from the structural equation modeling literature, and all code for conducting the prior sensitivity analysis is available in the online supplemental materials. (PsycINFO Database Record",1
https://doi.org/10.3758/pbr.16.5.798,Psychological interpretation of the ex-Gaussian and shifted Wald parameters: A diffusion model analysis,"A growing number of researchers use descriptive distributions such as the ex-Gaussian and the shifted Wald to summarize response time data for speeded two-choice tasks. Some of these researchers also assume that the parameters of these distributions uniquely correspond to specific cognitive processes. We studied the validity of this cognitive interpretation by relating the parameters of the ex-Gaussian and shifted Wald distributions to those of the Ratcliff diffusion model, a successful model whose parameters have well-established cognitive interpretations. In a simulation study, we fitted the ex-Gaussian and shifted Wald distributions to data generated from the diffusion model by systematically varying its parameters across a wide range of plausible values. In an empirical study, the two descriptive distributions were fitted to published data that featured manipulations of task difficulty, response caution, and a priori bias. The results clearly demonstrate that the ex-Gaussian and shifted Wald parameters do not correspond uniquely to parameters of the diffusion model. We conclude that researchers should resist the temptation to interpret changes in the ex-Gaussian and shifted Wald parameters in terms of cognitive processes. Supporting materials may be downloaded from http://pbr.psychonomic-journals .org/content/supplemental.",0
https://doi.org/10.1177/1471082x0901000404,A Bayesian model for repeated measures zero-inflated count data with application to outpatient psychiatric service use,"In applications involving count data, it is common to encounter an excess number of zeros. For example, in the study of outpatient service utilization, the number of utilization days will take on integer values, with many subjects having no utilization (zero values). Mixed distribution models, such as the zero-inflated Poisson and zero-inflated negative binomial, are often used to fit such data. A more general class of mixture models, called hurdle models, can be used to model zero deflation as well as zero inflation. Several authors have proposed frequentist approaches to fitting zero-inflated models for repeated measures. We describe a practical Bayesian approach which incorporates prior information, has optimal small-sample properties and allows for tractable inference. The approach can be easily implemented using standard Bayesian software. A study of psychiatric outpatient service use illustrates the methods.",0
https://doi.org/10.1111/j.2044-8317.2011.02032.x,A latent trait model for response times on tests employing the proportional hazards model,"For computer-administered tests, response times can be recorded conjointly with the corresponding responses. This broadens the scope of potential modelling approaches because response times can be analysed in addition to analysing the responses themselves. For this purpose, we present a new latent trait model for response times on tests. This model is based on the Cox proportional hazards model. According to this model, latent variables alter a baseline hazard function. Two different approaches to item parameter estimation are described: the first approach uses a variant of the Cox model for discrete time, whereas the second approach is based on a profile likelihood function. Properties of each estimator will be compared in a simulation study. Compared to the estimator for discrete time, the profile likelihood estimator is more efficient, that is, has smaller variance. Additionally, we show how the fit of the model can be evaluated and how the latent traits can be estimated. Finally, the applicability of the model to an empirical data set is demonstrated.",0
https://doi.org/10.1177/0013164411408412,On the Reliability and Validity of a Numerical Reasoning Speed Dimension Derived From Response Times Collected in Computerized Testing,Data from 181 college students were used to assess whether math reasoning item response times in computerized testing can provide valid and reliable measures of a speed dimension. The alternate forms reliability of the speed dimension was .85. A two-dimensional structural equation model suggests that the speed dimension is related to the accuracy of speeded responses. Speed factor scores were significantly correlated with performance on the ACT math scale. Results suggest that the speed dimension underlying response times can be reliably measured and that the dimension is related to the accuracy of performance under the pressure of time limits.,0
https://doi.org/10.1207/s15328007sem1204_2,Estimation of Reliability for Multiple-Component Measuring Instruments in Hierarchical Designs,"A method for estimation of reliability for multiple-component measuring instruments with clustered data is outlined. The approach is applicable with hierarchical designs where individuals are nested within higher order units and exhibit possibly related performance on components of a scale of interest. The procedure is developed within the framework of multilevel covariance structure modeling. The described method is useful for point and interval estimation of the degree of consistency of measurement with congeneric composites in hierarchical populations, and is illustrated with an empirical example.",0
https://doi.org/10.1214/06-ba117,A comparison of Bayesian and likelihood-based methods for fitting multilevel models,"We use simulation studies, whose design is realistic for educational andmedicalresearch(aswellasotherfleldsofinquiry),tocompareBayesianand likelihood-basedmethodsforflttingvariance-components(VC)andrandom-efiects logistic regression (RELR) models. The likelihood (and approximate likelihood) approachesweexaminearebasedonthemethodsmostwidelyusedincurrentap- plied multilevel (hierarchical) analyses: maximum likelihood (ML) and restricted ML(REML)forGaussianoutcomes,andmarginalandpenalizedquasi-likelihood (MQL and PQL) for Bernoulli outcomes. Our Bayesian methods use Markov chain Monte Carlo (MCMC) estimation, with adaptive hybrid Metropolis-Gibbs sampling for RELR models, and several difiuse prior distributions (i i1 (†;†) and U(0; 1 ) priors for variance components). For evaluation criteria we consider bias of point estimates and nominal versus actual coverage of interval estimates in re- peated sampling. In two-level VC models we flnd that (a) both likelihood-based and Bayesian approaches can be made to produce approximately unbiased esti- mates, although the automatic manner in which REML accomplishes this is an advantage, but (b) both approaches had di-culty achieving nominal coverage in smallsamplesandwithsmallvaluesoftheintraclasscorrelation. Withthethree- levelRELRmodelsweexamineweflndthat(c)quasi-likelihoodmethodsforesti- mating random-efiects variances perform badly with respect to bias and coverage intheexamplewesimulated,and(d)Bayesiandifiuse-priormethodsleadtowell- calibratedpointandintervalRELRestimates. Whileitistruethatthelikelihood- based methods we study are considerably faster computationally than MCMC, (i) steady improvements in recent years in both hardware speed and e-ciency of MonteCarloalgorithmsand(ii)thelackofcalibrationoflikelihood-basedmethods insomecommonhierarchicalsettingscombinetomakeMCMC-basedBayesianflt- tingofmultilevelmodelsanattractiveapproach,evenwithratherlargedatasets. Other analytic strategies based on less approximate likelihood methods are also possible butwouldbeneflt fromfurtherstudy ofthe type summarized here.",1
https://doi.org/10.1007/s11336-003-0974-7,"Cronbach’s α, Revelle’s β, and Mcdonald’s ωH: their relations with each other and two alternative conceptualizations of reliability","We make theoretical comparisons among five coefficients - Cronbach's Î±, Revelle's Î², McDonald's Ï‰ h, and two alternative conceptualizations of reliability. Though many end users and psychometricians alike may not distinguish among these five coefficients, we demonstrate formally their nonequivalence. Specifically, whereas there are conditions under which Î±, Î², and Ï‰ h are equivalent to each other and to one of the two conceptualizations of reliability considered here, we show that equality with this conceptualization of reliability and between Î± and Ï‰ h holds only under a highly restrictive set of conditions and that the conditions under which Î² equals Ï‰ h are only somewhat more general. The nonequivalence of Î±, Î², and Ï‰ h suggests that important information about the psychometric properties of a scale may be missing when scale developers and users only report Î± as is almost always the case. Â© 2005 The Psychometric Society.",0
https://doi.org/10.1214/09-aoas250,Hierarchical spatial models for predicting tree species assemblages across large domains,"Spatially explicit data layers of tree species assemblages, referred to as forest types or forest type groups, are a key component in large-scale assessments of forest sustainability, biodiversity, timber biomass, carbon sinks and forest health monitoring. This paper explores the utility of coupling georeferenced national forest inventory (NFI) data with readily available and spatially complete environmental predictor variables through spatially-varying multinomial logistic regression models to predict forest type groups across large forested landscapes. These models exploit underlying spatial associations within the NFI plot array and the spatially-varying impact of predictor variables to improve the accuracy of forest type group predictions. The richness of these models incurs onerous computational burdens and we discuss dimension reducing spatial processes that retain the richness in modeling. We illustrate using NFI data from Michigan, USA, where we provide a comprehensive analysis of this large study area and demonstrate improved prediction with associated measures of uncertainty.",0
https://doi.org/10.1037/a0025814,A time-varying effect model for intensive longitudinal data.,"Understanding temporal change in human behavior and psychological processes is a central issue in the behavioral sciences. With technological advances, intensive longitudinal data (ILD) are increasingly generated by studies of human behavior that repeatedly administer assessments over time. ILD offer unique opportunities to describe temporal behavioral changes in detail and identify related environmental and psychosocial antecedents and consequences. Traditional analytical approaches impose strong parametric assumptions about the nature of change in the relationship between time-varying covariates and outcomes of interest. This article introduces time-varying effect models (TVEMs) that explicitly model changes in the association between ILD covariates and ILD outcomes over time in a flexible manner. In this article, we describe unique research questions that the TVEM addresses, outline the model-estimation procedure, share a SAS macro for implementing the model, demonstrate model utility with a simulated example, and illustrate model applications in ILD collected as part of a smoking-cessation study to explore the relationship between smoking urges and self-efficacy during the course of the pre- and postcessation period.",0
https://doi.org/10.1007/s11336-009-9113-4,Using Threshold Autoregressive Models to Study Dyadic Interactions,"Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.",0
https://doi.org/10.1002/sim.3441,Meta-analysis of diagnostic test studies using individual patient data and aggregate data,"A meta-analysis of diagnostic test studies provides evidence-based results regarding the accuracy of a particular test, and usually involves synthesizing aggregate data (AD) from each study, such as the 2 by 2 tables of diagnostic accuracy. A bivariate random-effects meta-analysis (BRMA) can appropriately synthesize these tables, and leads to clinical results, such as the summary sensitivity and specificity across studies. However, translating such results into practice may be limited by between-study heterogeneity and that they relate to some 'average' patient across studies.In this paper we describe how the meta-analysis of individual patient data (IPD) from diagnostic studies can lead to clinical results more tailored to the individual patient. We develop IPD models that extend the BRMA framework to include study-level covariates, which help explain the between-study heterogeneity, and also patient-level covariates, which allow one to assess the effect of patient characteristics on test accuracy. We show how the inclusion of patient-level covariates requires a careful separation of within-study and across-study accuracy-covariate effects, as the latter are particularly prone to confounding. Our models are assessed through simulation and extended to allow IPD studies to be combined with AD studies, as IPD are not always available for all studies. Application is made to 23 studies assessing the accuracy of ear thermometers for diagnosing fever in children, with 16 IPD and 7 AD studies. The models reveal that between-study heterogeneity is partly explained by the use of different measurement devices, but there is no evidence that being an infant modifies diagnostic accuracy.",0
https://doi.org/10.1016/j.jmp.2005.02.004,A Bayesian approach to testing decision making axioms,"Abstract Theories of decision making are often formulated in terms of deterministic axioms, which do not account for stochastic variation that attends empirical data. This study presents a Bayesian inference framework for dealing with fallible data. The Bayesian framework provides readily applicable statistical procedures addressing typical inference questions that arise when algebraic axioms are tested against empirical data. The key idea of the Bayesian framework is to employ a prior distribution representing the parametric order constraints implied by a given axiom. Modern methods of Bayesian computation such as Markov chain Monte Carlo are used to estimate the posterior distribution, which provides the information that allows an axiom to be evaluated. Specifically, we adopt the Bayesian p -value as the criterion to assess the descriptive adequacy of a given model (axiom) and we use the deviance information criterion (DIC) to select among a set of candidate models. We illustrate the Bayesian framework by testing well-known axioms of decision making, including the axioms of monotonicity of joint receipt and stochastic transitivity.",0
https://doi.org/10.1093/biomet/74.4.817,A fast scoring algorithm for maximum likelihood estimation in unbalanced mixed models with nested random effects,On decrit un algorithme qui utilise des formules explicites pour l'inverse et le determinant de la matrice de covariance donnee par La Motte (1972) et evite l'inversion des grandes matrices,0
https://doi.org/10.1080/01621459.1983.10477920,Parametric Empirical Bayes Inference: Theory and Applications,"Abstract This article reviews the state of multiparameter shrinkage estimators with emphasis on the empirical Bayes viewpoint, particularly in the case of parametric prior distributions. Some successful applications of major importance are considered. Recent results concerning estimates of error and confidence intervals are described and illustrated with data.",0
https://doi.org/10.2307/1167125,The Analysis of Multilevel Data in Educational Research and Evaluation,,0
https://doi.org/10.1037/0021-9010.89.1.36,The Forgotten Ones? The Validity of Consideration and Initiating Structure in Leadership Research.,"This study provided a meta-analysis of the relationship of the Ohio State leadership behaviors--Consideration and Initiating Structure--with leadership. Overall, 163 independent correlations for Consideration and 159 correlations for Initiating Structure were analyzed. Results revealed that both Consideration (.48) and Initiating Structure (.29) have moderately strong, nonzero relations with leadership outcomes. Consideration was more strongly related to follower satisfaction (leader satisfaction, job satisfaction), motivation, and leader effectiveness, and Initiating Structure was slightly more strongly related to leader job performance and group-organization performance. Validities did vary by leadership measure, but in most cases validities generalized regardless of the measure used. Overall, the results provide important support for the validity of Initiating Structure and Consideration in leadership research.",0
https://doi.org/10.3102/1076998614547577,Design-Comparable Effect Sizes in Multiple Baseline Designs,"In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general framework for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small sample correction analogous to Hedges’s g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.",0
https://doi.org/10.1016/j.leaqua.2004.09.009,Applying multilevel confirmatory factor analysis techniques to the study of leadership,"Statistical issues associated with multilevel data are becoming increasingly important to organizational researchers. This paper concentrates on the issue of assessing the factor structure of a construct at aggregate levels of analysis. Specifically, we describe a recently developed procedure for performing multilevel confirmatory factor analysis (MCFA) [Muthen, B.O. (1990). Mean and covariance structure analysis of hierarchical data. Paper presented at the Psychometric Society, Princeton, NJ; Muthen, B.O. (1994). Multilevel covariance structure analysis. Sociological Methods and Research, 22, 376–398], and provide an illustrative example of its application to leadership data reflecting both the organizational and societal level of analysis. Overall, the results of our illustrative analysis support the existence of a valid societal-level leadership construct, and show the potential of this multilevel confirmatory factor analysis procedure for leadership research and the field of I/O psychology in general.",0
https://doi.org/10.3758/s13414-015-0834-4,Effect of Decision Load on Whole-Display Superiority in Change Detection,"Visual short-term memory (VSTM) refers to our ability in remembering visual information for a limited amount of time. In the VSTM literature, mixed findings have been reported regarding whether items are encoded individually or globally in the context of other items. This study adopted a color change detection task and manipulated color and spatial relations of the items on display to test whether inter-item relational information and processing can facilitate change detection performance. Results showed that only when a post-cue was presented to reduce decision load (Experiments 1 and 3), both color and spatial relations facilitated color change detection. However, when there was no post-cue to lessen the decision load, preserving spatial relations at test impaired color change detection (Experiment 2). Furthermore, spatial and color relational processing interactively affected color change detection. Benefit of the spatial relations was observed only when color grouping cues can aid change detection, and the utilization of color relations was optimized when spatial relations were preserved to cue the retrieval of color relations. Our results support the hierarchical representation hypothesis, which assumes that both individual items and item relations are encoded and maintained in VSTM. The amount of cognitive resources for retrieving different levels of representations is highly constrained by the decision load.",0
https://doi.org/10.1080/01621459.1994.10476829,The Collapsed Gibbs Sampler in Bayesian Computations with Applications to a Gene Regulation Problem,"Abstract This article describes a method of “grouping” and “collapsing” in using the Gibbs sampler and proves from an operator theory viewpoint that the method is in general beneficial. The norms of the forward operators associated with the corresponding nonreversible Markov chains are used to discriminate among different simulation schemes. When applied to Bayesian missing data problems, the idea of collapsing suggests skipping the steps of sampling parameter(s) values in standard data augmentation. By doing this, we obtain a predictive update version of the Gibbs sampler. A procedure of calculating the posterior odds ratio via the collapsed Gibbs sampler when incomplete observations are involved is presented. As an illustration of possible applications, three examples, along with a Bayesian treatment for identifying common protein binding sites in unaligned DNA sequences, are provided.",0
https://doi.org/10.1080/0267257x.2012.698637,Customer-perceived value in business-to-business relationships: A study of software customers,"Abstract Despite the importance of relationships in business-to-business (B2B) contexts, there is limited research as to what customers expect and value from relationships in industrial contexts. This study, therefore, seeks to understand customer-perceived value better by investigating actual and prospective customers in the software industry. A two-level analysis of customer perspectives on relationship attributes was conducted. First, semi-structured interviews were conducted with customers of a micro software firm. Insights from these interviews were then used to inform the second stage of the study, an online survey using Adaptive Conjoint Analysis, to identify the relative significance of these attributes. A total of 256 industrial buyers completed the survey. A new Customer Relationship Attributes Model (CRAM) is presented which encapsulates major attributes that current and prospective customers consider when entering into a relationship with their software supplier. The CRAM identifies five produ...",0
https://doi.org/10.1167/10.2.24,Distortions in recall from visual memory: Two classes of attractors at work,"In a trio of experiments, a matching procedure generated direct, analogue measures of short-term memory for the spatial frequency of Gabor stimuli. Experiment 1 showed that when just a single Gabor was presented for study, a retention interval of just a few seconds was enough to increase the variability of matches, suggesting that noise in memory substantially exceeds that in vision. Experiment 2 revealed that when a pair of Gabors was presented on each trial, the remembered appearance of one of the Gabors was influenced by: (1) the relationship between its spatial frequency and the spatial frequency of the accompanying, task-irrelevant non-target stimulus; and (2) the average spatial frequency of Gabors seen on previous trials. These two influences, which work on very different time scales, were approximately additive in their effects, each operating as an attractor for remembered appearance. Experiment 3 showed that a timely pre-stimulus cue allowed selective attention to curtail the influence of a task-irrelevant non-target, without diminishing the impact of the stimuli seen on previous trials. It appears that these two separable attractors influence distinct processes, with perception being influenced by the non-target stimulus and memory being influenced by stimuli seen on previous trials.",0
https://doi.org/10.1097/psy.0b013e3182736971,Multilevel Modeling in Psychosomatic Medicine Research,"The primary purpose of this study is to provide an overview of multilevel modeling for Psychosomatic Medicine readers and contributors. The article begins with a general introduction to multilevel modeling. Multilevel regression modeling at two levels is emphasized because of its prevalence in psychosomatic medicine research. Simulated data sets based on some core ideas from the Familias Unidas effectiveness study are used to illustrate key concepts including communication of model specification, parameter interpretation, sample size and power, and missing data. Input and key output files from Mplus and SAS are provided. A cluster randomized trial with repeated measures (i.e., three-level regression model) is then briefly presented with simulated data based on some core ideas from a cognitive-behavioral stress management intervention in prostate cancer.",0
https://doi.org/10.1027/1614-2241.1.3.86,Sufficient Sample Sizes for Multilevel Modeling,"Abstract. An important problem in multilevel modeling is what constitutes a sufficient sample size for accurate estimation. In multilevel analysis, the major restriction is often the higher-level sample size. In this paper, a simulation study is used to determine the influence of different sample sizes at the group level on the accuracy of the estimates (regression coefficients and variances) and their standard errors. In addition, the influence of other factors, such as the lowest-level sample size and different variance distributions between the levels (different intraclass correlations), is examined. The results show that only a small sample size at level two (meaning a sample of 50 or less) leads to biased estimates of the second-level standard errors. In all of the other simulated conditions the estimates of the regression coefficients, the variance components, and the standard errors are unbiased and accurate.",0
https://doi.org/10.1016/j.jpain.2013.10.001,Suppression of Voluntary Wheel Running in Rats Is Dependent on the Site of Inflammation: Evidence for Voluntary Running as a Measure of Hind Paw-Evoked Pain,"<h2>Abstract</h2> Decreased voluntary wheel running has recently been proposed as a preclinical pain measure for inflammatory pain, but whether this reflects pain evoked by use of the affected limbs is unknown. To assess the role of inflammation site as a determinant of this measure, complete Freund's adjuvant (CFA), formalin, or equivolume vehicle was subcutaneously injected into the plantar surface of the hind paws (bilateral) or L1 dorsum dermatome (leaving paws unaffected) of male Sprague Dawley rats. CFA-induced hind paw mechanical allodynia (<i>P</i> < .001) did not correlate with reduced voluntary wheel running. Intraplantar formalin did not attenuate voluntary running, despite eliciting robust licking/writhing/flinching behavior and hind paw mechanical allodynia (<i>P</i> < .001). Subcutaneous L1 dorsum dermatome formalin, but not CFA, induced licking/writhing/flinching behavior (<i>P</i> < .001), but neither induced hind paw mechanical allodynia or attenuated voluntary running. That voluntary running is decreased by hind paw CFA, but not by L1 dorsum CFA, implies that the behavior is a measure of CFA-induced pain evoked by use of the affected limbs rather than supraspinal pain processing that is independent of inflammation site. Furthermore, the results suggest that interpretation of voluntary wheel running data cannot simply be explained by correlation with mechanical allodynia. <h3>Perspective</h3> Whether decreased voluntary running is dependent on inflammation site is unknown. We show that intraplantar, but not L1 dorsum, CFA suppressed voluntary running and formalin-induced licking/writhing/flinching behavior but had no effect on voluntary running. These data suggest that suppressed voluntary running by CFA likely reflects pain evoked by use of the affected limbs.",0
https://doi.org/10.1037//0021-9010.87.4.797,Relationship of personality to performance motivation: A meta-analytic review.,"This article provides a meta-analysis of the relationship between the five-factor model of personality and 3 central theories of performance motivation (goal-setting, expectancy, and self-efficacy motivation). The quantitative review includes 150 correlations from 65 studies. Traits were organized according to the five-factor model of personality. Results indicated that Neuroticism (average validity = -.31) and Conscientiousness (average validity = .24) were the strongest and most consistent correlates of performance motivation across the 3 theoretical perspectives. Results further indicated that the validity of 3 of the Big Five traits - Neuroticism, Extraversion, and Conscientiousness - generalized across studies. As a set, the Big Five traits had an average multiple correlation of .49 with the motivational criteria, suggesting that the Big Five traits are an important source of performance motivation.",0
https://doi.org/10.1177/014662169301700402,Detection of Differential Item Functioning in the Graded Response Model,"Methods for detecting differential item func tioning (DIF) have been proposed primarily for the item response theory dichotomous response model. Three measures of DIF for the dichotomous response model are extended to include Samejima's graded response model: two measures based on area differences between item true score functions, and a χ 2 statistic for comparing differences in item parameters. An illustrative example is presented.",0
https://doi.org/10.1080/03610919508813280,Small sample characteristics of generalized estimating equations,"The aim of this study was to investigate the Type I error rate of hypothesis testing based on generalized estimating equations (GEE) for data characteristic of periodontal clinical trials. The data in these studies consist of a large number of binary responses from each subject and a small number of subjects (Haffajee et al. (1983), Goodson (1986), Jenkins et al. (1988)) Computer simulations were employed to investigate GEE based both on an empirical estimate of the variance-covariance matrix and a model-based estimate. Results from this investigation indicate that hypothesis testing based on GEE resulted in inappropriate Type I error rates when small samples are employed. Only an increase in the number of subjects to the point where it matched the number of observations per subject resulted in appropriate Type I error rates",0
https://doi.org/10.1214/07-aos501,Objective priors for the bivariate normal model,"Study of the bivariate normal distribution raises the full range of issues involving objective Bayesian inference, including the different types of objective priors (e.g., Jeffreys, invariant, reference, matching), the different modes of inference (e.g., Bayesian, frequentist, fiducial) and the criteria involved in deciding on optimal objective priors (e.g., ease of computation, frequentist performance, marginalization paradoxes). Summary recommendations as to optimal objective priors are made for a variety of inferences involving the bivariate normal distribution. In the course of the investigation, a variety of surprising results were found, including the availability of objective priors that yield exact frequentist inferences for many functions of the bivariate normal parameters, including the correlation coefficient.",0
https://doi.org/10.3102/1076998606298041,Combining Correlation Matrices: Simulation Analysis of Improved Fixed-Effects Methods,"The originally proposed multivariate meta-analysis approach for correlation matrices?analyze Pearson correlations, with each study's observed correlations replacing their population counterparts in its conditional-covariance matrix? performs poorly. Two refinements are considered: Analyze Fisher Z-transformed correlations, and substitute better estimates of correlations in the conditional covariances. Fixed-effects methods with and without each refinement were exam ined in a Monte Carlo study; number of studies and the distribution of within study sample sizes were varied. Both refinements improved element-wise point and interval estimates, as well as Type I error control for homogeneity tests, especially with many small studies. Practical recommendations and suggestions for future methodological work are offered. An appendix describes how to trans form Fisher-Z (co)variances to the Pearson-v metric.",0
https://doi.org/10.1086/208812,Effects of Prior Knowledge and Experience and Phase of the Choice Process on Consumer Decision Processes: A Protocol Analysis,Effects of prior knowledge and experience and phase of the choice on decision processes were investigated using a protocol coding scheme. Consumers with moderate knowledge and experience did more processing of available information than did the high or low groups. More knowledgeable consumers tended to process by brand. Consumers tended to use attribute-based evaluations in early and brand-based evaluations in later phases of choice.,0
https://doi.org/10.1177/0149206313501200,Bayesian Estimation and Inference,"This paper introduces the “Bayesian revolution” that is sweeping across multiple disciplines but has yet to gain a foothold in organizational research. The foundations of Bayesian estimation and inference are first reviewed. Then, two empirical examples are provided to show how Bayesian methods can overcome limitations of frequentist methods: (a) a structural equation model of testosterone’s effect on status in teams, where a Bayesian approach allows directly testing a traditional null hypothesis as a research hypothesis and allows estimating all possible residual covariances in a measurement model, neither of which are possible with frequentist methods; and (b) an ANOVA-style model from a true experiment of ego depletion’s effects on performance, where Bayesian estimation with informative priors allows results from all previous research (via a meta-analysis and other previous studies) to be combined with estimates of study effects in a principled manner, yielding support for hypotheses that is not obtained with frequentist methods. Data are available from the first author, code for the program Mplus is provided, and tables illustrate how to present Bayesian results. In conclusion, the many benefits and few hindrances of Bayesian methods are discussed, where the major hindrance has been an easily solvable lack of familiarity by organizational researchers.",0
https://doi.org/10.1002/sim.3577,A multivariate CAR model for improving the estimation of relative risks,"Disease mapping studies have been widely performed at univariate level, that is considering only one disease in the estimated models. Nonetheless, simultaneous modelling of different diseases can be a valuable tool both from the epidemiological and from the statistical point of view. In this paper we propose a model for multivariate disease mapping that generalizes the univariate conditional auto-regressive distribution. The proposed model is proven to be an effective alternative to existing multivariate models, mainly because it overcome some restrictive hypotheses underlying models previously proposed in this context. Model performances are checked via a simulation study and via application to a case study.",0
https://doi.org/10.1191/1740774505cn076oa,Design and analysis of clinical trials with clustering effects due to treatment,"Where patients receive therapy as a group, there are good theoretical reasons to believe that variation in the outcome will be smaller for patients treated in the same group than for patients treated in different groups. Similarly, where different therapists treat different groups of patients, outcome for patients treated by the same therapist may differ less than outcome for patients treated by different therapists. Clinical trials evaluating such therapies need to consider this potential lack of independence. As with cluster-randomized trials, this has implications for the precision of treatment effects estimates and statistical power. There are nevertheless differences between clustering due to the organization of treatment and that due to randomization. In cluster-randomized trials the distribution of cluster sizes in each treatment arm should be similar as a consequence of randomization unless there is differential loss to follow-up. With clustering due to therapy group or therapist, cluster size may differ systematically between treatment arms, due to size of therapy groups or differing health professional caseload. Intra-cluster correlation may also differ between treatment arms. The implications of differential cluster size and intracluster correlation for design and analysis will be illustrated by data from two trials, the first comparing nurse practitioner care with general practitioner care, and the second comparing a group therapy with individual treatment as usual. The special case where a group therapy or therapist is compared with an unclustered treatment is examined in detail using a simulation study. The implications of differential clustering effects for sample size and power are addressed. It is argued that the design and analysis of this type of trial should take account of possible heterogeneity in cluster size and intracluster correlation.",0
https://doi.org/10.1198/016214501753381841,Crossed Random Effect Models for Multiple Outcomes in a Study of Teratogenesis,"Human teratogens often manifest themselves through a broad spectrum of adverse effects. Although often not serious when considered individually, such outcomes taken together may represent a syndrome that can lead to serious developmental problems. Accordingly, studies that investigate the effect of human teratogens on fetal development typically record the presence or absence of a multitude of abnormalities, resulting in the data of multivariate binary form for each infant. Such studies typically have three objectives: (1) estimate an overall effect of exposure across outcomes, (2) identify subjects having the syndrome, and (3) identify those outcomes that constitute the syndrome so that doctors know what to look for when diagnosing the syndrome in other exposed newborns. This article proposes the use of a logistic regression model with crossed random effect structure to address all three questions simultaneously. We use the proposed models to analyze data from a study investigating the effects of in uter...",0
https://doi.org/10.1016/j.brat.2013.10.009,Applying the Quadruple Process model to evaluate change in implicit attitudinal responses during therapy for panic disorder,"This study explored the automatic and controlled processes that may influence performance on an implicit measure across cognitive-behavioral group therapy for panic disorder.The Quadruple Process model was applied to error scores from an Implicit Association Test evaluating associations between the concepts Me (vs. Not Me) + Calm (vs. Panicked) to evaluate four distinct processes: Association Activation, Detection, Guessing, and Overcoming Bias. Parameter estimates were calculated in the panic group (n = 28) across each treatment session where the IAT was administered, and at matched times when the IAT was completed in the healthy control group (n = 31).Association Activation for Me + Calm became stronger over treatment for participants in the panic group, demonstrating that it is possible to change automatically activated associations in memory (vs. simply overriding those associations) in a clinical sample via therapy. As well, the Guessing bias toward the calm category increased over treatment for participants in the panic group.This research evaluates key tenets about the role of automatic processing in cognitive models of anxiety, and emphasizes the viability of changing the actual activation of automatic associations in the context of treatment, versus only changing a person's ability to use reflective processing to overcome biased automatic processing.",0
https://doi.org/10.2307/1926450,Multicollinearity in Regression Analysis: Comment,"In recent paper this REVIEW,' Farrar and Glauber (hereafter FG) revisit problem of regression analysis. Viewing problem of as both facet and symptom of poor experimental design, 2 FG propose a three-stage hierarchy of increasing detailed tests presence, location, and pattern, 3 of multicollinearity. The first this series of three tests, on which other two are conditional, is desigped provide useful first measure of presence and severity of multicollinearity 4 sample on hand. Bartlett's well-known statistic testing joint distribution of sample correlations under assumption of vanishing parent correlations between variables is used by FG detecting multicollinearity. Bartlett shows that (natural) logarithm of intercorrelation determinant computed from sample drawn from multivariate, ortho-normal distribution, multiplied by factor k, is approximately distributed as Chi Square with v 1/2 n (n 1) degrees of freedom, where k = -[N 1 1/6 (2n + 5)], N is sample size and n is number of variables considered. If investigator concludes from first stage that exists and that it is severe enough warrant some action, FG propose regress consecutively each explanatory variable on remaining ones. The rp'silting F statistics will test for dependence of particular variables on other members 5 of set of explanatory variables. Finally, patterns of interdependence among independent variables are examined by testing significa-nce of partial correlations of every pair of explanatory variables, all other variables held constant. The main pillar of this three-level test is, of course, Bartlett's test which is properly used making inferences,6 under null hypothesis that all population correlations are zero. Since FG claim, however, that they are not interested drawing inferences from sample population (inferences from sample population . . . are possible . . . however, little importance is attached properties of population from which set of data has been drawn. Attention focuses largely, if not entirely, on sample itself 7), their use of Chi-Square statistic is questionable. Moreover, it is neither practical nor necessary assume orthogonality between parent economic variables, 4f one wishes make such inferences. Here we come heart of problem of multicollinearity. One may agree with FG that it is preferable think of in terms of [its] severity rather than its existence or nonexistence. 8 If one agrees with this approach, natural way proceed is indeed to define terms of departures from hypothesized statistical condition. 9 But what is this hypothesized condition? For FG this condition is the requirement that explanatory variables be truly independent of one another. 10 However, there is no such requirement least-squares solution. On contrary, least squares solu* I share with D. C. Farrar and R. R. Glauber my indebtedness Professor John R. Meyer who introduced us problem, and I am grateful his comments on an earlier draft. I am also grateful Professors J. Johnston, N. Wallace, D. Farrar, and R. Glauber valuable discussions. I am particularly thankful D. Farrar who did not spare his efforts order dig out old forgotten data, which enabled me recompute his regression equations. 1D. C. Farrar and R. R. Glauber, Multicollinearity Regression Analysis: The Problem Revisited, this REvIEw, XLIX (Feb. 1967). 2Ibid., p. 93. 3 Ibid., p. 104. ' Ibid., p. 101. BIbid., p. 104. Bartlett has originally developed this statistic order test number of meaningful components that can be extracted from set of variables. concise statement is given by Bartlett: A Note on Multiplying Factors Various x2 Approximations, Journal of Royal Statistical Society (B), XVI, no. 2 (1954), pp. 296-298. 7D. C. Farrar and R. R. Glauber, op. cit., 100. 8Ibid., p. 106. 9 Ibid., p. 92. 10Ibid., pp. 92 and 100.",0
https://doi.org/10.1037/a0035815,Linear and nonlinear associations between general intelligence and personality in Project TALENT.,"Research on the relations of personality traits to intelligence has primarily been concerned with linear associations. Yet, there are no a priori reasons why linear relations should be expected over nonlinear ones, which represent a much larger set of all possible associations. Using 2 techniques, quadratic and generalized additive models, we tested for linear and nonlinear associations of general intelligence (g) with 10 personality scales from Project TALENT (PT), a nationally representative sample of approximately 400,000 American high school students from 1960, divided into 4 grade samples (Flanagan et al., 1962). We departed from previous studies, including one with PT (Reeve, Meyer, & Bonaccio, 2006), by modeling latent quadratic effects directly, controlling the influence of the common factor in the personality scales, and assuming a direction of effect from g to personality. On the basis of the literature, we made 17 directional hypotheses for the linear and quadratic associations. Of these, 53% were supported in all 4 male grades and 58% in all 4 female grades. Quadratic associations explained substantive variance above and beyond linear effects (mean R² between 1.8% and 3.6%) for Sociability, Maturity, Vigor, and Leadership in males and Sociability, Maturity, and Tidiness in females; linear associations were predominant for other traits. We discuss how suited current theories of the personality-intelligence interface are to explain these associations, and how research on intellectually gifted samples may provide a unique way of understanding them. We conclude that nonlinear models can provide incremental detail regarding personality and intelligence associations.",0
https://doi.org/10.3102/10769986027003271,Modeling Incomplete Scaled Questionnaire Data with a Partial Credit Hierarchical Measurement Model,"The partial credit hierarchical measurement model (HMM) results when a partial credit IRT model and a hierarchical linear model are combined ( Bryk &amp; Raudenbush, 1992 ; Masters, 1982 ). This combined model enables the standard errors of parameters to be estimated accurately. The partial credit HMM is illustrated using a subset of data from the Sloan Study of Youth and Social Development, a five-year longitudinal project studying the career aspirations of adolescents. The data used for this study consisted of a subset of students’ responses to multiple administrations of an attitudinal questionnaire, as well as student-level covariates. Using student responses to seven seven-point semantic differential items tapping student mood, the partial credit HMM was used to explore the effects of gender and classroom activity upon student mood as students were engaged in a mathematics classroom. Gibbs sampling and the Metropolis-Hastings algorithm were used to impute values for the missing data and to estimate the parameters of the model. The results of the data analysis indicated that female students had lower mood than male students did for all classroom activities.",0
https://doi.org/10.1097/fbp.0b013e32834eafbc,The peripheral L-arginine–nitric oxide–cyclic GMP pathway and ATP-sensitive K+ channels are involved in the antinociceptive effect of crotalphine on neuropathic pain in rats,"Crotalphine, a 14 amino acid peptide first isolated from the venom of the South American rattlesnake Crotalus durissus terrificus, induces a peripheral long-lasting and opioid receptor-mediated antinociceptive effect in a rat model of neuropathic pain induced by chronic constriction of the sciatic nerve. In the present study, we further characterized the molecular mechanisms involved in this effect, determining the type of opioid receptor responsible for this effect and the involvement of the nitric oxide-cyclic GMP pathway and of K⁺ channels. Crotalphine (0.2 or 5 μg/kg, orally; 0.0006 μg/paw), administered on day 14 after nerve constriction, inhibited mechanical hyperalgesia and low-threshold mechanical allodynia. The effect of the peptide was antagonized by intraplantar administration of naltrindole, an antagonist of δ-opioid receptors, and partially reversed by norbinaltorphimine, an antagonist of κ-opioid receptors. The effect of crotalphine was also blocked by 7-nitroindazole, an inhibitor of the neuronal nitric oxide synthase; by 1H-(1,2,4) oxadiazolo[4,3-a]quinoxaline-1-one, an inhibitor of guanylate cyclase activation; and by glibenclamide, an ATP-sensitive K⁺ channel blocker. The results suggest that peripheral δ-opioid and κ-opioid receptors, the nitric oxide-cyclic GMP pathway, and ATP-sensitive K⁺ channels are involved in the antinociceptive effect of crotalphine. The present data point to the therapeutic potential of this peptide for the treatment of chronic neuropathic pain.",0
https://doi.org/10.1038/nature06860,Discrete fixed-resolution representations in visual working memory,"Limits on the storage capacity of working memory significantly affect cognitive abilities in a wide range of domains, but the nature of these capacity limits has been elusive. Some researchers have proposed that working memory stores a limited set of discrete, fixed-resolution representations, whereas others have proposed that working memory consists of a pool of resources that can be allocated flexibly to provide either a small number of high-resolution representations or a large number of low-resolution representations. Here we resolve this controversy by providing independent measures of capacity and resolution. We show that, when presented with more than a few simple objects, human observers store a high-resolution representation of a subset of the objects and retain no information about the others. Memory resolution varied over a narrow range that cannot be explained in terms of a general resource pool but can be well explained by a small set of discrete, fixed-resolution representations.",0
https://doi.org/10.1007/bf02293896,A bayesian approach to confirmatory factor analysis,"Confirmatory factor analysis is considered from a Bayesian viewpoint, in which prior information on parameter is incorporated in the analysis. An iterative algorithm is developed to obtain the Bayes estimates. A numerical example based on longitudinal data is presented. A simulation study is designed to compare the Bayesian approach with the maximum likelihood method. Â© 1981 The Psychometric Society.",0
https://doi.org/10.1080/01621459.1984.10478078,Reducing Bias in Observational Studies Using Subclassification on the Propensity Score,"The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Previous theoretical arguments have shown that subclassification on the propensity score will balance all observed covariates. Subclassification on an estimated propensity score is illustrated, using observational data on treatments for coronary artery disease. Five subclasses defined by the estimated propensity score are constructed that balance 74 covariates, and thereby provide estimates of treatment effects using direct adjustment. These subclasses are applied within sub-populations, and model-based adjustments are then used to provide estimates of treatment effects within these sub-populations. Two appendixes address theoretical issues related to the application: the effectiveness of subclassification on the propensity score in removing bias, and balancing properties of propensity scores with incomplete data. Â© 1984 Taylor & Francis Group, LLC.",0
https://doi.org/10.1111/j.1756-8765.2008.01010.x,A Bayesian Account of Reconstructive Memory,"It is well established that prior knowledge influences reconstruction from memory, but the specific interactions of memory and knowledge are unclear. Extending work by Huttenlocher et al. (Psychological Review, 98 [1991] 352; Journal of Experimental Psychology: General, 129 [2000] 220), we propose a Bayesian model of reconstructive memory in which prior knowledge interacts with episodic memory at multiple levels of abstraction. The combination of prior knowledge and noisy memory representations is dependent on familiarity. We present empirical evidence of the influences of prior knowledge at multiple levels of abstraction, showing that the reconstruction of familiar objects is influenced toward the specific prior for that object, while unfamiliar objects are influenced toward the overall category.",0
https://doi.org/10.1002/0471264385.wei0218,Growth Curve Analysis in Contemporary Psychological Research,"The term “growth curve” is used to describe data where: (1) the same entities are repeatedly observed, (2) the same procedures of measurement and scaling of observations are used, and (3) the timing of the observations is known. Growth curves are now common in many areas of psychological research, and some of these are presented here. The term “growth curve analysis” denotes the processes of describing, testing hypotheses, and making scientific inferences about the growth and change patterns in a wide range of time-related phenomena. In this sense, growth curve analyses are a specific form of the larger set of developmental and longitudinal research methods, but the unique features of growth data permit unique kinds of analyses. Formal models for the analysis of growth curves which have been developed in many different substantive domains are described here in five sections: (1) An introduction to growth curves, (2) linear models of growth, (3) multiple groups in growth curve models, (4) aspects of dynamic theory for growth models, and (5) multiple variables in growth curve analyses. We conclude with a discussion of future issues raised by the current growth models.",0
https://doi.org/10.1523/jneurosci.0076-11.2011,Caudal Granular Insular Cortex Is Sufficient and Necessary for the Long-Term Maintenance of Allodynic Behavior in the Rat Attributable to Mononeuropathy,"Mechanical allodynia, the perception of innocuous tactile stimulation as painful, is a severe symptom of chronic pain often produced by damage to peripheral nerves. Allodynia affects millions of people and remains highly resistant to classic analgesics and therapies. Neural mechanisms for the development and maintenance of allodynia have been investigated in the spinal cord, brainstem, thalamus, and forebrain, but manipulations of these regions rarely produce lasting effects. We found that long-term alleviation of allodynic manifestations is produced by discreetly lesioning a newly discovered somatosensory representation in caudal granular insular cortex (CGIC) in the rat, either before or after a chronic constriction injury of the sciatic nerve. However, CGIC lesions alone have no effect on normal mechanical stimulus thresholds. In addition, using electrophysiological techniques, we reveal a corticospinal loop that could be the anatomical source of the influence of CGIC on allodynia.",0
https://doi.org/10.1080/10705510701575396,Deciding on the Number of Classes in Latent Class Analysis and Growth Mixture Modeling: A Monte Carlo Simulation Study,"Mixture modeling is a widely applied data analysis technique used to identify unobserved heterogeneity in a population. Despite mixture models' usefulness in practice, one unresolved issue in the application of mixture models is that there is not one commonly accepted statistical indicator for deciding on the number of classes in a study population. This article presents the results of a simulation study that examines the performance of likelihood-based tests and the traditionally used Information Criterion (ICs) used for determining the number of classes in mixture modeling. We look at the performance of these tests and indexes for 3 types of mixture models: latent class analysis (LCA), a factor mixture model (FMA), and a growth mixture models (GMM). We evaluate the ability of the tests and indexes to correctly identify the number of classes at three different sample sizes (n = 200, 500, 1,000). Whereas the Bayesian Information Criterion performed the best of the ICs, the bootstrap likelihood ratio test ...",0
https://doi.org/10.1037/1082-989x.11.1.36,Local solutions in the estimation of growth mixture models.,"Finite mixture models are well known to have poorly behaved likelihood functions featuring singularities and multiple optima. Growth mixture models may suffer from fewer of these problems, potentially benefiting from the structure imposed on the estimated class means and covariances by the specified growth model. As demonstrated here, however, local solutions may still be problematic. Results from an empirical case study and a small Monte Carlo simulation show that failure to thoroughly consider the possible presence of local optima in the estimation of a growth mixture model can sometimes have serious consequences, possibly leading to adoption of an inferior solution that differs in substantively important ways from the actual maximum likelihood solution. Often, the defaults of current software need to be overridden to thoroughly evaluate the parameter space and obtain confidence that the maximum likelihood solution has in fact been obtained.",0
https://doi.org/10.1148/radiol.2015150034,Diagnosis of Calcific Tendonitis of the Rotator Cuff by Using Susceptibility-weighted MR Imaging,"To evaluate the diagnostic performance of susceptibility-weighted imaging (SWI) and standard shoulder joint magnetic resonance (MR) sequences in comparison to that of conventional radiography for the identification of calcifications in the rotator cuff in patients with calcific tendonitis.The institutional review board approved this prospective study. Written informed consent was obtained from all subjects. Fifty-four patients clinically suspected of having calcific tendonitis of the rotator cuff were included. On radiographs (the standard of reference), 27 patients had positive calcification findings, and 27 did not. Standard MR sequences and SWI, including magnitude and phase imaging, were performed. The diameter of calcifications was measured to assess intermodality correlations. Sensitivity, specificity, and intra- and interobserver agreement were calculated. Phantom measurements were performed to assess the detection limit of SWI.Fifty-six calcifications were detected with radiography in 27 patients. Most (55 calcifications, 98%) could be identified as calcifications by using SWI. Standard T1- and T2-weighted sequences were used to identify 33 calcifications (59%). SWI yielded a sensitivity of 98% (95% confidence interval [CI]: 0.943, 1) and specificity of 96% (95% CI: 0.886, 1) for the identification of calcifications when compared with radiography. Standard rotator cuff MR sequences yielded a sensitivity of 59% (95% CI: 0.422, 0.758) and specificity of 67% (95% CI: 0.493, 0.847). Diameter measurements demonstrated a high correlation between SWI and radiography (R(2) = 0.90), with overestimation of lesion diameter at SWI (mean ± standard deviation for SWI, 7.6 mm ± 5.4; for radiography, 5.3 mm ± 5.1). SWI yielded higher interobserver agreement (R(2) = 0.99, P < .001; 95% CI: 0.989, 0.996) compared with standard MR sequences (R(2) = 0.67, P = .62; 95% CI: 0.703, 0.899). In phantom experiments, SWI and computed tomography were used to identify small calcifications that were missed at radiography.SWI enables the reliable detection of calcifications in the rotator cuff in patients with calcific tendonitis by using conventional radiography as a reference and offers better sensitivity and specificity than standard rotator cuff MR sequences.",0
https://doi.org/10.1214/088342305000000016,Markov Chain Monte Carlo Methods and the Label Switching Problem in Bayesian Mixture Modeling,"In the past ten years there has been a dramatic increase of interest in the Bayesian analysis of finite mixture models. This is primarily because of the emergence of Markov chain Monte Carlo (MCMC) methods. While MCMC provides a convenient way to draw inference from complicated statistical models, there are many, perhaps underappreciated, problems associated with the MCMC analysis of mixtures. The problems are mainly caused by the nonidentifiability of the components under symmetric priors, which leads to so-called label switching in the MCMC output. This means that ergodic averages of component specific quantities will be identical and thus useless for inference. We review the solutions to the label switching problem, such as artificial identifiability constraints, relabelling algorithms and label invariant loss functions. We also review various MCMC sampling schemes that have been suggested for mixture models and discuss posterior sensitivity to prior specification.",0
https://doi.org/10.1177/0049124103260222,Autoregressive Latent Trajectory (ALT) Models A Synthesis of Two Traditions,"Although there are a variety of statistical methods available for the analysis of longitudinal panel data, two approaches are of particular historical importance: the autoregressive (simplex) model and the latent trajectory (curve) model. These two approaches have been portrayed as competing methodologies such that one approach is superior to the other. We argue that the autoregressive and trajectory models are special cases of a more encompassing model that we call the autoregressive latent trajectory (ALT) model. In this paper we detail the underlying statistical theory and mathematical identification of this model, and demonstrate the ALT model using two empirical data sets. The first reanalyzes a simulated repeated measures data set that was previously used to argue against the autoregressive model, and we illustrate how the ALT model can recover the true latent curve model. Second, we apply the ALT model to real family income data on N=3912 adults over a seven year period and find evidence for both autoregressive and latent trajectory processes. Extensions and limitations are discussed.",0
https://doi.org/10.1016/j.ssresearch.2014.05.011,"Homonegativity among first and second generation migrants in Europe: The interplay of time trends, origin, destination and religion","• There is few previous research into approval of homosexuality among migrants. • We analyze time trends, and the influence of origin, destination and religion. • A specific HAPC model is developed. • Homonegativity declines both over time as across cohorts. • Migrants conform to levels of homonegativity in the host society. Previous studies reported declining disapproval of homosexuality in Europe but have simultaneously identified the decelerating effect of religiosity and the higher disapproval of homosexuality among migrants. In this paper, we address disapproval of homosexuality among first- and second-generation migrants in Europe by assessing (1) period and cohort changes, (2) origin and destination country influences and (3) the role of religiosity. We develop a specific cross-classified multilevel design enabling us to simultaneously examine these influences. We test hypotheses using a subsample of the European Social Survey (ESS), containing 19,878 first and second generation migrants. The analyses lead to three important conclusions. Firstly, disapproval of homosexuality is declining both over time and across cohorts. Secondly, migrants conform to levels of disapproval of homosexuality among natives in the destination country, and this explains the decline among migrants over time. Thirdly, religion has a multi-faceted influence on levels of disapproval of homosexuality among migrants.",0
https://doi.org/10.1037/0022-3514.37.10.1742,A new round robin analysis of variance for social interaction data.,"Experimental social psychology has dealt primarily with situations that are not true social interactions; in a typical study, a subject responds to a fixed, artificial social stimulus such as a photograph, written description, or performance by a confederate. Although these artificial social stimuli provide experimental control over independent variables and can be analyzed using the types of statistical models originally developed for nonsocial experimental research, they provide little or no information about the interactive aspects of social behavior—the reciprocity or mutual contingency of the behavior of interaction partners. This paper describes a nonexperimenta l design specifically tailored to social interaction data that provides more information about individual differences and social influence in social interactions: a round robin design in which each person interacts with every other person. After a brief review of available models, a new and more general model for the analysis of social interaction data is presented, with an empirical demonstration using vocal activity data.",0
https://doi.org/10.5209/rev_sjop.2012.v15.n1.37348,Computerized Adaptive Testing: The Capitalization on Chance Problem,"This paper describes several simulation studies that examine the effects of capitalization on chance in the selection of items and the ability estimation in CAT, employing the 3-parameter logistic model. In order to generate different estimation errors for the item parameters, the calibration sample size was manipulated ( N = 500, 1000 and 2000 subjects) as was the ratio of item bank size to test length (banks of 197 and 788 items, test lengths of 20 and 40 items), both in a CAT and in a random test. Results show that capitalization on chance is particularly serious in CAT, as revealed by the large positive bias found in the small sample calibration conditions. For broad ranges of θ, the overestimation of the precision (asymptotic Se) reaches levels of 40%, something that does not occur with the RMSE (θ). The problem is greater as the item bank size to test length ratio increases. Potential solutions were tested in a second study, where two exposure control methods were incorporated into the item selection algorithm. Some alternative solutions are discussed.",0
https://doi.org/10.1080/01621459.1987.10478472,Asymptotic Properties of Maximum Likelihood Estimators and Likelihood Ratio Tests under Nonstandard Conditions,"Abstract Large sample properties of the likelihood function when the true parameter value may be on the boundary of the parameter space are described. Specifically, the asymptotic distribution of maximum likelihood estimators and likelihood ratio statistics are derived. These results generalize the work of Moran (1971), Chant (1974), and Chernoff (1954). Some of Chant's results are shown to be incorrect. The approach used in deriving these results follows from comments made by Moran and Chant. The problem is shown to be asymptotically equivalent to the problem of estimating the restricted mean of a multivariate Gaussian distribution from a sample of size 1. In this representation the Gaussian random variable corresponds to the limit of the normalized score statistic and the estimate of the mean corresponds to the limit of the normalized maximum likelihood estimator. Thus the limiting distribution of the maximum likelihood estimator is the same as the distribution of the projection of the Gaussian random v...",0
https://doi.org/10.1093/biomet/86.3.677,Joint mean-covariance models with applications to longitudinal data: unconstrained parameterisation,"SUMMARY We provide unconstrained parameterisation for and model a covariance using covariates. The Cholesky decomposition of the inverse of a covariance matrix is used to associate a unique unit lower triangular and a unique diagonal matrix with each covariance matrix. The entries of the lower triangular and the log of the diagonal matrix are unconstrained and have meaning as regression coefficients and prediction variances when regressing a measurement on its predecessors. An extended generalised linear model is introduced for joint modelling of the vectors of predictors for the mean and covariance subsuming the joint modelling strategy for mean and variance heterogeneity, Gabriel's antedependence models, Dempster's covariance selection models and the class of graphical models. The likelihood function and maximum likelihood estimators of the covariance and the mean parameters are studied when the observations are normally distributed. Applications to modelling nonstationary dependence structures and multivariate data are discussed and illustrated using real data. A graphical method, similar to that based on the correlogram in time series, is developed and used to identify parametric models for nonstationary covariances.",0
https://doi.org/10.1093/aje/kwq332,Odds Ratios for Mediation Analysis for a Dichotomous Outcome,"For dichotomous outcomes, the authors discuss when the standard approaches to mediation analysis used in epidemiology and the social sciences are valid, and they provide alternative mediation analysis techniques when the standard approaches will not work. They extend definitions of controlled direct effects and natural direct and indirect effects from the risk difference scale to the odds ratio scale. A simple technique to estimate direct and indirect effect odds ratios by combining logistic and linear regressions is described that applies when the outcome is rare and the mediator continuous. Further discussion is given as to how this mediation analysis technique can be extended to settings in which data come from a case-control study design. For the standard mediation analysis techniques used in the epidemiologic and social science literatures to be valid, an assumption of no interaction between the effects of the exposure and the mediator on the outcome is needed. The approach presented here, however, will apply even when there are interactions between the effect of the exposure and the mediator on the outcome.",0
https://doi.org/10.1016/s0377-2217(01)00147-3,Real world performance of choice-based conjoint models,"Conjoint analysis is one of the most important tools to support product development, pricing and positioning decisions in management practice. For this purpose various models have been developed. It is widely accepted that models that take consumer heterogeneity into account, outperform aggregate models in terms of hold-out tasks. The aim of our study is to investigate empirically whether predictions of choice-based conjoint models which incorporate heterogeneity can successfully be generalized to a whole market. To date no studies exist that examine the real world performance of choice-based conjoint models by use of aggregate scanner panel data. Our analysis is based on four commercial choice-based conjoint pricing studies including a total of 43 stock keeping units (SKU) and the corresponding weekly scanning data for approximately two years. An aggregate model serves as a benchmark for the performance of two models that take heterogeneity into account, hierarchical Bayes and latent class. Our empirical analysis demonstrates that, in contrast to the performance using hold-out tasks, the real world performance of hierarchical Bayes and latent class is similar to the performance of the aggregate model. Our results indicate that heterogeneity cannot be generalized to a whole market and suggest that aggregate models are sufficient to predict market shares. (author's abstract)",0
https://doi.org/10.1093/ser/mwu039,Between exclusion and calculating solidarity? Preferences for private versus public welfare provision and the size of the informal sector,"This article examines how the informal sector, as a group of potential 'free riders' for public welfare goods, relates to individual social policy preferences in low- and middle-income countries. The exclusion hypothesis proposes that a large informal sector lowers the preferences from formal workers and the middle- and high-income groups for social services to be provided by the state, and raises these groups' preferences for public welfare goods to become club goods. In contrast, the prospect hypothesis argues that formal workers, particularly the middle-income group, ally themselves to the informal sector to insure against the risk of future employment in informality. The study examines individual preferences for the provision of pensions and health care by either the state or private enterprises. The two competing hypotheses are tested with a hierarchical model using survey data from Latin America for 1995, 1998 and 2008. The findings offer support for the exclusion hypothesis. Ã‚Â© The Author 2015.",0
https://doi.org/10.1198/tech.2009.08161,Partitioning Degrees of Freedom in Hierarchical and Other Richly Parameterized Models,"Hodges & Sargent (2001) developed a measure of a hierarchical model's complexity, degrees of freedom (DF), that is consistent with definitions for scatterplot smoothers, interpretable in terms of simple models, and that enables control of a fit's complexity by means of a prior distribution on complexity. DF describes complexity of the whole fitted model but in general it is unclear how to allocate DF to individual effects. We give a new definition of DF for arbitrary normal-error linear hierarchical models, consistent with Hodges & Sargent's, that naturally partitions the n observations into DF for individual effects and for error. The new conception of an effect's DF is the ratio of the effect's modeled variance matrix to the total variance matrix. This gives a way to describe the sizes of different parts of a model (e.g., spatial clustering vs. heterogeneity), to place DF-based priors on smoothing parameters, and to describe how a smoothed effect competes with other effects. It also avoids difficulties with the most common definition of DF for residuals. We conclude by comparing DF to the effective number of parameters p(D) of Spiegelhalter et al (2002). Technical appendices and a dataset are available online as supplemental materials.",0
https://doi.org/10.1198/106186006x160050,Sampling Correlation Matrices in Bayesian Models With Correlated Latent Variables,"Hierarchical model specifications using latent variables are frequently used to reflect correlation structure in data. Motivated by the structure of a Bayesian multivariate probit model, we demonstrate a parameter-extended Metropolis-Hastings algorithm for sampling from the posterior distribution of a correlation matrix. Our sampling algorithms lead directly to two readily interpretable families of prior distributions for a correlation matrix. The methodology is illustrated through a simulation study and through an application with repeated binary outcomes on individuals from a study of a suicide prevention intervention.",0
https://doi.org/10.1160/th11-08-0586,"Network meta-analysis of prasugrel, ticagrelor, high- and standard-dose clopidogrel in patients scheduled for percutaneous coronary interventions","Summary Since novel antiplatelet treatments (prasugrel, ticagrelor, high-dose clopidogrel) have been predominantly tested against standard-dose clopidogrel, data on direct comparisons between these therapies are scarce. We therefore indirectly compared their efficacy and safety in patients undergoing percutaneous coronary intervention. Electronic databases were searched systematically to identify head-to-head randomised controlled trials (RCTs). Network meta-analysis was performed using generalised linear mixed models with adjustment for length of follow-up. Findings were corroborated by mixed treatment comparison through Bayesian methods. Fourteen RCTs were identified and included in the analysis (high- vs. standard-dose clopidogrel: 9 trials, prasugrel vs. high-dose clopidogrel: 2 trials, prasugrel vs. standard-dose clopidogrel: 2 trials, ticagrelor vs. standard-dose clopidogrel: 1 trial). No significant differences were found for efficacy outcomes except for stent thrombosis favouring prasugrel (vs. ticagrelor: odds ratio [OR] 0.63, 95% confidence interval [CI]: 0.42, 0.94; vs. high-dose clopidogrel: OR 0.70, 95%CI: 0.48, 1.01). Prasugrel exhibited a similar bleeding risk as high-dose clopidogrel, but more major (OR 1.43, 95%CI 1.07, 1.90) and major or minor bleeding (OR 1.36, 95%CI 1.09, 1.69) compared to ticagrelor. Ticagrelor was also associated with less major or minor bleeding compared to high-dose clopidogrel (OR 0.81, 95%CI 0.69, 0.96). No differences were seen for non CABG-related major bleeding between the three strategies. Results were corroborated in a subgroup analysis comprising only patients with acute coronary syndromes. In the absence of head-to-head clinical trials, network meta-analysis suggests potentially relevant differences in efficacy and bleeding risk among novel antiplatelet treatments and may thereby advance understanding of their differential therapeutic properties.",0
https://doi.org/10.1080/03050629.2014.948154,"Political Trust, Corruption, and Ratings of the IMF and the World Bank","There are only a handful of studies that examine public support for the IMF and World Bank. Public opinion data on attitudes to the economy feature prominently in these studies. Utilizing data from the Afrobarometer survey, we find that evaluations of the economy, ideology, and a range of sociodemographic factors including age, gender, employment status, health, education, and living conditions are not significantly related to ratings of effectiveness. Rather, we find that political trust and corruption—two very important concepts in the wider literature on individual-level attitudes toward international relations and foreign policy issues—are strongly associated with ratings of effectiveness.",0
https://doi.org/10.1007/978-3-319-19977-1_24,Mediation Analysis with Missing Data Through Multiple Imputation and Bootstrap,"A method using multiple imputation and bootstrap for dealing with missing data in mediation analysis is introduced and implemented in both SAS and R. Through simulation studies, it is shown that the method performs well for both MCAR and MAR data without and with auxiliary variables. It is also shown that the method can work for MNAR data if auxiliary variables related to missingness are included. The application of the method is demonstrated through the analysis of a subset of data from the National Longitudinal Survey of Youth. Mediation analysis with missing data can be conducted using the provided SAS macros and R package bmem.",0
https://doi.org/10.1093/bioinformatics/btw207,Sparse group factor analysis for biclustering of multiple data sources,"Abstract Motivation: Modelling methods that find structure in data are necessary with the current large volumes of genomic data, and there have been various efforts to find subsets of genes exhibiting consistent patterns over subsets of treatments. These biclustering techniques have focused on one data source, often gene expression data. We present a Bayesian approach for joint biclustering of multiple data sources, extending a recent method Group Factor Analysis to have a biclustering interpretation with additional sparsity assumptions. The resulting method enables data-driven detection of linear structure present in parts of the data sources. Results: Our simulation studies show that the proposed method reliably infers biclusters from heterogeneous data sources. We tested the method on data from the NCI-DREAM drug sensitivity prediction challenge, resulting in an excellent prediction accuracy. Moreover, the predictions are based on several biclusters which provide insight into the data sources, in this case on gene expression, DNA methylation, protein abundance, exome sequence, functional connectivity fingerprints and drug sensitivity. Availability and Implementation: http://research.cs.aalto.fi/pml/software/GFAsparse/ Contacts: kerstin.bunte@googlemail.com or samuel.kaski@aalto.fi",0
https://doi.org/10.1016/j.jmva.2009.04.014,Bayesian predictive densities based on superharmonic priors for the 2-dimensional Wishart model,Bayesian predictive densities for the 2-dimensional Wishart model are investigated. The performance of predictive densities is evaluated by using the Kullback–Leibler divergence. It is proved that a Bayesian predictive density based on a prior exactly dominates that based on the Jeffreys prior if the prior density satisfies some geometric conditions. An orthogonally invariant prior is introduced and it is shown that the Bayesian predictive density based on the prior is minimax and dominates that based on the right invariant prior with respect to the triangular group.,0
https://doi.org/10.1016/0749-5978(91)90020-t,The theory of planned behavior,"Research dealing with various aspects of* the theory of planned behavior (Ajzen, 1985, 1987) is reviewed, and some unresolved issues are discussed. In broad terms, the theory is found to be well supported by empirical evidence. Intentions to perform behaviors of different kinds can be predicted with high accuracy from attitudes toward the behavior, subjective norms, and perceived behavioral control; and these intentions, together with perceptions of behavioral control, account for considerable variance in actual behavior. Attitudes, subjective norms, and perceived behavioral control are shown to be related to appropriate sets of salient behavioral, normative, and control beliefs about the behavior, but the exact nature of these relations is still uncertain. Expectancy— value formulations are found to be only partly successful in dealing with these relations. Optimal rescaling of expectancy and value measures is offered as a means of dealing with measurement limitations. Finally, inclusion of past behavior in the prediction equation is shown to provide a means of testing the theory*s sufficiency, another issue that remains unresolved. The limited available evidence concerning this question shows that the theory is predicting behavior quite well in comparison to the ceiling imposed by behavioral reliability. © 1991 Academic Press. Inc.",0
https://doi.org/10.4135/9780857020994.n22,Maximum Likelihood and Bayesian Estimation for Nonlinear Structural Equation Models,,0
https://doi.org/10.1007/bf02294002,Estimation for structural equation models with missing data,"A direct method in handling incomplete data in general covariance structural models is investigated. Asymptotic statistical properties of the generalized least squares method are developed. It is shown that this approach has very close relationships with the maximum likelihood approach. Iterative procedures for obtaining the generalized least squares estimates, the maximum likelihood estimates, as well as their standard error estimates are derived. Computer programs for the confirmatory factor analysis model are implemented. A longitudinal type data set is used as an example to illustrate the results. Ã‚Â© 1986 The Psychometric Society.",0
https://doi.org/10.1123/jsep.2013-0043,A Point-by-Point Analysis of Performance in a Fencing Match: Psychological Processes Associated with Winning and Losing Streaks,"This study aimed to revisit the complex nature of serial dependency of performance during a match, examining the prospective associations between psychological processes and subsequent performance at the within-person level of analysis, and explore whether psychological processes are associated with the likelihood of winning series of points. A process-oriented sequential approach was used with 16 elite fencers during a simulated competition. Multilevel regression analyses revealed that serial dependency of performance fluctuates within a match. Results of a Bayesian multilevel structural equation model showed that prior performance subsequently influenced psychological processes. Although psychological processes did not predict performance in the subsequent point, successive winnings were associated with higher perceived control and task-oriented coping and lower negative affectivity compared with both losing streaks and nonstreaks. Overall, serial dependencies of performance are nonstationary during a match whereas psychological processes significantly differ in episodes of winning after winning versus losing after losing.",0
https://doi.org/10.1037/a0029856,An ideal observer analysis of visual working memory.,"Limits in visual working memory (VWM) strongly constrain human performance across many tasks. However, the nature of these limits is not well understood. In this article we develop an ideal observer analysis of human VWM by deriving the expected behavior of an optimally performing but limited-capacity memory system. This analysis is framed around rate-distortion theory, a branch of information theory that provides optimal bounds on the accuracy of information transmission subject to a fixed information capacity. The result of the ideal observer analysis is a theoretical framework that provides a task-independent and quantitative definition of visual memory capacity and yields novel predictions regarding human performance. These predictions are subsequently evaluated and confirmed in 2 empirical studies. Further, the framework is general enough to allow the specification and testing of alternative models of visual memory (e.g., how capacity is distributed across multiple items). We demonstrate that a simple model developed on the basis of the ideal observer analysis-one that allows variability in the number of stored memory representations but does not assume the presence of a fixed item limit-provides an excellent account of the empirical data and further offers a principled reinterpretation of existing models of VWM.",0
https://doi.org/10.1002/job.529,"To prosper, organizational psychology should … overcome methodological barriers to progress","Progress in organizational psychology (OP) research depends on the rigor and quality of the methods we use. This paper identifies ten methodological barriers to progress and offers suggestions for overcoming the barriers, in part or whole. The barriers address how we derive hypotheses from theories, the nature and scope of the questions we pursue in our studies, the ways we address causality, the manner in which we draw samples and measure constructs, and how we conduct statistical tests and draw inferences from our research. The paper concludes with recommendations for integrating research methods into our ongoing development goals as scholars and framing methods as tools that help us achieve shared objectives in our field. Copyright © 2008 John Wiley & Sons, Ltd.",0
https://doi.org/10.1007/s11336-013-9379-4,Hierarchical Bayesian Modeling for Test Theory Without an Answer Key,"Cultural Consensus Theory (CCT) models have been applied extensively across research domains in the social and behavioral sciences in order to explore shared knowledge and beliefs. CCT models operate on response data, in which the answer key is latent. The current paper develops methods to enhance the application of these models by developing the appropriate specifications for hierarchical Bayesian inference. A primary contribution is the methodology for integrating the use of covariates into CCT models. More specifically, both person- and item-related parameters are introduced as random effects that can respectively account for patterns of inter-individual and inter-item variability. Â© 2013, The Psychometric Society.",0
https://doi.org/10.1080/00273171.2010.531230,Nonlinear Structured Growth Mixture Models in M<i>plus</i>and OpenMx,"Growth mixture models (GMMs; Muthén & Muthén, 2000; Muthén & Shedden, 1999) are a combination of latent curve models (LCMs) and finite mixture models to examine the existence of latent classes that follow distinct developmental patterns. GMMs are often fit with linear, latent basis, multiphase, or polynomial change models because of their common use, flexibility in modeling many types of change patterns, the availability of statistical programs to fit such models, and the ease of programming. In this paper, we present additional ways of modeling nonlinear change patterns with GMMs. Specifically, we show how LCMs that follow specific nonlinear functions can be extended to examine the presence of multiple latent classes using the Mplus and OpenMx computer programs. These models are fit to longitudinal reading data from the Early Childhood Longitudinal Study-Kindergarten Cohort to illustrate their use.",0
https://doi.org/10.1177/0958928715573478,Attitudes towards student support: How positive feedback-effects prevent change in the Four Worlds of Student Finance,"This article provides a detailed analysis of individual preferences towards public financial aid to students from low-income families. Who favours/opposes such aid? What are the determinants of the respective preferences? I argue that three sets of factors jointly shape these preferences: materialistic self-interests, political attitudes, and the status quo of the higher education subsidy systems by generating positive feedback-effects. Results of multilevel ordered logit models utilizing the International Social Survey Program (ISSP) data for up to 22 countries over two decades indicate that self-interest matters: students strongly favour subsidies as do their parents, while those paying for the spending and those not expecting to benefit oppose such aid. Moreover, political attitudes are important: Supporters of redistribution and of increased public education spending in general, as well as leftwing voters, are much more likely to support students. On the macro-level, the findings suggest that positive feedback-effects exist: in countries with generous subsidy systems, public support for subsidies is higher. This article is the first to systematically analyse preferences towards higher education subsidies across countries and time and demonstrates how positive feedback-effects increasingly lock-in countries’ tuition-subsidy paths, making the systems resistant to (radical) change. As such, it speaks to the literature on the political economy of skill formation, the welfare state, public opinion and the public opinion–policy link.",0
https://doi.org/10.1080/10705511003659425,A Bayesian Approach for Nonlinear Structural Equation Models With Dichotomous Variables Using Logit and Probit Links,"Analysis of ordered binary and unordered binary data has received considerable attention in social and psychological research. This article introduces a Bayesian approach, which has several nice features in practical applications, for analyzing nonlinear structural equation models with dichotomous data. We demonstrate how to use the software WinBUGS and R2WinBUGS to obtain Bayesian estimates of the unknown parameters, estimates of latent variables, and the Deviance Information Criterion for model comparison. An illustrative example with an artificial data set is provided. Finally, simulation studies are conducted, not only to reveal the empirical performance of the Bayesian approach, but also to show that incorrectly treating binary data as ordinal, and vice versa, would produce misleading results.",0
https://doi.org/10.1080/00273171.2014.955604,Effects of Modeling the Heterogeneity on Inferences Drawn from Multilevel Designs,"This article uses Monte Carlo techniques to examine the effect of heterogeneity of variance in multilevel analyses in terms of relative bias, coverage probability, and root mean square error (RMSE). For all simulated data sets, the parameters were estimated using the restricted maximum-likelihood (REML) method both assuming homogeneity and incorporating heterogeneity into multilevel models. We find that (a) the estimates for the fixed parameters are unbiased, but the associated standard errors are frequently biased when heterogeneity is ignored; by contrast, the standard errors of the fixed effects are almost always accurate when heterogeneity is considered; (b) the estimates for the random parameters are slightly overestimated; (c) both the homogeneous and heterogeneous models produce standard errors of the variance component estimates that are underestimated; however, taking heterogeneity into account, the REML-estimations give correct estimates of the standard errors at the lowest level and lead to less underestimated standard errors at the highest level; and (d) from the RMSE point of view, REML accounting for heterogeneity outperforms REML assuming homogeneity; a considerable improvement has been particularly detected for the fixed parameters. Based on this, we conclude that the solution presented can be uniformly adopted. We illustrate the process using a real dataset.",0
https://doi.org/10.1037/a0015825,Estimation of IRT graded response models: Limited versus full information methods.,"The performance of parameter estimates and standard errors in estimating F. Samejima's graded response model was examined across 324 conditions. Full information maximum likelihood (FIML) was compared with a 3-stage estimator for categorical item factor analysis (CIFA) when the unweighted least squares method was used in CIFA's third stage. CIFA is much faster in estimating multidimensional models, particularly with correlated dimensions. Overall, CIFA yields slightly more accurate parameter estimates, and FIML yields slightly more accurate standard errors. Yet, across most conditions, differences between methods are negligible. FIML is the best election in small sample sizes (200 observations). CIFA is the best election in larger samples (on computational grounds). Both methods failed in a number of conditions, most of which involved 200 observations, few indicators per dimension, highly skewed items, or low factor loadings. These conditions are to be avoided in applications.",0
https://doi.org/10.1016/j.csda.2010.04.015,Alternating imputation posterior estimation of models with crossed random effects,"Generalized linear mixed models or latent variable models for categorical data are difficult to estimate if the random effects or latent variables vary at non-nested levels, such as persons and test items. Clayton and Rasbash (1999) suggested an Alternating Imputation Posterior (AIP) algorithm for approximate maximum likelihood estimation. For item response models with random item effects, the algorithm iterates between an item wing in which the item mean and variance are estimated for given person effects and a person wing in which the person mean and variance are estimated for given item effects. The person effects used for the item wing are sampled from the conditional posterior distribution estimated in the person wing and vice versa. Clayton and Rasbash (1999) used marginal quasi-likelihood (MQL) and penalized quasi-likelihood (PQL) estimation within the AIP algorithm, but this method has been shown to produce biased estimates in many situations, so we use maximum likelihood estimation with adaptive quadrature. We apply the proposed algorithm to the famous salamander mating data, comparing the estimates with many other methods, and to an educational testing dataset. We also present a simulation study to assess performance of the AIP algorithm and the Laplace approximation with different numbers of items and persons and a range of item and person variances.",0
https://doi.org/10.1214/105051606000000510,Harris recurrence of Metropolis-within-Gibbs and trans-dimensional Markov chains,"A ϕ-irreducible and aperiodic Markov chain with stationary probability distribution will converge to its stationary distribution from almost all starting points. The property of Harris recurrence allows us to replace “almost all” by “all,” which is potentially important when running Markov chain Monte Carlo algorithms. Full-dimensional Metropolis–Hastings algorithms are known to be Harris recurrent. In this paper, we consider conditions under which Metropolis-within-Gibbs and trans-dimensional Markov chains are or are not Harris recurrent. We present a simple but natural two-dimensional counter-example showing how Harris recurrence can fail, and also a variety of positive results which guarantee Harris recurrence. We also present some open problems. We close with a discussion of the practical implications for MCMC algorithms.",0
https://doi.org/10.1002/cjs.5550350108,Measuring the complexity of generalized linear hierarchical models,en,0
,Structural equation models of latent interaction and quadratic effects.,,0
https://doi.org/10.1111/rssc.12013,Hierarchical longitudinal models of relationships in social networks,Summary Motivated by the need to understand the dynamics of relationship formation and dissolution over time in real world social networks we develop a new longitudinal model for transitions in the relationship status of pairs of individuals (‘dyads’). We first specify a model for the relationship status of a single dyad and then extend it to account for important interdyad dependences (e.g. transitivity—‘a friend of a friend is a friend’) and heterogeneity. Model parameters are estimated by using Bayesian analysis implemented via Markov chain Monte Carlo sampling. We use the model to perform novel analyses of two diverse longitudinal friendship networks: an excerpt of the Teenage Friends and Lifestyle Study (a moderately sized network) and the Framingham Heart Study (a large network).,0
https://doi.org/10.1007/s11135-011-9503-4,Integrated analysis of content and construct validity of psychometric instruments,"Establishing adequacy of psychometric properties of an instrument involves acquisition and evaluation of evidence based on item content and internal structure. Content validity evidence consists of subject matter experts providing quantitative ratings of the extent to which items are a representative sample of targeted domain. Evidence of internal structure includes factor analytic studies and examination of item interrelationships based on item responses from participants. Although subject matter expert ratings and participant response data are traditionally analyzed separately, each serves to inform the other in important ways. We propose integrating subject matter experts' and participants' data seamlessly to establish a unified model of validity evidence. The approach is applied to an instrument designed to measure nursing home culture change (i. e., resident-centered care). The proposed method has been demonstrated to be useful with a posterior distribution resulting in stable estimates of psychometric parameters superior to traditional analytic approaches. To illustrate the efficacy of the methodology, we present a simulation study and discuss its place in psychometric methods. Â© 2011 Springer Science+Business Media B.V.",0
https://doi.org/10.1287/mksc.19.4.328.11789,A Hierarchical Bayesian Methodology for Treating Heterogeneity in Structural Equation Models,"Structural equation models are widely used in marketing and psychometric literature to model relationships between unobserved constructs and manifest variables and to control for measurement error. Most applications of structural equation models assume that data come from a homogeneous population. This assumption may be unrealistic, as individuals are likely to be heterogeneous in their perceptions and evaluations of unobserved constructs. In addition, individuals may exhibitdifferent measurement reliabilities. It is well-known in statistical literature that failure to account for unobserved sources of individual differences can resultin misleading inferences and incorrect conclusions. We develop a hierarchical Bayesian framework for modeling general forms of heterogeneity in partially recursive structural equation models. Our framework elucidates the motivations for accommodating heterogeneity and illustrates theoretically the types of misleading inferences that can result when unobserved heterogeneity is ignored. We describe in detail the choices that researchers can make in incorporating different forms of measurement and structural heterogeneity. Current random-coefficient models in psychometric literature can accommodate heterogeneity solely in mean structures. We extend these models by allowing for heterogeneity both in mean and covariance structures. Specifically, in addition to heterogeneity in measurement intercepts and factor means, we account for heterogeneity in factor covariance structure, measurement error, and structural parameters. Models such as random-coefficient factor analysis, random-coefficientsecond-order factor analysis, and random-coefficient, partially recursive simultaneous equation models are special cases of our proposed framework. We also develop Markov Chain Monte Carlo (MCMC) procedures to perform Bayesian inference in partially recursive, random-coefficient structural equation models. These procedures provide individual-specific estimates of the factor scores, structural coefficients, and other model parameters. We illustrate our approach using two applications. The first application illustrates our methods on synthetic data, whereas the second application uses consumer satisfaction data involving measurements on satisfaction, expectation disconfirmation, and performance variables obtained from a panel of subjects. Our results from the synthetic data application show that our Bayesian procedures perform well in recovering the true parameters. More importantly, we find that models that ignore heterogeneity can yield a severely distorted picture of the nature of associations among variables and can therefore generate misleading inferences. Specifically, we find that ignoring heterogeneity can result in inflated estimates of measurement reliability, wrong signs of factor covariances, and can yield attenuated model fit and standard errors. The results from the consumer satisfaction study show that individuals vary both in means and covariances and indicate that conventional psychometric methods are not appropriate for our data. In addition, we find that heterogeneous models outperform the standard structural equation model in predictive ability. Managerially, we show how one can use the individual-level factor scores and structural parameter estimates from the Bayesian approach to perform quadrantanalysis and refine marketing policy (e.g., develop a one-on-one marketing policy). The framework introduced in this paper and the inference procedures we describe should be of interest to researchers in a wide range of disciplines in which measurement error and unobserved heterogeneity are problematic. In particular, our approach is suitable for studies in which panel data or multiple observations are available for a given set of respondents or objects (e.g., firms, organizations, markets). At a practical level, our procedures can be used by managers and other policymakers to customize marketing activities or policies. Future research should extend our procedures to deal with the general nonrecursive structural equation model and to handle binary and ordinal data situations.",0
https://doi.org/10.1037/a0032138,Reliability estimation in a multilevel confirmatory factor analysis framework.,"Scales with varying degrees of measurement reliability are often used in the context of multistage sampling, where variance exists at multiple levels of analysis (e.g., individual and group). Because methodological guidance on assessing and reporting reliability at multiple levels of analysis is currently lacking, we discuss the importance of examining level-specific reliability. We present a simulation study and an applied example showing different methods for estimating multilevel reliability using multilevel confirmatory factor analysis and provide supporting Mplus program code. We conclude that (a) single-level estimates will not reflect a scale's actual reliability unless reliability is identical at each level of analysis, (b) 2-level alpha and composite reliability (omega) perform relatively well in most settings, (c) estimates of maximal reliability (H) were more biased when estimated using multilevel data than either alpha or omega, and (d) small cluster size can lead to overestimates of reliability at the between level of analysis. We also show that Monte Carlo confidence intervals and Bayesian credible intervals closely reflect the sampling distribution of reliability estimates under most conditions. We discuss the estimation of credible intervals using Mplus and provide R code for computing Monte Carlo confidence intervals.",0
https://doi.org/10.1177/0022343314551080,A dangerous discrepancy,"This study aims to uncover how horizontal inequality affects support for violent and nonviolent resistance among Palestinians in the West Bank and Gaza. National survey data are introduced to operationalize the mechanisms proposed in the horizontal inequality literature on the individual level. Results point to the operation of political and economic horizontal inequality mechanisms in the Palestinian case. Higher perceived status of civil and political rights is associated with a lower probability of supporting violent over nonviolent resistance. Individuals are also more likely to support violent over nonviolent resistance the larger the difference in household expenditure and consumer durable ownership between their own region and the closest Israeli subdistrict. Corresponding differences in educational attainment have no corresponding effect. The results demonstrate how economic and political horizontal inequality can increase the risk that an individual becomes part of the mobilizational potential of violent social movements. This can lead to participation in a wide range of supportive actions that increase the viability of such movements, ranging from material support to direct participation in violence. The level of public support for violent resistance in key constituencies could also influence public opinion sensitive actors like Hamas in their choice between violent and nonviolent resistance strategies.",0
https://doi.org/10.1080/01621459.1989.10478868,Optimal Matching for Observational Studies,"Abstract Matching is a common method of adjustment in observational studies. Currently, matched samples are constructed using greedy heuristics (or “stepwise” procedures) that produce, in general, suboptimal matchings. With respect to a particular criterion, a matched sample is suboptimal if it could be improved by changing the controls assigned to specific treated units, that is, if it could be improved with the data at hand. Here, optimal matched samples are obtained using network flow theory. In addition to providing optimal matched-pair samples, this approach yields optimal constructions for several statistical matching problems that have not been studied previously, including the construction of matched samples with multiple controls, with a variable number of controls, and the construction of balanced matched samples that combine features of pair matching and frequency matching. Computational efficiency is discussed. Extensive use is made of ideas from two essentially disjoint literatures, namely st...",0
https://doi.org/10.1093/acprof:oso/9780199533022.001.0001,"Bayesian Smoothing and Regression for Longitudinal, Spatial and Event History Data","Several recent advances in smoothing and semiparametric regression are presented in this book from a unifying, Bayesian perspective. Simulation-based full Bayesian Markov chain Monte Carlo (MCMC) inference, as well as empirical Bayes procedures closely related to penalized likelihood estimation and mixed models, are considered here. Throughout, the focus is on semiparametric regression and smoothing based on basis expansions of unknown functions and effects in combination with smoothness priors for the basis coefficients. Beginning with a review of basic methods for smoothing and mixed models, longitudinal data, spatial data, and event history data are treated in separate chapters. Worked examples from various fields such as forestry, development economics, medicine, and marketing are used to illustrate the statistical methods covered in this book. Most of these examples have been analysed using implementations in the Bayesian software, BayesX, and some with R Codes.",0
https://doi.org/10.1002/sim.4780110104,Bayesian subset analysis in a colorectal cancer clinical trial,"Subset analysis is the examination of treatment comparisons within groups of patients with restricted levels of patient characteristics. Such analyses are vulnerable to multiplicity effects. We examine the problem in the context of a proportional hazards model with terms for treatment, each of several dichotomous covariates representing the patient characteristics of interest, and treatment-by-covariate interaction effects. Parametrically, a subset-specific treatment effect is equal to the treatment effect term plus a linear combination of the interaction terms. We present Bayesian point and interval estimates under the assumption that the interaction terms are exchangeable and the prior distributions for the other regression parameters are locally uniform. This produces a shrinking of the estimated interaction effects towards zero, thereby discounting them and dealing in a natural way with multiplicity. We illustrate the method using results of a recent North Central Cancer Treatment Group/Mayo Clinic study in advanced colorectal cancer.",0
https://doi.org/10.1037/1082-989x.9.3.334,Structured Latent Curve Models for the Study of Change in Multivariate Repeated Measures.,"This article considers a structured latent curve model for multiple repeated measures. In a structured latent curve model, a smooth nonlinear function characterizes the mean response. A first-order Taylor polynomial taken with regard to the mean function defines elements of a restricted factor matrix that may include parameters that enter nonlinearly. Similar to factor scores, random coefficients are combined with the factor matrix to produce individual latent curves that need not follow the same form as the mean curve. Here the associations between change characteristics in multiple repeated measures are studied. A factor analysis model for covariates is included as a means of relating latent covariates to the factors characterizing change in different repeated measures. An example is provided.",0
https://doi.org/10.1080/10705510709336743,On Fitting Nonlinear Latent Curve Models to Multiple Variables Measured Longitudinally,"This article shows how nonlinear latent curve models may be fitted for simultaneous analysis of multiple variables measured longitudinally using Mx statistical software. Longitudinal studies often involve observation of several variables across time with interest in the associations between change characteristics of different variables measured within individuals. Other applications involve repeated measures for distinguishable individuals nested within small groups, such as families, with interest in the associations between change characteristics in variables for individuals within groups. This article shows how Mx can be used to carry out analysis of multiple variables measured over time where at least one variable is described by a function that includes one or more parameters that enter the model nonlinearly. An example is provided.",0
https://doi.org/10.1002/sim.2713,Bayesian analysis of structural equation models with multinomial variables and an application to type 2 diabetic nephropathy,"There is now increasing evidence proving that many complex diseases can be significantly influenced by correlated phenotype and genotype variables, as well as their interactions. Effective and rigorous assessment of such influence is difficult, because the number of phenotype and genotype variables of interest may not be small, and a genotype variable is an unordered categorical variable that follows a multinomial distribution. To address the problem, we establish a novel nonlinear structural equation model for analysing mixed continuous and multinomial data that can be missing at random. A confirmatory factor analysis model with Kronecker product is proposed for grouping the manifest continuous and multinomial variables into latent variables according to their functions; and a nonlinear structural equation is formulated to assess the linear and interaction effects of the independent latent variables to the dependent latent variables. Bayesian methods for estimation and model comparison are developed through Markov chain Monte Carlo techniques and path sampling. The newly developed methodologies are applied to a case-control cohort of type 2 diabetic patients with nephropathy.",0
https://doi.org/10.1016/j.jmp.2003.12.003,Mental architectures with selectively influenced but stochastically interdependent components,"Abstract The way external factors influence distribution functions for the overall time required to perform a mental task (such as responding to a stimulus, or solving a problem) may be informative as to the underlying mental architecture, the hypothetical network of interconnected processes some of which are selectively influenced by some of the external factors. Under the assumption that all processes contributing to the overall performance time are stochastically independent, several basic results have been previously established. These results relate patterns of response time distribution functions produced by manipulating external factors to such questions as whether the hypothetical constituent processes in the mental architecture enter AND gates or OR gates, and whether pairs of processes are sequential or concurrent. The present study shows that all these results are also valid for stochastically interdependent component times, provided the selective dependence of these components upon external factors is understood within the framework of a recently proposed theory of selective influence. According to this theory each component is representable as a function of three arguments: the factor set selectively influencing it, a component-specific source of randomness, and a source of randomness shared by all the components.",0
https://doi.org/10.3310/hta4380,Bayesian methods in health technology assessment: a review.,"Bayesian methods may be defined as the explicit quantitative use of external evidence in the design, monitoring, analysis, interpretation and reporting of a health technology assessment. In outline, the methods involve formal combination through the use of Bayes's theorem of: 1. a prior distribution or belief about the value of a quantity of interest (for example, a treatment effect) based on evidence not derived from the study under analysis, with 2. a summary of the information concerning the same quantity available from the data collected in the study (known as the likelihood), to yield 3. an updated or posterior distribution of the quantity of interest. These methods thus directly address the question of how new evidence should change what we currently believe. They extend naturally into making predictions, synthesising evidence from multiple sources, and designing studies: in addition, if we are willing to quantify the value of different consequences as a 'loss function', Bayesian methods extend into a full decision-theoretic approach to study design, monitoring and eventual policy decision-making. Nonetheless, Bayesian methods are a controversial topic in that they may involve the explicit use of subjective judgements in what is conventionally supposed to be a rigorous scientific exercise.This report is intended to provide: 1. a brief review of the essential ideas of Bayesian analysis 2. a full structured review of applications of Bayesian methods to randomised controlled trials, observational studies, and the synthesis of evidence, in a form which should be reasonably straightforward to update 3. a critical commentary on similarities and differences between Bayesian and conventional approaches 4. criteria for assessing the reporting of a Bayesian analysis 5. a comprehensive list of published 'three-star' examples, in which a proper prior distribution has been used for the quantity of primary interest 6. tutorial case studies of a variety of types 7. recommendations on how Bayesian methods and approaches may be assimilated into health technology assessments in a variety of contexts and by a variety of participants in the research process.The BIDS ISI database was searched using the terms 'Bayes' or 'Bayesian'. This yielded almost 4000 papers published in the period 1990-98. All resultant abstracts were reviewed for relevance to health technology assessment; about 250 were so identified, and used as the basis for forward and backward searches. In addition EMBASE and MEDLINE databases were searched, along with websites of prominent authors, and available personal collections of references, finally yielding nearly 500 relevant references. A comprehensive review of all references describing use of 'proper' Bayesian methods in health technology assessment (those which update an informative prior distribution through the use of Bayes's theorem) has been attempted, and around 30 such papers are reported in structured form. There has been very limited use of proper Bayesian methods in practice, and relevant studies appear to be relatively easily identified.Bayesian methods in the health technology assessment context 1. Different contexts may demand different statistical approaches. Prior opinions are most valuable when the assessment forms part of a series of similar studies. A decision-theoretic approach may be appropriate where the consequences of a study are reasonably predictable. 2. The prior distribution is important and not unique, and so a range of options should be examined in a sensitivity analysis. Bayesian methods are best seen as a transformation from initial to final opinion, rather than providing a single 'correct' inference. 3. The use of a prior is based on judgement, and hence a degree of subjectivity cannot be avoided. However, subjective priors tend to show predictable biases, and archetypal priors may be useful for identifying a reasonable range of prior opinion.",0
https://doi.org/10.1016/s0169-7161(05)25028-9,Bayesian Analysis of ROC Data,"This chapter highlights Bayesian analysis of ROC data. When measured on a dichotomous scale, the performance of a diagnostic test can be summarized by two quantities: the false positive fraction (FPF) and the true positive fraction (TPF). The FPF is defined as the fraction of “healthy” subjects who test positive, but who do not have the condition that the test is designed to detect. The TPF is the fraction of subjects, who are correctly diagnosed as having the condition. An ROC curve describes the tradeoff between FPF and TPF as the disease threshold is changed. More specifically, an ROC curve can be defined as the curve defined by the set of points. Moreover, the choice of a threshold determines the relative tradeoff between false positive rates and true positive rates. Receiver operating characteristics (ROC) methodology provides a framework for analyzing this tradeoff as the threshold applied to the diagnostic test is varied. The most common application of ROC methodology arises in the comparison of new diagnostic technologies. Finally, this chapter explores that ROC methodology is especially prevalent in the evaluation of radiological devices, where it has been used extensively to determine whether one imaging modality is superior to another.",0
https://doi.org/10.1080/01621459.1987.10478490,Empirical Bayes Confidence Intervals Based on Bootstrap Samples,"Abstract Consider the model with data generated by the following two-stage process. First, a parameter θ is sampled from a prior distribution G, and then an observation is sampled from the conditional distribution f(y | θ). If the prior distribution is known, then the Bayes estimate under squared error loss is the posterior expectation of θ conditional on the data y. For example, if G is Gaussian with mean μ and variance τ2 and f(y | θ) is Gaussian with mean θ and variance σ2, then the posterior distribution is Gaussian with mean B μ + (1 – B)y and variance σ2(1 – B), where B = σ2/(σ2 + τ2). Inferences about θ are based on this distribution. We study the application of the bootstrap to situations where the prior must be estimated from the data (empirical Bayes methods). For this model, we observe data Y T = [Y 1, …, Y K]T, each independent Y K following the compound model described previously. As first shown by James and Stein (1961), setting each θk equal to its estimated posterior mean, where , and , pr...",0
https://doi.org/10.1186/1742-2094-11-92,Improvement of spinal non-viral IL-10gene delivery by D-mannose as a transgene adjuvant to control chronic neuropathic pain,"Peri-spinal subarachnoid (intrathecal; i.t.) injection of non-viral naked plasmid DNA encoding the anti-inflammatory cytokine, IL-10 (pDNA-IL-10) suppresses chronic neuropathic pain in animal models. However, two sequential i.t. pDNA injections are required within a discrete 5 to 72-hour period for prolonged efficacy. Previous reports identified phagocytic immune cells present in the peri-spinal milieu surrounding the i.t injection site that may play a role in transgene uptake resulting in subsequent IL-10 transgene expression.In the present study, we aimed to examine whether factors known to induce pro-phagocytic anti-inflammatory properties of immune cells improve i.t. IL-10 transgene uptake using reduced naked pDNA-IL-10 doses previously determined ineffective. Both the synthetic glucocorticoid, dexamethasone, and the hexose sugar, D-mannose, were factors examined that could optimize i.t. pDNA-IL-10 uptake leading to enduring suppression of neuropathic pain as assessed by light touch sensitivity of the rat hindpaw (allodynia).Compared to dexamethasone, i.t. mannose pretreatment significantly and dose-dependently prolonged pDNA-IL-10 pain suppressive effects, reduced spinal IL-1β and enhanced spinal and dorsal root ganglia IL-10 immunoreactivity. Macrophages exposed to D-mannose revealed reduced proinflammatory TNF-α, IL-1β, and nitric oxide, and increased IL-10 protein release, while IL-4 revealed no improvement in transgene uptake. Separately, D-mannose dramatically increased pDNA-derived IL-10 protein release in culture supernatants. Lastly, a single i.t. co-injection of mannose with a 25-fold lower pDNA-IL-10 dose produced prolonged pain suppression in neuropathic rats.Peri-spinal treatment with D-mannose may optimize naked pDNA-IL-10 transgene uptake for suppression of allodynia, and is a novel approach to tune spinal immune cells toward pro-phagocytic phenotype for improved non-viral gene therapy.",0
https://doi.org/10.1177/1948550611407689,Your Best Self Helps Reveal Your True Self,"How does trying to make a positive impression on others impact the accuracy of impressions? In an experimental study, the impact of positive self-presentation on the accuracy of impressions was examined by randomly assigning targets to either “put their best face forward” or to a control condition with low self-presentation demands. First, self-presenters successfully elicited more positive impressions from others, being viewed as more normative and better liked than those less motivated to self-present. Importantly, self-presenters were also viewed with greater accuracy than control targets, being perceived more in line with their self-reported distinctive personality traits and their IQ test scores. Mediational analyses were consistent with the hypothesis that self-presenters were more engaging than controls, which in turn led these individuals to be viewed with greater distinctive self–other agreement. In sum, positive self-presentation facilitates more accurate impressions, indicating that putting one’s best self forward helps reveal one’s true self.",0
https://doi.org/10.1016/s0006-8993(00)02050-3,"Thermal hyperalgesia and mechanical allodynia produced by intrathecal administration of the human immunodeficiency virus-1 (HIV-1) envelope glycoprotein, gp120","Astrocytes and microglia in the spinal cord have recently been reported to contribute to the development of peripheral inflammation-induced exaggerated pain states. Both lowering of thermal pain threshold (thermal hyperalgesia) and lowering of response threshold to light tactile stimuli (mechanical allodynia) have been reported. The notion that spinal cord glia are potential mediators of such effects is based on the disruption of these exaggerated pain states by drugs thought to preferentially affect glial function. Activation of astrocytes and microglia can release many of the same substances that are known to mediate thermal hyperalgesia and mechanical allodynia. The aim of the present series of studies was to determine whether exaggerated pain states could also be created in rats by direct, intraspinal immune activation of astrocytes and microglia. The immune stimulus used was peri-spinal (intrathecal, i.t.) application of the Human Immunodeficiency Virus type 1 (HIV-1) envelope glycoprotein, gp120. This portion of HIV-1 is known to bind to and activate microglia and astrocytes. Robust thermal hyperalgesia (tail-flick, TF, and Hargreaves tests) and mechanical allodynia (von Frey and touch-evoked agitation tests) were observed in response to i.t. gp120. Heat denaturing of the complex protein structure of gp120 blocked gp120-induced thermal hyperalgesia. Lastly, both thermal hyperalgesia and mechanical allodynia to i.t. gp120 were blocked by spinal pretreatment with drugs (fluorocitrate and CNI-1493) thought to preferentially disrupt glial function.",0
https://doi.org/10.3758/bf03194548,Invariance of the psychometric function for character recognition across the visual field,"The psychometric function for recognition of singly presented digits as a function of digit contrast was measured at 2 degrees steps across the horizontal meridian of the visual field, under monocular and binocular viewing conditions. A maximum-likelihood staircase procedure was used in a 10-alternative forced-choice recognition paradigm to gather the data Both the Weibull and the logistic psychometric functions provide excellent fits to the observed data. The slopes of these functions at their point of inflection ranged from 4.0 to 5.0 proportion-correct/log10-unit contrast, for both monocular and binocular viewing and for all loci in the visual field. These slope values correspond to short-term measurements (around 30 trials, or 1 min) and do not include performance variations of longer duration; the latter are estimated to increase slope by a factor of about 1.5. A single psychometric function shape, centered around a threshold value, therefore describes recognition performance at all retinal loci and binocularity. An empirical comparison of slope results across the literature shows that the function's slope is about twice that reported for a number of detection tasks. The comparison of recognition contrast thresholds, percentage correct values, and other performance measures across studies requires the knowledge of the psychometric function's slope, and our results thus provide a firm basis for the study of low-contrast character recognition.",0
https://doi.org/10.1111/j.1745-3984.2002.tb01146.x,Item Parameter Estimation Under Conditions of Test Speededness: Application of a Mixture Rasch Model With Ordinal Constraints,"When tests are administered under fixed time constraints, test performances can be affected by speededness. Among other consequences, speededness can result in inaccurate parameter estimates in item response theory (IRT) models, especially for items located near the end of tests (Oshima, 1994). This article presents an IRT strategy for reducing contamination in item difficulty estimates due to speededness. Ordinal constraints are applied to a mixture Rasch model (Rost, 1990) so as to distinguish two latent classes of examinees: (a) a “speeded” class, comprised of examinees that had insufficient time to adequately answer end-of-test items, and (b) a “nonspeeded” class, comprised of examinees that had sufficient time to answer all items. The parameter estimates obtained for end-of-test items in the nonspeeded class are shown to more accurately approximate their difficulties when the items are administered at earlier locations on a different form of the test. A mixture model can also be used to estimate the class memberships of individual examinees. In this way, it can be determined whether membership in the speeded class is associated with other student characteristics. Results are reported for gender and ethnicity.",0
https://doi.org/10.1017/cbo9781139941433.011,State space modeling for analysis of behavior in learning experiments,"Introduction: During the process of learning the brain undergoes changes that can be observed at both the cellular and systems level. Being able to track accurately simultaneous changes in behavior and neural activity is key to understanding how the brain learns new tasks and information. Learning is studied in a large number of experimental paradigms involving, for example, testing effects on learning of brain lesions (Whishaw & Tomie 1991; Dias et al. 1997; Dusek & Eichenbaum 1997; Wise & Murray 1999; Fox et al. 2003; Kim & Frank 2009; Kayser & D'Esposito 2013), attentional modulation (Cook & Maunsell 2002; Hudson et al. 2009), optogenetic manipulation (Warden et al. 2012) and pharmacological interventions (Stefani et al. 2003). Studies are also performed to understand how learning is affected by aging (Harris & Wolbers 2012), stroke (Panarese et al. 2012) and psychological conditions including autism (Solomon et al. 2011) and synesthesia (Brang et al. 2013). The learning process is also studied in relation to changes in neural activity in specific brain regions (Jog et al. 1999; Wirth et al. 2003; Suzuki & Brown 2005; Brovelli et al. 2011; Mattfeld & Stark 2011). In most cases, the response accuracy of a subject is binary, with a one representing a correct response and a zero representing an incorrect response. In its raw form, binary response accuracy can be difficult to visualize, especially if the time series is long, and the exact time when learning occurs can be difficult to identify. Typically, an experimenter is interested in deriving two things from the learning data: a learning trial and a learning curve. The first item is the time point at which responses significantly change relative to a baseline value such as chance performance. The second is estimation of a curve that defines the probability of a correct response as a function of trial. From these estimates, it is possible to compare changes in learning with other measurements such as, for example, localized brain oxygen consumption (via fMRI) or electrical activity. Ã‚Â© Cambridge University Press 2015",0
https://doi.org/10.1037/a0025046,Emotional inertia prospectively predicts the onset of depressive disorder in adolescence.,"Emotional inertia refers to the degree to which a person's current emotional state is predicted by their prior emotional state, reflecting how much it carries over from one moment to the next. Recently, in a cross-sectional study, we showed that high inertia is an important characteristic of the emotion dynamics observed in psychological maladjustment such as depression. In the present study, we examined whether emotional inertia prospectively predicts the onset of first-episode depression during adolescence. Emotional inertia was assessed in a sample of early adolescents (N = 165) based on second-to-second behavioral coding of videotaped naturalistic interactions with a parent. Greater inertia of both negative and positive emotional behaviors predicted the emergence of clinical depression 2.5 years later. The implications of these findings for the understanding of the etiology and early detection of depression are discussed.",0
https://doi.org/10.1037/a0027539,A comparison of methods for estimating quadratic effects in nonlinear structural equation models.,"Two Monte Carlo simulations were performed to compare methods for estimating and testing hypotheses of quadratic effects in latent variable regression models. The methods considered in the current study were (a) a 2-stage moderated regression approach using latent variable scores, (b) an unconstrained product indicator approach, (c) a latent moderated structural equation method, (d) a fully Bayesian approach, and (e) marginal maximum likelihood estimation. Of the 5 estimation methods, it was found that overall the methods based on maximum likelihood estimation and the Bayesian approach performed best in terms of bias, root-mean-square error, standard error ratios, power, and Type I error control, although key differences were observed. Similarities as well as disparities among methods are highlight and general recommendations articulated. As a point of comparison, all 5 approaches were fit to a reparameterized version of the latent quadratic model to educational reading data.",0
https://doi.org/10.3102/1076998611417628,Profile-Likelihood Approach for Estimating Generalized Linear Mixed Models With Factor Structures,"In this article, the authors suggest a profile-likelihood approach for estimating complex models by maximum likelihood (ML) using standard software and minimal programming. The method works whenever setting some of the parameters of the model to known constants turns the model into a standard model. An important class of models that can be estimated this way is generalized linear mixed models with factor structures. Such models are useful in educational research, for example, for estimation of value-added teacher or school effects with persistence parameters and for analysis of large-scale assessment data using multilevel item response models with discrimination parameters. The authors describe the profile-likelihood approach, implement it in the R software, and apply the method to longitudinal data and binary item response data. Simulation studies and comparison with gllamm show that the profile-likelihood method performs well in both types of applications. The authors also briefly discuss other types of models that can be estimated using the profile-likelihood idea.",0
https://doi.org/10.3758/pbr.15.4.713,Assessing individual differences in categorical data,"In cognitive modeling, data are often categorical observations taken over participants and items. Usually subsets of these observations are pooled and analyzed by a cognitive model assuming the category counts come from a multinomial distribution with the same model parameters underlying all observations. It is well known that if there are individual differences in participants and/or items, a model analysis of the pooled data may be quite misleading, and in such cases it may be appropriate to augment the cognitive model with parametric random effects assumptions. On the other hand, if random effects are incorporated into a cognitive model that is not needed, the resulting model may be more flexible than the multinomial model that assumes no heterogeneity, and this may lead to overfitting. This article presents Monte Carlo statistical tests for directly detecting individual participant and/or item heterogeneity that depend only on the data structure itself. These tests are based on the fact that heterogeneity in participants and/or items results in overdispersion of certain category count statistics. It is argued that the methods developed in the article should be applied to any set of participant x item categorical data prior to cognitive model-based analyses.",0
https://doi.org/10.1080/09585192.2011.561227,Does pay for performance diminish intrinsic interest?,"One concern with pay for individual performance (PFIP) is that it may undermine intrinsic interest, thus having little or no positive net influence on performance. A major basis for this concern is cognitive evaluation theory [CET; Deci and Ryan (1985), Intrinsic Motivation and Self-Determination in Human Behavior, New York: Plenum Press]. Most evidence on CET, however, comes from non-work settings and, even in that arena, there is debate regarding the undermining effect of PFIP. There is little workplace-based evidence on the validity of the undermining hypothesis and none that makes use of data on between-employer differences in PFIP. Also, a close reading of CET, reinforced by recent developments, suggests that PFIP plans could, under common workplace conditions, have a positive, rather than negative, influence on intrinsic interest. To our knowledge, there is no research that examines between-organization differences in PFIP and how they relate to employee intrinsic interest. There is also no research on whether employees having a preference for PFIP plans are likely to gravitate to organizations using such plans. To the extent such attraction–selection–attrition or sorting processes take place, the likelihood of detrimental consequences (e.g. diminished intrinsic interest) of PFIP plans due to mismatches between how the organization pays and how the employees are motivated should be less likely. We find no evidence of a detrimental effect of PFIP plans on intrinsic interest. Instead, intrinsic interest is actually higher under PFIP. We also find that organizations placing greater emphasis on PFIP plans tend to have employees with motivation orientations matching their PFIP plans, which may reduce the probability of a detrimental effect of PFIP.",0
https://doi.org/10.1093/biomet/82.3.639,A note on the existence of the posterior distribution for a class of mixed models for binomial responses,SUMMARY Necessary and sufficient conditions are given for the existence of the posterior distribution of the variance components in a class of mixed models for binomial responses. The implications of our results are illustrated through an example.,0
https://doi.org/10.1136/lupus-2015-000110,Perceptions of racism in healthcare among patients with systemic lupus erythematosus: a cross-sectional study,"Racial disparities in the clinical outcomes of systemic lupus erythematosus (SLE) exist. Perceived racial discrimination may contribute to disparities in health.To determine if perceived racism in healthcare differs by race among patients with SLE and to evaluate its contribution to racial disparities in SLE-related outcomes.163 African-American (AA) and 180 white (WH) patients with SLE were enrolled. Structured interviews and chart reviews were done to determine perceptions of racism, SLE-related outcomes (Systemic Lupus International Collaborating Clinics (SLICC) Damage Index, SLE Disease Activity, Center for Epidemiologic Studies-Depression (CES-D)), and other variables that may affect perceptions of racism. Serial hierarchical multivariable logistic regression models were conducted. Race-stratified analyses were also performed.56.0% of AA patients compared with 32.8% of WH patients had high perceptions of discrimination in healthcare (p<0.001). This difference remained (OR 4.75 (95% CI 2.41 to 8.68)) after adjustment for background, identity and healthcare experiences. Female gender (p=0.012) and lower trust in physicians (p<0.001) were also associated with high perceived racism. The odds of having greater disease damage (SLICC damage index ≥2) were higher in AA patients than in WH patients (crude OR 1.55 (95% CI 1.01 to 2.38)). The odds of having moderate to severe depression (CES-D ≥17) were also higher in AA patients than in WH patients (crude OR 1.94 (95% CI 1.26 to 2.98)). When adjusted for sociodemographic and clinical characteristics, racial disparities in disease damage and depression were no longer significant. Among AA patients, higher perceived racism was associated with having moderate to severe depression (adjusted OR 1.23 (95% CI 1.05 to 1.43)) even after adjusting for sociodemographic and clinical variables.Perceptions of racism in healthcare were more common in AA patients than in WH patients with SLE and were associated with depression. Interventions aimed at modifiable factors (eg, trust in providers) may reduce higher perceptions of race-based discrimination in SLE.",0
https://doi.org/10.1186/1471-2458-4-48,"The ProActivetrial protocol – a randomised controlled trial of the efficacy of a family-based, domiciliary intervention programme to increase physical activity among individuals at high risk of diabetes [ISRCTN61323766]","BackgroundIncreasing prevalence of obesity and disorders associated with sedentary living constitute a major global public health problem. While previous evaluations of interventions to increase physical activity have involved communities or individuals with established disease, less attention has been given to interventions for individuals at risk of disease.Methods/design ProActiveaims to evaluate the efficacy of a theoretical, evidence- and family-based intervention programme to increase physical activity in a sedentary population, defined as being at-risk through having a parental family history of diabetes. Primary care diabetes or family history registers were used to recruit 365 individuals aged 30–50 years, screened for activity level. Participants were assigned by central randomisation to three intervention programmes: brief written advice (comparison group), or a psychologically based behavioural change programme, delivered either by telephone (distance group) or face-to-face in the family home over one year. The protocol-driven intervention programme is delivered by trained facilitators, and aims to support increases in physical activity through the introduction and facilitation of a range of self-regulatory skills (e.g. goal setting). The primary outcome is daytime energy expenditure and its ratio to resting energy expenditure, measured at baseline and one year using individually calibrated heart rate monitoring. Secondary measures include self-report of individual and family activity, psychological mediators of behaviour change, physiological and biochemical correlates, acceptability, and costs, measured at baseline, six months and one year. The primary intention to treat analysis will compare groups at one-year post randomisation. Estimation of the impact on diabetes incidence will be modelled using data from a parallel ten-year cohort study using similar measures.Discussion ProActiveis the first efficacy trial of an intervention programme to promote physical activity in a defined high-risk group accessible through primary care. The intervention programme is based on psychological theory and evidence; it introduces and facilitates the use of self-regulatory skills to support behaviour change and maintenance. The trial addresses a range of methodological weaknesses in the field by careful specification and quality assurance of the intervention programme, precise characterisation of participants, year-long follow-up and objective measurement of physical activity. Due to report in 2005, ProActivewill provide estimates of the extent to which this approach could assist at-risk groups who could benefit from changes in behaviours affecting health, and inform future pragmatic trials.",0
https://doi.org/10.1002/sim.3648,Flexibility of Bayesian generalized linear mixed models for oral health research,"Many outcome variables in oral research are characterized by positive values and heavy skewness in the right tail. Examples are provided by many distributions of dental variables such as DMF (decayed, missing, filled teeth) scores, oral health impact profile score, gingival index scores, and microbiologic counts. Moreover, heterogeneity in data arises when more than one tooth is studied for each patient, due to the clusterization. Over the past decade, linear mixed models (LMEs) have become a common statistical tool to account for within-subject correlation in data with repeated measures. When a normal error is reasonably assumed, estimates of LMEs are supported by many statistical packages. Such is not the case for skewed data, where generalized linear mixed models (GLMMs) are required. However, the current software available supports only special cases of GLMMs or relies on crude Laplace-type approximation of integrals. In this study, a Bayesian approach is taken to estimate GLMMs for clustered skewed dental data. A Gamma GLMM and a log-normal model are employed to allow for heterogeneity across clusters, deriving from the patient-operator-tooth susceptibility typical of this clinical context. A comparison to the frequentist framework is also provided. In our case, Gamma GLMM fits data better than the log-normal distribution, while providing more precise estimates compared with the likelihood approach. A key advantage of the Bayesian framework is its ability to readily provide a flexible approach for implementation while simultaneously providing a formal procedure for solving inference problems. Copyright © 2009 John Wiley & Sons, Ltd.",0
https://doi.org/10.1111/1475-6765.12088,Valence and satisfaction with democracy: A cross-national analysis of nine Western European democracies,"In recent studies, scholars have highlighted factors that influence citizen satisfaction with democracy, with particular emphasis on the role played by the institutional features of political systems, and ideology. This article presents the first empirical study of whether changes in important party characteristics can affect individuals’ satisfaction with democracy. Using a measure of parties’ character-valence derived from content analysis of news reports, evidence is presented that when governing parties’ images decline with respect to important valence-related attributes such as competence, unity and integrity, then citizen satisfaction with democracy similarly declines. However, this relationship is conditional on the performance of opposition parties. These findings are relevant to studies of regime support, political representation, democratic accountability and voter behaviour.",0
https://doi.org/10.1016/s0005-7894(02)80004-1,Testing for group membership effects during and after treatment: The example of group therapy for smoking cessation,"Behavioral interventions often are administered in groups, yet the effects of group membership rarely have been evaluated. The current research examined 33 groups of clients ( M = 5.5 clients per group, SD = 2.5) volunteering for a group smoking cessation intervention. The intervention consisted of 6 group therapy sessions over an 11-day period. Attendance at the sessions and smoking behavior during the 11-day period were the dependent variables. Hierarchical linear modeling (HLM) revealed a statistically significant ( p",0
https://doi.org/10.1198/jcgs.2010.08181,Computational Aspects Related to Inference in Gaussian Graphical Models With the G-Wishart Prior,"We describe a comprehensive framework for performing Bayesian inference for Gaussian graphical models based on the G-Wishart prior with a special focus on efficiently including nondecomposable graphs in the model space. We develop a new approximation method to the normalizing constant of a G-Wishart distribution based on the Laplace approximation. We review recent developments in stochastic search algorithms and propose a new method, the mode oriented stochastic search (MOSS), that extends these techniques and proves superior at quickly finding graphical models with high posterior probability. We then develop a novel stochastic search technique for multivariate regression models and conclude with a real-world example from the recent covariance estimation literature. Supplemental materials are available online.",0
https://doi.org/10.3758/bf03210858,Estimating psychometric functions in forced-choice situations: Significant biases found in threshold and slope estimations when small samples are used,"When a theoretical psychometric function is fitted to experimental data (as in the obtaining of a psychophysical threshold), maximum-likelihood or probit methods are generally used. In the present paper, the behavior of these curve-fitting methods is studied for the special case of forced-choice experiments, in which the probability of a subject's making a correct response by chance is not zero. A mathematical investigation of the variance of the threshold and slope estimators shows that, in this case, the accuracy of the methods is much worse, and their sensitivity to the way data are sampled is greater, than in the case in which chance level is zero. Further, Monte Carlo simulations show that, in practical situations in which only a finite number of observations are made, the mean threshold and slope estimates are significantly biased. The amount of bias depends on the curve-fitting method and on the range of intensity values, but it is always greater in forced-choice situations than when chance level is zero.",0
https://doi.org/10.1093/jeg/lbu005,Stylised fact or situated messiness? The diverse effects of increasing debt on national economic growth,"This article reanalyses data used by Reinhart and Rogoff (2010c, American Economic Review, 100: 573–78—RR), and later Herndon et al. (2013, Cambridge Journal of Economics, online, doi: 10.1093/cje/bet075) to consider the relationship between growth and debt in developed countries. The consistency over countries and the causal direction of RR’s so called ‘stylised fact’ is considered. Using multilevel models, we find that when the effect of debt on growth is allowed to vary, and linear time trends are fully controlled for, the average effect of debt on growth disappears, whilst country-specific debt relations vary significantly. Additionally, countries with high debt levels appear more volatile in their growth rates. Regarding causality, we develop a new method extending distributed lag models to multilevel situations. These models suggest the causal direction is predominantly growth-to-debt, and is consistent (with some exceptions) across countries. We argue that RR’s findings are too simplistic, with limited policy relevance, whilst demonstrating how multilevel models can explicate realistically complex scenarios.",0
https://doi.org/10.1016/j.jneumeth.2011.10.025,BSMac: A MATLAB toolbox implementing a Bayesian spatial model for brain activation and connectivity,"We present a statistical and graphical visualization MATLAB toolbox for the analysis of functional magnetic resonance imaging (fMRI) data, called the Bayesian Spatial Model for activation and connectivity (BSMac). BSMac simultaneously performs whole-brain activation analyses at the voxel and region of interest (ROI) levels as well as task-related functional connectivity (FC) analyses using a flexible Bayesian modeling framework (Bowman et al., 2008). BSMac allows for inputting data in either Analyze or Nifti file formats. The user provides information pertaining to subgroup memberships, scanning sessions, and experimental tasks (stimuli), from which the design matrix is constructed. BSMac then performs parameter estimation based on Markov Chain Monte Carlo (MCMC) methods and generates plots for activation and FC, such as interactive 2D maps of voxel and region-level task-related changes in neural activity and animated 3D graphics of the FC results. The toolbox can be downloaded from http://www.sph.emory.edu/bios/CBIS/. We illustrate the BSMac toolbox through an application to an fMRI study of working memory in patients with schizophrenia.",0
https://doi.org/10.3758/bf03192809,Using the open-source statistical language R to analyze the dichotomous Rasch model,"R, an open-source statistical language and data analysis tool, is gaining popularity among psychologists currently teaching statistics. R is especially suitable for teaching advanced topics, such as fitting the dichotomous Rasch model--a topic that involves transforming complicated mathematical formulas into statistical computations. This article describes R's use as a teaching tool and a data analysis software program in the analysis of the Rasch model in item response theory. It also explains thetheory behind, as well as an educator's goals for, fitting the Rasch model with joint maximum likelihood estimation. This article also summarizes the R syntax for parameter estimation and the calculation of fit statistics. The results produced by R is compared with the results obtained from MINISTEP and the output of a conditional logit model. The use of R is encouraged because it is free, supported by a network of peer researchers, and covers both basic and advanced topics in statistics frequently used by psychologists.",0
https://doi.org/10.1017/s0266466600011646,On Consistency and Inconsistency of Estimating Equations,"The primary concern is to establish a fairly general framework in which estimators resulting from estimating equations g nθ = 0 are not consistent. This leads on to consistency by an intuitive route. Asymptotic distributions of consistent estimators are also touched upon, and the results are applied to various examples.",0
https://doi.org/10.1111/bmsp.12041,Properties of hypothesis testing techniques and (Bayesian) model selection for exploration-based and theory-based (order-restricted) hypotheses,"In this paper, the performance of six types of techniques for comparisons of means is examined. These six emerge from the distinction between the method employed (hypothesis testing, model selection using information criteria, or Bayesian model selection) and the set of hypotheses that is investigated (a classical, exploration-based set of hypotheses containing equality constraints on the means, or a theory-based limited set of hypotheses with equality and/or order restrictions). A simulation study is conducted to examine the performance of these techniques. We demonstrate that, if one has specific, a priori specified hypotheses, confirmation (i.e., investigating theory-based hypotheses) has advantages over exploration (i.e., examining all possible equality-constrained hypotheses). Furthermore, examining reasonable order-restricted hypotheses has more power to detect the true effect/non-null hypothesis than evaluating only equality restrictions. Additionally, when investigating more than one theory-based hypothesis, model selection is preferred over hypothesis testing. Because of the first two results, we further examine the techniques that are able to evaluate order restrictions in a confirmatory fashion by examining their performance when the homogeneity of variance assumption is violated. Results show that the techniques are robust to heterogeneity when the sample sizes are equal. When the sample sizes are unequal, the performance is affected by heterogeneity. The size and direction of the deviations from the baseline, where there is no heterogeneity, depend on the effect size (of the means) and on the trend in the group variances with respect to the ordering of the group sizes. Importantly, the deviations are less pronounced when the group variances and sizes exhibit the same trend (e.g., are both increasing with group number).",0
https://doi.org/10.1111/1467-9868.00239,Inference for multivariate normal hierarchical models,"This paper provides a new method and algorithm for making inferences about the parameters of a two-level multivariate normal hierarchical model. One has observed J p-dimensional vector outcomes, distributed at level 1 as multivariate normal with unknown mean vectors and with known covariance matrices. At level 2, the unknown mean vectors also have normal distributions, with common unknown covariance matrix A and with means depending on known covariates and on unknown regression coefficients. The algorithm samples independently from the marginal posterior distribution of A by using rejection procedures. Functions such as posterior means and covariances of the level 1 mean vectors and of the level 2 regression coefficient are estimated by averaging over posterior values calculated conditionally on each value of A drawn. This estimation accounts for the uncertainty in A, unlike standard restricted maximum likelihood empirical Bayes procedures. It is based on independent draws from the exact posterior distributions, unlike Gibbs sampling. The procedure is demonstrated for profiling hospitals based on patients' responses concerning p = 2 types of problems (non-surgical and surgical). The frequency operating characteristics of the rule corresponding to a particular vague multivariate prior distribution are shown via simulation to achieve their nominal values in that setting.",0
https://doi.org/10.1186/s12874-015-0031-0,Meta-analysis of incidence rate data in the presence of zero events,"When summary results from studies of counts of events in time contain zeros, the study-specific incidence rate ratio (IRR) and its standard error cannot be calculated because the log of zero is undefined. This poses problems for the widely used inverse-variance method that weights the study-specific IRRs to generate a pooled estimate.We conducted a simulation study to compare the inverse-variance method of conducting a meta-analysis (with and without the continuity correction) with alternative methods based on either Poisson regression with fixed interventions effects or Poisson regression with random intervention effects. We manipulated the percentage of zeros in the intervention group (from no zeros to approximately 80 percent zeros), the levels of baseline variability and heterogeneity in the intervention effect, and the number of studies that comprise each meta-analysis. We applied these methods to an example from our own work in suicide prevention and to a recent meta-analysis of the effectiveness of condoms in preventing HIV transmission.As the percentage of zeros in the data increased, the inverse-variance method of pooling data shows increased bias and reduced coverage. Estimates from Poisson regression with fixed interventions effects also display evidence of bias and poor coverage, due to their inability to account for heterogeneity. Pooled IRRs from Poisson regression with random intervention effects were unaffected by the percentage of zeros in the data or the amount of heterogeneity.Inverse-variance methods perform poorly when the data contains zeros in either the control or intervention arms. Methods based on Poisson regression with random effect terms for the variance components are very flexible offer substantial improvement.",0
https://doi.org/10.1017/s113874160000514x,Sampling Plans for Fitting the Psychometric Function,"Research on estimation of a psychometric function Ψ has usually focused on comparing alternative algorithms to apply to the data, rarely addressing how best to gather the data themselves (i.e., what sampling plan best deploys the affordable number of trials). Simulation methods were used here to assess the performance of several sampling plans in yes–no and forced-choice tasks, including the QUEST method and several variants of up–down staircases and of the method of constant stimuli (MOCS). We also assessed the efficacy of four parameter estimation methods. Performance comparisons were based on analyses of usability (i.e., the percentage of times that a plan yields usable data for the estimation of all the parameters of Ψ) and of the resultant distributions of parameter estimates. Maximum likelihood turned out to be the best parameter estimation method. As for sampling plans, QUEST never exceeded 80% usability even when 1000 trials were administered and rendered accurate estimates of threshold but misestimated the remaining parameters. MOCS and up–down staircases yielded similar and acceptable usability (above 95% with 400–500 trials) and, although neither type of plan allowed estimating all parameters with optimal precision, each type appeared well suited to estimating a distinct subset of parameters. An analysis of the causes of this differential suitability allowed designing alternative sampling plans (all based on up–down staircases) for yes–no and forced-choice tasks. These alternative plans rendered near optimal distributions of estimates for all parameters. The results just described apply when the fitted Ψ has the same mathematical form as the actual Ψ generating the data; in case of form mismatch, all parameters except threshold were generally misestimated but the relative performance of all the sampling plans remained identical. Detailed practical recommendations are given.",0
https://doi.org/10.3389/fpsyg.2014.00181,Evaluation of model fit in nonlinear multilevel structural equation modeling,"Evaluating model fit in nonlinear multilevel structural equation models (MSEM) presents a challenge as no adequate test statistic is available. Nevertheless, using a product indicator approach a likelihood ratio test for linear models is provided which may also be useful for nonlinear MSEM. The main problem with nonlinear models is that product variables are nonnormally distributed. Although robust test statistics have been developed for linear SEM to ensure valid results under the condition of nonnormality, they were not yet investigated for nonlinear MSEM. In a Monte Carlo study, the performance of the robust likelihood ratio test was investigated for models with single-level latent interaction effects using the unconstrained product indicator approach. As overall model fit evaluation has a potential limitation in detecting the lack of fit at a single level even for linear models, level-specific model fit evaluation was also investigated using partially saturated models. Four population models were considered: a model with interaction effects at both levels, an interaction effect at the within-group level, an interaction effect at the between-group level, and a model with no interaction effects at both levels. For these models the number of groups, predictor correlation, and model misspecification was varied. The results indicate that the robust test statistic performed sufficiently well. Advantages of level-specific model fit evaluation for the detection of model misfit are demonstrated.",0
https://doi.org/10.1037/1082-989x.8.3.369,Statistical and Substantive Checking in Growth Mixture Modeling: Comment on Bauer and Curran (2003).,This commentary discusses the D. J. Bauer and P. J. Curran (2003) investigation of growth mixture modeling. Single-class modeling of nonnormal outcomes is compared with modeling with multiple latent trajectory classes. New statistical tests of multiple-class models are discussed. Principles for substantive investigation of growth mixture model results are presented and illustrated by an example of high school dropout predicted by low mathematics achievement development in Grades 7-10.,0
https://doi.org/10.14257/ijsh.2016.10.1.15,A Improved Statistical Model Analysis the Mental Health of Rural-to-Urban Migrants in China,"The mental health of rural-to-urban migrants in China is a critical issue. The aim of this study was to test the migrants' mental health. The findings drawn from this qualitative study of 769 migrants in Wuhan in 2012 based on the Bayesian structural equation model. Overall, the survey found that leisure plays the greatest positive role in migrants' mental health, as well as work, interpersonal relationships, and health status have a negative role in migrants' mental health. Thus, the government must set relevant regulations to help migrants establish a better life and work values to work energetically. China is an agricultural country, and farmers comprise more than 75% of the total population. A large number of farmers have poured into the cities over the past 30 years, which caused the rapid growth of the migrant population in China. The sixth census data released by the National Bureau of Statistics show that the number of migrants in China has reached 0.26 billion, which are the largest scale of labor migration in the history of mankind (CNBS, 2011). According to the National Bureau of Statistics, rural-to-urban migrants are those who migrate from the countryside to cities to seek more job opportunities and higher quality of life, but they have no permanent urban residency (CNBS, 2001). These migrants who migrated from rural to urban areas are affected by the limitations of the traditional urban-rural dual structure. Farmers can live and work in the cities, but they are not included in the city Hukou. Therefore, they are not included in the welfare and public distribution system of the city, and they do not enjoy the same treatment as the city residents in terms of labor and social security, health insurance, and children's education. They are the targets of social discrimination and are isolated in the edge of urban communities(Yang, Li, & Wang,2006; Wang et al., 2010; McGuire, Li, & Wang, 2009), causing their mental health to become a common issue of concern. The characteristics and demands of migrants have changed since China's 30-year economic reform. Young migrants occupy a large proportion of the total numbers (Liu,2007). This new generation of migrants usually aims for social mobility, with more prominent laddering migration characteristics. They are sensitive, self-recognized, and have a higher education level. They are different from their parents who are a tough breed and only pursue income. Young migrants aspire for a high quality of life and hope to enjoy the same social status as their peers in the cities. New factors may appear as migrants change in the 21 st century.",0
https://doi.org/10.1080/10485250802613558,A Bayesian nonparametric method for model evaluation: application to genetic studies,"Statistical models applied to genetic studies commonly assume linear relationships (between disease and risk factors) and simple distributional forms (by relying on asymptotic methods) for inference. However, when the sample size is small, inference using traditional asymptotic models can be problematic. Moreover, the gene-disease relationship is not always linear. In this article, we present a new nonparametric Bayesian method for model assessment, and we demonstrate the advantages of this approach particularly when the sample size is small and/or the true model is non-linear. We evaluate our approach on simulated data and find that it performs substantially better than alternative models. We also apply our method to two real studies: diagnosis of conventional high-grade non-metastatic osteosarcoma, and survival in Burkitt's lymphoma.",0
,Latent variable scores and their uses,,0
https://doi.org/10.1080/19466315.2013.852617,A Quantitative Process for Enhancing End of Phase 2 Decisions,"The objectives of the phase 2 stage in a drug development program are to evaluate the safety and tolerability of different doses, select a promising dose range, and look for early signs of activity. At the end of phase 2, a decision to initiate phase 3 studies is made that involves the commitment of considerable resources. This multifactorial decision, generally made by balancing the current condition of a development organization's portfolio, the future cost of development, the competitive landscape, and the expected safety and efficacy benefits of a new therapy, needs to be a good one. In this article, we present a practical quantitative process that has been implemented for drugs entering phase 2 at Amgen Ltd. to ensure a consistent and explicit evidence-based approach is used to contribute to decisions for new drug candidates. Broadly following this process will also help statisticians increase their strategic influence in drug development programs. The process is illustrated using an example from the pancreatic cancer indication. Embedded within the process is a predominantly Bayesian approach to predicting the probability of efficacy success in a future (frequentist) phase 3 program.",0
https://doi.org/10.1023/a:1011098109834,,"Most large-scale secondary data sets used in higher education research (e.g., NP-SAS or BPS) are constructed using complex survey sample designs where the population of interest is stratified on a number of dimensions and oversampled within certain of these strata. Moreover, these complex sample designs often cluster lower level units (e.g., students) within higher level units (e.g., colleges) to achieve efficiencies in the sampling process. Ignoring oversampling (unequal probability of selection) in complex survey designs presents problems when trying to make inferences-data from these designs are, in their raw form, admittedly nonrepresentative of the population to which they are designed to generalize. Ignoring the clustering of observations in these sampling designs presents a second set of problems when making inferences about variability in the population and testing hypotheses and usually leads to an increased likelihood of committing Type I errors (declaring something as an effect when in fact it is not). This article presents an extended example using complex sample survey data to demonstrate how researchers can address problems associated with oversampling and clustering of observations in these designs.",0
https://doi.org/10.1214/11-sts358,Covariance Estimation: The GLM and Regularization Perspectives,"Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-definiteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions.",0
https://doi.org/10.1177/0146621604271495,Controlling Item Exposure and Test Overlap in Computerized Adaptive Testing,"This article proposes an item exposure control method, which is the extension of the Sympson and Hetter procedure and can provide item exposure control at both the item and test levels. Item exposure rate and test overlap rate are two indices commonly used to track item exposure in computerized adaptive tests. By considering both indices, item exposure can be monitored at both the item and test levels. To control the item exposure rate and test overlap rate simultaneously, the modified procedure attempted to control not only the maximum value but also the variance of item exposure rates. Results indicated that the item exposure rate and test overlap rate could be controlled simultaneously by implementing the modified procedure. Item exposure control was improved and precision of trait estimation decreased when a prespecified maximum test overlap rate was stringent.",0
https://doi.org/10.3389/fnint.2012.00100,Adaptation to visual or auditory time intervals modulates the perception of visual apparent motion,"It is debated whether sub-second timing is subserved by a centralized mechanism or by the intrinsic properties of task-related neural activity in specific modalities (Ivry and Schlerf, 2008). By using a temporal adaptation task, we investigated whether adapting to different time intervals conveyed through stimuli in different modalities (i.e., frames of a visual Ternus display, visual blinking discs, or auditory beeps) would affect the subsequent implicit perception of visual timing, i.e., inter-stimulus interval (ISI) between two frames in a Ternus display. The Ternus display can induce two percepts of apparent motion (AM), depending on the ISI between the two frames: ""element motion"" for short ISIs, in which the endmost disc is seen as moving back and forth while the middle disc at the overlapping or central position remains stationary; ""group motion"" for longer ISIs, in which both discs appear to move in a manner of lateral displacement as a whole. In Experiment 1, participants adapted to either the typical ""element motion"" (ISI = 50 ms) or the typical ""group motion"" (ISI = 200 ms). In Experiments 2 and 3, participants adapted to a time interval of 50 or 200 ms through observing a series of two paired blinking discs at the center of the screen (Experiment 2) or hearing a sequence of two paired beeps (with pitch 1000 Hz). In Experiment 4, participants adapted to sequences of paired beeps with either low pitches (500 Hz) or high pitches (5000 Hz). After adaptation in each trial, participants were presented with a Ternus probe in which the ISI between the two frames was equal to the transitional threshold of the two types of motions, as determined by a pretest. Results showed that adapting to the short time interval in all the situations led to more reports of ""group motion"" in the subsequent Ternus probes; adapting to the long time interval, however, caused no aftereffect for visual adaptation but significantly more reports of group motion for auditory adaptation. These findings, suggesting amodal representation for sub-second timing across modalities, are interpreted in the framework of temporal pacemaker model.",0
https://doi.org/10.1214/aos/1176344064,Bayesian Inference for Causal Effects: The Role of Randomization,"Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabilistic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. Moreover, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical randomized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.",0
https://doi.org/10.3758/bf03193597,Recognition memory for realistic synthetic faces,"A series of experiments examined short-term recognition memory for trios of briefly presented, synthetic human faces derived from three real human faces. The stimuli were a graded series of faces, which differed by varying known amounts from the face of the average female. Faces based on each of the three real faces were transformed so as to lie along orthogonal axes in a 3-D face space. Experiment 1 showed that the synthetic faces' perceptual similarity structure strongly influenced recognition memory. Results were fit by a noisy exemplar model (NEMO) of perceptual recognition memory. The fits revealed thatrecognition memory was influenced both by the similarity of the probe to the series items and by the similarities among the series items themselves. Nonmetric multidimensional scaling (MDS) showed that the faces' perceptual representations largely preserved the 3-D space in which the face stimuli were arrayed. NEMO gave a better account of the results when similarity was defined as perceptual MDS similarity, rather than as the physical proximity of one face to another. Experiment 2 confirmed the importance of within-list homogeneity directly, without mediation of a model. We discuss the affinities and differences between visual memory for synthetic faces and memory for simpler stimuli.",0
https://doi.org/10.1016/j.neuroimage.2014.07.022,Interregional alpha-band synchrony supports temporal cross-modal integration,"In a continuously changing environment, time is a key property that tells us whether information from the different senses belongs together. Yet, little is known about how the brain integrates temporal information across sensory modalities. Using high-density EEG combined with a novel psychometric timing task in which human subjects evaluated durations of audiovisual stimuli, we show that the strength of alpha-band (8-12 Hz) phase synchrony between localizer-defined auditory and visual regions depended on cross-modal attention: during encoding of a constant 500 ms standard interval, audiovisual alpha synchrony decreased when subjects attended audition while ignoring vision, compared to when they attended both modalities. In addition, alpha connectivity during a variable target interval predicted the degree to which auditory stimulus duration biased time estimation while attending vision. This cross-modal interference effect was estimated using a hierarchical Bayesian model of a psychometric function that also provided an estimate of each individual's tendency to exhibit attention lapses. This lapse rate, in turn, was predicted by single-trial estimates of the stability of interregional alpha synchrony: when attending to both modalities, trials with greater stability in patterns of connectivity were characterized by reduced contamination by lapses. Together, these results provide new insights into a functional role of the coupling of alpha phase dynamics between sensory cortices in integrating cross-modal information over time.",0
https://doi.org/10.1027/1614-2241/a000062,How Low Can You Go?,"Whereas general sample size guidelines have been suggested when estimating multilevel models, they are only generalizable to a relatively limited number of data conditions and model structures, both of which are not very feasible for the applied researcher. In an effort to expand our understanding of two-level multilevel models under less than ideal conditions, Monte Carlo methods, through SAS/IML, were used to examine model convergence rates, parameter point estimates (statistical bias), parameter interval estimates (confidence interval accuracy and precision), and both Type I error control and statistical power of tests associated with the fixed effects from linear two-level models estimated with PROC MIXED. These outcomes were analyzed as a function of: (a) level-1 sample size, (b) level-2 sample size, (c) intercept variance, (d) slope variance, (e) collinearity, and (f) model complexity. Bias was minimal across nearly all conditions simulated. The 95% confidence interval coverage and Type I error rate tended to be slightly conservative. The degree of statistical power was related to sample sizes and level of fixed effects; higher power was observed with larger sample sizes and level-1 fixed effects.",0
https://doi.org/10.1016/s0047-259x(02)00053-2,Estimating the covariance matrix: a new approach,"In this paper, we consider the problem of estimating the covariance matrix and the generalized variance when the observations follow a nonsingular multivariate normal distribution with unknown mean. A new method is presented to obtain a truncated estimator that utilizes the information available in the sample mean matrix and dominates the James–Stein minimax estimator. Several scale equivariant minimax estimators are also given. This method is then applied to obtain new truncated and improved estimators of the generalized variance; it also provides a new proof to the results of Shorrock and Zidek (Ann. Statist. 4 (1976) 629) and Sinha (J. Multivariate Anal. 6 (1976) 617).",0
https://doi.org/10.1177/1740774507083434,The intermediate endpoint effect in logistic and probit regression,"Background An intermediate endpoint is hypothesized to be in the middle of the causal sequence relating an independent variable to a dependent variable. The intermediate variable is also called a surrogate or mediating variable and the corresponding effect is called the mediated, surrogate endpoint, or intermediate endpoint effect. Clinical studies are often designed to change an intermediate or surrogate endpoint and through this intermediate change influence the ultimate endpoint. In many intermediate endpoint clinical studies the dependent variable is binary, and logistic or probit regression is used. Purpose The purpose of this study is to describe a limitation of a widely used approach to assessing intermediate endpoint effects and to propose an alternative method, based on products of coefficients, that yields more accurate results. Methods The intermediate endpoint model for a binary outcome is described for a true binary outcome and for a dichotomization of a latent continuous outcome. Plots of true values and a simulation study are used to evaluate the different methods. Results Distorted estimates of the intermediate endpoint effect and incorrect conclusions can result from the application of widely used methods to assess the intermediate endpoint effect. The same problem occurs for the proportion of an effect explained by an intermediate endpoint, which has been suggested as a useful measure for identifying intermediate endpoints. A solution to this problem is given based on the relationship between latent variable modeling and logistic or probit regression. Limitations More complicated intermediate variable models are not addressed in the study, although the methods described in the article can be extended to these more complicated models. Conclusions Researchers are encouraged to use an intermediate endpoint method based on the product of regression coefficients. A common method based on difference in coefficient methods can lead to distorted conclusions regarding the intermediate effect. Clinical Trials 2007; 4: 499—513. http://ctj.sagepub.com",0
https://doi.org/10.1007/bf00844756,The pattern of influence of perceived behavioral control upon exercising behavior: An application of Ajzen's theory of planned behavior,"The aim of the present studies was to verify the basic assumptions underlying the theory of planned behavior for the prediction of exercising intentions and behavior among adults of the general population (study 1) and a group of pregnant women (study 2). In both studies, baseline data were collected at home with trained interviewers and with the use of paper-and-pencil questionnaires. The self-report on behavior was obtained 6 months (study 1) and between 8 and 9 months (study 2) after baseline data collection. In study 1, perceived behavioral control influenced behavior only through intention. In study 2, none of the Ajzen model variables was associated to exercising behavior. Nonetheless, intention was influenced by attitude, habit, and perceived behavioral control. The results of the present studies suggest that perceived behavioral control contributes to the understanding of intentions to exercise but not to the prediction of exercising behavior. Â© 1993 Plenum Publishing Corporation.",0
https://doi.org/10.1037/11383-021,Longitudinal Methods.,,0
https://doi.org/10.1371/journal.pone.0096606,Guilt in Bereavement: The Role of Self-Blame and Regret in Coping with Loss,"Despite the apparent centrality of guilt in complicating reactions following bereavement, scientific investigation has been limited. Establishing the impact of specific components associated with guilt could enhance understanding. The aim of this study was to examine the relationships between two guilt-related manifestations, namely self-blame and regret, with grief and depression. A longitudinal investigation was conducted 4-7 months, 14 months and 2 years post-loss. Participants were bereaved spouses (30 widows; 30 widowers); their mean age was 53.05 years. Results showed that self-blame was associated with grief at the initial time-point and with its decline over time. Such associations were not found for depression. Initial levels of regret were neither associated with initial levels of grief and depression, nor were they related to the decline over time in either outcome variable. These results demonstrate the importance of examining guilt-related manifestations independently, over time, and with respect to both generic and grief-specific outcome variables. A main conclusion is that self-blame (but not regret) is a powerful determinant of grief-specific difficulties following the loss of a loved one. Implications for intervention are considered.",0
https://doi.org/10.1093/oxfordhb/9780199233281.013.0020,Disorders of the auditory brain,"© Oxford University Press, 2010. All rights reserved. This article is concerned with the disorders of the auditory system and their clinical assessment. It shows the importance of obtaining the history and neurological examination before detailed psychophysical testing. The assessment of disorders of the central auditory system in clinic is a multidisciplinary exercise, involving a key collaboration between neurology and audiological medicine. Patients with a suspected disorder of the central auditory system require audiological evaluation of middle ear and cochlear function. Continuous EEG is sometimes needed to seek evidence of abnormal epileptic activity that might be associated with auditory processing disorders. The article considers disorders defined on the basis of positive auditory symptoms. It also discusses abnormal auditory cognition in common clinical disorders in which the auditory features do not define the disorder. It aims to define auditory deficits in terms of the level in the system at which they are caused.",0
https://doi.org/10.1080/01402382.2014.887878,The Non-Procedural Determinants of Responsiveness,"This article starts from the remarks by Peter Mair on the growing gap between responsiveness and responsibility – or middle-run responsiveness – and the declining capacity of parties to bridge that gap. It focuses on the empirical analysis of the association between economic and substantive democratic dimensions and responsiveness, which are highly relevant to the way in which parties compete and govern within contemporary democracies. Following an introduction of the topic, the second section puts forward key concepts and hypotheses; the third presents the operationalisation of the variables and the applied method; the fourth and primary empirical section of the article analyses the non-procedural determinants of political and economic responsiveness, including freedom and equality as well as several key economic structural factors. The concluding remarks recapitulate the main empirical findings and submit a number of aspects that party leaders ought to take into account when addressing the thorny issue ...",0
https://doi.org/10.1002/hec.1781,ACCOUNTING FOR BETWEEN-STUDY VARIATION IN INCREMENTAL NET BENEFIT IN VALUE OF INFORMATION METHODOLOGY,"Previous applications of value of information methods for determining optimal sample size in randomized clinical trials have assumed no between-study variation in mean incremental net benefit. By adopting a hierarchical model, we provide a solution for determining optimal sample size with this assumption relaxed. The solution is illustrated with two examples from the literature. Expected net gain increases with increasing between-study variation, reflecting the increased uncertainty in incremental net benefit and reduced extent to which data are borrowed from previous evidence. Hence, a trial can become optimal where current evidence is sufficient assuming no between-study variation. However, despite the expected net gain increasing, the optimal sample size in the illustrated examples is relatively insensitive to the amount of between-study variation. Further percentage losses in expected net gain were small even when choosing sample sizes that reflected widely different between-study variation.",0
https://doi.org/10.1177/01466210122031984,An Evaluation of a Markov Chain Monte Carlo Method for the Rasch Model,"The accuracy of the Gibbs sampling Markov chain monte carlo procedure was examined for estimating item and person ( .) parameters in the one-parameter logistic model. Four datasets were analyzed using the Gibbs sampling method, conditional maximum likelihood, marginal maximum likelihood, and joint maximum likelihood. Maximum likelihood and expected a posteriori. estimation methods were used with marginal maximum likelihood estimation of item parameters. Item parameter estimates from the four methods were almost identical;. estimates from Gibbs sampling were similar to those obtained from the expected a posteriori method.",0
https://doi.org/10.1093/oxfordhb/9780199934874.013.0013,Matching and Propensity Scores,"The popularity of matching techniques has increased considerably during the last decades. They are mainly used for matching treatment and control units to estimate causal treatment effects from observational studies or for integrating two or more data sets that share a common subset of covariates. In focusing on causal inference with observational studies, we discuss multivariate matching techniques and several propensity score methods, like propensity score matching, subclassification, inverse-propensity weighting, and regression estimation. In addition to the theoretical aspects, we give practical guidelines for implementing these techniques and discuss the conditions under which these techniques warrant a causal interpretation of the estimated treatment effect. In particular, we emphasize that the selection of covariates and their reliable measurement is more important than the choice of a specific matching strategy.",0
https://doi.org/10.4135/9781412986311.n19,Latent Variable Analysis: Growth Mixture Modeling and Related Techniques for Longitudinal Data,,0
https://doi.org/10.1111/j.1745-3984.2005.00012.x,Infeasibility in Automated Test Assembly Models: A Comparison Study of Different Methods,"Several techniques exist to automatically put together a test meeting a number of specifications. In an item bank, the items are stored with their characteristics. A test is constructed by selecting a set of items that fulfills the specifications set by the test assembler. Test assembly problems are often formulated in terms of a model consisting of restrictions and an objective to be maximized or minimized. A problem arises when it is impossible to construct a test from the item pool that meets all specifications, that is, when the model is not feasible. Several methods exist to handle these infeasibility problems. In this article, test assembly models resulting from two practical testing programs were reconstructed to be infeasible. These models were analyzed using methods that forced a solution (Goal Programming, Multiple-Goal Programming, Greedy Heuristic), that analyzed the causes (Relaxed and Ordered Deletion Algorithm (RODA), Integer Randomized Deletion Algorithm (IRDA), Set Covering (SC), and Item Sampling), or that analyzed the causes and used this information to force a solution (Irreducible Infeasible Set-Solver). Specialized methods such as the IRDA and the Irreducible Infeasible Set-Solver performed best. Recommendations about the use of different methods are given.",0
https://doi.org/10.1037/a0031541,A probabilistic clustering theory of the organization of visual short-term memory.,"Experimental evidence suggests that the content of a memory for even a simple display encoded in visual short-term memory (VSTM) can be very complex. VSTM uses organizational processes that make the representation of an item dependent on the feature values of all displayed items as well as on these items' representations. Here, we develop a probabilistic clustering theory (PCT) for modeling the organization of VSTM for simple displays. PCT states that VSTM represents a set of items in terms of a probability distribution over all possible clusterings or partitions of those items. Because PCT considers multiple possible partitions, it can represent an item at multiple granularities or scales simultaneously. Moreover, using standard probabilistic inference, it automatically determines the appropriate partitions for the particular set of items at hand and the probabilities or weights that should be allocated to each partition. A consequence of these properties is that PCT accounts for experimental data that have previously motivated hierarchical models of VSTM, thereby providing an appealing alternative to hierarchical models with prespecified, fixed structures. We explore both an exact implementation of PCT based on Dirichlet process mixture models and approximate implementations based on Bayesian finite mixture models. We show that a previously proposed 2-level hierarchical model can be seen as a special case of PCT with a single cluster. We show how a wide range of previously reported results on the organization of VSTM can be understood in terms of PCT. In particular, we find that, consistent with empirical evidence, PCT predicts biases in estimates of the feature values of individual items and also predicts a novel form of dependence between estimates of the feature values of different items. We qualitatively confirm this last prediction in 3 novel experiments designed to directly measure biases and dependencies in subjects' estimates.",0
https://doi.org/10.1080/10705511.2012.634712,Small Sample Properties of Bayesian Multivariate Autoregressive Time Series Models,"The aim of this study was to compare the small sample (N = 1, 3, 5, 10, 15) performance of a Bayesian multivariate vector autoregressive (BVAR-SEM) time series model relative to frequentist power and parameter estimation bias. A multivariate autoregressive model was developed based on correlated autoregressive time series vectors of varying lengths (T = 25, 50, 75, 100, 125) using Statistical Analysis System (SAS) version 9.2. Autoregressive components for the 5 series vectors included coefficients of .80, .70, .65, .50 and .40. Error variance components included values of .20, .20, .10, .15, and .15, with cross-lagged coefficients of .10, .10, .15, .10, and .10. A Monte Carlo study revealed that in comparison to frequentist methods, the Bayesian approach provided increased sensitivity for hypothesis testing and detecting Type I error.",1
https://doi.org/10.1177/0149206314539351,Social Influence Interpretation of Interpersonal Processes and Team Performance Over Time Using Bayesian Model Selection,"The team behavior literature is ambiguous about the relations between members’ interpersonal processes—task debate and task conflict—and team performance. From a social influence perspective, we show why members’ interpersonal processes determine team performance over time in small groups. Together, over time, dissenting in-group minorities who share information (via debate) with majorities, who selectively engage with them to consider their alternative proposals (via conflict), can improve their team performance (via innovation). The context/comparison model of social influence and its leniency contract extension to the special case of in-group minorities suggest a pattern of members’ interpersonal processes that unfolds over time to reconcile factions with the same social identity who hold different approaches to shared projects. Conditional on typical levels of task debate, we predict that (a) in early episodes, task conflict increases the relation between task debate and team performance; (b) in middle episodes, task conflict decreases the relation; and (c) in late episodes, task conflict increases the relation again. We explore our thesis using a longitudinal design with a sample of 60 student teams (360 individuals) working together for course credit over 5 months (21 weeks) to write a first business plan for a new venture. We use a multilevel structural equation modeling approach with Bayesian estimation. We found support for our theory expressed in informative hypotheses using Bayesian model selection. These results were not evident from conventional graphing and post hoc statistical probing of simple slopes against the null hypothesis.",0
https://doi.org/10.1007/bf00554126,Experimental analysis of choice,"Our paper reviews and summarizes the state-of-the-art in the design and analysis of consumer choice experiments. We emphasize experiments involving discrete choices, but also review related work on the design and analysis of ranking and resource allocation experiments. Major topics include 1) Choice experiments and conjoint analysis, 2) Random utility and constant utility probabilistic discrete choice models as a theoretical foundation for choice experiments, and 3) The design of choice experiments. Other topics include a) Experimental procedure, b) Model specification, c) Model estimation, and d) Model validation. Suggestions for future research are made with respect to each topic. Ã‚Â© 1991 Kluwer Academic Publishers.",0
https://doi.org/10.1111/j.1745-3984.1999.tb00557.x,Multiple Objective Test Assembly Problems,"Mathematical programming techniques for optimal test assembly are discussed. Most methods optimize a single objective: for instance, the amount of information in a test, subject to a number of constraints. However, some test assembly problems have multiple objectives. A recent example in the literature is the problem of assembling test that measure multiple traits, where the amount of information in the test about each different trait has to be maximized. The present paper proposes methods appropriate for solving multiple objective test assembly problems. An overview of multiple objective optimization methods is given. The impact of the method on the optimality of the solution is shown and the appropriateness of the methods is discussed. The methods are illustrated using an empirical example of a test assembly problem for a two-dimensional mathematics item pool.",0
https://doi.org/10.2333/bhmk.29.81,Beyond SEM: General Latent Variable Modeling,"This article gives an overview of statistical analysis with latent variables. Using traditional structural equation modeling as a starting point, it shows how the idea of latent variables captures a wide variety of statistical concepts, including random effects, missing data, sources of variation in hierarchical data, finite mixtures. latent classes, and clusters. These latent variable applications go beyond the traditional latent variable useage in psychometrics with its focus on measurement error and hypothetical constructs measured by multiple indicators. The article argues for the value of integrating statistical and psychometric modeling ideas. Different applications are discussed in a unifying framework that brings together in one general model such different analysis types as factor models, growth curve models, multilevel models, latent class models and discrete-time survival models. Several possible combinations and extensions of these models are made clear due to the unifying framework.",0
https://doi.org/10.3758/bf03214357,How to fit a response time distribution,"Among the most valuable tools in behavioral science is statistically fitting mathematical models of cognition to data-response time distributions, in particular. However, techniques for fitting distributions vary widely, and little is known about the efficacy of different techniques. In this article, we assess several fitting techniques by simulating six widely cited models of response time and using the fitting procedures to recover model parameters. The techniques include the maximization of likelihood and least squares fits of the theoretical distributions to different empirical estimates of the simulated distributions. A running example is used to illustrate the different estimation and fitting procedures. The simulation studies reveal that empirical density estimates are biased even for very large sample sizes. Some fitting techniques yield more accurate and less variable parameter estimates than do others. Methods that involve least squares fits to density estimates generally yield very poor parameter estimates.",0
https://doi.org/10.1111/j.1460-2466.2006.00317.x,"Democracy Based on Difference: Examining the Links Between Structural Heterogeneity, Heterogeneity of Discussion Networks, and Democratic Citizenship","This study explores the direct and indirect links between structural heterogeneity, network heterogeneity, and political participation. We review the often conflicting scholarship on discussion network heterogeneity and political participation and place it within a multilevel conceptual framework of heterogeneity. Based on this integrated theoretical model, our study uses a combination of macro-level and individual-level survey data from various sources. First, we use a cross-sectional national data set, based on a telephone survey with a probability sample of almost 800 adults. Second, we combine these individual-level data with county-level data on religious, political, and racial heterogeneity. Based on these data sets, we develop a path model linking structure, context, and networks into an integrated pathway to evaluate the direct and indirect effects of heterogeneity on political participation. Our results show positive links between structural and network heterogeneity that are both direct and indirect, that is, mediated through various communication processes.",0
https://doi.org/10.1007/bf02296961,An alternative two stage least squares (2SLS) estimator for latent variable equations,"The Maximum-likelihood estimator dominates the estimation of general structural equation models. Noniterative, equation-by-equation estimators for factor analysis have received some attention, but little has been done on such estimators for latent variable equations. I propose an alternative 2SLS estimator of the parameters in LISREL type models and contrast it with the existing ones. The new 2SLS estimator allows observed and latent variables to originate from nonnormal distributions, is consistent, has a known asymptotic covariance matrix, and is estimable with standard statistical software. Diagnostics for evaluating instrumental variables are described. An empirical example illustrates the estimator.",0
https://doi.org/10.1111/joop.12104,Data-analytic strategies for examining the effectiveness of daily interventions,"Interest in the use of ecological momentary interventions – that is, interventions that are implemented in participants' everyday lives – to change experiences and behaviours has grown rapidly in recent years. In particular, EMIs in which the intervention is delivered on a daily basis (daily interventions) can be easily combined with daily diary studies to analyse intervention effects on dependent variables (DVs) that fluctuate over time. This article first provides a typology of research designs to classify daily intervention studies according to (1) the type of assessment of the DV (global assessment on a small number of fixed occasions, daily assessment, and repeated assessment within days) and (2) the type of control (within-subjects vs. between-subjects designs) used in the studies. We then demonstrate how multilevel models can be used to examine the effects of a daily intervention on the mean levels of the DV. We differentiate between general effectiveness, differential effectiveness, and conditional effectiveness and show how these effects are represented in the models. As an illustration, we apply some of the models to data from a daily intervention workplace study (N = 51 full-time employees) that focused on the effects of savouring exercises on calm mood and vigour",0
https://doi.org/10.1177/1094428112457829,The Time Has Come,"The use of Bayesian methods for data analysis is creating a revolution in fields ranging from genetics to marketing. Yet, results of our literature review, including more than 10,000 articles publi...",0
https://doi.org/10.1177/0022022111430254,Unpacking Cultural Differences in Alexithymia,"The current study provides a cultural examination of alexithymia, a multifaceted personality construct that refers to a general deficit in the ability to identify and describe emotional states, and that has been linked to a number of psychiatric illnesses. Though this construct has been critiqued as heavily rooted in “Western” norms of emotional expression, it has not received much empirical attention from a cultural perspective. Recently, Ryder et al. (2008) found that higher levels of alexithymia among Chinese versus Euro-Canadian outpatients were explained by group differences in one component of alexithymia, externally oriented thinking (EOT); they proposed that Chinese cultural contexts may encourage EOT due to a greater emphasis on social relationships and interpersonal harmony rather than inner emotional experience. The current study examined the hypothesis that EOT is more strongly shaped by cultural values than are two other components of alexithymia, difficulty identifying feelings (DIF) and difficulty describing feelings (DDF). Euro-Canadian ( n = 271) and Chinese-Canadian ( n = 237) undergraduates completed measures of alexithymia and cultural values. Chinese-Canadians showed higher levels of EOT than Euro-Canadians ( p &lt; .001). EOT, and not DIF or DDF, was predicted by Modernization and Euro-American values in both groups. Furthermore, cultural values mediated the effect of group membership on levels of EOT. These results suggest that cultural differences in alexithymia may be explained by culturally based variations in the importance placed on emotions, rather than deficits in emotional processing. The study also raises questions about the measurement and meaning of EOT, particularly from a cross-cultural perspective.",0
https://doi.org/10.1207/s15327906mbr4004_4,Discrete Latent Markov Models for Normally Distributed Response Data,"Van de Pol and Langeheine (1990) presented a general framework for Markov modeling of repeatedly measured discrete data. We discuss analogical single indicator models for normally distributed responses. In contrast to discrete models, which have been studied extensively, analogical continuous response models have hardly been considered. These models are formulated as highly constrained multinormal finite mixture models (McLachlan & Peel, 2000). The assumption of conditional independence, which is often postulated in the discrete models, may be relaxed in the normal-based models. In these models, the observed correlation between two variables may thus be due to the presence of two or more latent classes and the presence of within-class dependence. The latter may be subjected to structural equation modeling. In addition to presenting various normal-based Markov models, we demonstrate how these models, formulated as multinormal finite mixtures, may be fitted using the freely available program Mx (Neale, Boker, Xie, & Maes, 2002). To illustrate the application of some of the models, we report the analysis of data relating to the understanding of the conservation of continuous quantity (i.e., a Piagetian construct).",0
https://doi.org/10.1111/j.1744-6570.1996.tb01801.x,USING RANDOM RATHER THAN FIXED EFFECTS MODELS IN META-ANALYSIS: IMPLICATIONS FOR SITUATIONAL SPECIFICITY AND VALIDITY GENERALIZATION,"Combining statistical information across studies (i.e., meta-analysis) is a standard research tool in applied psychology. The most common meta-analytic approach in applied psychology, the fixed effects approach, assumes that individual studies are homogeneous and are sampled from the same population. This model assumes that sampling error alone explains the majority of observed differences in study effect sizes and its use has lead some to challenge the notion of situational specificity in favor of validity generalization. We critique the fixed effects methodology and propose an advancement–the random effects model (RE) which provides estimates of how between-study differences influence the relationships under study. RE models assume that studies are heterogeneous since they are often conducted by different investigators under different settings. Parameter estimates of both models are compared and evidence in favor of the random effects approach is presented. We argue against use of the fixed effects model because it may lead to misleading conclusions about situational specificity.",0
https://doi.org/10.1890/11-1899.1,A generalized approach to modeling and estimating indirect effects in ecology,"The need to model and test hypotheses about complex ecological systems has led to a steady increase in use of path analytical techniques, which allow the modeling of multiple multivariate dependencies reflecting hypothesized causation and mechanisms. The aim is to achieve the estimation of direct, indirect, and total effects of one variable on another and to assess the adequacy of whole models. Path analytical techniques based on maximum likelihood currently used in ecology are rarely adequate for ecological data, which are often sparse, multi-level, and may contain nonlinear relationships as well as nonnormal response data such as counts or proportion data. Here I introduce a more flexible approach in the form of the joint application of hierarchical Bayes, Markov chain Monte Carlo algorithms, Shipley's d-sep test, and the potential outcomes framework to fit path models as well as to decompose and estimate effects. An example based on the direct and indirect interactions between ants, two insect herbivores, and a plant species demonstrates the implementation of these techniques, using freely available software.",0
https://doi.org/10.1080/03610919708813451,Information and other criteria in structural equation model selection,"This article presents the results of a simulation study evaluating information criteria in conjunction with other well-known criteria for model selection in structural equation modeling (SEM). Two sets of simulation experiments were performed. In both sets, sample sizes of n = 100,400,1000,6000 were used and the performance of 18 criteria was assessed by the frequency with which each of five analytic models was selected as best by each criterion in 500 replications. In the first set of experiments correctly specified analytic models (noncentrality parameter 0) were entertained in combination with misspecified ones, while in the second set all five models were misspecified. In both sets of experiments, we found that the information criteria perform better than the other criteria overall, but that Cudeck and Browne's cross-validation index ( CVI) remains an attractive option. Within the class of information criteria, Akaike's information criterion (AIC) is found to show some overfitting tendency. We demonst...",0
https://doi.org/10.1016/j.jrp.2013.09.004,The Mini-IPIP6: Tiny yet highly stable markers of Big Six personality,"Abstract We assessed the stability of a short-form six-factor personality measure over a one-year period in a large national probability sample (N = 4289). Personality was assessed using the Mini-IPIP6—a short-form measure assessing Extraversion, Agreeableness, Conscientiousness, Neuroticism, Openness to Experience, and Honesty-Humility. Standardized estimates calculated using Bayesian Structural Equation Modelling (BSEM) indicated that all six personality dimensions were extremely stable. An alternative model using Maximum Likelihood estimation, in which residual item variances were associated over repeated assessments, yielded similar findings. These results highlight the stability of personality in the general population, even when assessed using short-form scales. The use of Bayesian models to examine the stability of personality and their application for study of change in specific developmental periods is discussed.",0
https://doi.org/10.1177/0146621611427898,A Comparison of the LR and DFIT Frameworks of Differential Functioning Applied to the Generalized Graded Unfolding Model,"Recently, applied psychological measurement researchers have become interested in the application of the generalized graded unfolding model (GGUM), a parametric item response theory model that posits an ideal point conception of the relationship between latent attributes and observed item responses. Little attention has been given to considerations for the detection of differential item functioning (DIF) under the GGUM. In this article, the authors present a Monte Carlo simulation meant to assess the efficacy of the likelihood ratio (LR) and differential functioning of items and tests (DFIT) frameworks, two popular ways of detecting DIF. Findings indicate a marked superiority of the LR approach over DFIT in terms of true and false positive rates under the GGUM. The discussion centers on possible explanations for the poor performance of the DFIT framework in detecting DIF under the GGUM and addresses limitations of the current study as well as future research directions.",0
https://doi.org/10.1121/1.399058,Stimulus selection in adaptive psychophysical procedures,"In adaptive psychophysical procedures, the stimulus should be presented at a relatively high level rather than near the middle of the psychometric function, which is often defined as the ""threshold"" value. For some psychometric functions, the optimal stimulus placement level produces 84% to 94% correct responses in a two-alternative forced-choice task. This result is disquieting because the popular two-down one-up rule tracks a relatively low percentage of correct responses, 70.7%. Computer simulations and a variety of psychometric functions were used to confirm the validity of this analysis. These simulations also demonstrate that the precise form of the psychometric function is not critical in achieving the high efficiencies. Finally, data from human listeners indicate that the standard deviation of threshold estimates is indeed larger when the stimulus presented on each trial is at a stimulus level corresponding to 70.7% rather than 94% correct responses.",0
https://doi.org/10.1016/j.schres.2006.01.022,Switch and maintenance of task set in schizophrenia,"Task set maintenance and switching deficits are robust in schizophrenia. However, little is known about how these constructs are related to one another. The development of an improved understanding of set switching and maintenance deficits in schizophrenia requires that these constructs be explicated in terms of elementary cognitive processes rather than grouped into broad psychological concepts like executive functioning. A relevant dichotomy has been proposed in which sensory and perceptual (""attentional"") processes are distinguished from decisional (""intentional"") processes in task maintenance and switching; however, the contributions these processes make to performance deficits in schizophrenia is not known. In the present study, 30 participants with schizophrenia and 27 healthy comparisons completed a cued attentional set switching task. In addition to analyses of mean response times, the contributions of attentional and intentional processes to task performance were estimated using an ex-Gaussian distributional analysis. Schizophrenia was associated with a set maintenance deficit that was accounted for by an attentional, rather than intentional, dysfunction. Both groups showed significant switch costs that could be attributed to attentional processes, but there was no evidence for an attentional set switching deficit in schizophrenia. The findings suggest that set switching and set maintenance may reflect distinct cognitive deficits in schizophrenia and that they may be associated with unique information processing mechanisms.",0
https://doi.org/10.1080/03610918.2014.983648,Clustered data with small sample sizes: Comparing the performance of model-based and design-based approaches,Two classes of methods properly account for clustering of data: design-based methods and model-based methods. Estimates from both methods have been shown to be approximately equal with large sample...,0
https://doi.org/10.1080/10705511.2014.994744,A Specification Error Test That Uses Instrumental Variables to Detect Latent Quadratic and Latent Interaction Effects,"The relations between the latent variables in structural equation models are typically assumed to be linear in form. This article aims to explain how a specification error test using instrumental variables (IVs) can be employed to detect unmodeled interactions between latent variables or quadratic effects of latent variables. An empirical example is presented, and the results of a simulation study are reported to evaluate the sensitivity and specificity of the test and compare it with the commonly employed chi-square model test. The results show that the proposed test can identify most unmodeled latent interactions or latent quadratic effects in moderate to large samples. Furthermore, its power is higher when the number of indicators used to define the latent variables is large. Altogether, this article shows how the IV-based test can be applied to structural equation models and that it is a valuable tool for researchers using structural equation models.",0
https://doi.org/10.3758/s13428-016-0823-0,Performance of growth mixture models in the presence of time-varying covariates,"Growth mixture modeling is often used to identify unobserved heterogeneity in populations. Despite the usefulness of growth mixture modeling in practice, little is known about the performance of this data analysis technique in the presence of time-varying covariates. In the present simulation study, we examined the impacts of five design factors: the proportion of the total variance of the outcome explained by the time-varying covariates, the number of time points, the error structure, the sample size, and the mixing ratio. More precisely, we examined the impact of these factors on the accuracy of parameter and standard error estimates, as well as on the class enumeration accuracy. Our results showed that the consistent Akaike information criterion (CAIC), the sample-size-adjusted CAIC (SCAIC), the Bayesian information criterion (BIC), and the integrated completed likelihood criterion (ICL-BIC) proved to be highly reliable indicators of the true number of latent classes in the data, across design conditions, and that the sample-size-adjusted BIC (SBIC) also proved quite accurate, especially in larger samples. In contrast, the Akaike information criterion (AIC), the entropy, the normalized entropy criterion (NEC), and the classification likelihood criterion (CLC) proved to be unreliable indicators of the true number of latent classes in the data. Our results also showed that substantial biases in the parameter and standard error estimates tended to be associated with growth mixture models that included only four time points.",0
https://doi.org/10.1177/0001699315579923,Reciprocity as a trigger of social cooperation in contemporary immigration societies?,"While the system stabilizing function of reciprocity is widely acknowledged, much less attention has been paid to the argument that reciprocity might initiate social cooperation in the first place. This paper tests Gouldner’s early assumption that reciprocity may act as a ‘starting mechanism’ of social cooperation in consolidating societies. The empirical test scenario builds on unequal civic engagement between immigrants and nationals, as this engagement gap can be read as a lack of social cooperation in consolidating immigration societies. Empirical analyses using survey data on reciprocal norms and based on Bayesian hierarchical modelling lend support for Gouldner’s thesis, underlining thereby the relevance of reciprocity in today’s increasingly diverse societies: individual norms of altruistic reciprocity elevate immigrants’ propensity to volunteer, reducing thereby the engagement gap between immigrants and natives in the area of informal volunteering. In other words, compliance with altruistic reciprocity may trigger cooperation in social strata, where it is less likely to occur. The positive moderation of the informal engagement gap through altruistic reciprocity turns out to be most pronounced for immigrants who are least likely to engage in informal volunteering, meaning low, but also highly educated immigrants.",0
https://doi.org/10.1007/bf02295291,The robustness of estimates of total indirect effects in covariance structure models estimated by maximum,"The large sample distribution of total indirect effects in covariance structure models in well known. Using Monte Carlo methods, this study examines the applicability of the large sample theory to maximum likelihood estimates oftotal indirect effects in sample sizes of 50, 100, 200, 400, and 800. Two models are studied. Model 1 is a recursive model with observable variables and Model 2 is a nonrecursive model with latent variables. For the large sample theory to apply, the results suggest that sample szes of 200 or more and 400 or more are required for models such as Model 1 and Model 2, respectively. Â© 1990 The Psychometric Society.",0
https://doi.org/10.1007/978-1-137-56077-3_9,Climate Change and Reproductive Intentions in Europe,"The harsh impacts of climate change and its related hazards are increasingly being felt across the world. A large consensus has emerged among natural scientists about the nature and the impact of climate change. It is recognized that climate change is largely anthropogenic and that, in turn, a continuous worsening of environmental conditions has strong impacts on populations’ and individuals’ well-being (Lutz, 2010).",0
https://doi.org/10.1080/15305058.2013.870903,Factorial Structure of the Family Values Scale From a Multilevel-Multicultural Perspective,"In cross-cultural research, there is a tendency for researchers to draw inferences at the country level based on individual-level data. Such action implicitly and often mistakenly assumes that both the measuring instrument and its underlying construct(s) are operating equivalently across both levels. Based on responses from 5,482 college students sampled from 27 countries, we took a structural equation modeling approach to addressing this issue of level equivalence. Purposes of the study were: (a) to validate the hypothesized two-factor structure of the Family Values Scale (FV Scale; Georgas, 1999) within a multilevel framework that took individual- and country-level information into account; (b) to test equivalence of the FV Scale across individual and country levels; and (c) to evaluate relations between the FV Scale and three possibly important covariates—gender at the individual level, and affluence and religion at the country level. Implications of findings and importance of multilevel equivalence in...",0
https://doi.org/10.1111/j.0006-341x.2001.01173.x,Shrinkage Estimators for Covariance Matrices,"Estimation of covariance matrices in small samples has been studied by many authors. Standard estimators, like the unstructured maximum likelihood estimator (ML) or restricted maximum likelihood (REML) estimator, can be very unstable with the smallest estimated eigenvalues being too small and the largest too big. A standard approach to more stably estimating the matrix in small samples is to compute the ML or REML estimator under some simple structure that involves estimation of fewer parameters, such as compound symmetry or independence. However, these estimators will not be consistent unless the hypothesized structure is correct. If interest focuses on estimation of regression coefficients with correlated (or longitudinal) data, a sandwich estimator of the covariance matrix may be used to provide standard errors for the estimated coefficients that are robust in the sense that they remain consistent under misspecification of the covariance structure. With large matrices, however, the inefficiency of the sandwich estimator becomes worrisome. We consider here two general shrinkage approaches to estimating the covariance matrix and regression coefficients. The first involves shrinking the eigenvalues of the unstructured ML or REML estimator. The second involves shrinking an unstructured estimator toward a structured estimator. For both cases, the data determine the amount of shrinkage. These estimators are consistent and give consistent and asymptotically efficient estimates for regression coefficients. Simulations show the improved operating characteristics of the shrinkage estimators of the covariance matrix and the regression coefficients in finite samples. The final estimator chosen includes a combination of both shrinkage approaches, i.e., shrinking the eigenvalues and then shrinking toward structure. We illustrate our approach on a sleep EEG study that requires estimation of a 24 x 24 covariance matrix and for which inferences on mean parameters critically depend on the covariance estimator chosen. We recommend making inference using a particular shrinkage estimator that provides a reasonable compromise between structured and unstructured estimators.",0
https://doi.org/10.1214/ss/1032280214,Bootstrap confidence intervals,"This article surveys bootstrap methods for producing good approximate confidence intervals. The goal is to improve by an order of magnitude upon the accuracy of the standard intervals $\hat{\theta} \pm z^{(\alpha)} \hat{\sigma}$, in a way that allows routine application even to very complicated problems. Both theory and examples are used to show how this is done. The first seven sections provide a heuristic overview of four bootstrap confidence interval procedures: $BC_a$, bootstrap-t , ABC and calibration. Sections 8 and 9 describe the theory behind these methods, and their close connection with the likelihood-based confidence interval theory developed by Barndorff-Nielsen, Cox and Reid and others.",0
https://doi.org/10.1002/sim.4172,Multivariate meta‐analysis: Potential and promise,"The multivariate random effects model is a generalization of the standard univariate model. Multivariate meta-analysis is becoming more commonly used and the techniques and related computer software, although continually under development, are now in place. In order to raise awareness of the multivariate methods, and discuss their advantages and disadvantages, we organized a one day 'Multivariate meta-analysis' event at the Royal Statistical Society. In addition to disseminating the most recent developments, we also received an abundance of comments, concerns, insights, critiques and encouragement. This article provides a balanced account of the day's discourse. By giving others the opportunity to respond to our assessment, we hope to ensure that the various view points and opinions are aired before multivariate meta-analysis simply becomes another widely used de facto method without any proper consideration of it by the medical statistics community. We describe the areas of application that multivariate meta-analysis has found, the methods available, the difficulties typically encountered and the arguments for and against the multivariate methods, using four representative but contrasting examples. We conclude that the multivariate methods can be useful, and in particular can provide estimates with better statistical properties, but also that these benefits come at the price of making more assumptions which do not result in better inference in every case. Although there is evidence that multivariate meta-analysis has considerable potential, it must be even more carefully applied than its univariate counterpart in practice.",0
https://doi.org/10.1016/j.jbusvent.2011.01.001,Corporate effectuation: Entrepreneurial action and its impact on R&amp;D project performance,"Abstract Innovative products are widely recognized as an important source of competitive advantage. However, many companies have difficulties finding efficient and successful approaches to different types of R&D projects, particularly those that involve a high level of innovativeness. Therefore, the present study moves effectuation theory from the entrepreneurial context to R&D research. First, the characteristics of an effectual approach in the context of R&D projects are developed and differentiated from those of conventional prediction-based strategies (causation). Second, using a thorough qualitative and quantitative scale-development process to capture particularities of effectual and causal dimensions in the R&D context, expert interviews and a pilot study (123 R&D projects), the study develops a multi-factor measurement model of effectuation and causation. These measures are validated in a follow-up study with a larger sample of 400 projects. Third, the new measures are applied to test two central hypotheses: (a) effectuation is positively related to success in highly innovative contexts, (b) causation approaches are beneficial in projects with low levels of innovativeness. Overall, this study moves the effectuation logic from the entrepreneurial to the corporate R&D context, captures its particularities, and investigates its performance outcomes.",0
https://doi.org/10.1177/0049124194023002003,Estimator Conditioning Diagnostics for Covariance Structure Models,This article studies the utility of a general set of diagnostics for assessing conditioning problems in the covariance structure modeling framework. The diagnostics are based on extensions of the condition index and variance decomposition proportions advanced by Belsley and are based on using the covariance matrix of the estimates. A series of simulations with a variety of covariance structure models as well as a real data example show that these diagnostics are useful for gauging the sensitivity of parameter estimates to conditioning problems arising from collinearity in the raw data. The relationship between ill-conditioning and local identification as it pertains to the proposed diagnostics is also discussed. It is suggested that these diagnostics be implemented in existing covariance structure modeling software.,0
https://doi.org/10.1093/biomet/92.2.419,Hierarchical models for assessing variability among functions,"SUMMARY In many applications of functional data analysis, summarising functional variation based on fits, without taking account of the estimation process, runs the risk of attributing the estimation variation to the functional variation, thereby overstating the latter. For example, the first eigenvalue of a sample covariance matrix computed from estimated functions may be biased upwards. We display a set of estimated neuronal Poisson-process intensity functions where this bias is substantial, and we discuss two methods for account ing for estimation variation. One method uses a random-coefficient model, which requires all functions to be fitted with the same basis functions. An alternative method removes the same-basis restriction by means of a hierarchical Gaussian process model. In a small simulation study the hierarchical Gaussian process model outperformed the random coefficient model and greatly reduced the bias in the estimated first eigenvalue that would result from ignoring estimation variability. For the neuronal data the hierarchical Gaussian process estimate of the first eigenvalue was much smaller than the naive estimate that ignored variability due to function estimation. The neuronal setting also illustrates the benefit of incorporating alignment parameters into the hierarchical scheme.",0
https://doi.org/10.1037/0021-9010.92.5.1394,Choosing the best method for local validity estimation: Relative accuracy of meta-analysis versus a local study versus Bayes-analysis.,"This study assessed the relative accuracy of 3 techniques--local validity studies, meta-analysis, and Bayesian analysis--for estimating test validity, incremental validity, and adverse impact in the local selection context. Bayes-analysis involves combining a local study with nonlocal (meta-analytic) validity data. Using tests of cognitive ability and personality (conscientiousness) as predictors, an empirically driven selection scenario illustrates conditions in which each of the 3 estimation techniques performs best. General recommendations are offered for how to estimate local parameters, based on true population variability and the number of studies in the meta-analytic prior. Benefits of empirical Bayesian analysis for personnel selection are demonstrated, and equations are derived to help guide the choice of a local validity technique (i.e., meta-analysis vs. local study vs. Bayes-analysis).",0
https://doi.org/10.1207/s15328007sem0803_7,An Illustration of Second-Order Latent Growth Models,"Methods of latent curve analysis (latent growth modeling) have recently emerged as a versatile tool for investigating longitudinal change in measured variables. This article, using higher order factor models as suggested by McArdle (1988) and Tisak and Meredith (1990), illustrates latent curve analysis for the purpose of modeling longitudinal change directly in a latent construct. The construct of interest is assumed to be indicated by several measured variables, all of which are observed at the same multiple time points. Examples with simultaneous estimation of covariance and mean structures are provided for both a single group and a two-group scenario.",0
https://doi.org/10.1371/journal.pone.0081823,Publication Bias in Recent Meta-Analyses,"Positive results have a greater chance of being published and outcomes that are statistically significant have a greater chance of being fully reported. One consequence of research underreporting is that it may influence the sample of studies that is available for a meta-analysis. Smaller studies are often characterized by larger effects in published meta-analyses, which can be possibly explained by publication bias. We investigated the association between the statistical significance of the results and the probability of being included in recent meta-analyses.For meta-analyses of clinical trials, we defined the relative risk as the ratio of the probability of including statistically significant results favoring the treatment to the probability of including other results. For meta-analyses of other studies, we defined the relative risk as the ratio of the probability of including biologically plausible statistically significant results to the probability of including other results. We applied a Bayesian selection model for meta-analyses that included at least 30 studies and were published in four major general medical journals (BMJ, JAMA, Lancet, and PLOS Medicine) between 2008 and 2012.We identified 49 meta-analyses. The estimate of the relative risk was greater than one in 42 meta-analyses, greater than two in 16 meta-analyses, greater than three in eight meta-analyses, and greater than five in four meta-analyses. In 10 out of 28 meta-analyses of clinical trials, there was strong evidence that statistically significant results favoring the treatment were more likely to be included. In 4 out of 19 meta-analyses of observational studies, there was strong evidence that plausible statistically significant outcomes had a higher probability of being included.Publication bias was present in a substantial proportion of large meta-analyses that were recently published in four major medical journals.",0
,A parameterization for individual human growth curves.,,0
https://doi.org/10.1037/0033-295x.85.2.59,A theory of memory retrieval.,"A theory of memory retrieval is developed and is shown to apply over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe-memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe-memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with experimental data. The theory is applied to four item recognition paradigms (Sternberg, prememorized list, study-test, and continuous) and to speed-accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.",0
,Bayesian Analyses of Mediational Models for Survival Outcome,,0
https://doi.org/10.1207/s15327906mbr3001_1,The Effect of Different Forms of Centering in Hierarchical Linear Models,"Multilevel models are becoming increasingly used in applied educational social and economic research for the analysis of hierarchically nested data. In these random coefficient regression models the parameters are allowed to differ over the groups in which the observations are nested. For computational ease in deriving parameter estimates, predictors are often centered around the mean. In nested or grouped data, the option of centering around the grand mean is extended with an option to center within groups or contexts. Both are statistically sound ways to improve parameter estimation. In this article we study the effects of these two different ways of centering, in comparison to the use of raw scores, on the parameter estimates in random coefficient models. The conclusion is that centering around the group mean amounts to fitting a different model from that obtained by centering around the grand mean or by using raw scores. The choice between the two options for centering can only be made on a theoretical basis. Based on this study, we conclude that centering rules valid for simple models, such as the fixed coefficients regression model. are no longer applicable to more complicated models, such as the random coefficient model. We think researchers should be made aware of the consequences of the choice of particular centering options.",0
https://doi.org/10.2307/2171961,Nonparametric Tests of Stochastic Dominance in Income Distributions,"Tests for stochastic dominance, based upon extensions of the Goodness of Fit Test to the nonparametric comparison of income distributions, are proposed, implemented, and compared with indirect tests of second order stochastic dominance currently utilized in income distribution studies.",0
https://doi.org/10.2307/270723,Asymptotic Confidence Intervals for Indirect Effects in Structural Equation Models,"For comments on an earlier draft of this chapter and for detailed advice I am indebted to Robert M. Hauser, Halliman H. Winsborough, and Toni Richards, several anonymous reviewers, and the editor of this volume. I also wish to thank John Raisian, Nancy Rytina, and Barbara Mann for their comments and Mark Wilson for able research assistance. The opinions expressed here are the sole responsibility of the author.",0
https://doi.org/10.1080/00031305.2013.817357,Logistic Regression With Multiple Random Effects: A Simulation Study of Estimation Methods and Statistical Packages,"Several statistical packages are capable of estimating generalized linear mixed models and these packages provide one or more of three estimation methods: penalized quasi-likelihood, Laplace, and Gauss–Hermite. Many studies have investigated these methods’ performance for the mixed-effects logistic regression model. However, the authors focused on models with one or two random effects and assumed a simple covariance structure between them, which may not be realistic. When there are multiple correlated random effects in a model, the computation becomes intensive, and often an algorithm fails to converge. Moreover, in our analysis of smoking status and exposure to antitobacco advertisements, we have observed that when a model included multiple random effects, parameter estimates varied considerably from one statistical package to another even when using the same estimation method. This article presents a comprehensive review of the advantages and disadvantages of each estimation method. In addition, we compare the performances of the three methods across statistical packages via simulation, which involves two- and three-level logistic regression models with at least three correlated random effects. We apply our findings to a real dataset. Our results suggest that two packages—SAS GLIMMIX Laplace and SuperMix Gaussian quadrature—perform well in terms of accuracy, precision, convergence rates, and computing speed. We also discuss the strengths and weaknesses of the two packages in regard to sample sizes.",0
https://doi.org/10.2307/271084,Direct and Indirect Effects: Classical and Bootstrap Estimates of Variability,"The decomposition of effects in structural equation models has been of considerable interest to social scientists. Finite-sample or asymptotic results for the sampling distribution of estimators of direct effects are widely available. Statistical inferences about indirect effects have relied exclusively on asymptotic methods which assume that the limiting distribution of the estimator is normal, with a standard error derived from the delta method. We examine bootstrap procedures as another way to generate standard errors and confidence intervals and to estimate the sampling distributions of estimators of direct and indirect effects. We illustrate the classical and the bootstrap methods with three empirical examples. We find that in a moderately large sample, the bootstrap distribution of an estimator is close to that assumed with the",0
https://doi.org/10.2307/2983328,Improved Approximations for Multilevel Models with Binary Responses,"SUMMARY This paper discusses the use of improved approximations for the estimation of generalized linear multilevel models where the response is a proportion. Simulation studies by Rodriguez and Goldman have shown that in extreme situations large biases can occur, most notably when the response is binary, the number of level 1 units per level 2 unit is small and the underlying random parameter values are large. An improved approximation is introduced which largely eliminates the biases in the situation described by Rodriguez and Goldman. Keywortis: �BINARY RESPONSE; GENERALIZED LINEAR MODEL; HIERARCHICAL DATA; MARGINAL MODEL; MULTILEVEL MODEL; QUASI-LIKELIHOOD; UNIT-SPECIFIC MODEL",0
https://doi.org/10.1080/10629360600903866,Plausibility of multivariate normality assumption when multiply imputing non-Gaussian continuous outcomes: a simulation assessment,"Multiple imputation under the assumption of multivariate normality has emerged as a frequently used model-based approach in dealing with incomplete continuous data in recent years. Despite its simplicity and popularity, however, its plausibility has not been thoroughly evaluated via simulation. In this work, the performance of multiple imputation under a multivariate Gaussian model with unstructured covariances was examined on a broad range of simulated incomplete data sets that exhibit varying distributional characteristics such as skewness and multimodality that are not accommodated by a Gaussian model. Behavior of efficiency and accuracy measures was explored to determine the extent to which the procedure works properly. The conclusion drawn is that although the real data rarely conform with multivariate normality, imputation under the assumption of normality is a fairly reasonable tool, even when the assumption of normality is clearly violated; the fraction of missing information is high, especially w...",0
https://doi.org/10.1016/j.lindif.2012.10.002,Exploring gains in reading and mathematics achievement among regular and exceptional students using growth curve modeling,"Abstract Using four-wave longitudinal reading and mathematics data (4th to 7th grades) from a large urban school district, growth curve modeling was used as a tool for examining three research questions: Are achievement gaps closing in reading and mathematics? What are the associations between prior-achievement and growth across the reading and mathematics domains? Is there an association between the receipt of additional services (special education, English-as-second-language, free and reduced lunch program) and reading and mathematics achievement? Results showed that rates of growth in achievement diminished over time and achievement gaps closed in reading, but not mathematics. Reading ability was directly related to gains in mathematics. Analysis of the time-varying covariates showed that there tended to be positive effects of the receipt of English-as-second language instruction on both reading and mathematics achievement, whereas students receiving special education and free and reduced lunch programs consistently had lower academic achievement levels. Implications for the achievement literature are discussed.",0
https://doi.org/10.1006/jmps.1994.1001,Stochastic Dependencies in Parallel and Serial Models: Effects on Systems Factorial Interactions,"Abstract This paper examines the behavior of stochastically dependent serial and parallel processing models in the setting of a 2 x 2 factorial experiment. Interactions found in factorial experiments can provide insight into the underlying mental architecture operating in a given psychological task. Recent theoretical results classify mental networks according to the types of factorial interaction they predict when selectivity of the factors is assumed. When one allows this selectivity to break down through a stochastic dependence between processes, the characteristic patterns associated with distinct architectures are disturbed. We investigate the relationships among: (a) parallel and serial architectures, (b) positive, negative, and zero dependencies, and (c) types of mean reaction time factorial interaction. In particular, we show that in some cases, observable symptoms of a stochastic dependence arise. One of these, which we term a single factor reversal of mean processing times, arises as a result of a negative dependence under certain circumstances. Another characteristic of dependent systems that may contribute to their identifiability is that the interactions can be subadditive for some levels of the factors and superadditive at other levels. This change in contrast is not possible for independent serial and parallel models with selective influence.",0
,A stochastic unfolding model derived from the partial credit model,,0
https://doi.org/10.3102/1076998609332752,An Integrated Bayesian Model for DIF Analysis,"In this article, an integrated bayesian model for differential item functioning (DIF) analysis is proposed. The model is integrated in the sense of modeling the responses along with the DIF analysis. This approach allows DIF detection and explanation in a simultaneous setup. Previous empirical studies and/or subjective beliefs about the item parameters, including differential functioning behavior, may be conveniently expressed in terms of prior distributions. Values of indicator variables are estimated in the model, indicating which items have DIF and which do not; as a result, the data analyst may not be required to specify an “anchor set” of items that do not exhibit DIF a priori to identify the model. It reduces the iterative procedures that are commonly used for proficiency purification and DIF detection and explanation. Examples demonstrate the efficiency of this method in simulated and real situations.",0
https://doi.org/10.1037/a0033950,Indirect effects of fidelity to the family check-up on changes in parenting and early childhood problem behaviors.,"This study examines observations of client in-session engagement and fidelity of implementation to the Family Check-Up (FCU) as they relate to improvements in caregivers' positive behavior support (PBS) and children's problem behavior in the context of a randomized prevention trial. The psychometric properties of fidelity scores obtained with a new rating system are also explored.The FCU feedback sessions of 79 families with children with elevated problem behavior scores at age 2 were coded by trained raters of fidelity, who used an observational coding system developed specifically for this intervention model.Path analysis indicated that fidelity to the FCU results in greater caregiver engagement in the feedback session, which directly predicts improvements in caregivers' PBS 1 year later (β = 0.06, 95% CI [.007, .129]). Similarly, engagement and PBS directly predict reductions in children's problem behavior measured 2 years later (β = -0.24, 95% CI [-.664, -.019]).These results suggest fidelity within the context of this randomized intervention trial. Ratings of fidelity to the FCU covary with observed improvements in parenting and children's problem behavior in early childhood. Overall reliability of the fidelity scores was found to be acceptable, but some single-item reliability estimates were low, suggesting revisions to the rating system might be needed. Accurately assessing fidelity and understanding its relationship to change during intervention studies is an underdeveloped area of research and has revealed some inconsistent findings. Our results shed light on the mixed conclusions of previous studies, suggesting that future research ought to assess the role of intervening variable effects, such as observed engagement.",0
https://doi.org/10.1016/j.jeconom.2004.04.012,Practical propensity score matching: a reply to Smith and Todd,"Abstract This paper discusses propensity score matching in the context of Smith and Todd's (Does matching overcome Lalonde's critique of nonexperimental estimators, J. Econom., in press) reanalysis of Dehejia and Wahba (J. Am. Statist. Assoc. 97 (1999) 1053; National Bereau of Economics Research working Paper No. 6829, Rev. Econom. Statist., 2002, forthcoming). Propensity score methods require that a separate propensity score specification be estimated for each treatment group-comparison group combination. Furthermore, a researcher should always examine the sensitivity of the estimated treatment effect to small changes in the propensity score specification; this is a useful diagnostic on the quality of the comparison group. When these are borne in mind, propensity score methods are useful in analyzing all of the subsamples of the NSW data considered in Smith and Todd (Does matching overcome Lalonde's critique of nonexperimental estimators, J. Econom., in press).",0
https://doi.org/10.1093/acprof:oso/9780195173444.003.0006,Multilevel Autoregressive Modeling of Interindividual Differences in the Stability of a Process,"Most psychological phenomena have been studied through the consideration of data gathered at relatively few occasions of measurement over time. However, a current wave of diary-based studies has generated many databases with more intensively collected longitudinal data. The number of occasions can be greater than those gathered in a normal panel study, but less than the very high numbers of occasions typically found in the time-series domain. The same types of questions that could be answered based on a smaller number of occasions could also be proposed for these longer time series. Normally, scientific questions for which longitudinal data have been collected have been the focus with change and stability.",0
https://doi.org/10.1515/sagmb-2016-0051,A Bayesian semiparametric factor analysis model for subtype identification,"Abstract: Disease subtype identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to infer disease subtypes, which often lead to biologically meaningful insights into disease. Despite many successes, existing clustering methods may not perform well when genes are highly correlated and many uninformative genes are included for clustering due to the high dimensionality. In this article, we introduce a novel subtype identification method in the Bayesian setting based on gene expression profiles. This method, called BCSub, adopts an innovative semiparametric Bayesian factor analysis model to reduce the dimension of the data to a few factor scores for clustering. Specifically, the factor scores are assumed to follow the Dirichlet process mixture model in order to induce clustering. Through extensive simulation studies, we show that BCSub has improved performance over commonly used clustering methods. When applied to two gene expression datasets, our model is able to identify subtypes that are clinically more relevant than those identified from the existing methods.",0
https://doi.org/10.1002/hec.1198,Multilevel models for estimating incremental net benefits in multinational studies,"Multilevel models (MLMs) have been recommended for estimating incremental net benefits (INBs) in multicentre cost-effectiveness analysis (CEA). However, these models have assumed that the INBs are exchangeable and that there is a common variance across all centres. This paper examines the plausibility of these assumptions by comparing various MLMs for estimating the mean INB in a multinational CEA. The results showed that the MLMs that assumed the INBs were exchangeable and had a common variance led to incorrect inferences. The MLMs that included covariates to allow for systematic differences across the centres, and estimated different variances in each centre, made more plausible assumptions, fitted the data better and led to more appropriate inferences. We conclude that the validity of assumptions underlying MLMs used in CEA need to be critically evaluated before reliable conclusions can be drawn. Copyright © 2006 John Wiley & Sons, Ltd.",0
https://doi.org/10.1177/0013164409355693,The Multilevel Crossed Random Effects Growth Model for Estimating Teacher and School Effects: Issues and Extensions,"This article examines the multilevel linear crossed random effects growth model for estimating teacher and school effects from repeated measurements of student achievement. Results suggest that even a small degree of unmodeled nonlinearity can result in a substantial upward bias in the magnitude of the teacher effect, which raises concerns about its appropriateness for estimating teacher effects. To address this issue, a piecewise linear crossed random effect growth model is proposed. A comparison with the linear growth form shows that the piecewise specification provides more accurate estimates of teacher effects when achievement growth departs from linear growth across grade levels or over summer, which are prevalent conditions. Fitted examples using nationally representative data and Bayesian estimation methods are provided.",0
https://doi.org/10.1027/1614-2241/a000029,Sample Size and Accuracy of Estimates in Multilevel Models,"In a multilevel framework several researches have investigated the behavior of estimates in finite samples, particularly for continuous dependent variables. Some findings show poor precise estimates for the variance components. On the other hand, discrete response multilevel models have been investigated less widely. In this paper we analyze the influence of different factors on the accuracy of estimates and standard errors of estimates in a binary response 2-level model, through a Monte Carlo simulation study. We investigate the hypothesis of: (a) small sample sizes; (b) different intraclass correlation coefficients; (c) different numbers of quadrature points in the estimation procedure. Standard errors of estimates are studied through a noncoverage indicator. In all instances we have considered, the point estimates are unbiased (even with very small sample sizes), while the variance components are underestimated. The accuracy of the standard errors of variance estimates needs a very large number of groups.",0
https://doi.org/10.1007/s11336-012-9262-8,A Two-Step Bayesian Approach for Propensity Score Analysis: Simulations and Case Study,"A two-step Bayesian propensity score approach is introduced that incorporates prior information in the propensity score equation and outcome equation without the problems associated with simultaneous Bayesian propensity score approaches. The corresponding variance estimators are also provided. The two-step Bayesian propensity score is provided for three methods of implementation: propensity score stratification, weighting, and optimal full matching. Three simulation studies and one case study are presented to elaborate the proposed two-step Bayesian propensity score approach. Results of the simulation studies reveal that greater precision in the propensity score equation yields better recovery of the frequentist-based treatment effect. A slight advantage is shown for the Bayesian approach in small samples. Results also reveal that greater precision around the wrong treatment effect can lead to seriously distorted results. However, greater precision around the correct treatment effect parameter yields quite good results, with slight improvement seen with greater precision in the propensity score equation. A comparison of coverage rates for the conventional frequentist approach and proposed Bayesian approach is also provided. The case study reveals that credible intervals are wider than frequentist confidence intervals when priors are non-informative.",0
https://doi.org/10.1007/bf00170145,Analysis of longitudinal data using the hierarchical linear model,"The hierarchical linear model in a linear model with nested random coefficients, fruitfully used for multilevel research. A tutorial is presented on the use of this model for the analysis of longitudinal data, i.e., repeated data on the same subjects. An important advantage of this approach is that differences across subjects in the numbers and spacings of measurement occasions do not present a problem, and that changing covariates can easily be handled. The tutorial approaches the longitudinal data as measurements on populations of (subject-specific) functions.",0
https://doi.org/10.1111/1467-9574.00060,What are the advantages of MCMC based inference in latent variable models?,"Recent developments in Markov chain Monte Carlo [MCMC] methods have increased the popularity of Bayesian inference in many fields of research in economics, such as marketing research and financial econometrics. Gibbs sampling in combination with data augmentation allows inference in statistical/econometric models with many unobserved variables. The likelihood functions of these models may contain many integrals, which often makes a standard classical analysis difficult or even unfeasible. The advantage of the Bayesian approach using MCMC is that one only has to consider the likelihood function conditional on the unobserved variables. In many cases this implies that Bayesian parameter estimation is faster than classical maximum likelihood estimation. In this paper we illustrate the computational advantages of Bayesian estimation using MCMC in several popular latent variable models.",0
https://doi.org/10.1123/jsep.2014-0330,Bayesian Structural Equation Modeling in Sport and Exercise Psychology,"Bayesian statistics is on the rise in mainstream psychology, but applications in sport and exercise psychology research are scarce. In this article, the foundations of Bayesian analysis are introduced, and we will illustrate how to apply Bayesian structural equation modeling in a sport and exercise psychology setting. More specifically, we contrasted a confirmatory factor analysis on the Sport Motivation Scale II estimated with the most commonly used estimator, maximum likelihood, and a Bayesian approach with weakly informative priors for cross-loadings and correlated residuals. The results indicated that the model with Bayesian estimation and weakly informative priors provided a good fit to the data, whereas the model estimated with a maximum likelihood estimator did not produce a well-fitting model. The reasons for this discrepancy between maximum likelihood and Bayesian estimation are discussed as well as potential advantages and caveats with the Bayesian approach.",0
https://doi.org/10.1016/j.tvjl.2013.03.010,Effect of extended cefquinome treatment on clinical persistence or recurrence of environmental clinical mastitis,"The effectiveness of antibiotic treatment of clinical mastitis (CM) is classically evaluated using bacteriological cure, which provides a concise and objective way of assessing efficacy but does not reflect the situation in the field where persistence or recurrence of clinical signs lead to perceived treatment failure. If clinical signs persist or recur, intramammary (IMM) treatment is often extended or supplemented with parenteral therapy in the expectation of a more efficient elimination of clinical signs or a lower probability of recurrence. The objective of this study was to evaluate the efficacy against clinical persistence or recurrence of three cefquinome treatment regimes, standard 1.5-day intramammary (SIMM), 5-day extended intramammary (EIMM) and combination of EIMM plus 5-day extended parenteral (ECOMBO) treatment. The study was conducted on three dairy farms with a high recurrence rate of environmental mastitis. Efficacy was evaluated using a multi-level model at the quarter and at the cow level, based on the persistence or recurrence of clinical signs at any time during a 105-day period following the end of the initial treatment, independent of pathogen. The most prevalent pathogens were E. coli (16.9%) and S. uberis (11.97%). EIMM and ECOMBO significantly decreased the persistence or recurrence of CM by 8% and 6% at the quarter level and by 9% and 8% at the cow level, respectively. ECOMBO may not reduce the persistence or recurrence of CM beyond EIMM. Whilst extended treatment regimens offered an improved outcome in this study, the producer and practitioner need to carefully consider such regimens from the perspective of prudent antibiotic use.",0
https://doi.org/10.1287/mnsc.1100.1161,Assessing Joint Distributions with Isoprobability Contours,"We present a new method for constructing joint probability distributions of continuous random variables using isoprobability contours—sets of points with the same joint cumulative probability. This approach reduces the joint probability assessment into a one-dimensional cumulative probability assessment using a sequence of binary choices between various combinations of the variables of interest. The approach eliminates the need to assess directly the dependence, or association, between the variables. We discuss properties of isoprobability contours and methods for their assessment in practice. We also report results of a study in which subjects assessed the 50th percentile isoprobability contour of the joint distribution of weight and height. We use the data to show how to use the assessed contours to construct the joint distribution and to infer (indirectly) the dependence between the variables.",0
https://doi.org/10.4324/9781315092614-6,Two-Stage Least Squares Estimation of Interaction Effects,"This chapter provides a largely nontechnical description of an alternative technique to include interactions of latent variables in structural equation models. It also provides a generic description of a model with interactions of latent variables with multiple indicators. The chapter presents the two-stage least squares (2SLS) method to model such interactions. It discusses the D. A. Kenny and C. M. Judd and related methods for handling such interactions. If the new measurement equations and nonlinear constraints are included, Kenny and Judd showed that the coefficients for the nonlinear variables could be consistently estimated with generalized least squares estimation. The chapter compares the two techniques with the simulation and empirical data that Kenny and Judd provided in their original paper. It focuses on a 2SLS method to handle interactions of latent variables that have multiple indicators. The chapter concludes with comments and contrasts the alternative methods and their properties.",0
https://doi.org/10.1371/journal.pone.0087597,Modelling Pathways to Rubisco Degradation: A Structural Equation Network Modelling Approach,"'Omics analysis (transcriptomics, proteomics) quantifies changes in gene/protein expression, providing a snapshot of changes in biochemical pathways over time. Although tools such as modelling that are needed to investigate the relationships between genes/proteins already exist, they are rarely utilised. We consider the potential for using Structural Equation Modelling to investigate protein-protein interactions in a proposed Rubisco protein degradation pathway using previously published data from 2D electrophoresis and mass spectrometry proteome analysis. These informed the development of a prior model that hypothesised a pathway of Rubisco Large Subunit and Small Subunit degradation, producing both primary and secondary degradation products. While some of the putative pathways were confirmed by the modelling approach, the model also demonstrated features that had not been originally hypothesised. We used Bayesian analysis based on Markov Chain Monte Carlo simulation to generate output statistics suggesting that the model had replicated the variation in the observed data due to protein-protein interactions. This study represents an early step in the development of approaches that seek to enable the full utilisation of information regarding the dynamics of biochemical pathways contained within proteomics data. As these approaches gain attention, they will guide the design and conduct of experiments that enable 'Omics modelling to become a common place practice within molecular biology.",0
https://doi.org/10.1177/0146621612461727,The Influence of Item Calibration Error on Variable-Length Computerized Adaptive Testing,"Variable-length computerized adaptive testing (VL-CAT) allows both items and test length to be “tailored” to examinees, thereby achieving the measurement goal (e.g., scoring precision or classification) with as few items as possible. Several popular test termination rules depend on the standard error of the ability estimate, which in turn depends on the item parameter values. However, items are chosen on the basis of their parameter estimates, and capitalization on chance may occur. In this article, the authors investigated the effects of capitalization on chance on test length and classification accuracy in several VL-CAT simulations. The results confirm that capitalization on chance occurs in VL-CAT and has complex effects on test length, ability estimation, and classification accuracy. These results have important implications for the design and implementation of VL-CATs.",0
https://doi.org/10.1016/j.neuroscience.2009.09.046,Evidence for a role of heat shock protein-90 in toll like receptor 4 mediated pain enhancement in rats,"Spinal cord microglial toll-like receptor 4 (TLR4) has been implicated in enhancing neuropathic pain and opposing morphine analgesia. The present study was initiated to explore TLR4-mediated pain modulation by intrathecal lipopolysaccharide, a classic TLR4 agonist. However, our initial study revealed that intrathecal lipopolysaccharide failed to induce low-threshold mechanical allodynia in naive rats, suggestive that TLR4 agonism may be insufficient to enhance pain. These studies explore the possibility that a second signal is required; namely, heat shock protein-90 (HSP90). This candidate was chosen for study given its known importance as a regulator of TLR4 signaling. A combination of in vitro TLR4 cell signaling and in vivo behavioral studies of pain modulation suggest that TLR4-enhancement of neuropathic pain and TLR4-suppression of morphine analgesia each likely require HSP90 as a cofactor for the effects observed. In vitro studies revealed that dimethyl sulfoxide (DMSO) enhances HSP90 release, suggestive that this may be a means by which DMSO enhances TLR4 signaling. While 2 and 100 microg lipopolysaccharide intrathecally did not induce mechanical allodynia across the time course tested, co-administration of 1 microg lipopolysaccharide with a drug that enhances HSP90-mediated TLR4 signaling now induced robust allodynia. In support of this allodynia being mediated via a TLR4/HSP90 pathway, it was prevented or reversed by intrathecal co-administration of a HSP90 inhibitor, a TLR4 inhibitor, a microglia/monocyte activation inhibitor (as monocyte-derived cells are the predominant cell type expressing TLR4), and interleukin-1 receptor antagonist (as this proinflammatory cytokine is a downstream consequence of TLR4 activation). Together, these results suggest for the first time that TLR4 activation is necessary but not sufficient to induce spinally mediated pain enhancement. Rather, the data suggest that TLR4-dependent pain phenomena may require contributions by multiple components of the TLR4 receptor complex.",0
https://doi.org/10.1080/87565640801982486,Longitudinal Associations Between Reading and Mathematics Achievement,"The association between early reading skills and changes in mathematics was examined in a large, low-income sample to determine whether students who have a greater level of reading skills in early elementary school exhibit more rapid gains in tests of mathematics. The longitudinal associations between third grade reading comprehension and changes in three components of mathematics achievement (Problem Solving and Data Interpretation, Mathematical Concepts and Estimation, Mathematical Computation) from third through eighth grade were examined. Latent growth models were fit to the repeated assessments of each mathematics component and the students' third grade reading and global mathematics scores were included as predictors of the intercept and slope. Gender, poverty status, and ethnicity were included in the models as control variables. The results showed males and African-American students tended to have shallower rates of change than females and non-African-American/non-Hispanic students. In terms of the effect of reading on changes in mathematics, third grade reading comprehension was found to be a positive significant predictor of change for each component of mathematics, suggesting students with a greater level of reading achievement in early elementary school change more rapidly in mathematics skills controlling for prior mathematics skills and student characteristics. The largest effects were shown for the Problem Solving and Data Interpretation test, a test focused on the applications of mathematics knowledge, and the Mathematical Concepts and Estimation test. Negligible effects were found for changes in Mathematical Computation. Thus, early reading comprehension was shown to be related to a conceptual understanding of mathematics and the application of mathematics knowledge. These findings lend support for the notion that early reading skills are important for success in mathematics.",0
https://doi.org/10.1007/bf02294733,Markov chain estimation for test theory without an answer key,"This study develops Markov Chain Monte Carlo (MCMC) estimation theory for the General Condorcet Model (GCM), an item response model for dichotomous response data which does not presume the analyst knows the correct answers to the test a priori (answer key). In addition to the answer key, respondent ability, guessing bias, and difficulty parameters are estimated. With respect to data-fit, the study compares between the possible GCM formulations, using MCMC-based methods for model assessment and model selection. Real data applications and a simulation study show that the GCM can accurately reconstruct the answer key from a small number of respondents.",0
https://doi.org/10.1207/s15327906mbr3502_1,Design and Analysis of Monte Carlo Experiments: Attacking the Conventional Wisdom,"The design and analysis of Monte Carlo experiments, with special reference to structural equation modelling, is discussed in this article. These topics merit consideration, since the validity of the conclusions drawn from a Monte Carlo study clearly hinges on these features. It is argued that comprehensive Monte Carlo experiments can be implemented on a PC if the experiments are adequately designed. This is especially important when investigating modern computer intensive methodologies like resampling and Markov Chain Monte Carlo methods. We are faced with three fundamental challenges in Monte Carlo experimentation. The first problem is statistical precision, which concerns the reliability of the obtained results. External validity, on the other hand, depends on the number of experimental conditions, and is crucial for the prospects of generalising the results beyond the specific experiment. Finally, we face the constraint on available computer resources. The conventional wisdom in designing and analysing Monte Carlo experiments embodies no explicit specification of meta-model for analysing the output of the experiment, the use of case studies or full factorial designs as experimental plans, no use of variance reduction techniques, a large number of replications, and ""eyeballing"" of the results. A critical examination of the conventional wisdom is presented in this article. We suggest that the following alternative procedures should be considered. First of all, we argue that it is profitable to specify explicit meta-models, relating the chosen performance statistics and experimental conditions. Regarding the experimental plan, we recommend the use of incomplete designs, which will often result in considerable savings. We also consider the use of common random numbers in the simulation phase, since this may enhance the precision in estimating meta-models. The use of fewer replications per trial, enabling us to investigate an increased number of experimental conditions, should also be considered in order to improve the external validity at the cost of the conventionally excessive precision.",0
https://doi.org/10.1214/aos/1176346785,Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician,"A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.",0
https://doi.org/10.1093/biostatistics/kxp032,Modeling between-trial variance structure in mixed treatment comparisons,"In mixed treatment comparison (MTC) meta-analysis, modeling the heterogeneity in between-trial variances across studies is a difficult problem because of the constraints on the variances inherited from the MTC structure. Starting from a consistent Bayesian hierarchical model for the mean treatment effects, we represent the variance configuration by a set of triangle inequalities on the standard deviations. We take the separation strategy (Barnard and others, 2000) to specify prior distributions for standard deviations and correlations separately. The covariance matrix of the latent treatment arm effects can be employed as a vehicle to load the triangular constraints, which in addition allows incorporation of prior beliefs about the correlations between treatment effects. The spherical parameterization based on Cholesky decomposition (Pinheiro and Bates, 1996) is used to generate a positive-definite matrix for the prior correlations in Markov chain Monte Carlo (MCMC). Elicited prior information on correlations between treatment arms is introduced in the form of its equivalent data likelihood. The procedure is implemented in a MCMC framework and illustrated with example data sets from medical research practice.",0
https://doi.org/10.1111/j.1541-0420.2005.00377.x,Doubly Robust Estimation in Missing Data and Causal Inference Models,"The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.",0
https://doi.org/10.1037/1082-989x.5.2.230,Testing for robustness in Monte Carlo studies.,"Monte Carlo studies provide the information needed to help researchers select appropriate analytical procedures under design conditions in which the underlying assumptions of the procedures are not met. In Monte Carlo studies, the 2 errors that one could commit involve (a) concluding that a statistical procedure is robust when it is not or (b) concluding that it is not robust when it is. In previous attempts to apply standard statistical design principles to Monte Carlo studies, the less severe of these errors has been wrongly designated the Type I error. In this article, a method is presented for controlling the appropriate Type I error rate; the determination of the number of iterations required in a Monte Carlo study to achieve desired power is described; and a confidence interval for a test's true Type I error rate is derived. A robustness criterion is also proposed that is a compromise between W. G. Cochran's (1952) and J. V. Bradley's (1978) criteria.",0
https://doi.org/10.2202/1557-4679.1195,Estimating Multilevel Logistic Regression Models When the Number of Clusters is Low: A Comparison of Different Statistical Software Procedures,"Multilevel logistic regression models are increasingly being used to analyze clustered data in medical, public health, epidemiological, and educational research. Procedures for estimating the parameters of such models are available in many statistical software packages. There is currently little evidence on the minimum number of clusters necessary to reliably fit multilevel regression models. We conducted a Monte Carlo study to compare the performance of different statistical software procedures for estimating multilevel logistic regression models when the number of clusters was low. We examined procedures available in BUGS, HLM, R, SAS, and Stata. We found that there were qualitative differences in the performance of different software procedures for estimating multilevel logistic models when the number of clusters was low. Among the likelihood-based procedures, estimation methods based on adaptive Gauss-Hermite approximations to the likelihood (glmer in R and xtlogit in Stata) or adaptive Gaussian quadrature (Proc NLMIXED in SAS) tended to have superior performance for estimating variance components when the number of clusters was small, compared to software procedures based on penalized quasi-likelihood. However, only Bayesian estimation with BUGS allowed for accurate estimation of variance components when there were fewer than 10 clusters. For all statistical software procedures, estimation of variance components tended to be poor when there were only five subjects per cluster, regardless of the number of clusters.",0
https://doi.org/10.1002/sim.2530,Bayesian analysis of latent variable models with non-ignorable missing outcomes from exponential family,"To provide a comprehensive framework for analysing complex non-normal medical and biological data, we propose a Bayesian approach for a non-linear latent variable model with covariates, and non-ignorable missing data, under the exponential family of distributions. The non-ignorable missing mechanism is defined via a logistic regression model. Based on conjugate prior distributions, full conditional distributions for the implementation of Markov chain Monte Carlo methods in simulating observations from the joint posterior distribution are derived. These observations are used in computing the Bayesian estimates, as well as in implementing a path sampling procedure to evaluate the Bayes factor for model comparison. The proposed methods are illustrated using real data from a study on the non-adherence of hypertension patients.",0
https://doi.org/10.1111/j.1540-5907.2006.00187.x,"The Institutional Context of Tolerance for Ethnic Minorities: A Comparative, Multilevel Analysis of Western Europe","Drawing on recent insights in the nationalism and citizenship regime literatures, this article develops a macrotheoretical framework for understanding cross-national variations in tolerance of ethnic minorities. Specifically, it tests the hypothesis that the degree to which the dominant ethnic tradition or culture is institutionalized in the laws and policies of a nation-state affects citizen tolerance of ethnic minorities. Employing a multilevel regression model, it systematically tests the framework, as well as competing individual and country-level explanations, for all member states of the European Union in 1997. Results confirm a strong relationship between the laws governing the acquisition and expression of citizenship, that is, citizenship regime type, and individual tolerance judgments. Moreover, citizenship regime type has a strong mediating effect on three individual-level variables previously shown to predict tolerance: ingroup national identity, political ideology, and satisfaction with democracy.",0
https://doi.org/10.1177/0013164410366693,Improving Cognitive Diagnostic Computerized Adaptive Testing by Balancing Attribute Coverage: The Modified Maximum Global Discrimination Index Method,"This article proposes a new item selection method, namely, the modified maximum global discrimination index (MMGDI) method, for cognitive diagnostic computerized adaptive testing (CD-CAT). The new method captures two aspects of the appeal of an item: (a) the amount of contribution it can make toward adequate coverage of every attribute and (b) the amount of contribution it can make toward recovering the latent cognitive profile. A simulation study shows that the new method ensures adequate coverage of every attribute, which improves the validity of the test scores, and defensibility of the proposed uses of the test. Furthermore, compared with the original global discrimination index method, the MMGDI method improves the recovery rate of each attribute and of the entire cognitive profile, especially the latter. Therefore, the new method improves both the validity and reliability of the test scores from a CD-CAT program.",0
https://doi.org/10.1016/j.ecresq.2014.05.010,A typical morning in preschool: Observations of teacher–child interactions in German preschools,"Abstract The study examined the applicability and generalizability of the Classroom Assessment Scoring System Pre-K (CLASS Pre-K; Pianta, La Paro, & Hamre, 2008) and the associated conceptual Teaching through Interaction framework to understand classroom processes in the German early education system. Three broad domains describe effective teacher–child interactions: Emotional Support, Classroom Organization, and Instructional Support. In the present study, we observed teacher–child interactions in 63 classrooms drawn from 26 different preschools using the CLASS Pre-K. Consistent with research from the United States, CLASS Pre-K scores demonstrated that the quality of teacher–child interactions varied widely. Data indicated that the levels of Emotional Support and Classroom Organization were moderate. In contrast, the level of Instructional Support was rather low and even decreased over the course of the morning. Furthermore, Emotional Support was found to decrease over the day in classrooms with a higher child–teacher ratio. Results have important implications for policy and practice with regard to the quality of care and education in German preschools.",0
https://doi.org/10.1016/j.compedu.2015.05.005,Becoming more specific: Measuring and modeling teachers' perceived usefulness of ICT in the context of teaching and learning,"Studies on teachers' acceptance and use of information and communication technology (ICT) have revealed perceived usefulness to be a crucial determinant for integrating ICT in classrooms. In consequence, the present study focuses on teachers' perceived usefulness of ICT for teaching and learning and is aimed at describing its structure and relations to self-efficacy, ICT use, and teachers' age. By means of Bayesian analysis, we specified confirmatory factor-analytic and structural equation models to a large-scale data set of N?=?1190 Norwegian teachers. Our results supported the hypothesized four-factor structure of teachers' perceived usefulness of ICT, signifying different facets of ICT-related teaching goals in classrooms. Moreover, it was possible to disentangle general and specific components of the construct in nested factor models. In support of existing research, we found positive relations to self-efficacy and ICT use, but a negative relation to teachers' age. Our study provides evidence on a multidimensional conceptualization of teachers' perceived usefulness of ICT for teaching and learning, and verifies the relations to teacher-related characteristics. Implications for the measurement and modeling of the construct, and future research directions are discussed. Teachers' perceived usefulness of ICT for teaching and learning is multifaceted.A general factor and specific factors can be distinguished.Bayesian models with cross-loadings represent the structure of the construct.Perceived usefulness is positively related to self-efficacy and ICT use.Perceived usefulness is negatively related to teachers' age.",0
https://doi.org/10.1080/00220973.2015.1027805,The Impact of Sample Size and Other Factors When Estimating Multilevel Logistic Models,"The design of research studies utilizing binary multilevel models must necessarily incorporate knowledge of multiple factors, including estimation method, variance component size, or number of predictors, in addition to sample sizes. This Monte Carlo study examined the performance of random effect binary outcome multilevel models under varying methods of estimation, level-1 and level-2 sample size, outcome prevalence, variance component sizes, and number of predictors using SAS software. Mean estimates of statistical power were influenced primarily by sample sizes at both levels. In addition, confidence interval coverage and width and the likelihood of nonpositive definite random effect covariance matrices were impacted by variance component size and estimation method. The interactions of these and other factors with various model performance outcomes are explored.",0
https://doi.org/10.1007/bf00162520,Accelerating Monte Carlo Markov chain convergence for cumulative-link generalized linear models,"The ordinal probit, univariate or multivariate, is a generalized linear model (GLM) structure that arises frequently in such disparate areas of statistical applications as medicine and econometrics. Despite the straightforwardness of its implementation using the Gibbs sampler, the ordinal probit may present challenges in obtaining satisfactory convergence. We present a multivariate Hastings-within-Gibbs update step for generating latent data and bin boundary parameters jointly, instead of individually from their respective full conditionals. When the latent data are parameters of interest, this algorithm substantially improves Gibbs sampler convergence for large datasets. We also discuss Monte Carlo Markov chain (MCMC) implementation of cumulative logit (proportional odds) and cumulative complementary log-log (proportional hazards) models with latent data. Ã‚Â© 1996 Chapman & Hall.",0
https://doi.org/10.2307/2983178,Bayesian Analysis of Realistically Complex Models,SUMMARY Models with complex structure arise in many social science applications and appear natural candidates for the use of Markov chain Monte Carlo methods for inference. Conditional independence assumptions simplify the model specification and make estimation using Gibbs sampling particularly appropriate. Two examples are discussed: random effects models for repeated ordered categorical data and sensitivity analysis to assumptions concerning the mechanism underlying informative drop-out in a longitudinal study. The use of a program BUGS is demonstrated.,0
https://doi.org/10.1002/jsid.233,Measurement of minimum angle of resolution (MAR) in the stereoscopic display using the optotype of stereoscopic stimuli,"In stereoscopic images, the crossing point of the viewing directions of the two eyes determines the perceived depth. Assuming that accommodation is affected by the positions of the crossing point, the effect of crossing point on minimum angle of resolution (MAR) was investigated. For 40 participants, MAR was measured by two-alternative forced choice where Snellen optotype E of up and down directions were used as two kinds of stimuli. As the crossing point of the viewing direction of the left and right eyes moves farther from the sample display, the ability to identify the direction of letter E decreases at the optotype of the same line thickness. The change of MAR shows linear trends with respect to the optical power change that are the reciprocal of the distance from the participant to the crossing points located out of screen and on screen.",0
https://doi.org/10.1016/j.neuroimage.2017.01.052,Bayesian longitudinal low-rank regression models for imaging genetic data from longitudinal studies,"To perform a joint analysis of multivariate neuroimaging phenotypes and candidate genetic markers obtained from longitudinal studies, we develop a Bayesian longitudinal low-rank regression (L2R2) model. The L2R2 model integrates three key methodologies: a low-rank matrix for approximating the high-dimensional regression coefficient matrices corresponding to the genetic main effects and their interactions with time, penalized splines for characterizing the overall time effect, and a sparse factor analysis model coupled with random effects for capturing within-subject spatio-temporal correlations of longitudinal phenotypes. Posterior computation proceeds via an efficient Markov chain Monte Carlo algorithm. Simulations show that the L2R2 model outperforms several other competing methods. We apply the L2R2 model to investigate the effect of single nucleotide polymorphisms (SNPs) on the top 10 and top 40 previously reported Alzheimer disease-associated genes. We also identify associations between the interactions of these SNPs with patient age and the tissue volumes of 93 regions of interest from patients' brain images obtained from the Alzheimer's Disease Neuroimaging Initiative.",0
https://doi.org/10.1016/j.ijresmar.2003.04.001,Retention of latent segments in regression-based marketing models,"Abstract Product design and marketing mix decisions for segmented markets depend crucially on the correct specification of marketing models used as input to these decisions. With real-world data, the true number of segments in a market is unknown. Current evidence from simulation studies suggests that the accuracy of commonly used criteria for determining the number of segments in a market depends on the usage context, including the type of distribution being used to describe the data, the model specification, and the characteristics of the market. This study investigates via simulation the performance of seven segment retention criteria used with finite mixture regression models for normal data. This is one of the most important analysis contexts in marketing research since regression models are used, for example, in conjoint analysis and market response analysis, yet no previous study in either the marketing or statistics literatures explores the segment retention problem for mixture regression models. The study shows that one criterion, Akaike's Information Criterion (AIC) with a per-parameter penalty factor of 3 (AIC3), is clearly the best criterion to use across a wide variety of model specifications and data configurations, having the highest success rate and producing very low parameter bias. Currently, this criterion is rarely, if ever, used in the marketing literature.",0
https://doi.org/10.1002/(sici)1097-0258(19990715)18:13<1587::aid-sim141>3.0.co;2-z,Analysing repeated measurements data: a practical comparison of methods,"A variety of methods are available for analysing repeated measurements data where the outcome is continuous. However, there is little information on how established methods, such as summary statistics and repeated measures analysis of variance (RMAOV), compare in practice with methods that have become available to applied statisticians more recently, such as marginal models (based on generalized estimating equation methodology) and multilevel models (that is, hierarchical random effects models). The aim of this paper is to exemplify the use of these methods, and directly compare their results by application to a clinical trial data set. The focus is on practical aspects rather than technical issues. The data considered were taken from a clinical trial of treatments for asthma in 240 children, in which a baseline and four post-randomization measurements of outcomes were taken. The simplicity of the method of summary statistics using the post-randomization mean of observations provided a useful initial analysis. However, fixed time effects or treatment-time interactions cannot be included in such an analysis, and choice of appropriate weighting when there is substantial missing data is problematic. RMAOV, marginal models and multilevel models generally provided similar estimates and standard errors for the treatment effects, although in one example with a relatively complex variance structure the marginal model produced less efficient estimates. Two advantages of multilevel models are that they provide direct estimates of variance components which are often of interest in their own right, and that they can be naturally extended to handle multivariate outcomes.",0
https://doi.org/10.1016/j.jpain.2006.11.006,Walker 256 Tumor-Bearing Rats as a Model to Study Cancer Pain,"An animal model of cancer pain induced by injection of Walker 256 carcinoma cells into the plantar surface of rat hind paw is described. Tumor growth and the occurrence of metastasis were investigated by histopathological analysis. Tumor cell growth was also analyzed plethysmographically by the increase in paw volume. For characterization of pain symptoms, hyperalgesia, allodynia, and spontaneous pain were evaluated 5 to 8 days after cell injection. The volume of the inoculated paw started to increase on day 2 after inoculation, being 40% higher on day 5 after injection. At this time, there was a marked proliferation of tumor cells, with the presence of anaplastic and pleomorphic cells, nucleoli, and atypical mitotic features. On days 7 and 8 after injection, histopathological analysis of popliteal lymph nodes showed the presence of tumor cells. The intraplantar injection of Walker 256 cells caused hyperalgesia at day 5 after cell inoculation. Low-threshold mechanical allodynia was significant 2 days after cell injection, being increased on day 5. In addition, inoculation of tumor cells induced gross behavior, characterized by a significant increase in licking and lifting of the injected paw 5 days after injection. The pain-enhancing effect caused by cell inoculation was partially inhibited by indomethacin on day 2 after cell injection, whereas morphine blocked allodynia on days 2 and 5. These results indicate that intraplantar injection of Walker 256 cells cause pain symptoms characteristic of cancer pain. This experimental model can then be used to investigate new analgesic or anti-tumor drugs.This article presents a new animal model for studying cancer pain and metastasis. This model could help in understanding the mechanisms involved in cancer pain symptoms and may be used for the investigation of new analgesic or anti-tumor drugs.",0
https://doi.org/10.1037/met0000145,A comparison of Bayesian and frequentist model selection methods for factor analysis models.,"We compare the performances of well-known frequentist model fit indices (MFIs) and several Bayesian model selection criteria (MCC) as tools for cross-loading selection in factor analysis under low to moderate sample sizes, cross-loading sizes, and possible violations of distributional assumptions. The Bayesian criteria considered include the Bayes factor (BF), Bayesian Information Criterion (BIC), Deviance Information Criterion (DIC), a Bayesian leave-one-out with Pareto smoothed importance sampling (LOO-PSIS), and a Bayesian variable selection method using the spike-and-slab prior (SSP; Lu, Chow, & Loken, 2016). Simulation results indicate that of the Bayesian measures considered, the BF and the BIC showed the best balance between true positive rates and false positive rates, followed closely by the SSP. The LOO-PSIS and the DIC showed the highest true positive rates among all the measures considered, but with elevated false positive rates. In comparison, likelihood ratio tests (LRTs) are still the preferred frequentist model comparison tool, except for their higher false positive detection rates compared to the BF, BIC and SSP under violations of distributional assumptions. The root mean squared error of approximation (RMSEA) and the Tucker-Lewis index (TLI) at the conventional cut-off of approximate fit impose much more stringent ""penalties"" on model complexity under conditions with low cross-loading size, low sample size, and high model complexity compared with the LRTs and all other Bayesian MCC. Nevertheless, they provided a reasonable alternative to the LRTs in cases where the models cannot be readily constructed as nested within each other. (PsycINFO Database Record",0
https://doi.org/10.1016/j.envsoft.2013.10.006,"Exploring the application of participatory modeling approaches in the Sonora River Basin, Mexico","This study presents the results from evaluation of a hydrologic modeling workshop for 46 water resource decision makers in Hermosillo, Mexico. This region has serious, ongoing water quantity and quality problems. Our goals were to assess participants' perceptions of our workshop and associated hydrologic and water quality models and to learn whether it changed their perceptions of local water resource-related problems, causes, and solutions. We administered on-site pre-and post-workshop surveys to assess any changes and to collect evaluations of the workshop and models. A few about water quality problems changed significantly over the course of the workshop, but most measured perceptions did not. On average, participants rated the workshop highly and believed that the presented models could assist their future decision-making. These results could contribute to future watershed modeling workshop efforts. A participatory watershed modeling workshop in Hermosillo, Mexico was evaluated.Pre and post workshop surveys assessed changes in participants' perceptions.A few perceptions regarding water quality problems changed significantly.Participants believed that the models presented could assist future decision making.",0
https://doi.org/10.1207/s15324818ame1804_2,Bayesian or Non-Bayesian: A Comparison Study of Item Parameter Estimation in the Three-Parameter Logistic Model,"Through a large-scale simulation study, this article compares item parameter estimates obtained by the marginal maximum likelihood estimation (MMLE) and marginal Bayes modal estimation (MBME) procedures in the 3-parameter logistic model. The impact of different prior specifications on the MBME estimates is also investigated using carefully selected prior distributions. The results indicate that, in general, the MBME provides more accurate item parameter estimates than the MMLE procedure. The impact of different priors on the Bayesian estimates is modest when the examinee sample size is not extremely small.",0
https://doi.org/10.1176/appi.ajp.2007.07040704,Prevalence of Mental Illness in Immigrant and Non-Immigrant U.S. Latino Groups,"Although widely reported among Latino populations, contradictory evidence exists regarding the generalizability of the immigrant paradox, i.e., that foreign nativity protects against psychiatric disorders. The authors examined whether this paradox applies to all Latino groups by comparing estimates of lifetime psychiatric disorders among immigrant Latino subjects, U.S-born Latino subjects, and non-Latino white subjects.The authors combined and examined data from the National Latino and Asian American Study and the National Comorbidity Survey Replication, two of the largest nationally representative samples of psychiatric information.In the aggregate, risk of most psychiatric disorders was lower for Latino subjects than for non-Latino white subjects. Consistent with the immigrant paradox, U.S.-born Latino subjects reported higher rates for most psychiatric disorders than Latino immigrants. However, rates varied when data were stratified by nativity and disorder and adjusted for demographic and socioeconomic differences across groups. The immigrant paradox consistently held for Mexican subjects across mood, anxiety, and substance disorders, while it was only evident among Cuban and other Latino subjects for substance disorders. No differences were found in lifetime prevalence rates between migrant and U.S.-born Puerto Rican subjects.Caution should be exercised in generalizing the immigrant paradox to all Latino groups and for all psychiatric disorders. Aggregating Latino subjects into a single group masks significant variability in lifetime risk of psychiatric disorders, with some subgroups, such as Puerto Rican subjects, suffering from psychiatric disorders at rates comparable to non-Latino white subjects. Our findings thus suggest that immigrants benefit from a protective context in their country of origin, possibly inoculating them against risk for substance disorders, particularly if they emigrated to the United States as adults.",0
https://doi.org/10.1080/10705511.2014.937322,Evaluation of a Bayesian Approach to Estimating Nonlinear Mixed-Effects Mixture Models,"The growth mixture model has become increasingly popular, given the willingness to acknowledge developmental heterogeneity in populations. Typically, linear growth mixture models, based on polynomials or piecewise functions, are used in substantive applications and evaluated quantitatively through simulation. Growth mixture models that follow inherently nonlinear trajectories, referred to as nonlinear mixed-effects mixture models, have received comparatively little attention—likely due to estimation complexity. Previous work on the estimation of these models has involved multistep routines (Kelley, 2008), maximum likelihood estimation (MLE) via the E-M algorithm (Harring, 2005, 2012), Taylor series expansion and MLE within the structural equation modeling framework (Grimm, Ram, & Estabrook, 2010), and MLE by adaptive Gauss–Hermite quadrature (Codd & Cudeck, 2014). This article proposes and evaluates the use of Bayesian estimation with OpenBUGS (Lunn, Spiegelhalter, Thomas, & Best, 2009), a free program, a...",1
https://doi.org/10.1111/j.1745-3984.1990.tb00746.x,The Construction of Customized Two-Stage Tests,"In this paper mixed integer linear programming models for customizing two-stage tests are given. Model constraints are imposed with respect to test composition, administration time, inter-item dependencies, and other practical considerations. It is not difficult to modify the models to make them useful in constructing multistage tests.",0
https://doi.org/10.1109/tpami.1984.4767596,"Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images","We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.",0
https://doi.org/10.1016/j.jmva.2009.04.015,Modeling covariance matrices via partial autocorrelations,"We study the role of partial autocorrelations in the reparameterization and parsimonious modeling of a covariance matrix. The work is motivated by and tries to mimic the phenomenal success of the partial autocorrelations function (PACF) in model formulation, removing the positive-definiteness constraint on the autocorrelation function of a stationary time series and in reparameterizing the stationarity-invertibility domain of ARMA models. It turns out that once an order is fixed among the variables of a general random vector, then the above properties continue to hold and follows from establishing a one-to-one correspondence between a correlation matrix and its associated matrix of partial autocorrelations. Connections between the latter and the parameters of the modified Cholesky decomposition of a covariance matrix are discussed. Graphical tools similar to partial correlograms for model formulation and various priors based on the partial autocorrelations are proposed. We develop frequentist/Bayesian procedures for modelling correlation matrices, illustrate them using a real dataset, and explore their properties via simulations.",0
https://doi.org/10.1037/a0030543,Bayesian parametric estimation of stop-signal reaction time distributions.,"The cognitive concept of response inhibition can be measured with the stop-signal paradigm. In this paradigm, participants perform a 2-choice response time (RT) task where, on some of the trials, the primary task is interrupted by a stop signal that prompts participants to withhold their response. The dependent variable of interest is the latency of the unobservable stop response (stop-signal reaction time, or SSRT). Based on the horse race model (Logan & Cowan, 1984), several methods have been developed to estimate SSRTs. None of these approaches allow for the accurate estimation of the entire distribution of SSRTs. Here we introduce a Bayesian parametric approach that addresses this limitation. Our method is based on the assumptions of the horse race model and rests on the concept of censored distributions. We treat response inhibition as a censoring mechanism, where the distribution of RTs on the primary task (go RTs) is censored by the distribution of SSRTs. The method assumes that go RTs and SSRTs are ex-Gaussian distributed and uses Markov chain Monte Carlo sampling to obtain posterior distributions for the model parameters. The method can be applied to individual as well as hierarchical data structures. We present the results of a number of parameter recovery and robustness studies and apply our approach to published data from a stop-signal experiment.",0
https://doi.org/10.3758/pbr.16.1.80,Integrating episodic memories and prior knowledge at multiple levels of abstraction,"Prior knowledge can have a large influence on recall when the memory for the original event is error prone or incomplete. We investigated the interaction between memory and prior knowledge in a recall task involving natural objects such as fruits and vegetables. We first quantified prior knowledge for the sizes of objects in a norming experiment. We then assessed the influence of prior knowledge in a memory experiment in which we compared the actual size of objects shown during a study phase with the reconstructed size of an object during the test phase. Recall was biased both by the mean size of the specific object studied and by the mean size of all objects in the category. This result suggests that the influence of prior knowledge can come from multiple, hierarchically related levels of representation, such as the object-category and superordinate-category levels.",0
https://doi.org/10.1007/s10182-011-0168-z,Longitudinal dynamic analyses of cognition in the health and retirement study panel,"The purpose of this paper is to highlight some classic issues in the measurement of change and to show how contemporary solutions can be used to deal with some of these issues. Five classic issues will be raised here: (1) Separating individual changes from group differences; (2) options for incomplete longitudinal data over time, (3) options for nonlinear changes over time; (4) measurement invariance in studies of changes over time; and (5) new opportunities for modeling dynamic changes. For each issue we will describe the problem, and then review some contemporary solutions to these problems base on Structural Equation Models (SEM). We will fit these SEM to using existing panel data from the Health & Retirement Study (HRS) cognitive variables. This is not intended as an overly technical treatment, so only a few basic equations are presented, examples will be displayed graphically, and more complete references to the contemporary solutions will be given throughout.",0
https://doi.org/10.1586/14737167.5.5.531,Self-perceived health status of schizophrenic patients in Spain: analysis of geographic differences,"This report explores the use of regression models for estimating health status of schizophrenic patients from a Bayesian perspective. The aims are: to obtain a set of values of health states of the EQ-5D based on self-assessed health from a sample of schizophrenic patients; and to analyze the differences in the health status and in patients' perceptions of their health status between four mental health districts in Spain. The authors develop two linear models with dummy variables. The first model seeks to obtain an index of the health status of the patients using a visual analog scale as a dependent variable and the different dimensions of EQ-5D as regressors. The second model enables analysis of the differences between the self-assessed health status in the different geographic areas and also the differences between the patients' self-assessed health states, irrespective of their actual health state, in the different geographic areas. The analysis is done using a Bayesian approach with Gibbs sampling (computer program WinBUGS 1.4). Data concerning self-assessed EQ-5D with visual analog scale from four geographic areas of schizophrenic patients were obtained for the purposes of this analysis. The health status index for this sample was obtained and the differences for this index between the four geographic areas were analyzed. The study reveals variables that explain the differences in patients' health status and health state assessment. Four possible scenarios are considered.",0
https://doi.org/10.1348/000711003321645403,Bayesian model selection for mixtures of structural equation models with an unknown number of components,"This paper considers mixtures of structural equation models with an unknown number of components. A Bayesian model selection approach is developed based on the Bayes factor. A procedure for computing the Bayes factor is developed via path sampling, which has a number of nice features. The key idea is to construct a continuous path linking the competing models; then the Bayes factor can be estimated efficiently via grids in [0, 1] and simulated observations that are generated by the Gibbs sampler from the posterior distribution. Bayesian estimates of the structural parameters, latent variables, as well as other statistics can be produced as by-products. The properties and merits of the proposed procedure are discussed and illustrated by means of a simulation study and a real example.",0
https://doi.org/10.1177/01466216980222005,An Investigation of the Item Parameter Recovery Characteristics of a Gibbs Sampling Procedure,"The item parameter recovery characteristics of a Gibb's sampling method (Albert, 1992) for IRT item parameter estimation were investigated using a simulation study. The item parameters were estimated, under a normal ogive item response function model, using Gibbs sampling and BILOG (Mislevy &amp; Bock, 1989). The item parameter estimates were then equated to the metric of the underlying item parameters for tests with 10, 20, 30, and 50 items, and samples of 30, 60, 120, and 500 examinees. Summary statistics of the equating coefficients showed that Gibbs sampling and BILOG both produced trait scale metrics with units of measurement that were too small, but yielding a proper midpoint of the metric. When expressed in a common metric, the biases of the BILOG estimates of the item discriminations were uniformly smaller and less variable than those from Gibbs sampling. The biases of the item difficulty estimates yielded by the two estimation procedures were small and similar to each other. In addition, the item parameter recovery characteristics were comparable for the largest dataset of 50 items and 500 examinees. However, for short tests and sample sizes the item parameter recovery characteristics of BILOG were superior to those of the Gibbs sampling approach.",0
https://doi.org/10.1007/978-1-4757-3692-2,Observational Studies,,0
https://doi.org/10.1016/0167-9473(96)82296-1,"A review of two different approaches for the analysis of growth data using longitudinal mixed linear models: Comparing hierarchical linear regression (ML3, HLM) and repeated measures designs with structured covariance matrices (BMDP5V)","Abstract In this paper we review two approaches for the analysis of growth data by means of longitudinal mixed linear models. In these models the individual growth parameters, (most often) specifying polynomial growth curves, may vary randomly across individuals. This variation may in turn be accounted for by explaining variables. The first approach we discuss, is a type of multilevel model in which growth data are treated as having a hierarchical structure: measurements are ‘nested’ within individuals. The second is a version of a MANOVA repeated measures model employing a structured (error)covariance matrix. Of both approaches we examine the underlying statistical models and their interrelations. Apart from this theoretical comparison we review software by which they can be applied for real data analysis: two multilevel programs, ML3 and HLM, and one repeated measures program, BMDP5V. The programs are described and discussed with respect to several more general criteria, such as data setup and handling, implemented numerical routines and user friendliness, and, in particular, with respect to their application in longitudinal situations, i.e. their capabilities for the analysis of data on growth. Two data sets are used to compare the results of analyses performed by the three programs. Although both ways of specifying growth curve models show some shortcomings, each appears to be a fruitful method to handle growth data, theoretically, as well as in a practical sense. For the most part, shortcomings are induced by the accompanying software, developed within different scientific traditions. Applied to comparable problems, the three programs produce equivalent results.",0
https://doi.org/10.1016/j.jmp.2004.01.002,A note on the sampling properties of the Vincentizing (quantile averaging) procedure,"Abstract To assess the effect of a manipulation on a response time distribution, psychologists often use Vincentizing or quantile averaging to construct group or “average” distributions. We provide a theorem characterizing the large sample properties of the averaged quantiles when the individual RT distributions all belong to the same location-scale family. We then apply the theorem to estimating parameters for the quantile-averaged distributions. From the theorem, it is shown that parameters of the group distribution can be estimated by generalized least squares. This method provides accurate estimates of standard errors of parameters and can therefore be used in formal inference. The method is benchmarked in a small simulation study against both a maximum likelihood method and an ordinary least-squares method. Generalized least squares essentially is the only method based on the averaged quantiles that is both unbiased and provides accurate estimates of parameter standard errors. It is also proved that for location-scale families, performing generalized least squares on quantile averages is formally equivalent to averaging parameter estimates from generalized least squares performed on individuals. A limitation on the method is that individual RT distributions must be members of the same location-scale family.",0
https://doi.org/10.1177/0146621606291571,Test Design Optimization in CAT Early Stage with the Nominal Response Model,"The early stage of computerized adaptive testing (CAT) refers to the phase of the trait estimation during the administration of only a few items. This phase can be characterized by bias and instability of estimation. In this study, an item selection criterion is introduced in an attempt to lessen this instability: the D-optimality criterion. A polytomous unconstrained CAT simulation is carried out to evaluate this criterion's performance under different test premises. The simulation shows that the extent of early stage instability depends primarily on the quality of the item pool information and its size and secondarily on the item selection criteria. The efficiency of the D-optimality criterion is similar to the efficiency of other known item selection criteria. Yet, it often yields estimates that, at the beginning of CAT, display a more robust performance against instability.",0
https://doi.org/10.1016/j.jclinepi.2014.08.012,Predictive distributions were developed for the extent of heterogeneity in meta-analyses of continuous outcome data,"Estimation of between-study heterogeneity is problematic in small meta-analyses. Bayesian meta-analysis is beneficial because it allows incorporation of external evidence on heterogeneity. To facilitate this, we provide empirical evidence on the likely heterogeneity between studies in meta-analyses relating to specific research settings.Our analyses included 6,492 continuous-outcome meta-analyses within the Cochrane Database of Systematic Reviews. We investigated the influence of meta-analysis settings on heterogeneity by modeling study data from all meta-analyses on the standardized mean difference scale. Meta-analysis setting was described according to outcome type, intervention comparison type, and medical area. Predictive distributions for between-study variance expected in future meta-analyses were obtained, which can be used directly as informative priors.Among outcome types, heterogeneity was found to be lowest in meta-analyses of obstetric outcomes. Among intervention comparison types, heterogeneity was lowest in meta-analyses comparing two pharmacologic interventions. Predictive distributions are reported for different settings. In two example meta-analyses, incorporating external evidence led to a more precise heterogeneity estimate.Heterogeneity was influenced by meta-analysis characteristics. Informative priors for between-study variance were derived for each specific setting. Our analyses thus assist the incorporation of realistic prior information into meta-analyses including few studies.",0
https://doi.org/10.1002/sim.6737,Variational methods for fitting complex Bayesian mixed effects models to health data,"We consider approximate inference methods for Bayesian inference to longitudinal and multilevel data within the context of health science studies. The complexity of these grouped data often necessitates the use of sophisticated statistical models. However, the large size of these data can pose significant challenges for model fitting in terms of computational speed and memory storage. Our methodology is motivated by a study that examines trends in cesarean section rates in the largest state of Australia, New South Wales, between 1994 and 2010. We propose a group-specific curve model that encapsulates the complex nonlinear features of the overall and hospital-specific trends in cesarean section rates while taking into account hospital variability over time. We use penalized spline-based smooth functions that represent trends and implement a fully mean field variational Bayes approach to model fitting. Our mean field variational Bayes algorithms allow a fast (up to the order of thousands) and streamlined analytical approximate inference for complex mixed effects models, with minor degradation in accuracy compared with the standard Markov chain Monte Carlo methods.",0
https://doi.org/10.1080/00273170802034810,Evaluating Group-Based Interventions When Control Participants Are Ungrouped,"Individually randomized treatments are often administered within a group setting. As a consequence, outcomes for treated individuals may be correlated due to provider effects, common experiences within the group, and/or informal processes of socialization. In contrast, it is often reasonable to regard outcomes for control participants as independent, given that these individuals are not placed into groups. Although this kind of design is common in intervention research, the statistical models applied to evaluate the treatment effects are usually inconsistent with the resulting data structure, potentially leading to biased inferences. This article presents an alternative model that explicitly accounts for the fact that only treated participants are grouped. In addition to providing a useful test of the overall treatment effect, this approach also permits one to formally determine the extent to which treatment effects vary over treatment groups and whether there is evidence that individuals within treatment groups become similar to one another. This strategy is demonstrated with data from the Reconnecting Youth program for high school students at risk of school failure and behavioral disorders.",0
https://doi.org/10.3402/ejpt.v6.27503,Latent Growth Mixture Models to estimate PTSD trajectories,"No abstract available. (Published: 2 March 2015) Citation: European Journal of Psychotraumatology 2015, 6 : 27503 - http://dx.doi.org/10.3402/ejpt.v6.27503 This paper is part of the Special Issue: Estimating PTSD trajectories . More papers from this issue can be found at http://www.ejpt.net",0
https://doi.org/10.1093/ije/dyq248,A proposed method of bias adjustment for meta-analyses of published observational studies,"Interpretation of meta-analyses of published observational studies is problematic because of numerous sources of bias. We develop bias assessment, elicitation and adjustment methods, and apply them to a systematic review of longitudinal observational studies of the relationship between objectively measured physical activity and subsequent change in adiposity in children.We separated internal biases that reflect study quality from external biases that reflect generalizability to a target setting. Since published results were presented in different formats, these were all converted to correlation coefficients. Biases were considered as additive or proportional on the correlation scale. Opinions about the extent of each bias in each study, together with its uncertainty, were elicited in a formal process from quantitatively trained assessors for the internal biases and subject-matter specialists for the external biases. Bias-adjusted results for each study were combined across assessors using median pooling, and results combined across studies by random-effects meta-analysis.Before adjusting for bias, the pooled correlation is difficult to interpret because the studies varied substantially in quality and design, and there was considerable heterogeneity. After adjusting for both the internal and external biases, the pooled correlation provides a meaningful quantitative summary of all available evidence, and the confidence interval incorporates the elicited uncertainties about the extent of the biases. In the adjusted meta-analysis, there was no apparent heterogeneity.This approach provides a viable method of bias adjustment for meta-analyses of observational studies, allowing the quantitative synthesis of evidence from otherwise incompatible studies. From the meta-analysis of longitudinal observational studies, we conclude that there is no evidence that physical activity is associated with gain in body fat.",0
https://doi.org/10.3758/bf03211350,Statistical properties of forced-choice psychometric functions: Implications of probit analysis,"Probit analysis was applied to the problem of threshold estimation from psychometric functions derived from the two-alternative forced-choice (2AFC) method of constant stimuli. Threshold estimates from 2AFC experiments are surprisingly poor: They are about twice as variable as corresponding estimates based on the traditional yes-no method of constant stimuli, and their asymmetrical confidence limits are not readily predicted from conventional standard error formulas. All of these faults are exacerbated in small samples. Computer simulations demonstrated that, for small samples, the probit analysis equations do not give a valid estimate of threshold variability. The variability of staircase estimates of threshold cannot be less than the variability of threshold estimates derived from the method of constant stimuli given an optimum placement of trials. Hence our findings also define the minimum variability of all staircase estimators under the assumptions of probit analysis.",0
https://doi.org/10.1007/s11336-011-9215-7,Item Selection in Multidimensional Computerized Adaptive Testing—Gaining Information from Different Angles,"Over the past thirty years, obtaining diagnostic information from examinees' item responses has become an increasingly important feature of educational and psychological testing. The objective can be achieved by sequentially selecting multidimensional items to fit the class of latent traits being assessed, and therefore Multidimensional Computerized Adaptive Testing (MCAT) is one reasonable approach to such task. This study conducts a rigorous investigation on the relationships among four promising item selection methods: D-optimality, KL information index, continuous entropy, and mutual information. Some theoretical connections among the methods are demonstrated to show how information about the unknown vector Î¸ can be gained from different perspectives. Two simulation studies were carried out to compare the performance of the four methods. The simulation results showed that mutual information not only improved the overall estimation accuracy but also yielded the smallest conditional mean squared error in most region of Î¸. In the end, the overlap rates were calculated to empirically show the similarity and difference among the four methods. Â© 2011 The Psychometric Society.",0
https://doi.org/10.1080/08957340701796464,Investigation of a Nonparametric Procedure for Assessing Goodness-of-Fit in Item Response Theory,"Tests of model misfit are often performed to validate the use of a particular model in item response theory. Douglas and Cohen (2001) introduced a general nonparametric approach for detecting misfit under the two-parameter logistic model. However, the statistical properties of their approach, and empirical comparisons to other methods, have not been examined. In the present study, a Monte Carlo simulation study was used to examine the empirical Type I error rates and power of two nonparametric statistics based on the Douglas and Cohen (2001) approach. The procedures are compared to two commonly used goodness-of-fit statistics, S-X 2 (Orlando & Thissen, 2000) and BILOG's G 2 (Mislevy & Bock, 1990), across conditions varied by test length, sample size, and the percentage of misfitting items. Overall, the nonparametrically based statistics controlled the Type I error rate and exhibited the most power across all conditions. Due to its close association with a graphical representation of the item response func...",0
https://doi.org/10.1080/15332861.2015.1080056,A Conceptual Framework for Measuring E-fulfillment Dimensions: A Consumer Perspective,"The purpose of this study is to propose a conceptual framework for measuring the key dimensions of the e-fulfillment process and its relationship with online shopping satisfaction in pure e-tailing. The key dimensions relevant to the e-fulfillment process are explored in the literature during the first part of the study. These dimensions are categorized into three distinct processes of e-fulfillment: order procurement, order fulfillment, and product returns. In the second part of the study, the authors propose a conceptual framework and develop hypotheses to understand the relationship between the key dimensions of the e-fulfillment process, including e-business quality, product quality, availability, timeliness, condition, billing accuracy, ease of return, and online shopping satisfaction. Furthermore, the constructs in the proposed framework were empirically tested. This study will help e-tailers, academicians, and practitioners understand the consumers’ expectations regarding the key dimensions of the ...",0
https://doi.org/10.1287/mksc.1040.0070,Multicollinearity and Measurement Error in Structural Equation Models: Implications for Theory Testing,"The literature on structural equation models is unclear on whether and when multicollinearity may pose problems in theory testing (Type II errors). Two Monte Carlo simulation experiments show that multicollinearity can cause problems under certain conditions, specifically: (1) when multicollinearity is extreme, Type II error rates are generally unacceptably high (over 80%), (2) when multicollinearity is between 0.6 and 0.8, Type II error rates can be substantial (greater than 50% and frequently above 80%) if composite reliability is weak, explained variance (R 2 ) is low, and sample size is relatively small. However, as reliability improves (0.80 or higher), explained variance R 2 reaches 0.75, and sample becomes relatively large, Type II error rates become negligible. (3) When multicollinearity is between 0.4 and 0.5, Type II error rates tend to be quite small, except when reliability is weak, R 2 is low, and sample size is small, in which case error rates can still be high (greater than 50%). Methods for detecting and correcting multicollinearity are briefly discussed. However, since multicollinearity is difficult to manage after the fact, researchers should avoid problems by carefully managing the factors known to mitigate multicollinearity problems (particularly measurement error).",0
https://doi.org/10.1210/jc.2013-3450,Efficacy of Vitamin D Supplementation in Depression in Adults: A Systematic Review,"Randomized controlled trials (RCTs) investigating the efficacy of vitamin D (Vit D) in depression provided inconsistent results.We aim to summarize the evidence of RCTs to assess the efficacy of oral Vit D supplementation in depression compared to placebo.We searched electronic databases, two conference proceedings, and gray literature by contacting authors of included studies.We selected parallel RCTs investigating the effect of oral Vit D supplementation compared with placebo on depression in adults at risk of depression, with depression symptoms or a primary diagnosis of depression.Two reviewers independently extracted data from relevant literature.Classical and Bayesian random-effects meta-analyses were used to pool relative risk, odds ratio, and standardized mean difference. The quality of evidence was assessed using the Grading of Recommendations Assessment, Development and Evaluation tool.Six RCTs were identified with 1203 participants (72% females) including 71 depressed patients; five of the studies involved adults at risk of depression, and one trial used depressed patients. Results of the classical meta-analysis showed no significant effect of Vit D supplementation on postintervention depression scores (standardized mean difference = -0.14, 95% confidence interval = -0.41 to 0.13, P = .32; odds ratio = 0.93, 95% confidence interval = 0.54 to 1.59, P = .79). The quality of evidence was low. No significant differences were demonstrated in subgroup or sensitivity analyses. Similar results were found when Bayesian meta-analyses were applied.There is insufficient evidence to support the efficacy of Vit D supplementation in depression symptoms, and more RCTs using depressed patients are warranted.",0
https://doi.org/10.1016/0167-7152(90)90100-l,Full maximum likelihood analysis of structural equation models with polytomous variables,Abstract This paper is concerned with the analysis of structural equation models with polytomous variables. Identification conditions for the basic model are discussed. Theory for the full simultaneous maximum likelihood estimation of the thresholds and the covariance structure parameters is developed. An example is presented to illustrate the method.,0
https://doi.org/10.1027/1614-2241/a000083,A Hierarchical Bayesian Model With Correlated Residuals for Investigating Stability and Change in Intensive Longitudinal Data Settings,"The present paper’s focus is the modeling of interindividual and intraindividual variability in longitudinal data. We propose a hierarchical Bayesian model with correlated residuals, employing an autoregressive parameter AR(1) for focusing on intraindividual variability. The hierarchical model possesses four individual random effects: intercept, slope, variability, and autocorrelation. The performance of the proposed Bayesian estimation is investigated in simulated longitudinal data with three different sample sizes (N = 100, 200, 500) and three different numbers of measurement points (T = 10, 20, 40). The initial simulation values are selected according to the results of the first 20 measurement occasions from a longitudinal study on working memory capacity in 9th graders. Within this simulation study, we investigate the root mean square error (RMSE), bias, relative percentage bias, and the 90% coverage probability of parameter estimates. Results indicate that more accurate estimates are associated with a larger sample size. One exception to this tendency is the autocorrelation parameter, which shows more sensitivity to an increasing number of time points.",0
https://doi.org/10.1002/9780470035948,Bayesian Statistical Modelling,"Bayesian methods combine the evidence from the data at hand with previous quantitative knowledge to analyse practical problems in a wide range of areas. The calculations were previously complex, but it is now possible to routinely apply Bayesian methods due to advances in computing technology and the use of new sampling methods for estimating parameters. Such developments together with the availability of freeware such as WINBUGS and R have facilitated a rapid growth in the use of Bayesian methods, allowing their application in many scientific disciplines, including applied statistics, public health research, medical science, the social sciences and economics. Following the success of the first edition, this reworked and updated book provides an accessible approach to Bayesian computing and analysis, with an emphasis on the principles of prior selection, identification and the interpretation of real data sets. The second edition: Provides an integrated presentation of theory, examples, applications and computer algorithms. Discusses the role of Markov Chain Monte Carlo methods in computing and estimation. Includes a wide range of interdisciplinary applications, and a large selection of worked examples from the health and social sciences. Features a comprehensive range of methodologies and modelling techniques, and examines model fitting in practice using Bayesian principles. Provides exercises designed to help reinforce the reader's knowledge and a supplementary website containing data sets and relevant programs. Bayesian Statistical Modelling is ideal for researchers in applied statistics, medical science, public health and the social sciences, who will benefit greatly from the examples and applications featured. The book will also appeal to graduate students of applied statistics, data analysis and Bayesian methods, and will provide a great source of reference for both researchers and students. Praise for the First Edition: ""It is a remarkable achievement to have carried out such a range of analysis on such a range of data sets. I found this book comprehensive and stimulating, and was thoroughly impressed with both the depth and the range of the discussions it contains."" - ISI - Short Book Reviews. ""This is an excellent introductory book on Bayesian modelling techniques and data analysis"" - Biometrics. ""The book fills an important niche in the statistical literature and should be a very valuable resource for students and professionals who are utilizing Bayesian methods."" - Journal of Mathematical Psychology. Â© 2006 John Wiley & Sons Ltd. All Rights Reserved.",0
https://doi.org/10.1037/1040-3590.19.1.88,Constructing personality scales under the assumptions of an ideal point response process: Toward increasing the flexibility of personality measures.,"The main aim of this article is to explicate why a transition to ideal point methods of scale construction is needed to advance the field of personality assessment. The study empirically demonstrated the substantive benefits of ideal point methodology as compared with the dominance framework underlying traditional methods of scale construction. Specifically, using a large, heterogeneous pool of order items, the authors constructed scales using traditional classical test theory, dominance item response theory (IRT), and ideal point IRT methods. The merits of each method were examined in terms of item pool utilization, model-data fit, measurement precision, and construct and criterion-related validity. Results show that adoption of the ideal point approach provided a more flexible platform for creating future personality measures, and this transition did not adversely affect the validity of personality test scores.",0
https://doi.org/10.1177/0022146514557332,"Education, Mental Health, and Education-Labor Market Misfit","Higher-educated people experience enhanced mental health. We ponder whether the mental health benefits of educational attainment are limitless. At the individual level, we look at the impact of job-education mismatch. At the societal level, we hypothesize that diminishing economic returns on education limit its mental health benefits. Using a subsample of individuals aged 20 to 65 years (N = 28,288) from 21 countries in the European Social Survey (ESS 2006), we estimate the impact on depressive symptoms of characteristics at both the employee level (years of education and job-education mismatch) and the labor market/country level (the gap between the nontertiary and tertiary educated in terms of unemployment risks and earnings). The results show that educational attainment produces mental health benefits in most European countries. However, in some of the countries, these benefits are limited or even completely eliminated by education-labor market misfit.",0
https://doi.org/10.1080/01621459.1993.10476426,Bayesian Analysis for the Poly-Weibull Distribution,"Abstract In this article Bayesian analysis for a Poly-Weibull distribution using informative priors is discussed. This distribution typically arises when the data is the minimum of several Weibull failure times from competing risks. To perform the Bayesian computations, simulation using the Gibbs sampler is suggested. This can be used to find posterior moments, the marginal posterior probability density function, and the predictive risk or reliability.",0
https://doi.org/10.2307/1937887,Multicollinearity in Regression Analysis: The Problem Revisited,"T O MOST economists, the single equation least-squares regression model, like an old friend, is tried and true. Its properties and limitations have been extensively studied and documented and are, for the most part, wellknown. Any good text in econometrics can lay out the assumptions on which common versions of the model are based and provide a reasonably coherent perhaps even a lucid discussion of problems that arise as particular assumptions are violated. A short bibliography of definitive papers on such classical problems as non-normality, heteroscedasticity, serial correlation, feedback, etc., completes the job. As with most old friends, however, the longer one knows least squares, the more one learns about it. An admiration for its robustness under departures from many assumptions is sure to grow. The admiration must be tempered, however, by an appreciation of the model's sensitivity to certain other conditions. The requirement that explanatory variables be truly independent of one another is one of these. Proper of the model's classical problems ordinarily involves two separate stages: detection and correction. The DurbinWatson test for serial correlation, combined with Cochrane and Orcutt's suggested first differencing procedure, is an obvious example.' Bartlett's test for variance heterogeneity followed by a data transformation to restore homoscedasticity is another.2 No such treatment has been developed, however, for problems that arise as multicollinearity is encountered in regression analysis. Attention will focus here on what we consider to be the first step in a proper of the multicollinearity problem its detection, or diagnosis. Economists are coming more and more to agree that the second step, correction, requires the generation of additional information.3 Just how this information is to be obtained depends largely on the tastes of an investigator and on the specifics of a particular problem. It may involve additional primary data collection, the use of extraneous parameter estimates from secondary data sources, or the application of subjective information through constrained regression, or through Bayesian estimation procedures. Whatever its source, however, selectivity and thereby efficiency in generating the added information requires a systematic procedure for detecting its need i.e., for detecting the existence, measuring the extent, and pinpointing the location and causes of multicollinearity within a set of independent variables. Measures are proposed here that, in our opinion, fill this need. The paper's basic organization can be outlined briefly as follows. In the next section the multicollinearity problem's basic, formal nature is developed and illustrated. A discussion of historical approaches to the problem follows. With this as background, an attempt is made to define multicollinearity in terms of departures from a hypothesized statistical condition, and * The authors are Associate Professor of Finance at the Sloan School of Management, M.I.T., and Assistant Professor of Business Administration at the Harvard Business School, respectively. We are indebted to Professor John R. Meyer for introducing us to the multicollinearity problem and for advice and encouragement during the present effort to place it in perspective, and to Professors John Lintner and Robert Schlaifer for their comments and criticisms. Responsibility for specific interpretations, especially erroneous ones, remains our own. This research was supported by the Institute of Naval Studies, of which both authors were members at the time the work was conducted, and by grants from the Ford Foundation to both the Sloan School of Management and the Harvard Business School. Computation time and facilities were provided by the Computation Centers of Harvard and M.I.T. 1J Durbin and G. S. Watson, Testing for Serial Correlation in Least Squares Regression, Biometrika, 37-38, (1950-1951); and C. Cochrane and G. H. Orcutt, Application of Least Squares Regression to Relationships Containing Autocorrelated Error Terms Journal of the American Statistical Association, 44 (1949). 2F. David and J. Neyman, Extension of the Markoff Theorem on Least Squares, Statistical Research Memoirs, II (London, 1938). 'J. Johnston, Econometric Methods (McGraw-Hill, 1963), 207; J. Meyer and R. Glauber, Investment Decisions, Economic Forecasting, and Public Policy (Division of Research, Graduate School of Business Administration, Harvard University, 1964), 181 ff.",0
,The function of the vibrissae in the behavior of the white rat,,0
https://doi.org/10.1177/002224379503200401,"Psychometric Methods in Marketing Research: Part I, Conjoint Analysis",,0
https://doi.org/10.4324/9780203848852-8,Beyond multilevel regression modeling: Multilevel analysis in a general latent variable framework.,"The outline of the chapter is as follows.  Section 2.2 discusses two extensions of  two-level regression analysis, Section 2.3  discusses two-level path analysis and structural equation modeling, Section 2.4 presents an example of two-level exploratory  factor analysis (EFA), Section 2.5 discusses  two-level growth modeling using a two-part  model, Section 2.6 discusses an unconventional approach to three-level growth modeling, and Section 2.7 presents an example  of multilevel growth mixture modeling.",0
https://doi.org/10.1177/0146621602239475,Small Sample Estimation in Dichotomous Item Response Models: Effect of Priors Based on Judgmental Information on the Accuracy of Item Parameter Estimates,"Large item banks with properly calibrated test items are essential for ensuring the validity of computer-based tests. At the same time, item calibrations with small samples are desirable to minimize the amount of pretesting and limit item exposure. Bayesian estimation procedures show considerable promise with small examinee samples. The purposes of the study were (a) to examine how prior information for Bayesian item parameter estimation can be specified and (b) to investigate the relationship between sample size and the specification of prior information on the accuracy of item parameter estimates. The results of the simulation study were clear: Estimation of IRT model item parameters can be improved considerably. Improvements in the one-parameter model were modest; considerable improvements with the two- and three-parameter models were observed. Both the study of different forms of priors and ways to improve the judgmental data used in forming the priors appear to be promising directions for future research.",0
https://doi.org/10.1016/j.ijresmar.2003.11.002,Capturing consumer heterogeneity in metric conjoint analysis using Bayesian mixture models,"Abstract We address unobserved preference heterogeneity within an omitted variable framework which provides a theoretical rationale for more continuous preference distributions, multivariate normal in the limit. A comparison of the random coefficients model (RCM) and the latent class model (LCM) using simulated data illustrates that the RCM dominates the LCM if the underlying distribution is strictly continuous. The LCM dominates the RCM if the underlying distribution is strictly discrete once the sample is informative enough to support the true number of classes. The simulation further documents that the optimal number of classes in an LCM is an unrestricted function of the sample size if the underlying distribution is continuous. Finally, we present an application to the mineral water market, where a finite mixture with random effects model with two components performs best. All models are estimated fully Bayesian, and model comparisons are based on model likelihoods and analyses of holdout data.",0
,On the performance of multiple imputation for multivariate data with small sample size,,0
https://doi.org/10.1111/j.1745-3984.2011.00145.x,Restrictive Stochastic Item Selection Methods in Cognitive Diagnostic Computerized Adaptive Testing,"This paper proposes two new item selection methods for cognitive diagnostic computerized adaptive testing: the restrictive progressive method and the restrictive threshold method. They are built upon the posterior weighted Kullback-Leibler (KL) information index but include additional stochastic components either in the item selection index or in the item selection procedure. Simulation studies show that both methods are successful at simultaneously suppressing overexposed items and increasing the usage of underexposed items. Compared to item selection based upon (1) pure KL information and (2) the Sympson-Hetter method, the two new methods strike a better balance between item exposure control and measurement accuracy. The two new methods are also compared with Barrada et al.'s (2008) progressive method and proportional method.",0
https://doi.org/10.4324/9780203142738,New Participatory Dimensions in Civil Society,"1. Introduction: Democracy, Professionalization and Participation Jan W. van Deth and William A. Maloney Part 1: Professionalization and Democratic Politics 2. How to Domesticate Civil Society by Public-Private Partnerships: Evidence from German Local Health Policy Matthias Freise 3. Entrepreneurial Participation in International Local Politics: The Case of Marseille, European Capital of Culture for 2013 Nicolas Maisetti 4. New Issues, New Forms of Action? Climate Change and Environmental Activism in Britain Christopher Rootes 5. The Professionalization of EU's Civil Society: A Conceptual Framework Sabine Saurugger 6. The Democratic Contribution of Professionalized Representation William A. Maloney 7. Professionalized Supply-Side Mobilization: Are Financial Contributors 'Meaningful Participants'? Grant Jordan Part 2: Changing Democratic Engagement 8. New Modes of Participation and Norms of Citizenship Jan W. van Deth 9. A Remedy for Unequal Participation? How Welfare States Impact on Social and Political Engagement Isabelle Stadelmann-Steffen 10. Peripheral Participants: The Activation of the Politically Less Engaged in Advanced Democracies Eline A. de Rooij 11. Surrogates for the Underrepresented? Ideology and Participatory Inequality in Personal and Professional Political Action Tom W.G. van der Meer 12. The Stability of Individualized Collective Action: Results of a Panel Study among Belgian Late Adolescents Ellen Quintelier and Marc Hooghe 13. Youth Participation from the Top-Down: The Perspectives of Government and Community Sector Decision-Makers in Australia Ariadne Vromen 14. Conclusions: How Democratic is Professionalized and Individualized Political Action? Jan W. van Deth and William A. Maloney",0
https://doi.org/10.1348/000711007x230937,Incorporating randomness in the Fisher information for improving item-exposure control in CATs,"The most commonly employed item selection rule in a computerized adaptive test (CAT) is that of selecting the item with the maximum Fisher information for the estimated trait level. This means a highly unbalanced distribution of item-exposure rates, a high overlap rate among examinees and, for item bank management, strong pressure to replace items with a high discrimination parameter in the bank. An alternative for mitigating these problems involves, at the beginning of the test, basing item selection mainly on randomness. As the test progresses, the weight of information in the selection increases. In the present work we study, for two selection rules, the progressive methods (Revuelta & Ponsoda, 1998) and the proportional method (Segall, 2004a), different functions that define the weight of the random component according to the position in the test of the item to be administered. The functions were tested in simulated item banks and in an operative bank. We found that both the progressive and the proportional methods tolerate a high weight of the random component with minimal or zero loss of accuracy, while bank security and maintenance are improved.",0
https://doi.org/10.1037/a0034860,To thine own self be true: Psychological adjustment promotes judgeability via personality–behavior congruence.,"Well-adjusted individuals are highly judgeable in that their personalities tend to be seen more accurately than the personalities of less adjusted individuals (Colvin, 1993a, 1993b; Human & Biesanz, 2011a). The mechanisms behind this effect, however, are not well understood. How does adjustment facilitate judgeability? In the present video-perceptions study, we examined potential mechanisms through which adjustment could promote judgeability at 3 stages of the Realistic Accuracy Model (RAM; Funder, 1995): (a) cue relevance, (b) cue availability, and (c) cue detection. We found that well-adjusted individuals were more judgeable because they provided more relevant cues: Specifically, well-adjusted individuals behaved more in line with their distinctive personalities, which in turn led them to be seen more accurately. In contrast, neither cue availability nor detection could sufficiently account for the link between adjustment and judgeability. In sum, well-adjusted individuals are more judgeable because to their own selves, they are true.",0
https://doi.org/10.1214/aoms/1177729991,Heuristic Approach to the Kolmogorov-Smirnov Theorems,"Asymptotic theorems on the difference between the (empirical) distribution function calculated from a sample and the true distribution function governing the sampling process are well known. Simple proofs of an elementary nature have been obtained for the basic theorems of Komogorov and Smirnov by Feller, but even these proofs conceal to some extent, in their emphasis on elementary methodology, the naturalness of the results (qualitatively at least), and their mutual relations. Feller suggested that the author publish his own approach (which had also been used by Kac), which does not have these disadvantages, although rather deep analysis would be necessary for its rigorous justification. The approach is therefore presented (at one critical point) as heuristic reasoning which leads to results in investigations of this kind, even though the easiest proofs may use entirely different methods. No calculations are required to obtain the qualitative results, that is the existence of limiting distributions for large samples of various measures of the discrepancy between empirical and true distribution functions. The numerical evaluation of these limiting distributions requires certain results concerning the Brownian movement stochastic process and its relation to other Gaussian processes which will be derived in the Appendix.",0
https://doi.org/10.3758/bf03198390,An analysis of latency and interresponse time in free recall,"In four experiments, subjects freely recalled previously studied items while a voice key and computer recorded each item’s recall latency relative to the onset of the recall period. The measures of recall probability and mean recall latency were shown to be empirically independent, demonstrating that there exists no a priori relationship between the two. In all four experiments, latency distributions were fit well by the ex-Gaussian, suggesting that retrieval includes a brief normally distributed initiation stage followed by a longer exponentially distributed search stage. Further, the variation in mean latency stemmed from the variation in the duration of the search stage, not the initiation stage. Interresponse times (IRTs), the time elapsed between two successive item recalls, were analyzed as well. The growth of mean IRTs, plotted as a function of output position, was shown to be a simple function of the number of items not yet recalled. Finally, the mathematical nature of both free recall latency and IRT growth are shown to be consistent with a simple theoretical account of retrieval that depicts mean recall latency as a measure of the breadth of search.",0
https://doi.org/10.1007/s11222-007-9030-2,Bayesian parsimonious covariance estimation for hierarchical linear mixed models,"We consider a non-centered parameterization of the standard random-effects model, which is based on the Cholesky decomposition of the variance-covariance matrix. The regression type structure of the non-centered parameterization allows us to use Bayesian variable selection methods for covariance selection. We search for a parsimonious variance-covariance matrix by identifying the non-zero elements of the Cholesky factors. With this method we are able to learn from the data for each effect whether it is random or not, and whether covariances among random effects are zero. An application in marketing shows a substantial reduction of the number of free elements in the variance-covariance matrix.",0
https://doi.org/10.1037/a0017808,"Context, learning, and extinction.","A. Redish et al. (2007) proposed a reinforcement learning model of context-dependent learning and extinction in conditioning experiments, using the idea of ""state classification"" to categorize new observations into states. In the current article, the authors propose an interpretation of this idea in terms of normative statistical inference. They focus on renewal and latent inhibition, 2 conditioning paradigms in which contextual manipulations have been studied extensively, and show that online Bayesian inference within a model that assumes an unbounded number of latent causes can characterize a diverse set of behavioral results from such manipulations, some of which pose problems for the model of Redish et al. Moreover, in both paradigms, context dependence is absent in younger animals, or if hippocampal lesions are made prior to training. The authors suggest an explanation in terms of a restricted capacity to infer new causes.",0
,JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling,"JAGS is a program for Bayesian Graphical modelling which aims for compatibility with Classic BUGS. The program could eventually be developed as an R package. This article explains the motivations for this program, briefly describes the architecture and then discusses some ideas for a vectorized form of the BUGS language.",0
https://doi.org/10.1007/s001800000041,Implementation and performance issues in the Bayesian and likelihood fitting of multilevel models,"Computational Statistics, September 2000, Volume 15, Issue 3, pp 391-420",1
https://doi.org/10.1080/00949650212842,Marginal Likelihood for a Class of Bayesian Generalized Linear Models,"The Bayes factor has become an important tool for model selection. The marginal likelihoods are also important because they can be used to rank the models. In fact, the Bayes factor is the ratio of the marginal likelihoods for two models with proper prior densities. We discuss the marginal likelihood for a class of generalized linear models used in small area estimation for mortality data analysis. Computation in these models is intensive and requires the implementation of Markov chain Monte Carlo (MCMC) methods. A sophisticated method for computing the marginal likelihoods for generalized linear models using reduced Metropolis-Hastings (M-H) samplers has recently been introduced. Also, a much simpler method that uses the Laplace approximation has been proposed. Our method lies between these two in simplicity, and it uses importance sampling via a simple output analysis from a MCMC sampler. We also show that the new method can be approximated without using the MCMC sampler. We illustrate our methods for t...",0
https://doi.org/10.1016/s0304-3959(01)00354-2,A new model of sciatic inflammatory neuritis (SIN): induction of unilateral and bilateral mechanical allodynia following acute unilateral peri-sciatic immune activation in rats,"Immune activation near healthy peripheral nerves may have a greater role in creating pathological pain than previously recognized. We have developed a new model of sciatic inflammatory neuritis to assess how such immune activation may influence somatosensory processing. The present series of experiments reveal that zymosan (yeast cell walls) acutely injected around the sciatic nerve of awake unrestrained rats rapidly (within 3h) produces low threshold mechanical allodynia in the absence of thermal hyperalgesia. Low (4 microg) doses of zymosan produce both territorial and extra-territorial allodynia restricted to the ipsilateral hindpaw. Higher (40-400 microg) doses of zymosan again produce both territorial and extra-territorial allodynia. However, allodynia is now expressed both in the ipsilateral as well as contralateral hindpaws. Several lines of evidence are provided that the appearance of this contralateral ('mirror') allodynia reflects local actions of zymosan on the sciatic nerve rather than spread of this immune activator to the general circulation. Since many clinical neuropathies result from inflammation/infection of peripheral nerves rather than frank physical trauma, understanding how immune activation alters pain processing may suggest novel approaches to pain control.",0
https://doi.org/10.1093/scan/nsq099,One’s motor performance predictably modulates the understanding of others’ actions through adaptation of premotor visuo-motor neurons,"Neurons firing both during self and other's motor behavior (mirror neurons) have been described in the brain of vertebrates including humans. The activation of somatic motor programs driven by perceived behavior has been taken as evidence for mirror neurons' contribution to cognition. The inverse relation, that is the influence of motor behavior on perception, is needed for demonstrating the long-hypothesized causal role of mirror neurons in action understanding. We provide here conclusive behavioral and neurophysiological evidence for that causal role by means of cross-modal adaptation coupled with a novel transcranial magnetic stimulation (TMS)-adaptation paradigm. Blindfolded repeated motor performance of an object-directed action (push or pull) induced in healthy participants a strong visual after-effect when categorizing others' actions, as a result of motor-to-visual adaptation of visuo-motor neurons. TMS over the ventral premotor cortex, but not over the primary motor cortex, suppressed the after-effect, thus localizing the population of adapted visuo-motor neurons in the premotor cortex. These data are exquisitely consistent in humans with the existence of premotor mirror neurons that have access to the action meaning. We also show that controlled manipulation of the firing properties of this neural population produces strong predictable changes in the way we categorize others' actions.",0
https://doi.org/10.1016/j.jclinepi.2010.10.016,Mixed treatment comparison meta-analysis of altered fractionated radiotherapy and chemotherapy in head and neck cancer,"<h2>Abstract</h2><h3>Objective</h3> Different treatments have been investigated in head and neck cancers (HNCs) but not all of them have been appraised using pairwise comparison. This has resulted in failure to directly identify the best treatment with standard methods. Mixed treatment comparison (MTC) meta-analysis allows one to perform simultaneous inference regarding all treatments and select the best among them. <h3>Study Design and Setting</h3> We applied MTC models to the Meta-Analyses of Chemotherapy and Radiotherapy in HNC, which pooled individual patient data concerning more than 24,000 patients involved in 102 trials. Fixed- and random-effects models, models with or without consistency factors, possibly adapted to multiarm trials are discussed. <h3>Results</h3> Altered fractionated concomitant chemoradiotherapy (AF-CRT) leads to the highest probability of survival in nonmetastatic HNC. The probability that AF-CRT is the best treatment is 94% with random-effects models. There was no relevant inconsistency. When only the most recent trials were selected, AF-CRT and concomitant chemoradiotherapy (CRT) were the two best treatments. AF-CRT remains better than CRT but with a lower posterior probability. <h3>Conclusion</h3> MTC is a powerful method for investigating networks of randomized trials. Homogeneity, similarity of trial designs, populations, and the consistency of the network should be thoroughly checked.",0
https://doi.org/10.1198/jasa.2009.0237,Bayesian Mixture Labeling by Highest Posterior Density,"A fundamental problem for Bayesian mixture model analysis is label switching, which occurs as a result of the nonidentifiability of the mixture components under symmetric priors. We propose two labeling methods to solve this problem. The first method, denoted by PM(ALG), is based on the posterior modes and an ascending algorithm generically denoted ALG. We use each Markov chain Monte Carlo sample as the starting point in an ascending algorithm, and label the sample based on the mode of the posterior to which it converges. Our natural assumption here is that the samples converged to the same mode should have the same labels. The PM(ALG) labeling method has some computational advantages over other popular labeling methods. Additionally, it automatically matches the “ideal” labels in the highest posterior density credible regions. The second method does labeling by maximizing the normal likelihood of the labeled Gibbs samples. Using a Monte Carlo simulation study and a real dataset, we demonstrate the succes...",0
https://doi.org/10.1111/1467-9248.12183,Equal Partners in Dialogue? Participation Equality in a Transnational Deliberative Poll (Europolis),"By gathering a representative sample of citizens from all 27 EU Member States, the deliberative poll Europolis created the opportunity for the inclusion of a wide variety of European voices. Taking up claims of difference democrats who argue that informal hurdles to participation can endure even after individuals gain formal access to the floor, this article argues for an extended approach to evaluate equality in deliberative minipublics. Specifically, it assesses whether participants contributed in roughly equal measures to the discussion and whether their discussion partners considered their contributions on equal merits. In doing so, the article adds to the small but growing literature on deliberation that expresses reservations about taking the willingness to engage with others' claims for granted. In order to account for the intrinsically relational aspect of interpersonal communication, measures of social network analysis are introduced as possible tools to evaluate participation equality in deliberative encounters.",0
https://doi.org/10.1145/358315.358390,Generating gamma variates by a modified rejection technique,"A suitable square root transformation of a gamma random variable with mean a ≥ 1 yields a probability density close to the standard normal density. A modification of the rejection technique then begins by sampling from the normal distribution, being able to accept and transform the initial normal observation quickly at least 85 percent of the time (95 percent if a ≥ 4). When used with efficient subroutines for sampling from the normal and exponential distributions, the resulting accurate method is significantly faster than competing algorithms.",0
,R: A language and environment for statistical computing.,,0
https://doi.org/10.1111/j.1745-3984.2003.tb01149.x,A Method for Maintaining Scale Stability in the Presence of Test Speededness,"Administering tests under time constraints may result in poorly estimated item parameters, particularly for items at the end of the test (Douglas, Kim, Habing, & Gao, 1998; Oshima, 1994). Bolt, Cohen, and Wollack (2002) developed an item response theory mixture model to identify a latent group of examinees for whom a test is overly speeded, and found that item parameter estimates for end-of-test items in the nonspeeded group were similar to estimates for those same items when administered earlier in the test. In this study, we used the Bolt et al. (2002) method to study the effect of removing speeded examinees on the stability of a score scale over an II-year period. Results indicated that using only the nonspeeded examinees for equating and estimating item parameters provided a more unidimensional scale, smaller effects of item parameter drift (including fewer drifting items), and less scale drift (i.e., bias) and variability (i.e., root mean squared errors) when compared to the total group of examinees.",0
https://doi.org/10.1111/j.0006-341x.2004.00211.x,Maximum Likelihood Analysis of a General Latent Variable Model with Hierarchically Mixed Data,"A general two-level latent variable model is developed to provide a comprehensive framework for model comparison of various submodels. Nonlinear relationships among the latent variables in the structural equations at both levels, as well as the effects of fixed covariates in the measurement and structural equations at both levels, can be analyzed within the framework. Moreover, the methodology can be applied to hierarchically mixed continuous, dichotomous, and polytomous data. A Monte Carlo EM algorithm is implemented to produce the maximum likelihood estimate. The E-step is completed by approximating the conditional expectations through observations that are simulated by Markov chain Monte Carlo methods, while the M-step is completed by conditional maximization. A procedure is proposed for computing the complicated observed-data log likelihood and the BIC for model comparison. The methods are illustrated by using a real data set.",0
,Nonlinear structural equation models: The Kenny-Judd model with Interaction effects,,0
https://doi.org/10.1002/sim.5494,A fully Bayesian application of the Copas selection model for publication bias extended to network meta-analysis,"The Copas parametric model is aimed at exploring the potential impact of publication bias via sensitivity analysis, by making assumptions regarding the probability of publication of individual studies related to the standard error of their effect sizes. Reviewers often have prior assumptions about the extent of selection in the set of studies included in a meta-analysis. However, a Bayesian implementation of the Copas model has not been studied yet. We aim to present a Bayesian selection model for publication bias and to extend it to the case of network meta-analysis where each treatment is compared either with placebo or with a reference treatment creating a star-shaped network. We take advantage of the greater flexibility offered in the Bayesian context to incorporate in the model prior information on the extent and strength of selection. To derive prior distributions, we use both external data and an elicitation process of expert opinion.",0
https://doi.org/10.1016/j.jspi.2008.05.036,Propriety of posteriors in structured additive regression models: Theory and empirical evidence,"Abstract Structured additive regression comprises many semiparametric regression models such as generalized additive (mixed) models, geoadditive models, and hazard regression models within a unified framework. In a Bayesian formulation, non-parametric functions, spatial effects and further model components are specified in terms of multivariate Gaussian priors for high-dimensional vectors of regression coefficients. For several model terms, such as penalized splines or Markov random fields, these Gaussian prior distributions involve rank-deficient precision matrices, yielding partially improper priors. Moreover, hyperpriors for the variances (corresponding to inverse smoothing parameters) may also be specified as improper, e.g. corresponding to Jeffreys prior or a flat prior for the standard deviation. Hence, propriety of the joint posterior is a crucial issue for full Bayesian inference in particular if based on Markov chain Monte Carlo simulations. We establish theoretical results providing sufficient (and sometimes necessary) conditions for propriety and provide empirical evidence through several accompanying simulation studies.",0
https://doi.org/10.1207/s15366359mea0204_1,"A Manifesto on Psychology as Idiographic Science: Bringing the Person Back Into Scientific Psychology, This Time Forever","Psychology is focused on variation between cases (interindividual variation). Results thus obtained are considered to be generalizable to the understanding and explanation of variation within single cases (intraindividual variation). It is indicated, however, that the direct consequences of the classical ergodic theorems for psychology and psychometrics invalidate this conjectured generalizability: only under very strict conditions-which are hardly obtained in real psychological processes-can a generalization be made from a structure of interindividual variation to the analogous structure of intraindividual variation. Illustrations of the lack of this generalizability are given in the contexts of psychometrics, developmental psychology, and personality theory.",0
https://doi.org/10.1080/10705510802339106,A SAS Interface for Bayesian Analysis With WinBUGS,"Bayesian methods are becoming very popular despite some practical difficulties in implementation. To assist in the practical application of Bayesian methods, we show how to implement Bayesian analysis with WinBUGS as part of a standard set of SAS routines. This implementation procedure is first illustrated by fitting a multiple regression model and then a linear growth curve model. A third example is also provided to demonstrate how to iteratively run WinBUGS inside SAS for Monte Carlo simulation studies. The SAS codes used in this study are easily extended to accommodate many other models with only slight modification. This interface can be of practical benefit in many aspects of Bayesian methods because it allows the SAS users to benefit from the implementation of Bayesian estimation and it also allows the WinBUGS user to benefit from the data processing routines available in SAS.",0
https://doi.org/10.1016/j.amjmed.2009.05.021,Bayesian Meta-analysis of Hormone Therapy and Mortality in Younger Postmenopausal Women,"There is uncertainty over the risks and benefits of hormone therapy. We performed a Bayesian meta-analysis to evaluate the effect of hormone therapy on total mortality in younger postmenopausal women. This analysis synthesizes evidence from different sources, taking into account varying views on the issue.A comprehensive search from 1966 through January 2008 identified randomized controlled trials of at least 6 month's duration that evaluated hormone therapy in women with mean age <60 years and reported at least one death, and prospective observational cohort studies that evaluated the relative risk of mortality associated with hormone therapy after adjustment for confounding variables.The results were synthesized using a hierarchical random-effects Bayesian meta-analysis. The pooled results from 19 randomized trials, with 16,000 women (mean age 55 years) followed for 83,000 patient-years, showed a mortality relative risk of 0.73 (95% credible interval 0.52-0.96). When data from 8 observational studies were added to the analysis, the resultant relative risk was 0.72 (credible interval 0.62-0.82). The posterior probability that hormone therapy reduces total mortality in younger women is almost 1.The synthesis of data using Bayesian meta-analysis indicates a reduction in mortality in younger postmenopausal women taking hormone therapy compared with no treatment. This finding should be interpreted taking into account the potential benefits and harms of hormone therapy.",0
https://doi.org/10.1163/156856809788746309,Fixed vs. variable noise in 2AFC contrast discrimination: lessons from psychometric functions,"Recent discussion regarding whether the noise that limits 2AFC discrimination performance is fixed or variable has focused either on describing experimental methods that presumably dissociate the effects of response mean and variance or on reanalyzing a published data set with the aim of determining how to solve the question through goodness-of-fit statistics. This paper illustrates that the question cannot be solved by fitting models to data and assessing goodness-of-fit because data on detection and discrimination performance can be indistinguishably fitted by models that assume either type of noise when each is coupled with a convenient form for the transducer function. Thus, success or failure at fitting a transducer model merely illustrates the capability (or lack thereof) of some particular combination of transducer function and variance function to account for the data, but it cannot disclose the nature of the noise. We also comment on some of the issues that have been raised in recent exchange on the topic, namely, the existence of additional constraints for the models, the presence of asymmetric asymptotes, the likelihood of history-dependent noise, and the potential of certain experimental methods to dissociate the effects of response mean and variance.",0
https://doi.org/10.1016/j.jeconom.2006.07.014,Modeling and calculating the effect of treatment at baseline from panel outcomes,"We propose and examine a panel data model for isolating the effect of a treatment, taken once at baseline, from outcomes observed over subsequent time periods. In the model, the treatment intake and outcomes are assumed to be correlated, due to unobserved or unmeasured confounders. Intake is partly determined by a set of instrumental variables and the confounding on unobservables is modeled in a flexible way, varying both by time and treatment state. Covariate effects are assumed to be subject-specific and potentially correlated with other covariates. Estimation and inference is by Bayesian methods that are implemented by tuned Markov chain Monte Carlo methods. Because our analysis is based on the framework developed by Chib [2004. Analysis of treatment response data without the joint distribution of counterfactuals. Journal of Econometrics, in press], the modeling and estimation does not involve either the unknowable joint distribution of the potential outcomes or the missing counterfactuals. The problem of model choice through marginal likelihoods and Bayes factors is also considered. The methods are illustrated in simulation experiments and in an application dealing with the effect of participation in high school athletics on future labor market earnings.",0
https://doi.org/10.1177/0272431614537117,Teaching Through Interactions in Secondary School Classrooms,"Valid measurement of how students’ experiences in secondary school classrooms lead to gains in learning requires a developmental approach to conceptualizing classroom processes. This article presents a potentially useful theoretical model, the Teaching Through Interactions framework, which posits teacher-student interactions as a central driver for student learning and that teacher-student interactions can be organized into three major domains. Results from 1,482 classrooms provide evidence for distinct emotional, organizational, and instructional domains of teacher-student interaction. It also appears that a three-factor structure is a better fit to observational data than alternative one- and two-domain models of teacher-student classroom interactions, and that the three-domain structure is generalizable from 6th through 12th grade. Implications for practitioners, stakeholders, and researchers are discussed.",0
https://doi.org/10.1207/s15327906mbr3604_07,Hypothesis Testing and Model Comparison in Two-level Structural Equation Models,"One basic and important problem in two-level structural equation modeling is to find a good model for the observed sample data. This article demonstrates the use of the well-known Bayes factor in the Bayesian literature for hypothesis testing and model comparison in general two-level structural equation models. It is shown that the proposed methodology is flexible, and can be applied to situations with a wide variety of nonnested models. Moreover, some problems encountered in using existing methods for goodness-of-fit assessment of the proposed model can be alleviated. An illustrative example with some real data from an AIDS care study is presented.",0
https://doi.org/10.1002/sim.2672,Bayesian statistics in medicine: a 25 year review,"This review examines the state of Bayesian thinking as Statistics in Medicine was launched in 1982, reflecting particularly on its applicability and uses in medical research. It then looks at each subsequent five-year epoch, with a focus on papers appearing in Statistics in Medicine, putting these in the context of major developments in Bayesian thinking and computation with reference to important books, landmark meetings and seminal papers. It charts the growth of Bayesian statistics as it is applied to medicine and makes predictions for the future. From sparse beginnings, where Bayesian statistics was barely mentioned, Bayesian statistics has now permeated all the major areas of medical statistics, including clinical trials, epidemiology, meta-analyses and evidence synthesis, spatial modelling, longitudinal modelling, survival modelling, molecular genetics and decision-making in respect of new technologies.",0
https://doi.org/10.1002/sim.6131,Hierarchical network meta-analysis models to address sparsity of events and differing treatment classifications with regard to adverse outcomes,"Meta-analysis for adverse events resulting from medical interventions has many challenges, in part due to small numbers of such events within primary studies. Furthermore, variability in drug dose, potential differences between drugs within the same pharmaceutical class and multiple indications for a specific treatment can all add to the complexity of the evidence base.This paper explores the use of synthesis methods, incorporating mixed treatment comparisons, to estimate the risk of adverse events for a medical intervention, while acknowledging and modelling the complexity of the structure of the evidence base.The motivating example was the effect on malignancy of three anti-tumour necrosis factor (anti-TNF) drugs (etanercept, adalimumab and infliximab) indicated to treat rheumatoid arthritis. Using data derived from 13 primary studies, a series of meta-analysis models of increasing complexity were applied. Models ranged from a straightforward comparison of anti-TNF against non-anti-TNF controls, to more complex models in which a treatment was defined by individual drug and its dose. Hierarchical models to allow ‘borrowing strength’ across treatment classes and dose levels, and models involving constraints on the impact of dose level, are described.These models provide a flexible approach to estimating sparse, often adverse, outcomes associated with interventions. Each model makes its own set of assumptions, and approaches to assessing goodness of fit of the various models will usually be extremely limited in their effectiveness, due to the sparse nature of the data. Both methodological and clinical considerations are required to fit realistically complex models in this area and to evaluate their appropriateness. Copyright © 2014 John Wiley & Sons, Ltd.",0
https://doi.org/10.3102/1076998607302632,Effects on Scale Linking of Different Definitions of Criterion Functions for the IRT Characteristic Curve Methods,"Under item response theory, the characteristic curve methods (Haebara and Stocking-Lord methods) are used to link two ability scales from separate calibrations. The linking methods use their respective criterion functions that can be defined differently according to the symmetry- and distribution-related schemes. The symmetry-related scheme relates to which scale, targeted or transformed, should be used for the definition. The distribution-related scheme refers to a way of incorporating underlying ability distributions into the definition. Through simulations, this study examined if a certain optimal combination of the two schemes exists regardless of differences in proficiency distribution between samples involved in scale linking. Concurrent calibration was considered for comparative purposes and, across all nine combinations of proficiency distributions to be linked, its performance was better in linking accuracy than the linking methods. There was no optimal combination of the symmetry- and distribution-related schemes that led to minimal linking error across the nine combinations.",0
https://doi.org/10.1093/ije/29.1.158,Principles of multilevel modelling,"Multilevel modelling, also known as hierarchical regression, generalizes ordinary regression modelling to distinguish multiple levels of information in a model. Use of multiple levels gives rise to an enormous range of statistical benefits. To aid in understanding these benefits, this article provides an elementary introduction to the conceptual basis for multilevel modelling, beginning with classical frequentist, Bayes, and empirical-Bayes techniques as special cases. The article focuses on the role of multilevel averaging ('shrinkage') in the reduction of estimation error, and the role of prior information in finding good averages.",0
https://doi.org/10.1002/sim.3380,A random change point model for assessing variability in repeated measures of cognitive function,"Some cognitive functions undergo transitions in old age, which motivates the use of a change point model for the individual trajectory. The age when the change occurs varies between individuals and is treated as random. We illustrate the properties of a random change point model and use it for data from a Swedish study of change in cognitive function in old age. Variance estimates are obtained from Markov chain Monte Carlo simulation using Gibbs sampling. The random change point model is compared with models within the family of linear random effects models. The focus is on the ability to capture variability in measures of cognitive function. The models make different assumptions about the variance over the age span, and we demonstrate that the random change point model has the most reasonable structure. Copyright © 2008 John Wiley & Sons, Ltd.",0
https://doi.org/10.1111/j.1467-6494.1993.tb01031.x,Analyzing Individual Status and Change with Hierarchical Linear Models:Illustration with Depression in College Students,"A recently developed class of multilevel or hierarchical linear models (HLM) provides an intuitive and efficient way to estimate individual growth or change curves. The approach also models the between-subjects variation of the individual change curves with treatment factors and individual attributes. Unlike other repeated measures analysis methods common in the behavioral sciences, HLM allows the fit of data with unequal numbers of repeated observations for each subject, variable timing of observations, and missing data, features which are often characteristic of data from field studies. The application of HLM for the analysis of repeated psychological measures is discussed and illustrated here with depression data for college students. Strengths and limitations of the approach are discussed.",0
https://doi.org/10.3102/1076998606298044,Conditional Item-Exposure Control in Adaptive Testing Using Item-Ineligibility Probabilities,"Two conditional versions of the exposure-control method with item-ineligibility constraints for adaptive testing in van der Linden and Veldkamp (2004 ) are presented. The first version is for unconstrained item selection, the second for item selection with content constraints imposed by the shadow-test approach. In both versions, the exposure rates of the items are controlled using probabilities of item ineligibility given θ that adapt the exposure rates automatically to a goal value for the items in the pool. In an extensive empirical study with an adaptive version of the Law School Admission Test, the authors show how the method can be used to drive conditional exposure rates below goal values as low as 0.025. Obviously, the price to be paid for minimal exposure rates is a decrease in the accuracy of the ability estimates. This trend is illustrated with empirical data.",0
https://doi.org/10.1027/1614-2241.3.3.100,Challenges in Nonlinear Structural Equation Modeling,"Abstract. Challenges in evaluating nonlinear effects in multiple regression analyses include reliability, validity, multicollinearity, and dichotomization of continuous variables. While reliability and validity issues are solved by employing nonlinear structural equation modeling, multicollinearity remains a problem which may even be aggravated when using latent variable approaches. Further challenges of nonlinear latent analyses comprise the distribution of latent product terms, a problem especially relevant for approaches using maximum likelihood estimation methods based on multivariate normally distributed variables, and unbiased estimates of nonlinear effects under multicollinearity. The only methods that explicitly take the nonnormality of nonlinear latent models into account are latent moderated structural equations (LMS) and quasi-maximum likelihood (QML). In a small simulation study both methods yielded unbiased parameter estimates and correct estimates of standard errors for inferential statistics. The advantages and limitations of nonlinear structural equation modeling are discussed.",0
https://doi.org/10.1007/s11336-009-9141-0,Hierarchical Multinomial Processing Tree Models: A Latent-Trait Approach,"Multinomial processing tree models are widely used in many areas of psychology. A hierarchical extension of the model class is proposed, using a multivariate normal distribution of person-level parameters with the mean and covariance matrix to be estimated from the data. The hierarchical model allows one to take variability between persons into account and to assess parameter correlations. The model is estimated using Bayesian methods with weakly informative hyperprior distribution and a Gibbs sampler based on two steps of data augmentation. Estimation, model checks, and hypotheses tests are discussed. The new method is illustrated using a real data set, and its performance is evaluated in a simulation study. © 2009 The Psychometric Society.",0
https://doi.org/10.1037/a0016132,An effectiveness trial of a dissonance-based eating disorder prevention program for high-risk adolescent girls.,"Efficacy trials indicate that an eating disorder prevention program involving dissonance-inducing activities that decrease thin-ideal internalization reduces risk for current and future eating pathology, yet it is unclear whether this program produces effects under real-world conditions. The present effectiveness trial tested whether this program produced effects when school staff recruit participants and deliver the intervention. Adolescent girls with body image concerns (N = 306; M age = 15.7, SD = 1.1) randomized to the dissonance intervention showed significantly greater decreases in thin-ideal internalization, body dissatisfaction, dieting attempts, and eating disorder symptoms from pretest to posttest than did those assigned to a psychoeducational brochure control condition, with the effects for body dissatisfaction, dieting, and eating disorder symptoms persisting through 1-year follow-up. Effects were slightly smaller than those observed in a prior efficacy trial, suggesting that this program is effective under real-world conditions, but that facilitator selection, training, and supervision could be improved.",0
https://doi.org/10.1080/01621459.2014.983229,An Analysis of an Incomplete Marked Point Pattern of Heat-Related 911 Calls,"We analyze an incomplete marked point pattern of heat-related 911 calls between the years 2006–2010 in Houston, TX, to primarily investigate conditions that are associated with increased vulnerability to heat-related morbidity and, secondarily, build a statistical model that can be used as a public health tool to predict the volume of 911 calls given a time frame and heat exposure. We model the calls as arising from a nonhomogenous Cox process with unknown intensity measure. By using the kernel convolution construction of a Gaussian process, the intensity surface is modeled using a low-dimensional representation and properly adheres to circular domain constraints. We account for the incomplete observations by marginalizing the joint intensity measure over the domain of the missing marks and also demonstrate model based imputation. We find that spatial regions of high risk for heat-related 911 calls are temporally dynamic with the highest risk occurring in urban areas during the day. We also find that elde...",0
https://doi.org/10.1177/1465116514532556,"The Europeanization of interest groups: Group type, resources and policy area","Large variation exists in the extent to which national interest groups focus on European Union (EU) legislation and carry out their political activities in Brussels and Strasbourg. What explains this variation? We propose a series of hypotheses that suggest that business groups, and groups active in policy areas with high EU competence, are more Europeanized than other groups. The effect of group type, moreover, is conditional on the material resources a group possesses: we expect the difference between business and non-business groups to be largest for actors that are well endowed with material resources. Using novel data on 880 national associations, gained from a survey of interest groups in five European countries, we find support for these hypotheses. The article has implications for the literatures on lobbying, Europeanization, and theories of European integration.",0
https://doi.org/10.1177/014662168801200105,The Application of an Unfolding Model of the PIRT Type to the Measurement of Attitude,"Unfolding data for unidimensional variables con structed from direct responses (e.g., agreement or dis agreement) are characterized by single peaked func tions involving the locations of each person and each stimulus. A continuous discrirninal process, of the form postulated by Thurstone when he proposed his Law of Comparative Judgment, is suggested. This process is transformed to a qualitative dichotomous re sponse in which the probability of endorsement is governed by the square of the distance between the lo cations of the person and the stimulus. Maximum like lihood estimates of the parameters are derived, and it is shown that the information associated with any re sponse is a bimodal function of the difference between the person and stimulus locations. The feasibility of parameter estimation is demonstrated with a limited simulation study. The model is applied to a set of statements designed to measure attitudes toward capi tal punishment and scaled by the methods of Thur stone. The responses conformed to the unfolding mechanism, and the scale values of the statements are statistically equivalent to those obtained by Thur stone's methods. Index terms: Attitude measure ment, Developmental data, Discriminal process, Item response theory, Person response theory, Thurstone scaling, Unfolding data, Unidimensional scaling.",0
https://doi.org/10.1016/j.neuroscience.2014.09.020,"Activation of adult rat CNS endothelial cells by opioid-induced toll-like receptor 4 (TLR4) signaling induces proinflammatory, biochemical, morphological, and behavioral sequelae","CNS immune signaling contributes to deleterious opioid effects including hyperalgesia, tolerance, reward, and dependence/withdrawal. Such effects are mediated by opioid signaling at toll-like receptor 4 (TLR4), presumptively of glial origin. Whether CNS endothelial cells express TLR4 is controversial. If so, they would be well positioned for activation by blood-borne opioids, contributing to opioid-induced pro-inflammatory responses. These studies examined adult primary rat CNS endothelial cell responses to (-)-morphine or its mu opioid receptor (MOR)-inactive metabolite morphine-3-glucuronide (M3G), both known TLR4 agonists. We demonstrate that adult rat CNS endothelial cells express functional TLR4. M3G activated nuclear factor kappaB (NF-κB), increased tumor necrosis factor-α (TNFα) and cyclooxygenase-2 (COX2) mRNAs, and released prostaglandin E2 (PGE2) from these cells. (-)-Morphine-induced upregulation of TNFα mRNA and PGE2 release were unmasked by pre-treatment with nalmefene, a MOR antagonist without TLR4 activity (unlike CTAP, shown to have both MOR- and TLR4-activity), suggestive of an interplay between MOR and TLR4 co-activation by (-)-morphine. In support, MOR-dependent Protein Kinase A (PKA) opposed TLR4 signaling, as PKA inhibition (H-89) also unmasked (-)-morphine-induced TNFα and COX2 mRNA upregulation. Intrathecal injection of CNS endothelial cells, stimulated in vitro with M3G, produced TLR4-dependent tactile allodynia. Further, cortical suffusion with M3G in vivo induced TLR4-dependent vasodilation. Finally, endothelial cell TLR4 activation by lipopolysaccharide and/or M3G was blocked by the glial inhibitors AV1013 and propentofylline, demonstrating endothelial cells as a new target of such drugs. These data indicate that (-)-morphine and M3G can activate CNS endothelial cells via TLR4, inducing proinflammatory, biochemical, morphological, and behavioral sequelae. CNS endothelial cells may have previously unanticipated roles in opioid-induced effects, in phenomena blocked by presumptive glial inhibitors, as well as TLR4-mediated phenomena more broadly.",0
https://doi.org/10.1111/j.1745-3984.2005.00013.x,Increasing the Homogeneity of CAT's Item-Exposure Rates by Minimizing or Maximizing Varied Target Functions While Assembling Shadow Tests,"A computerized adaptive testing (CAT) algorithm that has the potential to increase the homogeneity of CAT's item-exposure rates without significantly sacrificing the precision of ability estimates was proposed and assessed in the shadow-test (van der Linden & Reese, 1998) CAT context. This CAT algorithm was formed by a combination of maximizing or minimizing varied target functions while assembling shadow tests. There were four target functions to be separately used in the first, second, third, and fourth quarter test of CAT. The elements to be used in the four functions were associated with (a) a random number assigned to each item, (b) the absolute difference between an examinee's current ability estimate and an item difficulty, (c) the absolute difference between an examinee's current ability estimate and an optimum item difficulty, and (d) item information. The results indicated that this combined CAT fully utilized all the items in the pool, reduced the maximum exposure rates, and achieved more homogeneous exposure rates. Moreover, its precision in recovering ability estimates was similar to that of the maximum item-information method. The combined CAT method resulted in the best overall results compared with the other individual CAT item-selection methods. The findings from the combined CAT are encouraging. Future uses are discussed.",0
https://doi.org/10.1111/j.2044-8309.1995.tb01058.x,The theory of planned behaviour: The effects of perceived behavioural control and self-efficacy,"The present study was undertaken to assess the utility of the theory of planned behaviour, using separate measures of the two variables that are considered to comprise the notion of perceived behavioural control, namely, beliefs concerning the controllability of the behaviour and efficacy expectancies. The study was concerned with the prediction of intentions to engage in regular exercise (for at least 20 minutes, three times a week for a fortnight) and actual exercise behaviour. A sample of 146 undergraduate subjects participated in the study. It was prospective in design; measures of attitudes, norms, intentions, perceived control and self-efficacy were obtained at the first wave of data collection, while actual behaviour was assessed two weeks later. The results of the study revealed support for the view that separate measures of self-efficacy and perceived behavioural control should be employed in the theory of planned behaviour. In the first instance, confirmatory factor analysis revealed that the two variables could be empirically distinguished. Second, the effects of perceived behavioural control and self-efficacy on behavioural intentions and actual behaviour differed. As predicted, efficacy expectancies influenced behavioural intentions, but not actual behaviour. In contrast, levels of perceived behavioural control had no effect on behavioural intentions, but emerged as a significant (positive) predictor of actual behaviour (there was also evidence that the effects of intentions on behaviour were moderated by the level of perceived behavioural control).",0
https://doi.org/10.1080/0022250x.1997.9990193,Consensus analysis of three‐way social network data,"Three‐way social network data occurs when every actor in a social network generates a digraph of the entire network. This paper presents a statistical model based on cultural consensus analysis for aggregating these separate digraphs into a single consensus digraph. In addition, the model allows estimation of separate hit and false alarm rates for each actor that can vary within each actor in different regions of the digraph. Several standard signal detection models are used to interpret the hit and false alarm parameters in terms of knowledge and response bias. A published three‐way data set by Kumbasar, Romney, and Batchelder (American Journal of Sociology, 1994) is analyzed, and the model reveals that both response bias and knowledge decrease with distance from ego.",0
https://doi.org/10.1207/s15328007sem0904_8,How to Use a Monte Carlo Study to Decide on Sample Size and Determine Power,"A common question asked by researchers is, What sample size do I need for my study? Over the years, several rules of thumb have been proposed. In reality there is no rule of thumb that applies to all situations. The sample size needed for a study depends on many factors, including the size of the model, distribution of the variables, amount of missing data, reliability of the variables, and strength of the relations among the variables. The purpose of this article is to demonstrate how substantive researchers can use a Monte Carlo study to decide on sample size and determine power. Two models are used as examples, a confirmatory factor analysis (CFA) model and a growth model. The analyses are carried out using the Mplus program (Muthen& Muthen 1998).",0
https://doi.org/10.1007/bf02294170,"The effect of sampling error on convergence, improper solutions, and goodness-of-fit indices for maximum likelihood confirmatory factor analysis","A Monte Carlo study assessed the effect of sampling error and model characteristics on the occurrence of nonconvergent solutions, improper solutions and the distribution of goodness-of-fit indices in maximum likelihood confirmatory factor analysis. Nonconvergent and improper solutions occurred more frequently for smaller sample sizes and for models with fewer indicators of each factor. Effects of practical significance due to sample size, the number of indicators per factor and the number of factors were found for GFI, AGFI, and RMR, whereas no practical effects were found for the probability values associated with the chi-square likelihood ratio test. Â© 1984 The Psychometric Society.",0
https://doi.org/10.1016/j.cct.2012.05.004,Comparison of methods for estimating the intraclass correlation coefficient for binary responses in cancer prevention cluster randomized trials,"The intraclass correlation coefficient (ICC) is a fundamental parameter of interest in cluster randomized trials as it can greatly affect statistical power. We compare common methods of estimating the ICC in cluster randomized trials with binary outcomes, with a specific focus on their application to community-based cancer prevention trials with primary outcome of self-reported cancer screening. Using three real data sets from cancer screening intervention trials with different numbers and types of clusters and cluster sizes, we obtained point estimates and 95% confidence intervals for the ICC using five methods: the analysis of variance estimator, the Fleiss-Cuzick estimator, the Pearson estimator, an estimator based on generalized estimating equations and an estimator from a random intercept logistic regression model. We compared estimates of the ICC for the overall sample and by study condition. Our results show that ICC estimates from different methods can be quite different, although confidence intervals generally overlap. The ICC varied substantially by study condition in two studies, suggesting that the common practice of assuming a common ICC across all clusters in the trial is questionable. A simulation study confirmed pitfalls of erroneously assuming a common ICC. Investigators should consider using sample size and analysis methods that allow the ICC to vary by study condition.",0
https://doi.org/10.1007/s00221-012-3354-7,Signal detection theory and vestibular perception: III. Estimating unbiased fit parameters for psychometric functions,"Psychophysics generally relies on estimating a subject's ability to perform a specific task as a function of an observed stimulus. For threshold studies, the fitted functions are called psychometric functions. While fitting psychometric functions to data acquired using adaptive sampling procedures (e.g., ""staircase"" procedures), investigators have encountered a bias in the spread (""slope"" or ""threshold"") parameter that has been attributed to the serial dependency of the adaptive data. Using simulations, we confirm this bias for cumulative Gaussian parametric maximum likelihood fits on data collected via adaptive sampling procedures, and then present a bias-reduced maximum likelihood fit that substantially reduces the bias without reducing the precision of the spread parameter estimate and without reducing the accuracy or precision of the other fit parameters. As a separate topic, we explain how to implement this bias reduction technique using generalized linear model fits as well as other numeric maximum likelihood techniques such as the Nelder-Mead simplex. We then provide a comparison of the iterative bootstrap and observed information matrix techniques for estimating parameter fit variance from adaptive sampling procedure data sets. The iterative bootstrap technique is shown to be slightly more accurate; however, the observed information technique executes in a small fraction (0.005 %) of the time required by the iterative bootstrap technique, which is an advantage when a real-time estimate of parameter fit variance is required.",0
https://doi.org/10.1002/acp.3143,The Presence of a Weapon Shrinks the Functional Field of View,"Summary  This study examined whether the functional field of view shrinks by the presence of a weapon or the increase of emotional arousal. In Experiment 1, participants viewed two types of pictures depicting scenes involving weapons or control objects and were asked to identify digits presented at the periphery when the pictures disappeared. The results showed that the presence of a weapon impaired identification of the peripheral digits, even when the pictures were equal with respect to emotional arousal level. In Experiment 2, participants viewed emotionally arousing pictures or neutral pictures, neither of which included weapons, and they were asked to identify digits presented at the periphery when the pictures disappeared. The results revealed that the increased emotional arousal did not impair identification of the peripheral digits. These results indicate that the functional field of view shrinks because of the presence of a weapon but not because of increased emotional arousal.Copyright © 2015 John Wiley & Sons, Ltd.",0
https://doi.org/10.1177/014662169802200406,A Method for Obtaining Standard Errors and Confidence Intervals of Composite Reliability for Congeneric Items,"A method for obtaining standard errors and confidence intervals of composite reliability co-efficients based on the bootstrap method is proposed. Using a structural equation modeling framework for estimating composite reliability of congeneric measures (Raykov, 1997), the approach takes repeated samples from a given sample and then estimates reliability. This approach extends earlier research on the distribution of scale reliability that is primarily concerned with Cronbach's coefficient alpha, and represents a general method based on fewer assumptions. It also permits testing hypotheses about composite reliability. Its use is demonstrated on simulated data.",0
https://doi.org/10.1214/08-ba318rej,Rejoinder,"In the main article I presented a series of objections to Bayesian inference, written in the voice of a hypothetical anti-Bayesian statistician. Here I respond to these objections along with some other comments made by four discussants. Â© 2008 International Society for Bayesian Analysis.",0
https://doi.org/10.1007/978-3-319-07503-7_21,Model Selection Criteria for Latent Growth Models Using Bayesian Methods,"Research in applied areas, such as statistical, psychological, behavioral, and educational areas, often involves the selection of the best available model from among a large set of candidate models. Considering that there is no well-defined model selection criterion in a Bayesian context and that latent growth mixture models are becoming popular in many areas, the goal of this study is to investigate the performance of a series of model selection criteria in the framework of latent growth mixture models with missing data and outliers in a Bayesian context. This study conducted five simulation studies to cover different cases, including latent growth curve models with missing data, latent growth curve models with missing data and outliers, growth mixture models with missing data and outliers, extended growth mixture models with missing data and outliers, and latent growth models with different classes. Simulation results show that almost all the proposed criteria can effectively identify the true models. This study also illustrated the application of these model selection criteria in real data analysis. The results will help inform the selection of growth models by researchers seeking to provide states with accurate estimates of the growth of their students. Â© Springer International Publishing Switzerland 2015.",0
https://doi.org/10.1123/japa.14.1.74,The Effect of Aging and Tennis Playing on Coincidence-Timing Accuracy,"This study examined the effect of tennis playing on the coincidence timing (CT) of older adults. Young, younger-old and older-old (20–30, 60–69, and 70–79 years old, respectively) tennis players and nonplayers were asked to synchronize a simple response (pressing a button) with the arrival of a moving stimulus at a target. Results showed that the older tennis players responded with a slight bias similar to that of the young players. Two experiments were conducted to determine whether the elimination of age effects through tennis playing was a result of maintaining basic perceptuomotor and perceptual processes or of some possible compensation strategy. The results revealed that the age-related increase in the visuomotor delay was significantly correlated with CT performance in older nonplayers but not in older tennis players. These results suggest that playing tennis is beneficial to older adults, insofar as they remained as accurate as younger ones despite less efficient perceptuomotor processes. This supports the compensation hypothesis.",0
https://doi.org/10.1348/096317909x476333,Job resources and flow at work: Modelling the relationship via latent growth curve and mixture model methodology,"The aim of the present three-wave follow-up study (n = 335) among employees of an employment agency was to investigate the association between job resources and work-related flow utilizing both variable- and person-oriented approaches. In addition, emotional exhaustion was studied as a moderator of the job resources–flow relationship, and as a predictor of the development of job resources and flow. The variable-oriented approach, based on latent growth curve analyses, revealed that the levels of job resources and flow at work, as well as changes in these variables, were positively associated with each other. The person-oriented inspection with the growth mixture modelling identified four trajectories based on the mean levels of job resources and flow and on the changes of these mean levels over time: (a) moderate work-related resources (n = 166), (b) declining work-related resources (n = 87), (c) high work-related resources (n = 46), and (d) low work-related resources (n = 36). Exhaustion was found to be an important predictor of job resources and flow, but it did not moderate their mutual association. Specifically, a low level of exhaustion was found to predict high levels of job resources and flow. Overall, these results suggest the importance of a person-oriented view of motivational processes at work. In addition, in order to fully understand positive motivational processes it seems important to investigate the role of negative well-being states as well.",0
https://doi.org/10.3102/1076998615606109,A Multilevel CFA-MTMM Model for Nested Structurally Different Methods,"The numerous advantages of structural equation modeling (SEM) for the analysis of multitrait–multimethod (MTMM) data are well known. MTMM-SEMs allow researchers to explicitly model the measurement error, to examine the true convergent and discriminant validity of the given measures, and to relate external variables to the latent trait as well as the latent method factors in the model. According to Eid et al. (2008) different MTMM measurement designs require different types of MTMM-SEMs. Eid et al. (2008) proposed three different MTMM-SEMs for measurement designs with (a) structurally different methods, (b) interchangeable methods, and (c) a combination of both types of methods. In the present work, we extend this taxonomy to a multilevel correlated traits–correlated methods minus one [CTC(M − 1)] model for nested structurally different methods. The new model enables researchers to study method effects on both measurement levels (i.e., within and between clusters, classes, schools, etc.) and evaluate the convergent and discriminant validity of the measures. The statistical performance of the model is examined by a simulation study, and recommendations for the application of the model are given.",0
https://doi.org/10.1186/1297-9686-44-10,Genetic evaluation of mastitis liability and recovery through longitudinal analysis of transition probabilities,"Many methods for the genetic analysis of mastitis use a cross-sectional approach, which omits information on, e.g., repeated mastitis cases during lactation, somatic cell count fluctuations, and recovery process. Acknowledging the dynamic behavior of mastitis during lactation and taking into account that there is more than one binary response variable to consider, can enhance the genetic evaluation of mastitis.Genetic evaluation of mastitis was carried out by modeling the dynamic nature of somatic cell count (SCC) within the lactation. The SCC patterns were captured by modeling transition probabilities between assumed states of mastitis and non-mastitis. A widely dispersed SCC pattern generates high transition probabilities between states and vice versa. This method can model transitions to and from states of infection simultaneously, i.e. both the mastitis liability and the recovery process are considered. A multilevel discrete time survival model was applied to estimate breeding values on simulated data with different dataset sizes, mastitis frequencies, and genetic correlations.Correlations between estimated and simulated breeding values showed that the estimated accuracies for mastitis liability were similar to those from previously tested methods that used data of confirmed mastitis cases, while our results were based on SCC as an indicator of mastitis. In addition, unlike the other methods, our method also generates breeding values for the recovery process.The developed method provides an effective tool for the genetic evaluation of mastitis when considering the whole disease course and will contribute to improving the genetic evaluation of udder health.",0
https://doi.org/10.1007/s11336-013-9358-9,Nonlinear Random-Effects Mixture Models for Repeated Measures,"A mixture model for repeated measures based on nonlinear functions with random effects is reviewed. The model can include individual schedules of measurement, data missing at random, nonlinear functions of the random effects, of covariates and of residuals. Individual group membership probabilities and individual random effects are obtained as empirical Bayes predictions. Although this is a complicated model that combines a mixture of populations, nonlinear regression, and hierarchical models, it is straightforward to estimate by maximum likelihood using SAS PROC NLMIXED. Many different models can be studied with this procedure. The model is more general than those that can be estimated with most special purpose computer programs currently available because the response function is essentially any form of nonlinear regression. Examples and sample code are included to illustrate the method. Ã‚Â© 2013 The Psychometric Society.",0
https://doi.org/10.1080/00273171.2017.1306432,Bayesian SEM for Specification Search Problems in Testing Factorial Invariance,"Specification search problems refer to two important but under-addressed issues in testing for factorial invariance: how to select proper reference indicators and how to locate specific non-invariant parameters. In this study, we propose a two-step procedure to solve these issues. Step 1 is to identify a proper reference indicator using the Bayesian structural equation modeling approach. An item is selected if it is associated with the highest likelihood to be invariant across groups. Step 2 is to locate specific non-invariant parameters, given that a proper reference indicator has already been selected in Step 1. A series of simulation analyses show that the proposed method performs well under a variety of data conditions, and optimal performance is observed under conditions of large magnitude of non-invariance, low proportion of non-invariance, and large sample sizes. We also provide an empirical example to demonstrate the specific procedures to implement the proposed method in applied research. The importance and influences are discussed regarding the choices of informative priors with zero mean and small variances. Extensions and limitations are also pointed out.",0
https://doi.org/10.1111/j.1744-6570.2001.tb00223.x,ESTIMATION OF SAMPLING VARIANCE OF CORRELATIONS IN META-ANALYSIS,"Monte Carlo simulations were conducted to compare the performance of the traditional (Fisher, 1954) and mean (Hunter & Schmidt, 1990) estimators of the sampling variance of correlations in meta-analysis. The mean estimator differs from the traditional estimator in that it uses the mean observed correlation, averaged across studies, in the sampling variance formula. The simulations investigated the homogeneous (i.e., no true correlation variance across studies) and heterogeneous case (i.e., true correlation variance across studies). Results reveal that, compared to the traditional estimator, the mean estimator provides less negatively biased estimates of sampling variance in the homogeneous and heterogeneous cases and more positively biased estimates in the heterogenous case. Thus, results support the use of the mean estimator unless strong, theory-based hypotheses regarding moderating effects exist.",0
https://doi.org/10.1037//0022-3514.83.1.126,The statistical analysis of data from small groups.,"The authors elaborate the complications and the opportunities inherent in the statistical analysis of small-group data. They begin by discussing nonindependence of group members' scores and then consider standard methods for the analysis of small-group data and determine that these methods do not take into account this nonindependence. A new method is proposed that uses multilevel modeling and allows for negative nonindependence and mutual influence. Finally, the complications of interactions, different group sizes, and differential effects are considered. The authors strongly urge that the analysis model of data from small-group studies should mirror the psychological processes that generate those data.",0
https://doi.org/10.1037//1082-989x.7.1.41,Multiphase mixed-effects models for repeated measures data.,"Behavior that develops in phases may exhibit distinctively different rates of change in one time period than in others. In this article, a mixed-effects model for a response that displays identifiable regimes is reviewed. An interesting component of the model is the change point. In substantive terms, the change point is the time when development switches from one phase to another. In a mixed-effects model, the change point can be a random coefficient. This possibility allows individuals to make the transition from one phase to another at different ages or after different lengths of time in treatment. Two examples are reviewed in detail, both of which can be estimated with software that is widely available.",0
,Robust inference using weighted least squares and quadratic estimating equations in latent variable modeling with categorical and continuous outcomes,,0
https://doi.org/10.1080/10705510903438872,Structural Equation Models of Latent Interactions: An Appropriate Standardized Solution and Its Scale-Free Properties,"Standardized parameter estimates are routinely used to summarize the results of multiple regression models of manifest variables and structural equation models of latent variables, because they facilitate interpretation. Although the typical standardization of interaction terms is not appropriate for multiple regression models, straightforward alternatives are well known (Aiken & West, 1991; Friedrich, 1982). Whereas the analogous problem exists for the estimation of latent interactions in structural equation modeling (SEM), the problem is more complex and apparently has not been resolved. Here we demonstrate that the appropriate “standardized” parameter estimates are easily formulated from parameter estimates routinely available from existing SEM software packages. Some properties of the appropriate “standardized” solution are mathematically derived, including the demonstration that the main and interaction effects are scale-free, as are the factor loadings. These desirable properties of the standardized...",0
https://doi.org/10.1111/rssa.12022,Fitting multilevel multivariate models with missing data in responses and covariates that may include interactions and non-linear terms,"Summary The paper extends existing models for multilevel multivariate data with mixed response types to handle quite general types and patterns of missing data values in a wide range of multilevel generalized linear models. It proposes an efficient Bayesian modelling approach that allows missing values in covariates, including models where there are interactions or other functions of covariates such as polynomials. The procedure can also be used to produce multiply imputed complete data sets. A simulation study is presented as well as the analysis of a longitudinal data set. The paper also shows how existing multiprocess models for handling endogeneity can be extended by the framework proposed.",0
https://doi.org/10.1097/ijg.0b013e31820bd1fd,Improved Prediction of Rates of Visual Field Loss in Glaucoma Using Empirical Bayes Estimates of Slopes of Change,"To describe and test a new methodology for estimation of rates of progressive visual field loss in glaucoma.This observational cohort study enrolled 643 eyes of 368 patients recruited from the Diagnostic Innovations in Glaucoma Study, followed for an average of 6.5±2.0 years. The visual field index was used to evaluate degree of visual field loss in standard automated perimetry. Growth mixture models were used to evaluate visual field index changes over time. Empirical Bayes estimates of best linear unbiased predictions (BLUPs) were used to obtain slopes of change based on the first 5 visual fields for each eye. These slopes were then used to predict future observations. The same procedure was done for ordinary least squares (OLS) estimates. The mean square error of the predictions was used to compare the predictive performance of the different methods.The growth mixture model successfully identified subpopulations of nonprogressors, slow, moderate, and fast progressors. The mean square error was significantly higher for OLS compared with the BLUP method (32.3 vs 13.9, respectively; P<0.001), indicating a better performance of the BLUP method to predict future observations. The benefit of BLUP predictions was especially evident in eyes with moderate and fast rates of change.Empirical Bayes estimates of rates of change performed significantly better than the commonly used technique of OLS regression in predicting future observations. Use of BLUP estimates should be considered when evaluating rates of functional change in glaucoma and predicting future impairment from the disease.",0
https://doi.org/10.1093/biomet/76.3.622,Restricted unbiased iterative generalized least-squares estimation,SUMMARY It is shown that the iterative least-squares procedure for estimating the parameters in a general multilevel random coefficients linear model can be modified to produce unbiased estimates of the random parameters. In the multivariate normal case these are equivalent to restricted maximum likelihood estimates.,0
https://doi.org/10.1007/bf02294110,Bayesian estimation in the two-parameter logistic model,"A Bayesian procedure is developed for the estimation of parameters in the two-parameter logistic item response model. Joint modal estimates of the parameters are obtained and procedures for the specification of prior information are described. Through simulation studies it is shown that Bayesian estimates of the parameters are superior to maximum likelihood estimates in the sense that they are (a) more meaningful since they do not drift out of range, and (b) more accurate in that they result in smaller mean squared differences between estimates and true values. © 1985 The Psychometric Society.",0
https://doi.org/10.1080/15332660802508364,Does “Made in …” Also Apply to Services? An Empirical Assessment of the Country-of-Origin Effect in Service Settings,"ABSTRACT The country-of-origin (COO) effect is one of the most prominent phenomena in the field of international marketing. Its influence on consumer quality perception, as well as on purchase decision, is strongly supported by a notable amount of empirical work. However, despite the obvious managerial relevance, most COO studies have been conducted with respect to products, whereas the impact of COO in service settings is a woefully underresearched area. This article fills that void by using limit conjoint analysis to empirically test the role of COO effect for services in two experimental settings. Specifically, the study investigates how much the relative importance of COO changes if additional quality cues are available for the consumer. Results lend support for the relevance of COO effects for services and provide useful implications for ways to utilize COO effects in international services marketing.",0
https://doi.org/10.1093/biomet/58.3.545,Recovery of inter-block information when block sizes are unequal,"SUMMARY A method is proposed for estimating intra-block and inter-block weights in the analysis of incomplete block designs with block sizes not necessarily equal. The method consists of maximizing the likelihood, not of all the data, but of a set of selected error contrasts. When block sizes are equal results are identical with those obtained by the method of Nelder (1968) for generally balanced designs. Although mainly concerned with incomplete block designs the paper also gives in outline an extension of the modified maximum likelihood procedure to designs with a more complicated block structure. In this paper we consider the estimation of weights to be used in the recovery of interblock information in incomplete block designs with possibly unequal block sizes. The problem can also be thought of as one of estimating constants and components of variance from data arranged in a general two-way classification when the effects of one classification are regarded as fixed and the effects of the second classification are regarded as random. Nelder (1968) described the efficient estimation of weights in generally balanced designs, in which the blocks are usually, although not always, of equal size. Lack of balance resulting from unequal block sizes is, however, common in some experimental work, for example in animal breeding experiments. The maximum likelihood procedure described by Hartley & Rao (1967) can be used but does not give the same estimates as Nelder's method in the balanced case. As will be shown, the two methods in effect use the same weighted sums of squares of residuals but assign different expectations. In the maximum likelihood approach, expectations are taken over a conditional distribution with the treatment effects fixed at their estimated values. In contrast Nelder uses unconditional expectations. The difference between the two methods is analogous to the well-known difference between two methods of estimating the variance o2 of a normal distribution, given a random sample of n values. Both methods use the same total sum of squares of deviations. But",0
,Discovering statistics using SPSS for Windows.,"The book Discovering Statistics Using SPSS for Windows is exactly that! Since it calculates amazingly fast, in the recent years, the computer has become the most useful and helpful tool for the researchers in almost every field of knowledge be it open and distance education, psychology, sociology, management or else. Quality research not only depends on fair data collection techniques but also on the respectable treatment of that data. Once the data have been collected, what is most important is the storage of data, their fast and 'as it is' retrieval when required, and their proper processing and analyses. Computers are of immense help in all these operations. However, it is interesting to note that computer is just a tool and what it will vomit (or produce) depends entirely upon the fingers and mind of its operator (or the researcher in the research settings). The computer has no mind (up to now!). So, it is the researcher who should be held responsible for the unfair treatment of the data. Proper treatment of the research data not only requires research attitude but also research aptitude, and proficiency and a solid background in statistical treatment of the data. For this purpose, some good textbooks are available that explain statistical theory and techniques. And some other good textbooks are also there on the role of computers and statistical software (e.g. SPSS) in the practical application of these statistical procedures employed on the data through computers. Here, the present book by Andy Field is a rare and excellent combination of both statistical theory as well as statistical treatment of the data using SPSS under one umbrella.",0
https://doi.org/10.1080/00273171.2012.673924,Estimating the Reliability of Aggregated and Within-Person Centered Scores in Ecological Momentary Assessment,"A procedure for estimating the reliability of test scores in the context of ecological momentary assessment (EMA) was proposed to take into account the characteristics of EMA measures. Two commonly used test scores in EMA were considered: the aggregated score (AGGS) and the within-person centered score (WPCS). Conceptually, AGGS and WPCS represent the interindividual differences and the intraindividual differences, respectively. The reliability coefficients for AGGS and WPCS were derived using a multilevel factor model with a serial correlation structure framework. Point estimates and confidence intervals of these coefficients were obtained using Mx ( Neale, Boker, Xie, & Maes, 2004 ). A simulation study showed that the proposed procedure performed well empirically. Diary data from Huang (2009) , which recorded daily joy level of 110 undergraduate students for 8 days, was used to illustrate the applicability of the proposed method.",0
https://doi.org/10.1214/aos/1176345010,Empirical Bayes Estimation of the Multivariate Normal Covariance Matrix,"Let $\mathbf{S}_{p \times p}$ have a Wishart distribution with scale matrix $\Sigma$ and $k$ degrees of freedom. Estimators of $\Sigma$ are given for each of the loss functions $L_1(\hat{\Sigma}, \Sigma) = \operatorname{tr} (\hat{\Sigma}\Sigma^{-1}) - \log \det (\hat{\Sigma}\Sigma^{-1}) - p$ and $L_2(\hat{\Sigma}, \Sigma) = \operatorname{tr} (\hat{\Sigma}\Sigma^{-1} - I)^2$. The obvious estimators of $\Sigma$ are the scalar multiples of $\mathbf{S}$, i.e., $a\mathbf{S}$ where $0 < a \leqslant 1/k$. (Recall that $(1/k)\mathbf{S}$ is unbiased.) For each problem $(\Sigma, \hat{\Sigma}, L_i), i = 1, 2$, we provide empirical Bayes estimators which dominate $a\mathbf{S}$ by a substantial amount. It is seen that the uniform reduction in the risk function determined by $L_2$ is at least $100(p + 1)/(k + p + 1){\tt\%}$. Dominance results for $L_1$ and $L_2$ were first given by James and Stein.",0
https://doi.org/10.1177/01466216970212004,Implementation of Marginal Bayesian Estimation with Four-Parameter Beta Prior Distributions,"Item parameter estimation for the threeparameter logistic model (3PLM) is sometimes problematic. The estimation algorithm of the 3PLM maximum likelihood estimation procedure often fails, which results in invalid parameter estimates. A procedure based on the marginal Bayesian estimation method introduced by Bock &amp; Aitkin (1981), Swaminathan &amp; Gifford (1985, 1986), and Mislevy &amp; Bock (1990) is proposed here to improve the item parameter estimates for the 3PLM. Four-parameter beta distributions are used as prior distributions for estimating item parameters. A computer simulation study suggested that implementing the marginal Bayesian estimation algorithm with four-parameter beta prior distributions and then updating the priors with empirical means of the updated intermediate estimates can improve item parameter estimation when accurate prior information about the unknown parameters is not available.",0
,Applications of ideal point approaches to scale construction and scoring in personality measurement: The development of a six-faceted measure of conscientiousness.,,0
https://doi.org/10.1177/0013164414527448,Maximum Likelihood Item Easiness Models for Test Theory Without an Answer Key,Cultural consensus theory (CCT) is a data aggregation technique with many applications in the social and behavioral sciences. We describe the intuition and theory behind a set of CCT models for continuous type data using maximum likelihood inference methodology. We describe how bias parameters can be incorporated into these models. We introduce two extensions to the basic model in order to account for item rating easiness/difficulty. The first extension is a multiplicative model and the second is an additive model. We show how the multiplicative model is related to the Rasch model. We describe several maximum-likelihood estimation procedures for the models and discuss issues of model fit and identifiability. We describe how the CCT models could be used to give alternative consensus-based measures of reliability. We demonstrate the utility of both the basic and extended models on a set of essay rating data and give ideas for future research.,0
https://doi.org/10.1016/j.csda.2005.02.008,MCMC algorithms for constrained variance matrices,The problem of finding a generic algorithm for applying Markov chain Monte Carlo (MCMC) estimation procedures to statistical models that include variance matrices with additional parameter constraints is considered. Such problems can be split between additional constraints across variance matrices and within variance matrices. The case of additional constraints across variance matrices is considered here for the first time and a review of existing work on the case of additional parameter constraints within a variance matrix is given. Two simple single-site updating random walk Metropolis algorithms are described which have the advantage of generality in that they can be applied to virtually all scenarios. Four applications where these methods can be used in practice are given. Some situations when such single-site algorithms break down are described and multiple-site alternatives are briefly discussed.,0
https://doi.org/10.1007/978-0-387-35768-3,Finite Mixture and Markov Switching Models,"WINNER OF THE 2007 DEGROOT PRIZE!  The prominence of finite mixture modelling is greater than ever. Many important statistical topics like clustering data, outlier treatment, or dealing with unobserved heterogeneity involve finite mixture models in some way or other. The area of potential applications goes beyond simple data analysis and extends to regression analysis and to non-linear time series analysis using Markov switching models.  For more than the hundred years since Karl Pearson showed in 1894 how to estimate the five parameters of a mixture of two normal distributions using the method of moments, statistical inference for finite mixture models has been a challenge to everybody who deals with them. In the past ten years, very powerful computational tools emerged for dealing with these models which combine a Bayesian approach with recent Monte simulation techniques based on Markov chains. This book reviews these techniques and covers the most recent advances in the field, among them bridge sampling techniques and reversible jump Markov chain Monte Carlo methods.  It is the first time that the Bayesian perspective of finite mixture modelling is systematically presented in book form. It is argued that the Bayesian approach provides much insight in this context and is easily implemented in practice. Although the main focus is on Bayesian inference, the author reviews several frequentist techniques, especially selecting the number of components of a finite mixture model, and discusses some of their shortcomings compared to the Bayesian approach.  The aim of this book is to impart the finite mixture and Markov switching approach to statistical modelling to a wide-ranging community. This includes not only statisticians, but also biologists, economists, engineers, financial agents, market researcher, medical researchers or any other frequent user of statistical models. This book should help newcomers to the field to understand how finite mixture and Markov switching models are formulated, what structures they imply on the data, what they could be used for, and how they are estimated. Researchers familiar with the subject also will profit from reading this book. The presentation is rather informal without abandoning mathematical correctness. Previous notions of Bayesian inference and Monte Carlo simulation are useful but not needed.",0
https://doi.org/10.1037/met0000065,Improving transparency and replication in Bayesian statistics: The WAMBS-Checklist.,"Bayesian statistical methods are slowly creeping into all fields of science and are becoming ever more popular in applied research. Although it is very attractive to use Bayesian statistics, our personal experience has led us to believe that naively applying Bayesian methods can be dangerous for at least 3 main reasons: the potential influence of priors, misinterpretation of Bayesian features and results, and improper reporting of Bayesian results. To deal with these 3 points of potential danger, we have developed a succinct checklist: the WAMBS-checklist (When to worry and how to Avoid the Misuse of Bayesian Statistics). The purpose of the questionnaire is to describe 10 main points that should be thoroughly checked when applying Bayesian analysis. We provide an account of ""when to worry"" for each of these issues related to: (a) issues to check before estimating the model, (b) issues to check after estimating the model but before interpreting results, (c) understanding the influence of priors, and (d) actions to take after interpreting results. To accompany these key points of concern, we will present diagnostic tools that can be used in conjunction with the development and assessment of a Bayesian model. We also include examples of how to interpret results when ""problems"" in estimation arise, as well as syntax and instructions for implementation. Our aim is to stress the importance of openness and transparency of all aspects of Bayesian estimation, and it is our hope that the WAMBS questionnaire can aid in this process. (PsycINFO Database Record",0
,Combining estimates of effect size.,,0
https://doi.org/10.1111/j.1540-5907.2008.00352.x,Economic Inequality and Intolerance: Attitudes toward Homosexuality in 35 Democracies,"Using hierarchical linear models fitted to data from the World Values Survey and national statistics for 35 countries, this article builds on the postmaterialist thesis by assessing the impact of economic inequality across and within nations on attitudes toward homosexuality. It provides evidence that tolerance tends to decline as national income inequality rises. For professionals and managers, the results also support the postmaterialist argument that economic development leads to more tolerant attitudes. On the other hand, attitudes of the working class are generally less tolerant, and contrary to expectations of the postmaterialist thesis, are seemingly unaffected by economic development. In other words, economic development influences attitudes only for those who benefit most. These findings have political implications, suggesting that state policies that have the goal of economic growth but fail to consider economic inequality may contribute to intolerant social and political values, an attribute widely considered detrimental for the health of democracy. I nglehart’s (1987, 1990, 1997) postmaterialist thesis suggests that liberal values result from democracy, economic development, and modernization. Important to this argument is the idea that the prosperity of modern societies allows people to shift their attentionfrommaterialconcernstoso-called“postmaterialist” concerns, such as social issues and self-expression. This shift to postmaterialist values allegedly results in greater social tolerance. Moreover, by focusing on average value differences across country and time, the postmaterialist thesis implies that economic prosperity affects the attitudes of citizens in all economic positions within a given nationinmuchthesameway.Althoughinequalitywithin nationsisnotentirelydiscounted,thetheoryassumesthat once economic development reaches a particularly high level, inequality lessens to the point that it no longer influences values (Inglehart 1987). The present research takes issue with the assumption that national economic prosperity affects all members within a nation in a similar manner. Given the vast differences in economic conditions and life chances according to income group, social class, and occupation, even within rich democracies, it is not controversial to argue",0
https://doi.org/10.1111/j.1467-985x.2008.00593.x,Multivariate meta-analysis: the effect of ignoring within-study correlation,"Summary. Multivariate meta-analysis allows the joint synthesis of summary estimates from multiple end points and accounts for their within-study and between-study correlation. Yet practitioners usually meta-analyse each end point independently. I examine the role of within-study correlation in multivariate meta-analysis, to elicit the consequences of ignoring it. Using analytic reasoning and a simulation study, the within-study correlation is shown to influence the ‘borrowing of strength’ across end points, and wrongly ignoring it gives meta-analysis results with generally inferior statistical properties; for example, on average it increases the mean-square error and standard error of pooled estimates, and for non-ignorable missing data it increases their bias. The influence of within-study correlation is only negligible when the within-study variation is small relative to the between-study variation, or when very small differences exist across studies in the within-study covariance matrices. The findings are demonstrated by applied examples within medicine, dentistry and education. Meta-analysts are thus encouraged to account for the correlation between end points. To facilitate this, I conclude by reviewing options for multivariate meta-analysis when within-study correlations are unknown; these include obtaining individual patient data, using external information, performing sensitivity analyses and using alternatively parameterized models.",0
https://doi.org/10.1007/bf02294318,Bayesian estimation and testing of structural equation models,"The Gibbs sampler can be used to obtain samples of arbitrary size from the posterior distribution over the parameters of a structural equation model (SEM) given covariance data and a prior distribution over the parameters. Point estimates, standard deviations and interval estimates for the parameters can be computed from these samples. If the prior distribution over the parameters is uninformative, the posterior is proportional to the likelihood, and asymptotically the inferences based on the Gibbs sample are the same as those based on the maximum likelihood solution, for example, output from LISREL or EQS. In small samples, however, the likelihood surface is not Gaussian and in some cases contains local maxima. Nevertheless, the Gibbs sample comes from the correct posterior distribution over the parameters regardless of the sample size and the shape of the likelihood surface. With an informative prior distribution over the parameters, the posterior can be used to make inferences about the parameters of underidentified models, as we illustrate on a simple errors-in-variables model.",0
https://doi.org/10.1111/bmsp.12058,Latent growth curve analysis with dichotomous items: Comparing four approaches,"A Monte Carlo study was used to compare four approaches to growth curve analysis of subjects assessed repeatedly with the same set of dichotomous items: A two-step procedure first estimating latent trait measures using MULTILOG and then using a hierarchical linear model to examine the changing trajectories with the estimated abilities as the outcome variable; a structural equation model using modified weighted least squares (WLSMV) estimation; and two approaches in the framework of multilevel item response models, including a hierarchical generalized linear model using Laplace estimation, and Bayesian analysis using Markov chain Monte Carlo (MCMC). These four methods have similar power in detecting the average linear slope across time. MCMC and Laplace estimates perform relatively better on the bias of the average linear slope and corresponding standard error, as well as the item location parameters. For the variance of the random intercept, and the covariance between the random intercept and slope, all estimates are biased in most conditions. For the random slope variance, only Laplace estimates are unbiased when there are eight time points.",0
https://doi.org/10.1007/s00221-013-3738-3,The role of spatiotemporal and spectral cues in segregating short sound events: evidence from auditory Ternus display,"Previous studies using auditory sequences with rapid repetition of tones revealed that spatiotemporal cues and spectral cues are important cues used to fuse or segregate sound streams. However, the perceptual grouping was partially driven by the cognitive processing of the periodicity cues of the long sequence. Here, we investigate whether perceptual groupings (spatiotemporal grouping vs. frequency grouping) could also be applicable to short auditory sequences, where auditory perceptual organization is mainly subserved by lower levels of perceptual processing. To find the answer to that question, we conducted two experiments using an auditory Ternus display. The display was composed of three speakers (A, B and C), with each speaker consecutively emitting one sound consisting of two frames (AB and BC). Experiment 1 manipulated both spatial and temporal factors. We implemented three 'within-frame intervals' (WFIs, or intervals between A and B, and between B and C), seven 'inter-frame intervals' (IFIs, or intervals between AB and BC) and two different speaker layouts (inter-distance of speakers: near or far). Experiment 2 manipulated the differentiations of frequencies between two auditory frames, in addition to the spatiotemporal cues as in Experiment 1. Listeners were required to make two alternative forced choices (2AFC) to report the perception of a given Ternus display: element motion (auditory apparent motion from sound A to B to C) or group motion (auditory apparent motion from sound 'AB' to 'BC'). The results indicate that the perceptual grouping of short auditory sequences (materialized by the perceptual decisions of the auditory Ternus display) was modulated by temporal and spectral cues, with the latter contributing more to segregating auditory events. Spatial layout plays a less role in perceptual organization. These results could be accounted for by the 'peripheral channeling' theory. Â© 2013 Springer-Verlag Berlin Heidelberg.",0
https://doi.org/10.1080/00220970109599492,Using Multilevel Modeling in Large-Scale Planned Variation Educational Experiments: Improving Understanding of Intervention Effects,"The author shows how one can combine a large-scale planned variation experimental design and multilevel analysis to address research questions that go beyond the issue of overall treatment effectiveness. In particular, the design and analysis approach presented here can address the following research questions: how an educational intervention produces its results, for whom the intervention works, and whether the intervention works well across different contexts. The author used data based on a large-scale educational field experiment to describe multilevel models that one can use to address this set of research questions. Important design and analysis considerations are highlighted, especially with regard to assessing the impact of school-level characteristics.",0
https://doi.org/10.1121/1.406696,A maximum‐likelihood method for estimating thresholds in a yes–no task,"A maximum-likelihood procedure for estimating threshold values in a yes-no task is presented. In computer simulations of this procedure, it is demonstrated that the variability of the threshold estimates is little affected by the density of the hypotheses tested for a fixed range, or by serious misestimates of the slope of the psychometric functions. The threshold value is also largely independent of the starting value of the signal. The standard deviation of the threshold estimates appears to decrease with the square root of the number of trials, with a 2- to 3-dB standard deviation possible if only 12 trials are used in the threshold estimates. Data are presented using human listeners tested on 5 days. Two threshold estimates, based on 12 trials, were made at each of the six audiometric frequencies on each day. The mean data appear sensible, and the standard deviation of the measured thresholds is about 3 dB. Using this procedure, it takes less than 3 min to measure the audiogram for a single ear.",0
https://doi.org/10.1002/sim.5745,Bayesian multivariate meta-analysis with multiple outcomes,"There has been a recent growth in developments of multivariate meta-analysis. We extend the methodology of Bayesian multivariate meta-analysis to the situation when there are more than two outcomes of interest, which is underexplored in the current literature. Our objective is to meta-analyse summary data from multiple outcomes simultaneously, accounting for potential dependencies among the data. One common issue is that studies do not all report all of the outcomes of interests, and we take an approach relying on marginal modelling of only the reported data. We employ a separation prior for the between-study variance-covariance matrix, which offers an improvement on the conventional inverse-Wishart prior, showing robustness in estimation and flexibility in incorporating prior information. Particular challenges arise when the number of outcomes is large relative to the number of studies because the number of parameters in the variance-covariance matrix can become substantial and there can be very little information with which to estimate between-study correlation coefficients. We explore assumptions that reduce the number of parameters in this matrix, including assumptions of homogenous variances, homogenous correlations for certain outcomes and positive correlation coefficients. We illustrate the methods with an example data set from the Cochrane Database of Systematic Reviews.",0
https://doi.org/10.1080/00273170902949719,The Effects of Educational Diversity in a National Sample of Law Students: Fitting Multilevel Latent Variable Models in Data With Categorical Indicators,"Controversy surrounding the use of race-conscious admissions can be partially resolved with improved empirical knowledge of the effects of racial diversity in educational settings. We use a national sample of law students nested in 64 law schools to test the complex and largely untested theory regarding the effects of educational diversity on student outcomes. Social scientists who study these outcomes frequently encounter both latent variables and nested data within a single analysis. Yet, until recently, an appropriate modeling technique has been computationally infeasible, and consequently few applied researchers have estimated appropriate models to test their theories, sometimes limiting the scope of their research question. Our results, based on disaggregated multilevel structural equation models, show that racial diversity is related to a reduction in prejudiced attitudes and increased perceived exposure to diverse ideas and that these effects are mediated by more frequent interpersonal contact with diverse peers. These findings provide support for the idea that administrative manipulation of educational diversity may lead to improved student outcomes. Admitting a racially/ethnically diverse student body provides an educational experience that encourages increased exposure to diverse ideas and belief systems.",0
https://doi.org/10.1037/a0020511,Rational approximations to rational models: Alternative algorithms for category learning.,"Rational models of cognition typically consider the abstract computational problems posed by the environment, assuming that people are capable of optimally solving those problems. This differs from more traditional formal models of cognition, which focus on the psychological processes responsible for behavior. A basic challenge for rational models is thus explaining how optimal solutions can be approximated by psychological processes. We outline a general strategy for answering this question, namely to explore the psychological plausibility of approximation algorithms developed in computer science and statistics. In particular, we argue that Monte Carlo methods provide a source of rational process models that connect optimal solutions to psychological processes. We support this argument through a detailed example, applying this approach to Anderson's (1990, 1991) rational model of categorization (RMC), which involves a particularly challenging computational problem. Drawing on a connection between the RMC and ideas from nonparametric Bayesian statistics, we propose 2 alternative algorithms for approximate inference in this model. The algorithms we consider include Gibbs sampling, a procedure appropriate when all stimuli are presented simultaneously, and particle filters, which sequentially approximate the posterior distribution with a small number of samples that are updated as new data become available. Applying these algorithms to several existing datasets shows that a particle filter with a single particle provides a good description of human inferences.",0
,"The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models",,0
https://doi.org/10.1016/j.prevetmed.2013.10.009,"Temporal associations between low body condition, lameness and milk yield in a UK dairy herd","Previous work has hypothesised that cows in low body condition become lame. We tested this in a prospective longitudinal study. Body condition score (BCS), causes of lameness and milk yield were collected from a 600-cow herd over 44-months. Mixed effect binomial models and a continuous outcome model were used to investigate the associations between lameness, BCS and milk yield. In total, 14,320 risk periods were obtained from 1137 cows. There were 1510 lameness treatments: the most common causes of lameness were sole ulcer (SU) (39%), sole haemorrhage (SH) (13%), digital dermatitis (DD) (10%) and white line disease (WLD) (8%). These varied by year and year quarter. Body condition was scored at 60-day intervals. BCS ranged from 1 to 5 with a mean of 2.5, scores were higher in very early lactation but varied widely throughout lactation; approximately 45% of scores were <2.5. The key finding was that BCS<2.5 was associated with an increased risk of treatment for lameness in the following 0-2 months and >2-4 months for all causes of lameness and also specifically for SU/WLD lameness. BCS<2.5 was associated with an increased risk of treatment for SH in the following 0-2 months but not >2-4 months. There was no such association with DD. All lameness, SU/WLD, SH and DD were significantly more likely to occur in cows that had been lame previously, but the effect of BCS was present even when all repeat cases of lameness were excluded from the analysis. Milk yield was significantly higher and fell in the month before treatment in cows lame with SU/WLD but it was not significantly higher for cows that were treated for DD compared with non-lame cows. These findings support the hypothesis that low BCS contributes to the development of horn related claw lameness but not infectious claw diseases in dairy cows. One link between low BCS and lameness is a thin digital cushion which has been proposed as a trigger for claw horn disease. Cows with BCS 2 produced more milk than cows with BCS 2.5, however, this was only approximately 100 kg difference in yield over a 305-day lactation. Given the increased risk of lameness in cows with BCS 2, the direct costs of lameness and the small variability in milk yield by BCS, preventing cows from falling to BCS<2.5 would improve cow welfare and be economically beneficial.",0
https://doi.org/10.1037/0033-2909.96.1.201,Estimating the nonlinear and interactive effects of latent variables.,"Describes a procedure that enables researchers to estimate nonlinear and interactive effects of latent variables in structural equation models. Given that the latent variables are normally distributed, the parameters of such models can be estimated. To do this, products of the measured variables are used as indicators of latent product variables. Estimation must be done using a procedure that allows nonlinear constraints on parameters. The procedure is demonstrated in 3 examples. The 1st 2 examples use artificial data with known parameter values. These parameters are successfully recovered by the procedure. The final complex example uses national election survey data. (14 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1984 American Psychological Association.",0
https://doi.org/10.1185/03007995.2015.1135110,Network meta-analysis of treatments for type 2 diabetes mellitus following failure with metformin plus sulfonylurea,"Aims The efficacy and safety of sodium-glucose linked transporters (SGLT2s) plus metformin and a sulfonylurea (MET + SU) for the treatment of type 2 diabetes mellitus (T2DM) in patients who fail to achieve glycemic control with MET + SU, relative to other triple therapies licensed in the EU, were estimated. Methods A systematic literature review and network meta-analysis (NMA) of randomized controlled trials (RCTs) involving anti-diabetes treatments added to MET + SU were conducted.Of 2236 abstracts identified through a systematic literature review, 30 RCTs published between 2003 and 2013 were included. RCTs ranged from 12 to 52 weeks in duration, included 28 to 1274 patients, were of parallel design, and most were open-label. Comparators included placebo (reference treatment), SGLT2 inhibitors, dipeptidyl peptidase-4 (DPP-4) inhibitors, thiazolidinediones (TZDs), alpha-glucosidase inhibitors (AGIs), meglitinides, glucagon-like peptide 1 (GLP-1) analogues, and basal, bolus, and biphasic insulin, all added on to MET + SU, as well as basal and biphasic insulin added to MET and monotherapy. The mean change (%) in HbA1c levels compared to placebo was -0.86 for SGLT2 inhibitors, -0.68 for DPP-4 inhibitors, -0.93 for TZDs, and -1.07 for GLP-1 analogues, respectively. Only SGLT2 inhibitors and GLP-1 analogues led to a weight loss (-1.71 kg and -1.14 kg, respectively) and decrease in systolic blood pressure (SBP; -3.73 mmHg and -2.90 mmHg, respectively), while all other treatments showed either an increase or no changes in weight or SBP. Conclusion SGLT2 inhibitors are at least as effective as other classes of antidiabetic agents at controlling HbA1c levels, while providing the additional benefits of weight loss and reducing SBP. Additionally, since the risk of hypoglycemia is similar or reduced with SGLT2 inhibitors, patients do not have to trade off efficacy for tolerability. Similar findings were observed for GLP-1 analogues.",0
https://doi.org/10.1080/02664763.2012.710896,A generalized Q–Q plot for longitudinal data,"Most biomedical research is carried out using longitudinal studies. The method of generalized estimating equations (GEEs) introduced by Liang and Zeger [Longitudinal data analysis using generalized linear models, Biometrika 73 (1986), pp. 13-22] and Zeger and Liang [Longitudinal data analysis for discrete and continuous outcomes, Biometrics 42 (1986), pp. 121-130] has become a standard method for analyzing non-normal longitudinal data. Since then, a large variety of GEEs have been proposed. However, the model diagnostic problem has not been explored intensively. Oh et al. [Modeldiagnostic plots for repeated measures data using the generalized estimating equations approach, Comput. Statist. Data Anal. 53 (2008), pp. 222-232] proposed residual plots based on the quantile-quantile (Q-Q) plots of the chi(2)-distribution for repeated-measures data using the GEE methodology. They considered the Pearson, Anscombe and deviance residuals. In this work, we propose to extend this graphical diagnostic using a generalized residual. A simulation study is presented as well as two examples illustrating the proposed generalized Q-Q plots.",0
https://doi.org/10.1590/1678-7153.201528302,Intrinsic Motivation Inventory: Psychometric Properties in the Context of First Language and Mathematics Learning,"Intrinsic Motivation Inventory (IMI) is a multidimensional measurement grounded on the Self-Determination Theory (SDT) used in assessing the subjective experiences of participants when developing an activity. The aim of this study is to analyze the characteristics of IMI among Portuguese students, testing four organizational models (unidimensional, multidimensional, hierarchical and bi-factor). A total of 3685 students from the 5th to the 12th grades (50.4% boys) participated in the study (M = 13.67, SD = 2.26). Two versions of IMI were used (First Language and Mathematics) with twenty-one items distributed over five subscales: Enjoyment, Perceived Competence, Pressure/Tension, Perceived Choice and Value/Utility. The confirmatory factor analysis corroborated the multidimensionality of intrinsic motivation, and that the bi-factor model presented the best fit indexes. This model showed the existence of one general factor, resulting from the contribution of all individual dimensions and the particularities of most of them. Furthermore, results also highlighted satisfactory reliability scores both through Cronbach's alpha scores and Composite reliability scores. These results indicate that this scale is appropriate to evaluate the underlying constructs of the theoretical model of SDT and allows for the calculation of a global measure of intrinsic motivation, as well as specific measures for their predictors.",0
https://doi.org/10.1002/sim.2112,How vague is vague? A simulation study of the impact of the use of vague prior distributions in MCMC using WinBUGS,"There has been a recent growth in the use of Bayesian methods in medical research. The main reasons for this are the development of computer intensive simulation based methods such as Markov chain Monte Carlo (MCMC), increases in computing power and the introduction of powerful software such as WinBUGS. This has enabled increasingly complex models to be fitted. The ability to fit these complex models has led to MCMC methods being used as a convenient tool by frequentists, who may have no desire to be fully Bayesian. Often researchers want 'the data to dominate' when there is no prior information and thus attempt to use vague prior distributions. However, with small amounts of data the use of vague priors can be problematic. The results are potentially sensitive to the choice of prior distribution. In general there are fewer problems with location parameters. The main problem is with scale parameters. With scale parameters, not only does one have to decide the distributional form of the prior distribution, but also whether to put the prior distribution on the variance, standard deviation or precision. We have conducted a simulation study comparing the effects of 13 different prior distributions for the scale parameter on simulated random effects meta-analysis data. We varied the number of studies (5, 10 and 30) and compared three different between-study variances to give nine different simulation scenarios. One thousand data sets were generated for each scenario and each data set was analysed using the 13 different prior distributions. The frequentist properties of bias and coverage were investigated for the between-study variance and the effect size. The choice of prior distribution was crucial when there were just five studies. There was a large variation in the estimates of the between-study variance for the 13 different prior distributions. With a large number of studies the choice of prior distribution was less important. The effect size estimated was not biased, but the precision with which it was estimated varied with the choice of prior distribution leading to varying coverage intervals and, potentially, to different statistical inferences. Again there was less of a problem with a larger number of studies. There is a particular problem if the between-study variance is close to the boundary at zero, as MCMC results tend to produce upwardly biased estimates of the between-study variance, particularly if inferences are based on the posterior mean. The choice of 'vague' prior distribution can lead to a marked variation in results, particularly in small studies. Sensitivity to the choice of prior distribution should always be assessed.",0
https://doi.org/10.1037/0033-2909.119.1.166,Latent variable interaction and quadratic effect estimation: A two-step technique using structural equation analysis.,"The author proposes an alternative estimation technique for latent variable interactions and quadraties. Available techniques for specifying these variables in structural equation models require adding variables or constraint equations that can produce specification tedium and errors or estimation difficulties. The proposed technique avoids these difficulties and may be useful for EQS, LISREL 7, and LISREL 8 users. First, measurement parameters for indicator Ioadings and errors of linear latent variables are estimated in a measurement model that excludes the interaction and quadratic variables. Next, these estimates are used to calculate values for the indicator loadings and error variances ofthe interaction and quadratic latent variables. Then, these calculated values are specified as constants in the structural model containing the interaction and quadratic variables. Interaction and quadratic effects are routinely reported for categorical independent variables (i.e., in analysis of variance) frequently to aid in the interpretation of significant main effects. However, interaction and quadratic effects are less frequently reported for continuous independent variables.",0
https://doi.org/10.1037/a0026728,"The effect of exposure duration on visual character identification in single, whole, and partial report.","The psychometric function of single-letter identification is typically described as a function of stimulus intensity. However, the effect of stimulus exposure duration on letter identification remains poorly described. This is surprising because the effect of exposure duration has played a central role in modeling performance in whole and partial report (Shibuya & Bundesen, 1988). Therefore, we experimentally investigated visual letter identification as a function of exposure duration. We compared the exponential, the gamma, and the Weibull psychometric functions, all with a temporal offset included, as well as the ex-Gaussian, the log-logistic, and finally the squared-logistic, which is a psychometric function that to our knowledge has not been described before. The log-logistic and the squared-logistic psychometric function fit well to experimental data. Also, we conducted an experiment to test the ability of the psychometric functions to fit single-letter identification data, at different stimulus contrast levels; also here the same psychometric functions prevailed. Finally, after insertion into Bundesen's Theory of Visual Attention (Bundesen, 1990), the same psychometric functions enable closer fits to data from a previous whole and partial report experiment.",0
https://doi.org/10.1080/13506285.2013.844963,Environment sensitivity in hierarchical representations,,0
https://doi.org/10.1093/ije/dys041,"Predicting the extent of heterogeneity in meta-analysis, using empirical data from the Cochrane Database of Systematic Reviews","Many meta-analyses contain only a small number of studies, which makes it difficult to estimate the extent of between-study heterogeneity. Bayesian meta-analysis allows incorporation of external evidence on heterogeneity, and offers advantages over conventional random-effects meta-analysis. To assist in this, we provide empirical evidence on the likely extent of heterogeneity in particular areas of health care.Our analyses included 14 886 meta-analyses from the Cochrane Database of Systematic Reviews. We classified each meta-analysis according to the type of outcome, type of intervention comparison and medical specialty. By modelling the study data from all meta-analyses simultaneously, using the log odds ratio scale, we investigated the impact of meta-analysis characteristics on the underlying between-study heterogeneity variance. Predictive distributions were obtained for the heterogeneity expected in future meta-analyses.Between-study heterogeneity variances for meta-analyses in which the outcome was all-cause mortality were found to be on average 17% (95% CI 10-26) of variances for other outcomes. In meta-analyses comparing two active pharmacological interventions, heterogeneity was on average 75% (95% CI 58-95) of variances for non-pharmacological interventions. Meta-analysis size was found to have only a small effect on heterogeneity. Predictive distributions are presented for nine different settings, defined by type of outcome and type of intervention comparison. For example, for a planned meta-analysis comparing a pharmacological intervention against placebo or control with a subjectively measured outcome, the predictive distribution for heterogeneity is a log-normal (-2.13, 1.58(2)) distribution, which has a median value of 0.12. In an example of meta-analysis of six studies, incorporating external evidence led to a smaller heterogeneity estimate and a narrower confidence interval for the combined intervention effect.Meta-analysis characteristics were strongly associated with the degree of between-study heterogeneity, and predictive distributions for heterogeneity differed substantially across settings. The informative priors provided will be very beneficial in future meta-analyses including few studies.",0
https://doi.org/10.1177/0149206314551964,Improving the Meta-Analytic Assessment of Effect Size Variance With an Informed Bayesian Prior,"Meta-analytic estimation of effect size variance is critical for determining the degree to which a relationship or finding generalizes across contexts. In most meta-analyses, population effect size variability is estimated by subtracting expected sampling error variance from observed variance, using only information from a limited set of available studies. We propose an improved Bayesian variance estimation technique that incorporates findings from previous meta-analytic research through an informed prior distribution of likely levels of effect size variance. The logic of exchangeability as a conceptual foundation for using an informed prior is explicated. On the basis of Monte Carlo simulations, we find the traditional method of meta-analytic variance estimation the most biased and least accurate technique across all sizes of meta-analyses considered. The Bayesian methodology incorporating an informed prior proved to be the most accurate and overall least biased of all estimation methods. Conceptual advantages and limitations that must be taken into account when incorporating an informed prior to estimate variability of effect sizes in a meta-analysis are also discussed.",0
https://doi.org/10.1186/1471-2288-14-103,Methods for calculating confidence and credible intervals for the residual between-study variance in random effects meta-regression models,"Meta-regression is becoming increasingly used to model study level covariate effects. However this type of statistical analysis presents many difficulties and challenges. Here two methods for calculating confidence intervals for the magnitude of the residual between-study variance in random effects meta-regression models are developed. A further suggestion for calculating credible intervals using informative prior distributions for the residual between-study variance is presented.Two recently proposed and, under the assumptions of the random effects model, exact methods for constructing confidence intervals for the between-study variance in random effects meta-analyses are extended to the meta-regression setting. The use of Generalised Cochran heterogeneity statistics is extended to the meta-regression setting and a Newton-Raphson procedure is developed to implement the Q profile method for meta-analysis and meta-regression. WinBUGS is used to implement informative priors for the residual between-study variance in the context of Bayesian meta-regressions.Results are obtained for two contrasting examples, where the first example involves a binary covariate and the second involves a continuous covariate. Intervals for the residual between-study variance are wide for both examples.Statistical methods, and R computer software, are available to compute exact confidence intervals for the residual between-study variance under the random effects model for meta-regression. These frequentist methods are almost as easily implemented as their established counterparts for meta-analysis. Bayesian meta-regressions are also easily performed by analysts who are comfortable using WinBUGS. Estimates of the residual between-study variance in random effects meta-regressions should be routinely reported and accompanied by some measure of their uncertainty. Confidence and/or credible intervals are well-suited to this purpose.",0
https://doi.org/10.1093/ije/dyi312,Bayesian perspectives for epidemiological research: I. Foundations and basic methods,"One misconception (of many) about Bayesian analyses is that prior distributions introduce assumptions that are more questionable than assumptions made by frequentist methods; yet the assumptions in priors can be more reasonable than the assumptions implicit in standard frequentist models. Another misconception is that Bayesian methods are computationally difficult and require special software. But perfectly adequate Bayesian analyses can be carried out with common software for frequentist analysis. Under a wide range of priors, the accuracy of these approximations is just as good as the frequentist accuracy of the software--and more than adequate for the inaccurate observational studies found in health and social sciences. An easy way to do Bayesian analyses is via inverse-variance (information) weighted averaging of the prior with the frequentist estimate. A more general method expresses the prior distributions in the form of prior data or 'data equivalents', which are then entered in the analysis as a new data stratum. That form reveals the strength of the prior judgements being introduced and may lead to tempering of those judgements. It is argued that a criterion for scientific acceptability of a prior distribution is that it be expressible as prior data, so that the strength of prior assumptions can be gauged by how much data they represent.",0
https://doi.org/10.3102/1076998610375835,On the Importance of Reliable Covariate Measurement in Selection Bias Adjustments Using Propensity Scores,"The effect of unreliability of measurement on propensity score (PS) adjusted treatment effects has not been previously studied. The authors report on a study simulating different degrees of unreliability in the multiple covariates that were used to estimate the PS. The simulation uses the same data as two prior studies. Shadish, Clark, and Steiner showed that a PS formed from many covariates demonstrably reduced selection bias, while Steiner, Cook, Shadish, and Clark identified the subsets of covariates from the larger set that were most effective for bias reduction. Adding different degrees of random error to these covariates in a simulation, the authors demonstrate that unreliability of measurement can degrade the ability of PSs to reduce bias. Specifically, increases in reliability only promote bias reduction, if the covariates are effective in reducing bias to begin with. Increasing or decreasing the reliability of covariates that do not effectively reduce selection bias makes no difference at all.",0
https://doi.org/10.1037/a0035234,Factorial comparison of working memory models.,"Three questions have been prominent in the study of visual working memory limitations: (a) What is the nature of mnemonic precision (e.g., quantized or continuous)? (b) How many items are remembered? (c) To what extent do spatial binding errors account for working memory failures? Modeling studies have typically focused on comparing possible answers to a single one of these questions, even though the result of such a comparison might depend on the assumed answers to both others. Here, we consider every possible combination of previously proposed answers to the individual questions. Each model is then a point in a 3-factor model space containing a total of 32 models, of which only 6 have been tested previously. We compare all models on data from 10 delayed-estimation experiments from 6 laboratories (for a total of 164 subjects and 131,452 trials). Consistently across experiments, we find that (a) mnemonic precision is not quantized but continuous and not equal but variable across items and trials; (b) the number of remembered items is likely to be variable across trials, with a mean of 6.4 in the best model (median across subjects); (c) spatial binding errors occur but explain only a small fraction of responses (16.5% at set size 8 in the best model). We find strong evidence against all 6 documented models. Our results demonstrate the value of factorial model comparison in working memory.",0
https://doi.org/10.1037/1082-989x.13.2.150,Constrained versus unconstrained estimation in structural equation modeling.,"Recently, R. D. Stoel, F. G. Garre, C. Dolan, and G. van den Wittenboer (2006) reviewed approaches for obtaining reference mixture distributions for difference tests when a parameter is on the boundary. The authors of the present study argue that this methodology is incomplete without a discussion of when the mixtures are needed and show that they only become relevant when constrained difference tests are conducted. Because constrained difference tests can hide important model misspecification, a reliable way to assess global model fit under constrained estimation would be needed. Examination of the options for assessing model fit under constrained estimation reveals that no perfect solutions exist, although the conditional approach of releasing a degree of freedom for each active constraint appears to be the most methodologically sound one. The authors discuss pros and cons of constrained and unconstrained estimation and their implementation in 5 popular structural equation modeling packages and argue that unconstrained estimation is a simpler method that is also more informative about sources of misfit. In practice, researchers will have trouble conducting constrained difference tests appropriately, as this requires a commitment to ignore Heywood cases. Consequently, mixture distributions for difference tests are rarely appropriate.",0
https://doi.org/10.1002/sim.2423,Meta-analysis of heterogeneously reported trials assessing change from baseline,"This paper considers the quantitative synthesis of published comparative study results when the outcome measures used in the individual studies and the way in which they are reported varies between studies. Whilst the former difficulty may be overcome, at least to a limited extent, by the use of standardized effects, the latter is often more problematic. Two potential solutions to this problem are; sensitivity analyses and a fully Bayesian approach, in which pertinent external information is included. Both approaches are illustrated using the results of two systematic reviews and meta-analyses which consider the difference in mean change in systolic blood pressure and the difference in physical functioning between an intervention and control group. The two examples illustrate that by adopting a fully Bayesian approach, as opposed to undertaking sensitivity analyses assuming fixed values for unknown parameters, the overall intervention effect can be estimated with greater uncertainty, but that assessing the sensitivity of results to choice of prior distributions in such analyses is crucial.",0
https://doi.org/10.2501/ijmr-2013-058,Reviews of Market Drivers of New Product Performance: Effects and Relationships,"This study adopts a meta-analytic approach to review the performance effects of the market predictors of new product performance and their structural relationships. Based on empirical findings from the relevant studies published before 2011, this study has a number of interesting findings. First, market orientation, competitor orientation, product advantage and launch proficiency are the dominant drivers of new product performance. Second, market orientation, marketing synergy, product advantage and competitive intensity have significant effects on new product performance. Third, product advantage serves as an important intermediary between the market predictors and new product performance. Fourth, product innovativeness per se does not affect new product performance. Finally, launch proficiency translates the effect of market orientation into new product performance. These findings not only identify the dominant market drivers of new product performance, but also profile the routes leading to better new product performance. Some important implications for market research and practice are also provided.",0
https://doi.org/10.1080/00273171.2010.483387,Using a Multivariate Multilevel Polytomous Item Response Theory Model to Study Parallel Processes of Change: The Dynamic Association Between Adolescents' Social Isolation and Engagement With Delinquent Peers in the National Youth Survey,"The application of multidimensional item response theory models to repeated observations has demonstrated great promise in developmental research. It allows researchers to take into consideration both the characteristics of item response and measurement error in longitudinal trajectory analysis, which improves the reliability and validity of the latent growth curve (LGC) model. The purpose of this study is to demonstrate the potential of Bayesian methods and the utility of a comprehensive modeling framework, the one combining a measurement model (e.g., a multidimensional graded response model, MGRM) with a structural model (e.g., an associative latent growth curve analysis, ALGC). All analyses are implemented in WinBUGS 1.4.3 ( Spiegelhalter, Thomas, Best, & Lunn, 2003 ), which allows researchers to use Markov chain Monte Carlo simulation methods to fit complex statistical models and circumvent intractable analytic or numerical integrations. The utility of this MGRM-ALGC modeling framework was investigated with both simulated and empirical data, and promising results were obtained. As the results indicate, being a flexible multivariate multilevel model, this MGRM-ALGC model not only produces item parameter estimates that are readily estimable and interpretable but also estimates the corresponding covariation in the developmental dimensions. In terms of substantive interpretation, as adolescents perceived themselves more socially isolated, the chance that they are engaged with delinquent peers becomes profoundly larger. Generally, boys have a higher initial exposure extent than girls. However, there is no gender difference associated with other latent growth parameters.",0
https://doi.org/10.1207/s15327906mb340203,Structural Equation Modeling with Small Samples: Test Statistics,"Structural equation modeling is a well-known technique for studying relationships among multivariate data. In practice, high dimensional nonnormal data with small to medium sample sizes are very common, and large sample theory, on which almost all modeling statistics are based, cannot be invoked for model evaluation with test statistics. The most natural method for nonnormal data, the asymptotically distribution free procedure, is not defined when the sample size is less than the number of nonduplicated elements in the sample covariance. Since normal theory maximum likelihood estimation remains defined for intermediate to small sample size, it may be invoked but with the probable consequence of distorted performance in model evaluation. This article studies the small sample behavior of several test statistics that are based on maximum likelihood estimator, but are designed to perform better with nonnormal data. We aim to identify statistics that work reasonably well for a range of small sample sizes and distribution conditions. Monte Carlo results indicate that Yuan and Bentler's recently proposed F-statistic performs satisfactorily.",0
https://doi.org/10.1080/10705519809540105,Interactions of latent variables in structural equation models,"Interactions of variables occur in a variety of statistical analyses. The best known procedures for models with interactions of latent variables are technically demanding. Not only does the potential user need to be familiar with structural equation modeling (SEM), but the researcher must be familiar with programming nonlinear and linear constraints and must be comfortable with fairly large and complicated models. This article provides a largely nontechnical description of an alternative two‐stage least squares (2SLS) technique to include interactions of latent variables in SEM. The method requires the selection of instrumental variables and we give rules for their selection in the most common cases. We compare the 2SLS method to the alternatives. Some of the important advantages of the 2SLS are that it can handle nonnormal observed variables, is readily available in major statistical software packages, and has a known asymptotic distribution. In providing the comparisons, we reanalyze all the interaction...",0
https://doi.org/10.1111/1467-985x.00283,A Bayesian approach to Markov modelling in cost-effectiveness analyses: application to taxane use in advanced breast cancer,"Summary. The paper demonstrates how cost-effectiveness decision analysis may be implemented from a Bayesian perspective, using Markov chain Monte Carlo simulation methods for both the synthesis of relevant evidence input into the model and the evaluation of the model itself. The desirable aspects of a Bayesian approach for this type of analysis include the incorporation of full parameter uncertainty, the ability to perform all the analysis, including each meta-analysis, in a single coherent model and the incorporation of expert opinion either directly or regarding the relative credibility of different data sources. The method is described, and its ease of implementation demonstrated, through a practical example to evaluate the cost-effectiveness of using taxanes for the second-line treatment of advanced breast cancer compared with conventional treatment. For completeness, the results from the Markov chain Monte Carlo simulation model are compared and contrasted with those from a classical Monte Carlo simulation model.",0
https://doi.org/10.1214/ss/1032209662,R. A. Fisher and multivariate analysis,This paper reviews R. A. Fisher's many fundamental contri- butions to multivariate statistical analysis-from the derivation of the distribution of the sample correlation coefficient to discriminant analysis. The emphasis here is on the conceptual and mathematical development. All of his papers on multivariate analysis will be included in this survey.,0
https://doi.org/10.1016/s0169-7161(05)25034-4,Bayesian Aspects of Small Area Estimation,"This chapter deals with the Bayesian aspects of small area estimation. It discusses only some aspects of small area estimation, particularly the model based EB and HB approach. Models play a major role in modern small area estimation. There are several advantages of model based inference over the traditional indirect estimators. EB produce approximate estimates whereas HB provides accurate estimates. For HB analysis, the standard Bayesian model diagnostics tools are applied so far. The chapter addresses the basic models, and mentions some extensions of those models. This chapter focuses mainly on empirical Bayes and hierarchical Bayes estimators related to small area models. The Bayes estimators are usually obtained under squared error loss. This chapter concludes that Empirical Bayes estimators are obtained as estimated posterior mean, while for hierarchical Bayes usually subjective prior distributions are assumed for the model parameters.",0
https://doi.org/10.1167/12.6.25,The psychometric function: The lapse rate revisited,"In their influential paper, Wichmann and Hill (2001) have shown that the threshold and slope estimates of a psychometric function may be severely biased when it is assumed that the lapse rate equals zero but lapses do, in fact, occur. Based on a large number of simulated experiments, Wichmann and Hill claim that threshold and slope estimates are essentially unbiased when one allows the lapse rate to vary within a rectangular prior during the fitting procedure. Here, I replicate Wichmann and Hill's finding that significant bias in parameter estimates results when one assumes that the lapse rate equals zero but lapses do occur, but fail to replicate their finding that freeing the lapse rate eliminates this bias. Instead, I show that significant and systematic bias remains in both threshold and slope estimates even when one frees the lapse rate according to Wichmann and Hill's suggestion. I explain the mechanisms behind the bias and propose an alternative strategy to incorporate the lapse rate into psychometric function models, which does result in essentially unbiased parameter estimates.",0
https://doi.org/10.3102/1076998607302626,Using Response Times for Item Selection in Adaptive Testing,"Response times on items can be used to improve item selection in adaptive testing provided that a probabilistic model for their distribution is available. In this research, the author used a hierarchical modeling framework with separate first-level models for the responses and response times and a second-level model for the distribution of the ability and speed parameters in the population of test takers. The framework allows the author to retrofit an empirical prior distribution for the ability parameter on each occurrence of a new response time. In an example with an adaptive version of the Law School Admission Test (LSAT), the author shows how this additional update of the posterior distribution of the ability leads to a substantial improvement of the ability estimator. Two ways of applying the procedure in real-world adaptive testing are discussed.",0
https://doi.org/10.1007/bf02294554,Finite mixtures in confirmatory factor-analysis models,"In this paper, various types of finite mixtures of confirmatory factor-analysis models are proposed for handling data heterogeneity. Under the proposed mixture approach, observations are assumed to be drawn from mixtures of distinct confirmatory factor-analysis models. But each observation does not need to be identified to a particular model prior to model fitting. Several classes of mixture models are proposed. These models differ by their unique representations of data heterogeneity. Three different sampling schemes for these mixture models are distinguished. A mixed type of the these three sampling schemes is considered throughout this article. The proposed mixture approach reduces to regular multiple-group confirmatory factor-analysis under a restrictive sampling scheme, in which the structural equation model for each observation is assumed to be known. By assuming a mixture of multivariate normals for the data, maximum likelihood estimation using the EM (Expectation-Maximization) algorithm and the AS (Approximate-Scoring) method are developed, respectively. Some mixture models were fitted to a real data set for illustrating the application of the theory. Although the EM algorithm and the AS method gave similar sets of parameter estimates, the AS method was found computationally more efficient than the EM algorithm. Some comments on applying the mixture approach to structural equation modeling are made.",0
https://doi.org/10.1080/14783360903492694,Rethinking perceived service quality: An alternative to hierarchical and multidimensional models,"Measuring perceived service quality continues to be a controversial topic in management literature. Brady and Cronin's (2001) hierarchical and multidimensional model overcame several limitations of previously proposed models. Nevertheless, Brady and Cronin's conceptualisation has been the object of substantial criticism. This research describes the most important limitations of the Brady and Cronin's (2001) model, pointing out some newly identified drawbacks to this kind of conceptualisation and to using the methodology for analysing service quality models. To overcome these shortcomings, a new procedure is proposed, based on the proper identification of service quality attributes and on the study of unobserved heterogeneity in customer perceptions. An application of this procedure to the insurance industry shows the possible advantages of using this method for studying perceived quality in services. The results of the empirical study confirm the presence of several latent classes formed by customers with...",0
https://doi.org/10.1198/tast.2009.08140,Comparison of Software Algorithms for Calculating REML Wald Type Confidence Limits for the Between-Group Variance Component in a Small Sample One-Way Random Effects Model Example,"Confidence limits for variance components in mixed effects models can be readily estimated by procedures such as PROC MIXED (SAS®), MIXED (SPSS®), LME (S-PLUS®), and XTMIXED (Stata®). For a small sample unbalanced one-way random effects model case study, the REML Wald type confidence limits for the between-group variance obtained from PROC MIXED differ substantially from those from MIXED, LME, and XTMIXED. Simulations indicate that the Satterthwaite approximation used by PROC MIXED results in highly inflated confidence interval lengths, while the interval lengths from the other procedures are usually similar but also biased upward. The coverage rates of the confidence limits from all four procedures appear above the nominal value. Extreme upper limits and problems with computing the confidence limits are also possible, especially with PROC MIXED. In general, one should avoid the naive use of statistical software packages when estimating REML confidence limits for variance components with small datasets, a...",0
https://doi.org/10.1007/bf02295737,A Bayesian analysis of finite mixtures in the LISREL model,"In this paper, we propose a Bayesian framework for estimating finite mixtures of the LISREL model. The basic idea in our analysis is to augment the observed data of the manifest variables with the latent variables and the allocation variables. The Gibbs sampler is implemented to obtain the Bayesian solution. Other associated statistical inferences, such as the direct estimation of the latent variables, establishment of a goodness-of-fit assessment for a posited model, Bayesian classification, residual and outlier analyses, are discussed. The methodology is illustrated with a simulation study and a real example.",0
https://doi.org/10.1207/s15327906mbr3001_3,A Simulation Study of Mediated Effect Measures,"Analytical solutions for point and variance estimators of the mediated effect, the ratio of the mediated to the direct effect, and the proportion of the total effect that is mediated were studied with statistical simulations. We compared several approximate solutions based on the multivariate delta method and second order Taylor series expansions to the empirical standard deviation of each estimator and theoretical standard error when available. The simulations consisted of 500 replications of three normally distributed variables for eight sample sizes (N = 10, 25, 50, 100, 500, 1000, and 5000) and 64 parameter value combinations. The different solutions for the standard error of the indirect effect were very similar for sample sizes of at least 50, except when the independent variable was dichotomized. A sample size of at least 500 was needed for accurate point and variance estimates of the proportion mediated. The point and variance estimates of the ratio of the mediated to nonmediated effect did not stabilize until the sample size was 2,000 for the all continuous variable case. Implications for the estimation of mediated effects in experimental and nonexperimental studies are discussed.",0
https://doi.org/10.1037/apl0000026,An item analysis of the Conditional Reasoning Test of Aggression.,"This manuscript uses item response theory (IRT) to estimate item characteristics of the Conditional Reasoning Test of Aggression (CRT-A). Using a sample size of 5,511 respondents, the present analysis provides an accurate assessment of the capability of the CRT-A to measure latent aggression. The one-parameter logistic (1PL) model, two-parameter logistic (2PL) model, and three-parameter logistic (3PL) model are compared before the item analysis. Results suggest that the 2PL model is the most appropriate dichotomous IRT model for describing the item characteristics of the CRT-A. Potential multdimensionality in the CRT-A is also examined. Results suggest that CRT-A items work as theoretically intended, with the probability of selecting an aggressive response increasing with latent trait levels. Information curves indicate that the CRT-A is best suited for use with individuals who are high on latent aggression. Exploratory analyses include an examination of polytomous IRT models and DIF comparing student and employee respondents. The results have implications for future research using the CRT-A as well as the identification of populations appropriate for measurement using this assessment tool.",0
https://doi.org/10.1002/eat.10109,Healthy weight control and dissonance-based eating disorder prevention programs: Results from a controlled trial,"Because universal psychoeducational eating disorder prevention programs have had little success, we developed and evaluated two interventions for high-risk populations: a healthy weight control intervention and a dissonance-based intervention.Adolescent girls (N = 148) with body image concerns were randomized to one of these interventions or to a waitlist control group. Participants completed baseline, termination, and 1, 3, and 6-month follow-up surveys.Participants in both interventions reported decreased thin-ideal internalization, negative affect, and bulimic symptoms at termination and follow-up relative to controls. However, no effects were observed for body dissatisfaction or dieting and effects diminished over time.Results provide evidence that both interventions effectively reduce bulimic pathology and risk factors for eating disturbances.",0
https://doi.org/10.3102/10769986030001059,Using Propensity Score Subclassification for Multiple Treatment Doses to Evaluate a National Antidrug Media Campaign,"In 1998, the U.S. Office of National Drug Control Policy launched a national media campaign in an effort to reduce and prevent drug use among young Americans. Because the campaign was implemented nationwide, there is no control group available for use in evaluating the effects of the campaign. Nevertheless, it is possible to use propensity score methods to evaluate the effects of the campaign. However, because teens receive varying degrees of exposure to the media campaign, it is necessary to apply propensity score methods that accommodate multiple treatment doses. This work extends that of previous authors to subclassification on the propensity score for observational studies with multiple treatment doses, rather than matching on the propensity score, and proposes modifications to accommodate complex survey data. This methodology is illustrated using data from a pilot study for the media campaign evaluation.",0
https://doi.org/10.17705/1cais.03476,Bayesian Structural Equation Models for Cumulative Theory Building in Information Systems―A Brief Tutorial Using BUGS and R,"Structural equation models (SEM) are frequently used in information systems (IS) to analyze and test theoretical propositions. As IS researchers frequently reuse measurement instruments and adapt or extend theories, they frequently re-estimate regression relationships in their SEM that have been examined in previous studies. We advocate the use of Bayesian estimation of structural equation models as an aid to cumulative theory building; Bayesian statistics offer a statistically sound way to incorporate prior knowledge into SEM estimation, allowing researchers to keep a “running tally” of the best estimates of model parameters. This tutorial on the application of Bayesian principles to SEM estimation discusses when and why the use of Bayesian estimation should be considered by IS researchers, presents an illustrative example using best practices, and makes recommendations to guide IS researchers in the application of Bayesian SEM.",0
https://doi.org/10.1016/j.jspi.2004.10.012,Discussion on the paper by Pardoe and Weidner,,0
https://doi.org/10.1371/journal.pone.0091710,Joint Bayesian Inference Reveals Model Properties Shared between Multiple Experimental Conditions,"Statistical modeling produces compressed and often more easily interpretable descriptions of experimental data in form of model parameters. When experimental manipulations target selected parameters, it is necessary for their interpretation that other model components remain constant. For example, psychophysicists use dose rate models to describe how behavior changes as a function of a single stimulus variable. The main interest is on shifts of this function induced by experimental manipulation, assuming invariance in other aspects of the function. Combining several experimental conditions in a joint analysis that takes such invariance constraints into account can result in a complex model for which no robust standard procedures are available. We formulate a solution for the joint analysis through repeated applications of standard procedures by allowing an additional assumption. This way, experimental conditions can be analyzed separately such that all conditions are implicitly taken into account. We investigate the validity of the supplementary assumption through simulations. Furthermore, we present a natural way to check whether a joint treatment is appropriate. We illustrate the method for the specific case of the psychometric function; however the procedure applies to other models that encompass multiple experimental conditions.",0
https://doi.org/10.7326/m13-2886,Random-Effects Meta-analysis of Inconsistent Effects: A Time for Change,"A primary goal of meta-analysis is to improve the estimation of treatment effects by pooling results of similar studies. This article explains how the most widely used method for pooling heterogeneous studies--the Der Simonian-Laird (DL) estimator--can produce biased estimates with falsely high precision. A classic example is presented to show that use of the DL estimator can lead to erroneous conclusions. Particular problems with the DL estimator are discussed, and several alternative methods for summarizing heterogeneous evidence are presented. The authors support replacing universal use of the DL estimator with analyses based on a critical synthesis that recognizes the uncertainty in the evidence,focuses on describing and explaining the probable sources of variation in the evidence, and uses random-effects estimates that provide more accurate confidence limits than the DL estimator.",0
https://doi.org/10.7916/d8rj4sgp,Estimating High Dimensional Covariance Matrices and Its Applications,"Estimating covariance matrices is an important part of portfolio selection, risk management, and asset pricing. This paper reviews the recent development in estimating high dimensional covariance matrices, where the number of variables can be greater than the number of observations. The limitations of the sample covariance matrix are discussed. Several new approaches are presented, including the shrinkage method, the observable and latent factor method, the Bayesian approach, and the random matrix theory approach. For each method, the construction of covariance matrices is given. The relationships among these methods are discussed.",0
https://doi.org/10.1515/jbnst-2008-0407,The Hausman Test Statistic can be Negative even Asymptotically,"Summary We show that under the alternative hypothesis the Hausman chi-square test statistic can be negative not only in small samples but even asymptotically. Therefore in large samples such a result is only compatible with the alternative and should be interpreted accordingly. Applying a known insight from finite samples, this can only occur if the different estimation precisions (often the residual variance estimates) under the null and the alternative both enter the test statistic. In finite samples, using the absolute value of the test statistic is a remedy that does not alter the null distribution and is thus admissible. Even for positive test statistics the relevant covariance matrix difference should be routinely checked for positive semi-definiteness, because we also show that otherwise test results may be misleading. Of course the preferable solution still is to impose the same nuisance parameter (i.e., residual variance) estimate under the null and alternative hypotheses, if the model context permits that with relative ease. We complement the likelihood-based exposition by a formal proof in an omitted-variable context, we present simulation evidence for the test of panel random effects, and we illustrate the problems with a panel homogeneity test.",0
https://doi.org/10.1080/10618600.2016.1164709,Combining Functional Data Registration and Factor Analysis,"ABSTRACTWe extend the definition of functional data registration to encompass a larger class of registration models. In contrast to traditional registration models, we allow for registered functions that have more than one primary direction of variation. The proposed Bayesian hierarchical model simultaneously registers the observed functions and estimates the two primary factors that characterize variation in the registered functions. Each registered function is assumed to be predominantly composed of a linear combination of these two primary factors, and the function-specific weights for each observation are estimated within the registration model. We show how these estimated weights can easily be used to classify functions after registration using both simulated data and a juggling dataset. Supplementary materials for this article are available online.",0
https://doi.org/10.1080/02699931.2012.667392,Getting stuck in depression: The roles of rumination and emotional inertia,"Like many other mental disorders, depression is characterised by psychological inflexibility. Two instances of such inflexibility are rumination: repetitive cognitions focusing on the causes and consequences of depressive symptoms; and emotional inertia: the tendency for affective states to be resistant to change. In two studies, we tested the predictions that: (1) rumination and emotional inertia are related; and (2) both independently contribute to depressive symptoms. We examined emotional inertia of subjective affective experiences in daily life among a sample of non-clinical undergraduates (Study 1), and of affective behaviours during a family interaction task in a sample of clinically depressed and non-depressed adolescents (Study 2), and related it to self-reported rumination and depression severity. In both studies, rumination (particularly the brooding facet) and emotional inertia (particularly of sad/dysphoric affect) were positively associated, and both independently predicted depression severity. These findings demonstrate the importance of studying both cognitive and affective inflexibility in depression.",0
https://doi.org/10.1177/0165025407077764,Bayesian analysis of longitudinal data using growth curve models,"Bayesian methods for analyzing longitudinal data in social and behavioral research are recommended for their ability to incorporate prior information in estimating simple and complex models. We first summarize the basics of Bayesian methods before presenting an empirical example in which we fit a latent basis growth curve model to achievement data from the National Longitudinal Survey of Youth. This step-by-step example illustrates how to analyze data using both noninformative and informative priors. The results show that in addition to being an alternative to the maximum likelihood estimation (MLE) method, Bayesian methods also have unique strengths, such as the systematic incorporation of prior information from previous studies. These methods are more plausible ways to analyze small sample data compared with the MLE method.",0
https://doi.org/10.1177/0146167298242002,"Emotional Reactivity to Everyday Problems, Affective Inertia, and Neuroticism","A naturalistic diary recording study was conducted to assess affective responses to everyday stress. Community-residing male participants made diary recordings regarding problem occurrence and mood several times a day for 8 days. In addition to reporting more frequent daily problems, persons scoring high in neuroticism were more reactive to stressors and were more distressed by recurrent problems than were persons scoring low in neuroticism. New problems affected everyone comparably. There was also evidence of affective inertia, such that bad mood was more likely to carry over to the next assessment. This lag effect tended to be stronger among more neurotic individuals.",0
https://doi.org/10.1002/(sici)1097-0258(19960615)15:11<1069::aid-sim220>3.0.co;2-q,ON DESIGN CONSIDERATIONS AND RANDOMIZATION-BASED INFERENCE FOR COMMUNITY INTERVENTION TRIALS,"This paper discusses design considerations and the role of randomization-based inference in randomized community intervention trials. We stress that longitudinal follow-up of cohorts within communities often yields useful information on the effects of intervention on individuals, whereas cross-sectional surveys can usefully assess the impact of intervention on group indices of health. We also discuss briefly special design considerations, such as sampling cohorts from targeted subpopulations (for example, heavy smokers), matching the communities, calculating sample size, and other practical issues. We present randomization tests for matched and unmatched cohort designs. As is well known, these tests necessarily have proper size under the strong null hypothesis that treatment has no effect on any community response. It is less well known, however, that the size of randomization tests can exceed nominal levels under the 'weak' null hypothesis that intervention does not affect the average community response. Because this weak null hypothesis is of interest in community intervention trials, we study the size of randomization tests by simulation under conditions in which the weak null hypothesis holds but the strong null hypothesis does not. In unmatched studies, size may exceed nominal levels under the weak null hypothesis if there are more intervention than control communities and if the variance among community responses is larger among control communities than among intervention communities; size may also exceed nominal levels if there are more control than intervention communities and if the variance among community responses is larger among intervention communities. Otherwise, size is likely near nominal levels. To avoid such problems, we recommend use of the same numbers of control and intervention communities in unmatched designs. Pair-matched designs usually have size near nominal levels, even under the weak null hypothesis. We have identified some extreme cases, unlikely to arise in practice, in which even the size of pair-matched studies can exceed nominal levels. These simulations, however, tend to confirm the robustness of randomization tests for matched and unmatched community intervention trials, particularly if the latter designs have equal numbers of intervention and control communities. We also describe adaptations of randomization tests to allow for covariate adjustment, missing data, and application to cross-sectional surveys. We show that covariate adjustment can increase power, but such power gains diminish as the random component of variation among communities increases, which corresponds to increasing intraclass correlation of responses within communities. We briefly relate our results to model-based methods of inference for community intervention trials that include hierarchical models such as an analysis of variance model with random community effects and fixed intervention effects. Although we have tailored this paper to the design of community intervention trials, many of the ideas apply to other experiments in which one allocates groups or clusters of subjects at random to intervention or control treatments.",0
,Bayesian Structural Equation Models for Cumulative Theory Building in Information Systems.,"Theories are sets of causal relationships between constructs and their proxy indicator variables. Theories are tested and their numerical parameters are estimated using statistical models of latent and observed variables. A considerable amount of theoretical development in Information Systems occurs by theory extension or adaptation. Moreover, researchers are encouraged to reuse existing measurement instruments when possible. As a consequence, there are many cases when a relationship between two variables (latent and/or observed) is re-estimated in a new study with a new sample or in a new context. To aid in cumulative theory building, a re-estimation of parameters should take into account our prior knowledge about their likely values. In this paper, we show how Bayesian statistical models can provide a statistically sound way of incorporating prior knowledge into parameter estimation, allowing researchers to keep a “running tally” of the best estimates of model parameters.",0
https://doi.org/10.1111/j.1745-3984.2004.tb01112.x,Effects of Practical Constraints on Item Selection Rules at the Early Stages of Computerized Adaptive Testing,"The purpose of this study was to compare the effects of four item selection rules—(1) Fisher information (F), (2) Fisher information with a posterior distribution (FP), (3) Kullback-Leibler information with a posterior distribution (KP), and (4) completely randomized item selection (RN)—with respect to the precision of trait estimation and the extent of item usage at the early stages of computerized adaptive testing. The comparison of the four item selection rules was carried out under three conditions: (1) using only the item information function as the item selection criterion; (2) using both the item information function and content balancing; and (3) using the item information function, content balancing, and item exposure control. When test length was less than 10 items, FP and KP tended to outperform F at extreme trait levels in Condition 1. However, in more realistic settings, it could not be concluded that FP and KP outperformed F, especially when item exposure control was imposed. When test length was greater than 10 items, the three nonrandom item selection procedures performed similarly no matter what the condition was, while F had slightly higher item usage.",0
https://doi.org/10.1177/0013164403258402,The Effect of Multicollinearity on Multilevel Modeling Parameter Estimates and Standard Errors,"This study investigates the quality of multilevel model parameter estimates and standard errors as a function of varying magnitudes of correlation among Level 1 predictors and model characteristics. The results of the study showthat with multicollinearity presented at Level 1 of a two-level mixed-effects linear model, the fixed-effect parameter estimates produce relatively unbiased values; however, the variance and covariance component estimates produce downwardly biased values except for Level 1 variance (&lt; 5%). The standard errors associated with the parameter estimates are also biased under varied magnitudes of Level 1 predictor correlation.",0
https://doi.org/10.1080/10503300701796992,Statistical analysis of group-administered intervention data: Reanalysis of two randomized trials,"Group-administered interventions often create statistical dependencies, which, if ignored, increase the rate of Type I errors. The authors analyzed data from two randomized trials involving group interventions to document the impact of statistical dependency on tests of intervention effects and to provide estimates of statistical dependency. Intraclass correlations ranged from .02 to .12. Adjusting for dependencies increased p values for the tests of intervention effects. The increase in the p values depended on the magnitude of the statistical dependence and available degrees of freedom. Results suggest that the literature may overstate the efficacy of group interventions and imply that it will be important to study why groups create dependencies. The authors discuss how dependencies impact statistical power and how researchers can address this concern.",0
https://doi.org/10.3758/s13428-013-0395-1,Statistical power of latent growth curve models to detect quadratic growth,"Latent curve models (LCMs) have been used extensively to analyze longitudinal data. However, little is known about the power of LCMs to detect nonlinear trends when they are present in the data. For this study, we utilized simulated data to investigate the power of LCMs to detect the mean of the quadratic slope, Type I error rates, and rates of nonconvergence during the estimation of quadratic LCMs. Five factors were examined: the number of time points, growth magnitude, interindividual variability, sample size, and the R 2s of the measured variables. The results showed that the empirical Type I error rates were close to the nominal value of 5 %. The empirical power to detect the mean of the quadratic slope was affected by the simulation factors. Finally, a substantial proportion of samples failed to converge under conditions of no to small variation in the quadratic factor, small sample sizes, and small R 2 of the repeated measures. In general, we recommended that quadratic LCMs be based on samples of (a) at least 250 but ideally 400, when four measurement points are available; (b) at least 100 but ideally 150, when six measurement points are available; (c) at least 50 but ideally 100, when ten measurement points are available. Ã‚Â© 2013 Psychonomic Society, Inc.",0
https://doi.org/10.1016/j.pain.2004.02.018,HIV-1 gp120 Stimulates proinflammatory cytokine-mediated pain facilitation via activation of nitric oxide synthase-I (nNOS),"It has become clear that spinal cord glia (microglia and astrocytes) importantly contribute to the creation of exaggerated pain responses. One model used to study this is peri-spinal (intrathecal, i.t.) administration of gp120, an envelope protein of HIV-1 known to activate glia. Previous studies demonstrated that i.t. gp120 produces pain facilitation via the release of glial proinflammatory cytokines. The present series of studies tested whether spinal nitric oxide (NO) contributes to i.t. gp120-induced mechanical allodynia and, if so, what effect NO has on spinal proinflammatory cytokines. gp120 stimulation of acutely isolated lumbar dorsal spinal cords released NO as well as proinflammatory cytokines (tumor necrosis factor-alpha, interleukin-1beta (IL1), interleukin-6 (IL6)), thus identifying NO as a candidate mediator of gp120-induced behavioral effects. Behaviorally, identical effects were observed when gp120-induced mechanical allodynia was challenged by i.t. pre-treatment with either a broad-spectrum nitric oxide synthase (NOS) inhibitor (L-NAME) or 7-NINA, a selective inhibitor of NOS type-I (nNOS). Both abolished gp120-induced mechanical allodynia. While the literature pre-dominantly documents that proinflammatory cytokines stimulate the production of NO rather than the reverse, here we show that gp120-induced NO increases proinflammatory cytokine mRNA levels (RT-PCR) and both protein expression and protein release (serial ELISA). Furthermore, gp120 increases mRNA for IL1 converting enzyme and matrix metalloproteinase-9, enzymes responsible for activation and release of proinflammatory cytokines.",0
https://doi.org/10.1016/j.ijresmar.2009.06.003,A meta-analysis of the determinants of organic sales growth,"Abstract We present the results of a meta-analysis on drivers of organic sales growth conducted using a Hierarchical Bayes estimation technique. Based on a comprehensive review of a diverse set of literatures on organic sales growth, we identify eleven drivers of organic sales growth performance of firms: (i) innovation, (ii) marketing orientation (iii) advertising (iv) interorganizational networks, (v) entrepreneurial orientation, (vi) management capacity, (vii) firm age, (viii) firm size, (ix) competition, (x) munificence, and (xi) dynamism. Among the variables under a manager's control, innovation, advertising, market orientation, interorganizational networks, entrepreneurial orientation and managerial capacity serve as positive drivers of organic growth. Older firms and firms operating in dynamic and competitive environments face constraints in terms of organic growth. We find that the omission of marketing variables in empirical models biases the elasticities of eight of the drivers of organic growth. Three study design characteristics impact the magnitude of elasticity of organic growth drivers: using cross-sectional data instead of panel data, using growth rates instead of absolute change as operationalization of growth and using market share instead of sales as a measure of revenues.",0
https://doi.org/10.1016/j.jspi.2012.01.005,Reference priors for linear models with general covariance structures,"Abstract We develop a new class of reference priors for linear models with general covariance structures. A general Markov chain Monte Carlo algorithm is also proposed for implementing the computation. We present several examples to demonstrate the results: Bayesian penalized spline smoothing, a Bayesian approach to bivariate smoothing for a spatial model, and prior specification for structural equation models.",0
https://doi.org/10.1111/j.1467-985x.2007.00487.x,A hierarchical modelling framework for identifying unusual performance in health care providers,"Summary.  A wide variety of statistical methods have been proposed for detecting unusual performance in cross-sectional data on health care providers. We attempt to create a unified framework for comparing these methods, focusing on a clear distinction between estimation and hypothesis testing approaches, with the corresponding distinction between detecting ‘extreme’ and ‘divergent’ performance. When assuming a random-effects model the random-effects distribution forms the null hypothesis, and there appears little point in testing whether individual effects are greater or less than average. The hypothesis testing approach uses p-values as summaries and brings with it the standard problems of multiple testing, whether Bayesian or classical inference is adopted. A null random-effects formulation allows us to answer appropriate questions of the type: ‘is a particular provider worse than we would expect the true worst provider (but still part of the null distribution) to be'? We outline a broad three-stage strategy of exploratory detection of unusual providers, detailed modelling robust to potential outliers and confirmation of unusual performance, illustrated by using two detailed examples. The concepts are most easily handled within a Bayesian analytic framework using Markov chain Monte Carlo methods, but the basic ideas should be generally applicable.",0
https://doi.org/10.1080/19312458.2012.679848,Advantages of Monte Carlo Confidence Intervals for Indirect Effects,"Monte Carlo simulation is a useful but underutilized method of constructing confidence intervals for indirect effects in mediation analysis. The Monte Carlo confidence interval method has several distinct advantages over rival methods. Its performance is comparable to other widely accepted methods of interval construction, it can be used when only summary data are available, it can be used in situations where rival methods (e.g., bootstrapping and distribution of the product methods) are difficult or impossible, and it is not as computer-intensive as some other methods. In this study we discuss Monte Carlo confidence intervals for indirect effects, report the results of a simulation study comparing their performance to that of competing methods, demonstrate the method in applied examples, and discuss several software options for implementation in applied settings.",0
https://doi.org/10.1080/0094965031000147713,Implementation of a robust bayesian method,"In this work we study robustness in Bayesian models through a generalization of the Normal distribution. We show new appropriate techniques in order to deal with this distribution in Bayesian inference. Then we propose two approaches to decide, in some applications, if we should replace the usual Normal model by this generalization. First, we pose this dilemma as a model rejection problem, using diagnostic measures. In the second approach we evaluate the model's predictive efficiency. We illustrate those perspectives with a simulation study, a non linear model and a longitudinal data model.",0
https://doi.org/10.1037/0021-9010.88.1.100,Using theory to evaluate personality and job-performance relations: A socioanalytic perspective.,"The authors used socioanalytic theory to understand individual differences in people's performance at work. Specifically, if predictors and criteria are aligned by using theory, then the meta-analytic validity of personality measures exceeds that of atheoretical approaches. As performance assessment moved from general to specific job criteria, all Big Five personality dimensions more precisely predicted relevant criterion variables, with estimated true validities of .43 (Emotional Stability), .35 (Extraversion-Ambition), .34 (Agreeableness), .36 (Conscientiousness), and .34 (Intellect-Openness to Experience).",0
https://doi.org/10.1002/bimj.201300288,Evidence synthesis for count distributions based on heterogeneous and incomplete aggregated data,"The analysis of count data is commonly done using Poisson models. Negative binomial models are a straightforward and readily motivated generalization for the case of overdispersed data, that is, when the observed variance is greater than expected under a Poissonian model. Rate and overdispersion parameters then need to be considered jointly, which in general is not trivial. Here, we are concerned with evidence synthesis in the case where the reporting of data is rather heterogeneous, that is, events are reported either in terms of mean event counts, the proportion of event-free patients, or rate estimates and standard errors. Either figure carries some information about the relevant parameters, and it is the joint modeling that allows for coherent inference on the parameters of interest. The methods are motivated and illustrated by a systematic review in chronic obstructive pulmonary disease.",0
https://doi.org/10.1016/j.jspi.2006.08.008,Empirical Bayes estimation in finite population sampling under functional measurement error models,"Abstract The paper considers simultaneous estimation of finite population means for several strata. A model-based approach is taken, where the covariates in the super-population model are subject to measurement errors. Empirical Bayes (EB) estimators of the strata means are developed and an asymptotic expression for the MSE of the EB estimators is provided. It is shown that the proposed EB estimators are “first order optimal” in the sense of Robbins [1956. An empirical Bayes approach to statistics. In: Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, vol. 1, University of California Press, Berkeley, pp. 157–164], while the regular EB estimators which ignore the measurement error are not.",0
https://doi.org/10.1007/0-306-47531-6_13,Testlet Response Theory: An Analog for the 3PL Model Useful in Testlet-Based Adaptive Testing,"The invention of short multiple choice test items provided an enormous technical and practical advantage for test developers; certainly the items could be scored easily, but that was just one of the reasons for their popular adoption in the early part of the 20th century. A more important reason was the increase in validity offered because of the speed with which such items could be answered. This meant that a broad range of content specifications could be addressed, and hence an examinee need no longer be penalized because of an unfortunate choice of constructed response (e.g., essay) question. These advantages, as well as many others (see Anastasi, 1976, 415-417) led the multiple choice format to become, by far, the dominant form used in large-scale standardized mental testing throughout this century. Nevertheless, this breakthrough in test construction, dominant at least since the days of Army is currently being reconsidered. Critics of tests that are made up of large numbers of short questions suggest that decontextualized items yield a task that is abstracted too far from the domain of inference for many potential uses. For several reasons, only one of them as a response to this criticism, variations in test theory were considered that would allow the retention of the shortanswer format while at the same time eliminating the shortcomings expressed by those critics. One of these variations was the development of item response theory (IRT), an analytic breakthrough in test scoring. A key feature of IRT is that examinee responses are conceived of as reflecting evidence of a particular location on a single underlying latent",0
https://doi.org/10.1037/0022-3514.51.6.1173,"The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations.","In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators. Â© 1986 American Psychological Association.",0
https://doi.org/10.1007/bf02294365,On structural equation modeling with data that are not missing completely at random,"A general latent variable model is given which includes the specification of a missing data mechanism. This framework allows for an elucidating discussion of existing general multivariate theory bearing on maximum likelihood estimation with missing data. Here, missing completely at random is not a prerequisite for unbiased estimation in large samples, as when using the traditional listwise or pairwise present data approaches. The theory is connected with old and new results in the area of selection and factorial invariance. It is pointed out that in many applications, maximum likelihood estimation with missing data may be carried out by existing structural equation modeling software, such as LISREL and LISCOMP. Several sets of artifical data are generated within the general model framework. The proposed estimator is compared to the two traditional ones and found superior.",0
https://doi.org/10.1016/0893-6080(95)00014-3,Convergence results for the EM approach to mixtures of experts architectures,"The Expectation-Maximization (EM) algorithm is an iterative approach to maximum likelihood parameter estimation. Jordan and Jacobs (1993) recently proposed an EM algorithm for the mixture of experts architecture of Jacobs, Jordan, Nowlan and Hinton (1991) and the hierarchical mixture of experts architecture of Jordan and Jacobs (1992). They showed empirically that the EM algorithm for these architectures yields significantly faster convergence than gradient ascent. In the current paper we provide a theoretical analysis of this algorithm. We show that the algorithm can be regarded as a variable metric algorithm with its searching direction having a positive projection on the gradient of the log likelihood. We also analyze the convergence of the algorithm and provide an explicit expression for the convergence rate. In addition, we describe an acceleration technique that yields a significant speedup in simulation experiments.",0
https://doi.org/10.1002/sim.3124,A Bayesian approach for sample size determination in method comparison studies,"Studies involving two methods for measuring a continuous response are regularly conducted in health sciences to evaluate agreement of a method with itself and agreement between methods. Notwithstanding their wide usage, the design of such studies, in particular, the sample size determination, has not been addressed in the literature when the goal is the simultaneous evaluation of intra- and inter-method agreement. We fill this need by developing a simulation-based Bayesian methodology for determining sample sizes in a hierarchical model framework. Unlike a frequentist approach, it takes into account uncertainty in parameter estimates. This methodology can be used with any scalar measure of agreement available in the literature. We demonstrate this for four currently used measures. The proposed method is applied to an ongoing proteomics project, where we use pilot data to determine the number of individuals and the number of replications needed to evaluate the agreement between two methods for measuring protein ratios. We also apply our method to determine the sample size for an experiment involving measurement of blood pressure. Copyright © 2007 John Wiley & Sons, Ltd.",0
https://doi.org/10.1207/s15328007sem0803_1,The Consequences of Ignoring Multilevel Data Structures in Nonhierarchical Covariance Modeling,"This study examined the effects of ignoring multilevel data structures in nonhierarchical covariance modeling using a Monte Carlo simulation. Multilevel sample data were generated with respect to 3 design factors: (a) intraclass correlation, (b) group and member configuration, and (c) the models that underlie the between-group and within-group variance components associated with multilevel data. Covariance models that ignored the multilevel structure were then fit to the data. Results indicated that when variables exhibit minimal levels of intraclass correlation, the chi-square model/data fit statistic, the parameter estimators, and the standard error estimators are relatively unbiased. However, as the level of intraclass correlation increases, the chi-square statistic, the parameters, and their standard errors all exhibit estimation problems. The specific group/member configurations as well as the underlying between-group and within-group model structures further exacerbate the estimation problems encoun...",0
https://doi.org/10.1109/icemt.2010.5657604,Notice of Retraction: A study of equating of Computer-Based College English Placement Test,"This paper was to investigate the application of item response theory to specify an appropriate item response model for the equating of the mixed-format of multiple-choice items and open-ended items, since equating is the precondition of the development of an item bank for the Computer-Based College English Placement Test. It was shown that the equating approach based on common-item nonequivalent groups design and the two-parameter logistic model is ideal to be used to calibrate item and ability parameters in two alternate test forms since this model fits data of all listening and reading sections in both forms.",0
https://doi.org/10.1177/1094428104263672,Using Generalized Estimating Equations for Longitudinal Data Analysis,"The generalized estimating equation (GEE) approach of Zeger and Liang facilitates analysis of data collected in longitudinal, nested, or repeated measures designs. GEEs use the generalized linear model to estimate more efficient and unbiased regression parameters relative to ordinary least squares regression in part because they permit specification of a working correlation matrix that accounts for the form of within-subject correlation of responses on dependent variables of many different distributions, including normal, binomial, and Poisson. The author briefly explains the theory behind GEEs and their beneficial statistical properties and limitations and compares GEEs to suboptimal approaches for analyzing longitudinal data through use of two examples. The first demonstration applies GEEs to the analysis of data from a longitudinal lab study with a counted response variable; the second demonstration applies GEEs to analysis of data with a normally distributed response variable from subjects nested within branch offices ofan organization.",0
https://doi.org/10.1111/j.0006-341x.2003.00097.x,Modeling Longitudinal Data with Nonignorable Dropouts Using a Latent Dropout Class Model,"In longitudinal studies with dropout, pattern-mixture models form an attractive modeling framework to account for nonignorable missing data. However, pattern-mixture models assume that the components of the mixture distribution are entirely determined by the dropout times. That is, two subjects with the same dropout time have the same distribution for their response with probability one. As that is unlikely to be the case, this assumption made lead to classification error. In addition, if there are certain dropout patterns with very few subjects, which often occurs when the number of observation times is relatively large, pattern-specific parameters may be weakly identified or require identifying restrictions. We propose an alternative approach, which is a latent-class model. The dropout time is assumed to be related to the unobserved (latent) class membership, where the number of classes is less than the number of observed patterns; a regression model for the response is specified conditional on the latent variable. This is a type of shared-parameter model, where the shared ""parameter"" is discrete. Parameter estimates are obtained using the method of maximum likelihood. Averaging the estimates of the conditional parameters over the distribution of the latent variable yields estimates of the marginal regression parameters. The methodology is illustrated using longitudinal data on depression from a study of HIV in women.",0
https://doi.org/10.1080/10705510701758265,A Simulation Study Comparison of Bayesian Estimation With Conventional Methods for Estimating Unknown Change Points,"The main purpose of this research is to evaluate the performance of a Bayesian approach for estimating unknown change points using Monte Carlo simulations. The univariate and bivariate unknown change point mixed models were presented and the basic idea of the Bayesian approach for estimating the models was discussed. The performance of Bayesian estimation was evaluated using simulation studies of longitudinal data with different sample sizes, varying change point values, different levels of Level-1 variances, and univariate versus bivariate outcomes. The numerical results compared the performance of the Bayesian methods with the first-order Taylor expansion method and the adaptive Gaussian quadrature method implemented in SAS PROC NLMIXED. These simulation results showed that the first-order Taylor expansion method and the adaptive Gaussian quadrature method were sensitive to the initial values, making the results somewhat unreliable. In contrast, these simulation results showed that Bayesian estimation w...",0
https://doi.org/10.1198/016214504000001187,Causal Inference With General Treatment Regimes,"In this article we develop the theoretical properties of the propensity function, which is a generalization of the propensity score of Rosenbaum and Rubin. Methods based on the propensity score have long been used for causal inference in observational studies; they are easy to use and can effectively reduce the bias caused by nonrandom treatment assignment. Although treatment regimes need not be binary in practice, the propensity score methods are generally confined to binary treatment scenarios. Two possible exceptions have been suggested for ordinal and categorical treatments. In this article we develop theory and methods that encompass all of these techniques and widen their applicability by allowing for arbitrary treatment regimes. We illustrate our propensity function methods by applying them to two datasets; we estimate the effect of smoking on medical expenditure and the effect of schooling on wages. We also conduct simulation studies to investigate the performance of our methods.",0
https://doi.org/10.1167/iovs.13-12032,"Perimetric Evaluation of Saccadic Latency, Saccadic Accuracy, and Visual Threshold for Peripheral Visual Stimuli in Young Compared With Older Adults","Using a novel automated perimetry technique, we tested the hypothesis that older adults will have increased latency and decreased accuracy of saccades, as well as higher visual thresholds, to peripheral visual stimuli when compared with younger adults.We tested 20 healthy subjects aged 18 to 30 years (""young"") and 21 healthy subjects at least 60 years old (""older"") for detection of briefly flashed peripheral stimuli of differing sizes in eight locations along the horizontal meridian (±4°, ±12°, ±20°, and ±28°). With the left eye occluded, subjects were instructed to look quickly toward any seen stimuli. Right eye movements were recorded with an EyeLink 1000 infrared camera system. Limiting our analysis to the four stimulus positions in the nasal hemifield (-4°, -12°, -20°, and -28°), we evaluated for group-level differences in saccadic latency, accuracy, and visual threshold at each stimulus location.Saccadic latency increased as stimulus size decreased in both groups. Older subjects had significantly increased saccadic latencies (at all locations; P < 0.05), decreased accuracies (at all locations; P < 0.05), and higher visual thresholds (at the -12°, -20°, and -28° locations; P < 0.05). Additionally, there were significant relationships between visual threshold and latency, visual threshold and accuracy, and latency and accuracy (P < 0.0001).Older adults have increased latency and decreased accuracy of saccades, as well as higher visual thresholds, to peripheral visual stimuli when compared with younger adults. Saccadic latency and accuracy are related to visual threshold, suggesting that saccadic latency and accuracy could be useful as perimetric outcome measures.",0
https://doi.org/10.1525/aa.1986.88.2.02a00020,Culture as Consensus: A Theory of Culture and Informant Accuracy,"This paper presents and tests a formal mathematical model for the analysis of informant responses to systematic interview questions. We assume a situation in which the ethnographer does not know how much each informant knows about the cultural domain under consideration nor the answers to the questions. The model simultaneously provides an estimate of the cultural competence or knowledge of each informant and an estimate of the correct answer to each question asked of the informant. The model currently handles true-false, multiple-choice, andfill-in-the-blank type question formats. In familiar cultural domains the model produces good results from as few as four informants. The paper includes a table showing the number of informants needed to provide stated levels of confidence given the mean level of knowledge among the informants. Implications are discussed.",0
https://doi.org/10.3389/fnbeh.2014.00040,Sex differences in mechanical allodynia: how can it be preclinically quantified and analyzed?,"Translating promising preclinical drug discoveries to successful clinical trials remains a significant hurdle in pain research. Although animal models have significantly contributed to understanding chronic pain pathophysiology, the majority of research has focused on male rodents using testing procedures that produce sex difference data that do not align well with comparable clinical experiences. Additionally, the use of animal pain models presents ongoing ethical challenges demanding continuing refinement of preclinical methods. To this end, this study sought to test a quantitative allodynia assessment technique and associated statistical analysis in a modified graded nerve injury pain model with the aim to further examine sex differences in allodynia. Graded allodynia was established in male and female Sprague Dawley rats by altering the number of sutures placed around the sciatic nerve and quantified by the von Frey test. Linear mixed effects modeling regressed response on each fixed effect (sex, oestrus cycle, pain treatment). On comparison with other common von Frey assessment techniques, utilizing lower threshold filaments than those ordinarily tested, at 1 s intervals, appropriately and successfully investigated female mechanical allodynia, revealing significant sex and oestrus cycle difference across the graded allodynia that other common behavioral methods were unable to detect. Utilizing this different von Frey approach and graded allodynia model, a single suture inflicting less allodynia was sufficient to demonstrate exaggerated female mechanical allodynia throughout the phases of dioestrus and pro-oestrus. Refining the von Frey testing method, statistical analysis technique and the use of a graded model of chronic pain, allowed for examination of the influences on female mechanical nociception that other von Frey methods cannot provide.",0
https://doi.org/10.1007/s10742-015-0139-z,Multilevel Poisson sample selection models and alternative methods for estimating hospital effects on long-term outcomes,"Hospital care focuses on improving patients’ long-term quality of life, yet hospital quality metrics typically focus on short-term processes. Attempting to understand a patient’s long-term process introduces sample selection bias since patients must survive the hospitalization in order to observe post-hospitalization outcomes. As a result, proper analysis of long-term outcomes should account for clustering, due to the hierarchical structure of hospital data, as well as sample selection bias. The objective of this paper was to evaluate random effect parameter estimation and higher-level ranking of long-term count outcomes and short-term selection processes in the presence of cluster and selection bias by comparing multilevel Poisson models, multilevel zero-inflated Poisson models, and multilevel Poisson sample selection models (MPSSMs) in a series of simulations. We simulated an outcome resembling a post-discharge Poisson count with a pre-specified selection process determining a patient’s hospitalization survival with each hospital having a unique effect on both processes. In order to clarify the methodology, we also analyzed a real-world hospital dataset involving a count outcome conditioned on the selection process of hospital survival. Across all simulations, the random effect parameter estimates were directly compared and the empirical Bayes estimates were extracted, ranked, and compared using the Spearman rank correlation. Results show that the MPSSM produces more accurate random effect parameter estimates and higher-level empirical Bayes ranks. When modeling multilevel effects on long-term count outcomes observed after a short-term selection process, higher-level effects are more reliably measured using MPSSMs. © 2015, Springer Science+Business Media New York (outside the USA).",0
https://doi.org/10.1007/s40273-013-0067-0,A Choice That Matters?,"Decision-analytic cost-effectiveness (CE) models combine many different parameters like transition probabilities, event probabilities, utilities and costs, which are often obtained after meta-analysis. The method of meta-analysis may affect the CE estimate.Our aim was to perform a simulation study that compares the performance of different methods of meta-analysis, especially with respect to model-based health economic (HE) outcomes.A reference patient population of 50,000 was simulated from which sets of samples were drawn. Each sample drawn represented a clinical trial comparing two fictitious interventions. In several scenarios, the heterogeneity between these trials was varied, by drawing one or more of the trials from predefined subpopulations. Parameter estimates from these trials were combined using frequentist fixed (FFE) and random effects (FRE), and Bayesian fixed (BFE) and random effects (BRE) meta-analysis. The pooled parameter estimates were entered into a probabilistic cost-effectiveness Markov model. The four methods of meta-analysis resulted in different parameter estimates and HE outcomes, which were compared with the true values in the reference population. Performance statistics were: (1) the percentage of repetitions that the confidence interval of the probabilistic sensitivity analysis covers the true value (coverage), (2) the difference between the estimated and true value (bias), (3) the mean absolute value of the bias (MAD) and (4) the percentage of repetitions that result in a statistically significant difference between the two interventions (statistical power). As the differences between methods could be due to chance, we repeated every step of the analysis 1,000 times to study whether differences were systematic.FFE, FRE and BFE lead to different parameter estimates, but, when entered into the model, they do not lead to large differences in the point estimates of the HE outcomes, even in scenarios where we built in heterogeneity. Random effects methods do not necessarily reduce bias when heterogeneity is added to the trials, and may even increase bias in certain situations. BRE tends to overestimate uncertainty reflected in the CE acceptability curve.FFE, FRE and BFE lead to comparable HE outcomes. BRE tends to overestimate uncertainty. Based on this study, we recommend FRE as the preferred method of meta-analysis.",0
https://doi.org/10.1037/a0019390,Perceiver effects as projective tests: What your perceptions of others say about you.,"In 3 studies, we document various properties of perceiver effects--or how an individual generally tends to describe other people in a population. First, we document that perceiver effects have consistent relationships with dispositional characteristics of the perceiver, ranging from self-reported personality traits and academic performance to well-being and measures of personality disorders, to how liked the person is by peers. Second, we document that the covariation in perceiver effects among trait dimensions can be adequately captured by a single factor consisting of how positively others are seen across a wide range of traits (e.g., how nice, interesting, trustworthy, happy, and stable others are generally seen). Third, we estimate the 1-year stability of perceiver effects and show that individual differences in the typical perception of others have a level of stability comparable to that of personality traits. The results provide compelling evidence that how individuals generally perceive others is a stable individual difference that reveals much about the perceiver's own personality.",0
https://doi.org/10.3389/fpsyg.2016.00289,The Importance of Isomorphism for Conclusions about Homology: A Bayesian Multilevel Structural Equation Modeling Approach with Ordinal Indicators,"We describe a Monte Carlo study examining the impact of assuming item isomorphism (i.e., equivalent construct meaning across levels of analysis) on conclusions about homology (i.e., equivalent structural relations across levels of analysis) under varying degrees of non-isomorphism in the context of ordinal indicator multilevel structural equation models (MSEMs). We focus on the condition where one or more loadings are higher on the between level than on the within level to show that while much past research on homology has ignored the issue of psychometric isomorphism, psychometric isomorphism is in fact critical to valid conclusions about homology. More specifically, when a measurement model with non-isomorphic items occupies an exogenous position in a multilevel structural model and the non-isomorphism of these items is not modeled, the within level exogenous latent variance is under-estimated leading to over-estimation of the within level structural coefficient, while the between level exogenous latent variance is overestimated leading to underestimation of the between structural coefficient. When a measurement model with non-isomorphic items occupies an endogenous position in a multilevel structural model and the non-isomorphism of these items is not modeled, the endogenous within level latent variance is under-estimated leading to under-estimation of the within level structural coefficient while the endogenous between level latent variance is over-estimated leading to over-estimation of the between level structural coefficient. The innovative aspect of this article is demonstrating that even minor violations of psychometric isomorphism render claims of homology untenable. We also show that posterior predictive p-values for ordinal indicator Bayesian MSEMs are insensitive to violations of isomorphism even when they lead to severely biased within and between level structural parameters. We highlight conditions where poor estimation of even correctly specified models rules out empirical examination of isomorphism and homology without taking precautions, for instance, larger Level-2 sample sizes, or using informative priors.",0
https://doi.org/10.1167/15.14.6,Independence of the completion effect from the noncompletion effect in illusory contour perception,"Spatially separated object information can be effortlessly completed in the visual system, as demonstrated by the well-known Kanizsa-type illusory contours. The perception of illusory contours is closely associated with the spatial configuration of contour fragments, leading to the long-lasting difficulty in distinguishing the effect of the completion process that interpolates the contour fragments from the effect of the noncompletion process that analyzes the contour fragments. However, a close relationship does not necessarily imply nonindependence, e.g., two people may show similar behaviors in one situation but may not in another situation. Inspired by this simple common sense, we conducted a contour discrimination task (i.e., discriminating between the interpolated contours) and a fragment discrimination task (i.e., discriminating between the physically-specified contour fragments) for Kanizsa squares and Kanizsa circles. The performance difference between the contour and fragment discrimination tasks was much larger for Kanizsa circles than for Kanizsa squares. This independence of the completion effect--as indicated by the performance in the contour task--from the noncompletion effect--as indicated by the performance in the fragment task--provides new insights into the understanding of the mechanism of visual completion.",0
https://doi.org/10.1214/12-ba730,On the Half-Cauchy Prior for a Global Scale Parameter,"This paper argues that the half-Cauchy distribution should replace the inverseGamma distribution as a default prior for a top-level scale parameter in Bayesian hierarchical models, at least for cases where a proper prior is necessary. Our arguments involve a blend of Bayesian and frequentist reasoning, and are intended to complement the original case made by Gelman (2006) in support of the folded-t family of priors. First, we generalize the half-Cauchy prior to the wider class of hypergeometric inverted-beta priors. We derive expressions for posterior moments and marginal densities when these priors are used for a top-level normal variance in a Bayesian hierarchical model. We go on to prove a proposition that, together with the results for moments and marginals, allows us to characterize the frequentist risk of the Bayes estimators under all global-shrinkage priors in the class. These theoretical results, in turn, allow us to study the frequentist properties of the half-Cauchy prior versus a wide class of alternatives. The half-Cauchy occupies a sensible “middle ground” within this class: it performs very well near the origin, but does not lead to drastic compromises in other parts of the parameter space. This provides an alternative, classical justification for the repeated, routine use of this prior. We also consider situations where the underlying mean vector is sparse, where we argue that the usual conjugate choice of an inverse-gamma prior is particularly inappropriate, and can lead to highly distorted posterior inferences. Finally, we briefly summarize some open issues in the specification of default priors for scale terms in hierarchical models.",0
https://doi.org/10.1080/10705510709336744,Detecting Mixtures From Structural Model Differences Using Latent Variable Mixture Modeling: A Comparison of Relative Model Fit Statistics,"The accuracy of structural model parameter estimates in latent variable mixture modeling was explored with a 3 (sample size) × 3 (exogenous latent mean difference) × 3 (endogenous latent mean difference) × 3 (correlation between factors) × 3 (mixture proportions) factorial design. In addition, the efficacy of several likelihood-based statistics (Akaike's Information Criterion [AIC], Bayesian Information Ctriterion [BIC], the sample-size adjusted BIC [ssBIC], the consistent AIC [CAIC], the Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test [aVLMR]), classification-based statistics (CLC [classification likelihood information criterion], ICL-BIC [integrated classification likelihood], normalized entropy criterion [NEC], entropy), and distributional statistics (multivariate skew and kurtosis test) were examined to determine which statistics best recover the correct number of components. Results indicate that the structural parameters were recovered, but the model fit statistics were not exceedingly accurat...",0
https://doi.org/10.1111/biom.12411,Nonparametric Bayes modeling for case control studies with many predictors,"It is common in biomedical research to run case-control studies involving high-dimensional predictors, with the main goal being detection of the sparse subset of predictors having a significant association with disease. Usual analyses rely on independent screening, considering each predictor one at a time, or in some cases on logistic regression assuming no interactions. We propose a fundamentally different approach based on a nonparametric Bayesian low rank tensor factorization model for the retrospective likelihood. Our model allows a very flexible structure in characterizing the distribution of multivariate variables as unknown and without any linear assumptions as in logistic regression. Predictors are excluded only if they have no impact on disease risk, either directly or through interactions with other predictors. Hence, we obtain an omnibus approach for screening for important predictors. Computation relies on an efficient Gibbs sampler. The methods are shown to have high power and low false discovery rates in simulation studies, and we consider an application to an epidemiology study of birth defects.",0
https://doi.org/10.1207/s15328007sem1304_1,On the Merits of Orthogonalizing Powered and Product Terms: Implications for Modeling Interactions Among Latent Variables,"The goals of this article are twofold: (a) briefly highlight the merits of residual centering for representing interaction and powered terms in standard regression contexts (e.g., Lance, 1988), and (b) extend the residual centering procedure to represent latent variable interactions. The proposed method for representing latent variable interactions has potential advantages over extant procedures. First, the latent variable interaction is derived from the observed covariation pattern among all possible indicators of the interaction. Second, no constraints on particular estimated parameters need to be placed. Third, no recalculations of parameters are required. Fourth, model estimates are stable and interpretable. In our view, the orthogonalizing approach is technically and conceptually straightforward, can be estimated using any structural equation modeling software package, and has direct practical interpretation of parameter estimates. Its behavior in terms of model fit and estimated standard errors is v...",0
https://doi.org/10.1177/0013164403260196,The Analysis of Repeated Measurements with Mixed-Model Adjusted F Tests,"One approach to the analysis of repeated measures data allows researchers to model the covariance structure of their data rather than presume a certain structure, as is the case with conventional univariate and multivariate test statistics. This mixed-model approach, available through SAS PROC MIXED, was compared to a Welch-James type statistic. The Welch-James approach is known to provide generally robust tests of treatment effects in a repeated measures between-by within-subjects design under assumption violations given certain sample size requirements. The mixed-model F tests were based on Kenward-Roger’s adjusted degrees of freedom solution, an approach specifically proposed for small sample settings. The authors investigated Type I error control for repeated measures main and interaction effects in unbalanced designs when normality and covariance homogeneity assumptions did not hold. The mixed-model Kenward-Roger’s adjusted F tests showed superior Type I error control in small sample size conditions in which the Welch-James type statistic was nonrobust; power rates, however, did not favor one approach over the other.",0
https://doi.org/10.1068/i0411,Surprise Leads to Noisier Perceptual Decisions,"Surprising events in the environment can impair task performance. This might be due to complete distraction, leading to lapses during which performance is reduced to guessing. Alternatively, unpredictability might cause a graded withdrawal of perceptual resources from the task at hand and thereby reduce sensitivity. Here we attempt to distinguish between these two mechanisms. Listeners performed a novel auditory pitch—duration discrimination, where stimulus loudness changed occasionally and incidentally to the task. Responses were slower and less accurate in the surprising condition, where loudness changed unpredictably, than in the predictable condition, where the loudness was held constant. By explicitly modelling both lapses and changes in sensitivity, we found that unpredictable changes diminished sensitivity but did not increase the rate of lapses. These findings suggest that background environmental uncertainty can disrupt goal-directed behaviour. This graded processing strategy might be adaptive in potentially threatening contexts, and reflect a flexible system for automatic allocation of perceptual resources.",0
https://doi.org/10.1023/a:1020281327116,,"This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.",0
https://doi.org/10.1177/0272431613511331,Single-Level and Multilevel Mediation Analysis,"Mediation analysis is a statistical approach used to examine how the effect of an independent variable on an outcome is transmitted through an intervening variable (mediator). In this article, we provide a gentle introduction to single-level and multilevel mediation analyses. Using single-level data, we demonstrate an application of structural equation modeling (SEM) in estimating mediation models with multiple mediators and multiple outcomes. We also describe the estimation and interpretation of a 2→1→1 multilevel mediation model in which the first, second, and third numbers correspond to the measurement levels of the independent, mediator, and outcome variables, respectively. We present two numerical examples that use simulated data based on published studies in adolescence research to demonstrate how to specify, estimate, and interpret the results of single-level and multilevel mediation analyses to answer key research questions. We briefly discuss underlying assumptions required to make a valid causal interpretation of a mediation analysis.",0
https://doi.org/10.1016/j.psychsport.2015.11.004,Using self-determination theory to understand motivation for walking: Instrument development and model testing using Bayesian structural equation modelling,"Abstract Objective The motivational processes underpinning walking behaviour are not well understood. This study aimed to develop walking-specific motivation measures drawn from self-determination theory (SDT), assess the psychometric properties of the measures, incorporating Baysesian structural equation modelling (BSEM), and examine how these variables relate to walking behaviour. Method Participants (n = 298; mean age = 41.69; S.D. = 11.06; male = 57) completed the Behavioural Regulations in Walking Questionnaire (BRWQ), Psychological Needs Satisfaction for Walking Scale (PNSWS) and the IPAQ-long form, from which measures of workplace, transport and leisure walking were extracted. BSEM was used to test the hypothesized factor structures of the BRWQ and PNSWS. Internal reliabilities were assessed using the composite reliability coefficient. Convergent and discriminant validity were assessed by examining the relationships between the variables in relation to established theory. Results BSEM showed excellent fit for the BRWQ and PNSWS measurement models. The scales demonstrated good internal consistency. The associations within and between the BRWQ and PNSWS subscales were generally as expected. The relationship between the BRWQ subscales and walking for transport and leisure were also generally as expected, but there were no significant relationships for walking at work. Two PNSWS subscales were significantly related to walking for leisure, but no significant relationships were evident for walking for transport and at work. Conclusions There is preliminary evidence for the acceptable psychometric properties of instruments to measure SDT constructs in walking, and the findings highlight the advantages of BSEM. The findings also suggest that the motivational processes underpinning walking may vary by type of walking.",0
https://doi.org/10.1080/01621459.1987.10478410,Better Bootstrap Confidence Intervals,"Abstract We consider the problem of setting approximate confidence intervals for a single parameter θ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, , can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.",0
https://doi.org/10.1037/1082-989x.9.2.147,"The Persistence of Underpowered Studies in Psychological Research: Causes, Consequences, and Remedies.","Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The ""curse of multiplicities"" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing.",0
https://doi.org/10.1016/j.jsp.2010.06.003,First graders' literacy and self-regulation gains: The effect of individualizing student instruction,"We examined the effect of individualizing student instruction (ISI; N=445 students, 46 classrooms) on first graders' self-regulation gains compared to a business-as-usual control group. Self-regulation, conceptualized as a constellation of executive skills, was positively associated with academic development. We hypothesized that the ISI intervention's emphasis on teacher planning and organization, classroom management, and the opportunity for students to work independently and in small groups would promote students' self-regulation. We found no main effect of ISI on self-regulation gains. However, for students with weaker initial self-regulation, ISI was associated with greater self-regulation gains compared to peers in control classrooms. The ISI effect on self-regulation was greater when the intervention was more fully implemented.",0
https://doi.org/10.1177/0013164410387594,The Influence of Dimensionality on Parameter Estimation Accuracy in the Generalized Graded Unfolding Model,"The generalized graded unfolding model (GGUM) is an ideal point model of responding that is consistent with the Thurstonian theory of respondent behavior. Ideal point models have recently generated interest in the realms of attitude and personality assessment. One unclear aspect of applying ideal point models is the influence of multidimensionality on GGUM item and person parameters estimation accuracy. Using simulated data, the authors tested the influence of the balance, or ratio, of items loading onto two dimensions, the degree of bidimensionality and sample size on parameter estimation accuracy. The results suggest that bidimensionality and the proportion of items loading onto a second trait increases estimation error. The second trait was chosen in estimation when a large number of the items in the survey reflected a highly irrelevant second trait. Estimation error was greater for persons and items at the extreme ends of the continuum; positive estimates were biased upward and positive parameters downward. The results suggest that although the GGUM chooses another trait in estimation, in most cases conventional fit analyses and checks for item parameter extremity are likely to be successful in identifying items measuring another trait. Furthermore, the conditions in which the trait being estimated may not be clear should be rare in practice. The implications of these results for researchers who wish to apply these models to real-life data are discussed.",0
https://doi.org/10.1007/bf02294403,The area between two item characteristic curves,"Formulas for computing the exact signed and unsigned areas between two item characteristic curves (ICCs) are presented. It is further shown that when the c parameters are unequal, the area between two ICCs is infinite. The significance of the exact area measures for item bias research is discussed. Â© 1988 The Psychometric Society.",0
https://doi.org/10.1111/bmsp.12054,A mixture hierarchical model for response times and response accuracy,"In real testing, examinees may manifest different types of test-taking behaviours. In this paper we focus on two types that appear to be among the more frequently occurring behaviours – solution behaviour and rapid guessing behaviour. Rapid guessing usually happens in high-stakes tests when there is insufficient time, and in low-stakes tests when there is lack of effort. These two qualitatively different test-taking behaviours, if ignored, will lead to violation of the local independence assumption and, as a result, yield biased item/person parameter estimation. We propose a mixture hierarchical model to account for differences among item responses and response time patterns arising from these two behaviours. The model is also able to identify the specific behaviour an examinee engages in when answering an item. A Monte Carlo expectation maximization algorithm is proposed for model calibration. A simulation study shows that the new model yields more accurate item and person parameter estimates than a non-mixture model when the data indeed come from two types of behaviour. The model also fits real, high-stakes test data better than a non-mixture model, and therefore the new model can better identify the underlying test-taking behaviour an examinee engages in on a certain item.",0
https://doi.org/10.1111/1467-985x.00206,Improved estimation procedures for multilevel models with binary response: a case‐study,"During recent years, analysts have been relying on approximate methods of inference to estimate multilevel models for binary or count data. In an earlier study of random-intercept models for binary outcomes we used simulated data to demonstrate that one such approximation, known as marginal quasi-likelihood, leads to a substantial attenuation bias in the estimates of both fixed and random effects whenever the random effects are non-trivial. In this paper, we fit three-level random-intercept models to actual data for two binary outcomes, to assess whether refined approximation procedures, namely penalized quasi-likelihood and second-order improvements to marginal and penalized quasi-likelihood, also underestimate the underlying parameters. The extent of the bias is assessed by two standards of comparison: exact maximum likelihood estimates, based on a Gauss–Hermite numerical quadrature procedure, and a set of Bayesian estimates, obtained from Gibbs sampling with diffuse priors. We also examine the effectiveness of a parametric bootstrap procedure for reducing the bias. The results indicate that second-order penalized quasi-likelihood estimates provide a considerable improvement over the other approximations, but all the methods of approximate inference result in a substantial underestimation of the fixed and random effects when the random effects are sizable. We also find that the parametric bootstrap method can eliminate the bias but is computationally very intensive.",0
https://doi.org/10.1191/096228001678227794,Bayesian methods in meta-analysis and evidence synthesis,"This paper reviews the use of Bayesian methods in meta-analysis. Whilst there has been an explosion in the use of meta-analysis over the last few years, driven mainly by the move towards evidence-based healthcare, so too Bayesian methods are being used increasingly within medical statistics. Whilst in many meta-analysis settings the Bayesian models used mirror those previously adopted in a frequentist formulation, there are a number of specific advantages conferred by the Bayesian approach. These include: full allowance for all parameter uncertainty in the model, the ability to include other pertinent information that would otherwise be excluded, and the ability to extend the models to accommodate more complex, but frequently occurring, scenarios. The Bayesian methods discussed are illustrated by means of a meta-analysis examining the evidence relating to electronic fetal heart rate monitoring and perinatal mortality in which evidence is available from a variety of sources.",0
https://doi.org/10.1161/strokeaha.108.515460,Longitudinal Analysis of Quality of Life for Stroke Survivors Using Latent Curve Models,"For the survivors, activities of daily living, handicap, and depression have a significant impact on health-related quality of life (HRQOL). How the dynamic changes of these variables relate to HRQOL over time in the subacute phase of stroke recovery has not been investigated. The objective of this study was to study longitudinal behaviors of HRQOL of the stroke survivors in relation to the changes in activities of daily living, handicap, and depression after stroke.This was a prospective cohort study of first disabling patients with stroke. Subjects were interviewed at 3, 6, and 12 months after stroke for modified Barthel Index, London Handicap Scale, Geriatric Depression Scale, and the World Health Organization Quality of Life questionnaire (abbreviated Hong Kong version). A latent curve model was developed to analyze how the dynamic changes in activities of daily living, handicap, and depressive mood related to the changes in HRQOL.Two hundred forty-seven of 303 patients (82%) followed up at 3 months after stroke could complete the quality-of-life questionnaire. Their mean age was 68.8 years. The latent curve model analysis revealed that initial physical health HRQOL was independently associated with activities of daily living, handicap, and depression. The other 3 HRQOL domain scores were primarily associated with depression only. The rates of change in all 4 domains of HRQOL were significantly and inversely associated with rate of change in the Geriatric Depression Scale only.Change in mood in the postacute phase of stroke recovery is the most significant determinant of change in HRQOL. More attention should be paid to the detection and management of poststroke depression.",0
https://doi.org/10.1207/s15327906mbr3602_06,Multilevel Modeling of Individual and Group Level Mediated Effects,"This article combines procedures for single-level mediational analysis with multilevel modeling techniques in order to appropriately test mediational effects in clustered data. A simulation study compared the performance of these multilevel mediational models with that of single-level mediational models in clustered data with individual- or group-level initial independent variables, individual- or group-level mediators, and individual level outcomes. The standard errors of mediated effects from the multilevel solution were generally accurate, while those from the single-level procedure were downwardly biased, often by 20% or more. The multilevel advantage was greatest in those situations involving group-level variables, larger group sizes, and higher intraclass correlations in mediator and outcome variables. Multilevel mediational modeling methods were also applied to data from a preventive intervention designed to reduce intentions to use steroids among players on high school football teams. This example illustrates differences between single-level and multilevel mediational modeling in real-world clustered data and shows how the multilevel technique may lead to more accurate results.",0
https://doi.org/10.1080/10705511.2011.557338,Estimation of MIMIC Model Parameters with Multilevel Data,"The purpose of this simulation study was to assess the performance of latent variable models that take into account the complex sampling mechanism that often underlies data used in educational, psychological, and other social science research. Analyses were conducted using the multiple indicator multiple cause (MIMIC) model, which is a flexible and effective tool for relating observed and latent variables. The data were simulated in a hierarchical framework (e.g., individuals nested in schools) so that a multilevel modeling approach would be appropriate. Analyses were conducted accounting for and not accounting for the nested data to determine the impact of ignoring such multilevel data structures in full structural equation models. Results highlight the differences in modeling results when the analytic strategy is congruent with the data structure and what occurs when this congruency is absent. Type I error rates and power for the standard and multilevel methods were similar for within-cluster variables ...",0
https://doi.org/10.1177/0003122415573049,Paradoxes of Social Policy,"Korpi and Palme’s (1998) classic “The Paradox of Redistribution and Strategies of Equality” claims that universal social policy better reduces poverty than social policies targeted at the poor. This article revisits Korpi and Palme’s classic, and in the process, explores and informs a set of enduring questions about social policy, politics, and social equality. Specifically, we investigate the relationships between three dimensions of welfare transfers—transfer share (the average share of household income from welfare transfers), low-income targeting, and universalism—and poverty and preferences for redistribution. We analyze rich democracies like Korpi and Palme, but we also generalize to a broader sample of developed and developing countries. Consistent with Korpi and Palme, we show (1) poverty is negatively associated with transfer share and universalism; (2) redistribution preferences are negatively associated with low-income targeting; and (3) universalism is positively associated with transfer share. Contrary to Korpi and Palme, redistribution preferences are not related to transfer share or universalism; and low-income targeting is neither positively associated with poverty nor negatively associated with transfer share. Therefore, instead of the “paradox of redistribution” we propose two new paradoxes of social policy: non-complementarity and undermining. The non-complementarity paradox entails a mismatch between the dimensions that matter to poverty and the dimension that matters to redistribution preferences. The undermining paradox emphasizes that the dimension (transfer share) that most reduces poverty tends to increase with the one dimension (low-income targeting) that reduces support for redistribution.",0
https://doi.org/10.1111/j.1467-842x.2012.00647.x,COMBINING INDIVIDUAL PARTICIPANT DATA AND SUMMARY STATISTICS FROM BOTH CONTINUOUSLY VALUED AND BINARY VARIABLES TO ESTIMATE REGRESSION PARAMETERS,"Summary  Recent research has extended standard methods for meta-analysis to more general forms of evidence synthesis, where the aim is to combine different data types or data summaries that contain information about functions of multiple parameters to make inferences about the parameters of interest. We consider one such scenario in which the goal is to make inferences about the association between a primary binary exposure and continuously valued outcome in the context of several confounding exposures, and where the data are available in various different forms: individual participant data (IPD) with repeated measures, sample means that have been aggregated over strata, and binary data generated by thresholding the underlying continuously valued outcome measure. We show that an estimator of the population mean of a continuously valued outcome can be constructed using binary threshold data provided that a separate estimate of the outcome standard deviation is available. The results of a simulation study show that this estimator has negligible bias but is less efficient than the sample mean – the minimum variance ratio is based on a Taylor series expansion. Combining this estimator with sample means and IPD from different sources (such as a series of published studies) using both linear and probit regression does, however, improve the precision of estimation considerably by incorporating data that would otherwise have been excluded for being in the wrong format. We apply these methods to investigate the association between the G277S mutation in the transferrin gene and serum ferritin (iron) levels separately in pre- and post-menopausal women based on data from three published studies.",0
https://doi.org/10.1007/978-3-319-01264-3_9,Bayesian Methods for Conjoint Analysis-Based Predictions: Do We Still Need Latent Classes?,"AbstractRecently, more and more Bayesian methods have been proposed for modeling heterogeneous preference structures of consumers (see, e.g., Allenby et al., J Mark Res 32:152–162, 1995, 35:384–389, 1998; Baier and Polasek, Stud Classif Data Anal Knowl Organ 22:413–421, 2003; Otter et al., Int J Res Mark 21(3):285–297, 2004). Comparisons have shown that these new methods compete well with the traditional ones where latent classes are used for this purpose (see Ramaswamy and Cohen (2007) Latent class models for conjoint analysis. In: Gustafsson A, Herrmann A, Huber (eds) Conjoint measurement – methods and applications, 4th edn. Springer, Berlin, pp 295–320) for an overview on these traditional methods). This applies especially when the prediction of choices among products is the main objective (e.g. Moore et al., Mark Lett 9(2):195–207, 1998; Andrews et al., J Mark Res 39:479–487, 2002a; 39:87–98, 2002b; Moore, Int J Res Mark 21:299–312, 2004; Karniouchina et al., Eur J Oper Res 19(1):340–348, 2009, with comparative results). However, the question is still open whether this superiority still holds when the latent class approach is combined with the Bayesian one. This paper responds to this question. Bayesian methods with and without latent classes are used for modeling heterogeneous preference structures of consumers and for predicting choices among competing products. The results show a clear superiority of the combined approach over the purely Bayesian one. It seems that we still need latent classes for conjoint analysis-based predictions.KeywordsRoot Mean Square ErrorPosterior DistributionLatent ClassPredictive ValidityMarket SegmentThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.",0
https://doi.org/10.22329/amr.v13i1.2835,Analyzing Structural Relations in Multivariate Dyadic Binary Data,"In social network studies, most often only a single relation (or link) between the actors is investigated. When more than one link has been recorded, the two-way sociomatrix becomes a three-way array with the set of links being the third way. In this paper, we present a model which simultaneously accounts for the three ways in the data. Random effects are used to model the between-actor variability, both on senders and receivers side. In addition, structural relations between the linking variables are investigated. The model is applied to a study of popularity and strength in a class of students. It is shown that popularity can be seen as a linear function of strength on the receivers’ side, but not on the senders’ side.",0
,Windowed Sine Bursts: In Search of Optimal Test Signals for Detecting the Threshold of Audibility of Temporal Decays,"A slow decay in an audio signal is perceived as ringing and is commonly caused by room modes. This affects the perception of intelligibility, clarity, definition, and spatial rendering. A method has previously been devised to find the threshold of audibility of the decay in low-frequency narrow-band signals. One of the test signals in the largescale listening test will be a low-frequency sine burst, but spectral spreading at the start and end of the test signal acts as an additional non-modal cue. This effect is removed by windowing, for example a half Hann. The aim of this paper is to determine the window length required (threshold) to render the end of the test signal free from audible spectral spreading. The Parameter Estimation by Sequential Testing (PEST) method and calibrated headphones (to remove factors associated with the listening environment) are used in subjective listening tests. The window length threshold is found to be constant above 200 Hz but rises exponentially towards low frequencies, and is replay level dependent. Threshold may be related to the absolute threshold of hearing, masking curves and/or auditory filter bandwidth.",0
https://doi.org/10.1371/journal.pone.0096002,A Bayesian Geostatistical Moran Curve Model for Estimating Net Changes of Tsetse Populations in Zambia,"For the first time a Bayesian geostatistical version of the Moran Curve, a logarithmic form of the Ricker stock recruitment curve, is proposed that is able to give an estimate of net change in population demographic rates considering components such as fertility and density dependent and density independent mortalities. The method is applied to spatio-temporally referenced count data of tsetse flies obtained from fly-rounds. The model is a linear regression with three components: population rate of change estimated from the Moran curve, an explicit spatio-temporal covariance, and the observation error optimised within a Bayesian framework. The model was applied to the three main climate seasons of Zambia (rainy--January to April, cold-dry--May to August, and hot-dry--September to December) taking into account land surface temperature and (seasonally changing) cattle distribution. The model shows a maximum positive net change during the hot-dry season and a minimum between the rainy and cold-dry seasons. Density independent losses are correlated positively with day-time land surface temperature and negatively with night-time land surface temperature and cattle distribution. The inclusion of density dependent mortality increases considerably the goodness of fit of the model. Cross validation with an independent dataset taken from the same area resulted in a very accurate estimate of tsetse catches. In general, the overall framework provides an important tool for vector control and eradication by identifying vector population concentrations and local vector demographic rates. It can also be applied to the case of sustainable harvesting of natural populations.",0
https://doi.org/10.1093/biomet/71.1.1,On the existence of maximum likelihood estimates in logistic regression models,"SUMMARY The problems of existence, uniqueness and location of maximum likelihood estimates in log linear models have received special attention in the literature (Haberman, 1974, Chapter 2; Wedderburn, 1976; Silvapulle, 1981). For multinomial logistic regression models, we prove existence theorems by considering the possible patterns of data points, which fall into three mutually exclusive and exhaustive categories: complete separation, quasicomplete separation and overlap. Our results suggest general rules for identifying infinite parameter estimates in log linear models for frequency tables.",0
https://doi.org/10.1038/384356a0,Genetic and environmental contributions to the acquisition of a motor skill,"Practice, with feedback, is a fundamental variable that influences the aquisition of motor skills: with it, everyone improves, but some improve more than others. This simple fact has led to frequent debate over the relative importance of genetic and environmental influences on motor learning. In principle these factors could influence subjects' initial level of proficiency, their rate of improvement or their final level of attainment. The problem has been investigated using the rotary pursuit (RP) task, in which subjects learn to track a rotating target with a stylus; this is a factorially pure task which is relatively unaffected by cognitive or verbal factors. Earlier studies of twins reared together indicated that heredity was the primary factor responsible for individual differences in motor skill. Here we have studied learning in a sample of monozygotic (MZA) and dizygotic (DZA) twins who had been reared apart. Heritability of performance was high even in the initial phase, and increased with practice. The rate of learning was also significantly heritable. We propose that the effect of practice is to decrease the effect of environmental variation (previous learning) and increase the relative strength of genetic influences on motor performance.",0
https://doi.org/10.1080/10705511.2014.937849,A Bayesian Approach to Multilevel Structural Equation Modeling With Continuous and Dichotomous Outcomes,"Multilevel Structural equation models are most often estimated from a frequentist framework via maximum likelihood. However, as shown in this article, frequentist results are not always accurate. Alternatively, one can apply a Bayesian approach using Markov chain Monte Carlo estimation methods. This simulation study compared estimation quality using Bayesian and frequentist approaches in the context of a multilevel latent covariate model. Continuous and dichotomous variables were examined because it is not yet known how different types of outcomes—most notably categorical—affect parameter recovery in this modeling context. Within the Bayesian estimation framework, the impact of diffuse, weakly informative, and informative prior distributions were compared. Findings indicated that Bayesian estimation may be used to overcome convergence problems and improve parameter estimate bias. Results highlight the differences in estimation quality between dichotomous and continuous variable models and the importance o...",1
https://doi.org/10.3102/10769986029002201,Asymptotic Effect of Misspecification in the Random Part of the Multilevel Model,"The authors examine the asymptotic effect of omitting a random coefficient in the multilevel model and derive expressions for the change in (a) the variance components estimator and (b) the estimated variance of the fixed effects estimator. They apply the method of moments, which yields a closed form expression for the omission effect. In practice, the model parameters are estimated by maximum likelihood; however, since the moment estimator and the maximum likelihood estimator are both consistent, the presented expression for the change in the variance components estimator asymptotically holds for the maximum likelihood estimator as well. The results are illustrated with an analysis of mathematics performance data.",0
https://doi.org/10.1097/jom.0b013e3180329a8d,The PHLAME (Promoting Healthy Lifestyles: Alternative Models??? Effects) Firefighter Study: Outcomes of Two Models of Behavior Change,"PHLAME's (Promoting Healthy Lifestyles: Alternative Models' Effects) objective was to assess and compare two means to promote healthy lifestyles.Prospective trial among 599 firefighters randomized by station to 1) team-centered curriculum, 2) one-on-one motivational interviewing (MI), and 3) controls. Assessment included dietary behavior, physical activity, weight, and general well-being at baseline and 12 months. Program effects were determined using an analysis of covariance (ANCOVA) based approach, and models for relationships were evaluated with path analysis.Both interventions were acceptable and delivered with high fidelity. The team and MI programs increased fruit and vegetable consumption (P < 0.01 and 0.05, respectively) and general well-being (P < 0.01). Significantly less weight gain occurred in both (P < 0.05). A cross-sectional model was consistent with mediation differing between interventions.Both a team-centered and individual-oriented intervention promoted healthy behaviors. The scripted team curriculum is innovative, exportable, and may enlist influences not accessed with individual formats.",0
https://doi.org/10.3102/1076998607307359,Extended Generalized Linear Latent and Mixed Model,"The generalized linear latent and mixed modeling (GLLAMM framework) includes many models such as hierarchical and structural equation models. However, GLLAMM cannot currently accommodate some models because it does not allow some parameters to be random. GLLAMM is extended to overcome the limitation by adding a submodel that specifies a distribution of the additional random effects (Extended-GLLAMM). The extension is extremely simple to implement through the Bayesian framework with the WinBUGS software. Our approach is illustrated through the analysis of data from a youth tobacco cessation study.",0
https://doi.org/10.1016/j.visres.2006.04.022,Bayesian adaptive estimation of threshold versus contrast external noise functions: The quick TvC method,"External noise paradigms, measuring contrast threshold as a function of external noise contrast (the “ TvC ” function), provide a valuable tool for studying perceptual mechanisms. However, measuring TvC functions at the multiple performance criteria needed to constrain observer models has previously involved demanding data collection (often > 2000 trials). To ease this task, we developed a novel Bayesian adaptive procedure, the “quick TvC ” or “ qTvC ” method, to rapidly estimate multiple TvC functions, by adapting a strategy originally developed to estimate psychometric threshold and slope [Cobo-Lewis, A. B. (1996). An adaptive method for estimating multiple parameters of a psychometric function. Journal of Mathematical Psychology , 40 , 353–354; Kontsevich, L. L., & Tyler, C. W. (1999). Bayesian adaptive estimation of psychometric slope and threshold. Vision Research , 39 (16), 2729–2737]. Exploiting the regularities observed in empirical TvC functions, the qTvC method estimates three parameters: the optimal threshold c 0 , the critical noise level N c , and the common slope, η , of log-parallel psychometric functions across external noise conditions. Before each trial, the qTvC uses a one-step-ahead search to select the stimulus (jointly defined by signal and noise contrast) that minimizes the expected entropy of the three-dimensional posterior probability distribution, p ( N c , c 0 , η ). The method’s accuracy and precision, for estimating TvC functions at three performance criteria (65%, 79%, and 92% correct), were evaluated using Monte-Carlo simulations and a psychophysical task. Simulations showed that less than 300 trials were needed to estimate TvC functions at three widely separated criteria with good accuracy (bias < 5%) and precision (mean root mean square error <1.5 dB). Using an orientation identification task, we found excellent agreement (weighted r 2 > .95) between TvC estimates obtained with the qTvC and the method of constant stimuli, although the qTvC used only 12% of the data collection (240 vs 1920 trials). The qTvC may hold considerable practical value for applying the external noise method to study mechanisms of observer state changes and special populations. We suggest that the same adaptive strategy can be applied to directly estimate other classical functions, such as the contrast sensitivity function, elliptical equi-discrimination contours, and sensory memory decay functions.",0
https://doi.org/10.1080/00273171.2013.796280,Response Time Modeling Based on the Proportional Hazards Model,"Response time data are regularly analyzed in psychology. When several response times are assessed per participant, it is common practice to use latent trait models in order to account for the dependency of the response times from the same participant. One such model is the proportional hazards model with random effects. Despite its popularity in survival analysis, this model is rarely used in psychology because of the difficulty of model estimation when latent variables are present. In this article, a new estimation method is proposed. This method is based on the rank correlation matrix containing Kendall's Tau coefficients and unweighted least squares estimation ( Kendall, 1938 ). Compared with marginal maximum likelihood estimation, the new estimation approach is simple, not computationally intensive, and almost as efficient. Additionally, the approach allows the implementation of a test for model fit. Feasibility of the estimation method and validity of the fit test is demonstrated with a simulation study. An application of the model to a real data set is provided.",0
https://doi.org/10.1007/s10928-008-9108-2,Comparing models for perfluorooctanoic acid pharmacokinetics using Bayesian analysis,"Selecting the appropriate pharmacokinetic (PK) model given the available data is investigated for perfluorooctanoic acid (PFOA), which has been widely analyzed with an empirical, one-compartment model. This research examined the results of experiments [Kemper R. A., DuPont Haskell Laboratories, USEPA Administrative Record AR-226.1499 (2003)] that administered single oral or iv doses of PFOA to adult male and female rats. PFOA concentration was observed over time; in plasma for some animals and in fecal and urinary excretion for others. There were four rats per dose group, for a total of 36 males and 36 females. Assuming that the PK parameters for each individual within a gender were drawn from the same, biologically varying population, plasma and excretion data were jointly analyzed using a hierarchical framework to separate uncertainty due to measurement error from actual biological variability. Bayesian analysis using Markov Chain Monte Carlo (MCMC) provides tools to perform such an analysis as well as quantitative diagnostics to evaluate and discriminate between models. Starting from a one-compartment PK model with separate clearances to urine and feces, the model was incrementally expanded using Bayesian measures to assess if the expansion was supported by the data. PFOA excretion is sexually dimorphic in rats; male rats have bi-phasic elimination that is roughly 40 times slower than that of the females, which appear to have a single elimination phase. The male and female data were analyzed separately, keeping only the parameters describing the measurement process in common. For male rats, including excretion data initially decreased certainty in the one-compartment parameter estimates compared to an analysis using plasma data only. Allowing a third, unspecified clearance improved agreement and increased certainty when all the data was used, however a significant amount of eliminated PFOA was estimated to be missing from the excretion data. Adding an additional PK compartment reduced the unaccounted-for elimination to amounts comparable to the cage wash. For both sexes, an MCMC estimate of the appropriateness of a model for a given data type, the Deviance Information Criterion, indicated that this two-compartment model was better suited to describing PFOA PK. The median estimate was 142.1 Ã‚Â± 37.6 ml/kg for the volume of the primary compartment and 1.24 Ã‚Â± 1.1 ml/kg/h for the clearances of male rats and 166.4 Ã‚Â± 46.8 ml/kg and 30.3 Ã‚Â± 13.2 ml/kg/h, respectively for female rats. The estimates for the second compartment differed greatly with gender-volume 311.8 Ã‚Â± 453.9 ml/kg with clearance 3.2 Ã‚Â± 6.2 for males and 1400 Ã‚Â± 2507.5 ml/kg and 4.3 Ã‚Â± 2.2 ml/kg/h for females. The median estimated clearance was 12 Ã‚Â± 6% to feces and 85 Ã‚Â± 7% to urine for male rats and 8 Ã‚Â± 6% and 77 Ã‚Â± 9% for female rats. We conclude that the available data may support more models for PFOA PK beyond two-compartments and that the methods employed here will be generally useful for more complicated, including PBPK, models. Ã‚Â© 2008 Springer Science+Business Media, LLC.",0
https://doi.org/10.1214/aos/1176325750,Markov Chains for Exploring Posterior Distributions,"Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.",0
https://doi.org/10.1016/s0169-7161(06)26010-3,10 Structural Equation Modeling,"Structural equation modeling (SEM), by segregating measurement errors from the true scores of attributes, provides a methodology to model the latent variables, such as attitudes, IQ, personality traits, political liberalism or conservatism, and socio-economic status, directly. The methodology of SEM has enjoyed tremendous developments since 1970, and is now widely applied. The idea of multiple indicators for a latent variable is from factor analysis. SEM is often regarded as an extension of factor analysis in the psychometric literature. This methodology also covers several widely used statistical models in various disciplines. This chapter presents several specific models before introducing the general mean and covariance structures. It is noted that unlike the exploratory factor analysis (EFA) model, the zero loadings in a general confirmatory factor analysis (CFA) model are specified a priori based on subject-matter knowledge, and their number and placement guarantee that the model is identified without any rotational constraints.",0
https://doi.org/10.1002/tea.20313,How and when does complex reasoning occur? Empirically driven development of a learning progression focused on complex reasoning about biodiversity,"In order to compete in a global economy, students are going to need resources and curricula focusing on critical thinking and reasoning in science. Despite awareness for the need for complex reasoning, American students perform poorly relative to peers on international standardized tests measuring complex thinking in science. Research focusing on learning progressions is one effort to provide more coherent science curricular sequences and assessments that can be focused on complex thinking about focal science topics. This article describes an empirically driven, five-step process to develop a 3-year learning progression focusing on complex thinking about biodiversity. Our efforts resulted in empirical results and work products including: (1) a revised definition of learning progressions, (2) empirically driven, 3-year progressions for complex thinking about biodiversity, (3) an application of statistical approaches for the analysis of learning progression products, (4) Hierarchical Linear Modeling results demonstrating significant student achievement on complex thinking about biodiversity, and (5) Growth Model results demonstrating strengths and weaknesses of the first version of our curricular units. The empirical studies present information to inform both curriculum and assessment development. For curriculum development, the role of learning progressions as templates for the development of organized sequences of curricular units focused on complex science is discussed. For assessment development, learning progression-guided assessments provide a greater range and amount of information that can more reliably discriminate between students of differing abilities than a contrasting standardized assessment measure that was also focused on biodiversity content. © 2009 Wiley Periodicals, Inc. J Res Sci Teach 46: 610–631, 2009",0
https://doi.org/10.1111/rssc.12107,Objective Bayesian estimation of the probability of default,"Summary Reliable estimation of the probability of default (PD) of a customer is one of the most important tasks in credit risk modelling for banks applying the internal ratings-based approach under the Basel II–III framework. Motivated by the desire to analyse reliably a low default portfolio of non-profit housing companies, we consider PD estimation within a Bayesian framework and develop objective priors for the parameter θ representing the PD in the Gaussian and the Student t single-factor models. A marginal reference prior and limiting versions of it are presented and their posterior propriety is studied. The priors are shown to be direct generalizations of the Jeffreys prior in the binomial model. We use Markov chain Monte Carlo strategies to sample efficiently from the posterior distributions and compare the developed priors on the grounds of the frequentist properties of the resulting Bayesian inferences with subjective priors previously proposed in the literature. Finally, the analysis of the non-profit housing companies portfolio highlights the ultility of the methodological developments.",0
https://doi.org/10.1002/env.850,Evaluation of Bayesian models for focused clustering in health data,"This paper examines the ability of Bayesian hierarchical models to recover evidence of disease risk excess around a fixed location. This location can be a putative source of health hazard, such as an incinerator, mobile phone mast or dump site. While Bayesian models are convenient to use for modeling, it is useful to consider how well these models perform in the true risk scenarios. In what follows, we evaluate the ability of these models to recover the true risk under simulation. It is surprising that the resulting posterior parameters estimates are heavily biased. Using the credible intervals for distance decline parameter to assess ‘coverage or power’ of detecting distance effect, the ‘power’ decreases with increasing correlation in the background population effect. The inclusion of correlated heterogeneity in models does affect the ability of the models to detect the stronger distance decline scenarios. The uncorrelated heterogeneity seems little affect this ability however. Copyright © 2007 John Wiley & Sons, Ltd.",0
https://doi.org/10.1136/bmjopen-2015-009013,Antiplatelet regimens in the long-term secondary prevention of transient ischaemic attack and ischaemic stroke: an updated network meta-analysis,"To examine the comparative efficacy and safety of different antiplatelet regimens in patients with prior non-cardioembolic ischaemic stroke or transient ischaemic attack.Systematic review and network meta-analysis.As on 31 March 2015, all randomised controlled trials that investigated the effects of antiplatelet agents in the long-term (≥ 3 months) secondary prevention of non-cardioembolic transient ischaemic attack or ischaemic stroke were searched and identified.The primary outcome measure of efficacy was serious vascular events (non-fatal stroke, non-fatal myocardial infarction and vascular death). The outcome measure of safety was any bleeding.A total of 36 randomised controlled trials (82,144 patients) were included. Network meta-analysis showed that cilostazol was significantly more effective than clopidogrel (OR 0.77, 95% credible interval 0.60-0.98) and low-dose (75-162 mg daily) aspirin (0.69, 0.55-0.86) in the prevention of serious vascular events. Aspirin (50 mg daily) plus dipyridamole (400 mg daily) and clopidogrel reduced the risk of serious vascular events compared with low-dose aspirin; however, the difference was not statistically significant. Furthermore, low-dose aspirin was as effective as higher daily doses. Cilostazol was associated with a significantly lower bleeding risk than most of the other regimens. Moreover, aspirin plus clopidogrel was associated with significantly more haemorrhagic events than other regimens. Direct comparisons showed similar results as the network meta-analysis.Cilostazol was significantly more effective than aspirin and clopidogrel alone in the long-term prevention of serious vascular events in patients with prior non-cardioembolic ischaemic stroke or transient ischaemic attack. Cilostazol was associated with a significantly lower bleeding risk than low-dose aspirin (75-162 mg daily) and aspirin (50 mg daily) plus dipyridamole (400 mg daily). Low-dose aspirin was as effective as higher daily doses. However, further large, randomised, controlled, head-to-head trials are needed, especially in non-Asian ethnic groups.",0
https://doi.org/10.1016/j.spl.2015.02.022,Further results on estimation of covariance matrix,"Abstract This paper considers the problem of estimating a covariance matrix under Stein’s loss. Sufficient conditions for the modified Efron–Morris estimator to be minimax under weighted quadratic loss are shown, which provide a general method for improving the estimator of the covariance matrix.",0
https://doi.org/10.1007/bf02295278,A unifying expression for the maximal reliability of a linear composite,"A formally simple expression for the maximal reliability of a linear composite is provided, its theoretical implications and its relation to existing results for reliability are discussed.",0
https://doi.org/10.1177/0164027599212006,A Multilevel Model of Early Retirement Decisions among Autoworkers in Plants with Different Futures,"During the period of their 1986-1989 General Motors (GM)-United Auto Workers (UAW) contract, about 17% of all GM autoworkers who were eligible to elect early retirement did so. Those who did were distinctive in theoretically expected ways, with expectations defined by individual characteristics such as age, physical health, and pension wealth. But some of the workers were employed in plants that GM had decided to abandon. Did that difference in organizational context make a difference in individual workers’ decisions about early retirement? Would workers who chose to take early retirement and who were employed in plants scheduled to close have made the same decision had their plants not been selected for closure? If the rate of early retirement was higher in plants scheduled to close, and it was, how did that difference relate to the process by which individual workers reached their decisions? These are some of the questions asked and answered through multilevel analyses of data from a probability sample of GM’s autoworkers. These analyses generate findings not detected in single-level analyses of the same data.",0
https://doi.org/10.1111/jora.12066,Processes Linking Father Absence to Educational Attainment Among African American Females,"Previous researchers have assessed how father absence influences both socioeconomic and reproductive development, but have not analyzed these relationships jointly or used theoretical support for explaining why father absence affects socioeconomic and reproductive development, especially for African American girls. Guided by the family economic stress model and psychosocial acceleration theory, this study investigates how and why these relationships exist for 532 African American females. Structural equation modeling revealed that longer duration of father absence predicted lowered educational attainment in early adulthood via lower income, increased economic pressure, accelerated reproductive development, and lowered educational expectations. We include recommendations for the use of these perspectives to better understand the interplay between family socioeconomic processes and reproductive maturation in African American females.",0
https://doi.org/10.1017/cbo9781139022422,Design and Analysis of Long-term Ecological Monitoring Studies,"To provide useful and meaningful information, long-term ecological programs need to implement solid and efficient statistical approaches for collecting and analyzing data. This volume provides rigorous guidance on quantitative issues in monitoring, with contributions from world experts in the field. These experts have extensive experience in teaching fundamental and advanced ideas and methods to natural resource managers, scientists and students. The chapters present a range of tools and approaches, including detailed coverage of variance component estimation and quantitative selection among alternative designs; spatially balanced sampling; sampling strategies integrating design- and model-based approaches; and advanced analytical approaches such as hierarchical and structural equation modelling. Making these tools more accessible to ecologists and other monitoring practitioners across numerous disciplines, this is a valuable resource for any professional whose work deals with ecological monitoring. Supplementary example software code is available online at www.cambridge.org/9780521191548.",0
https://doi.org/10.1300/j426v02n04_05,The Persistent Sciatic Inflammatory Neuropathy (SIN) Rat Model of Neuropathic Pain Does Not Involve Small-Fiber Axon Damage,"ABSTRACTMost neuropathic pain conditions are caused by damage to peripheral nociceptive neurons known as “small-fibers.” Examination of punch skin biopsies immunolabeled with pan-axonal markers has identified degeneration of distal nociceptive axons as a pathological hallmark of most neuralgias including numerous polyneuropathies, postherpetic neuralgia, and complex regional pain syndrome. Similar axonopathy is present in several rodent models of neuralgia (chronic constriction injury and spared nerve injury). Here we have measured the density of distal cutaneous small-fiber axons in rats with persistent mechanical allodynia caused by the sciatic inflammatory neuropathy (SIN), a model of painful neuritis. We have evaluated whether perineural immune activation sufficient to cause evoked-pain behaviors is also associated with loss of distal epidermal innervation.SIN was created in adult male Sprague-Dawley rats by perineural zymosan administration using established methods. Unoperated rats provided controls...",0
https://doi.org/10.1080/00273171.2015.1090899,A Bayesian Approach to More Stable Estimates of Group-Level Effects in Contextual Studies,"Multilevel analyses are often used to estimate the effects of group-level constructs. However, when using aggregated individual data (e.g., student ratings) to assess a group-level construct (e.g., classroom climate), the observed group mean might not provide a reliable measure of the unobserved latent group mean. In the present article, we propose a Bayesian approach that can be used to estimate a multilevel latent covariate model, which corrects for the unreliable assessment of the latent group mean when estimating the group-level effect. A simulation study was conducted to evaluate the choice of different priors for the group-level variance of the predictor variable and to compare the Bayesian approach with the maximum likelihood approach implemented in the software Mplus. Results showed that, under problematic conditions (i.e., small number of groups, predictor variable with a small ICC), the Bayesian approach produced more accurate estimates of the group-level effect than the maximum likelihood approach did.",0
https://doi.org/10.1167/13.7.3,"The psi-marginal adaptive method: How to give nuisance parameters the attention they deserve (no more, no less)","Adaptive testing methods serve to maximize the information gained regarding the values of the parameters of a psychometric function (PF). Such methods typically target only one or two (""threshold"" and ""slope"") of the PF's four parameters while assuming fixed values for the ""nuisance"" parameters (""guess rate"" and ""lapse rate""). Here I propose the ""psi-marginal"" adaptive method, which can target nuisance parameters but only when this is the most optimal manner in which to reduce uncertainty in the value of the parameters of primary interest. The method is based on Kontsevich and Tyler's (1999) psi-method. However, in the proposed method a posterior distribution defined across all parameters of unknown value is maintained. Each of the parameters is specified either as a parameter of primary interest whose estimation should be optimized or as a nuisance parameter whose estimation should be subservient to the estimation of the parameters of primary interest. Critically, selection of stimulus intensities is based on the expected information gain in the marginal posterior distribution defined across the parameters of interest only. The appeal of this method is that it will target nuisance parameters adaptively and only when doing so maximizes the expected information gain regarding the values of the parameters of interest. Simulations indicate that treating the lapse rate as a nuisance parameter in the psi-marginal method results in smaller bias and higher precision in threshold and slope estimates compared to the original psi method. The method is highly flexible and various other uses are discussed.",0
https://doi.org/10.1111/jtsa.12182,A Bayesian Non-Parametric Dynamic AR Model for Multiple Time Series Analysis,"In this article, we propose a Bayesian non-parametric model for the analysis of multiple time series. We consider an autoregressive structure of order p for each of the series and borrow strength across the series by considering a common error population that is also evolving in time. The error populations (distributions) are assumed non-parametric whose law is based on a series of dependent Polya trees with zero median. This dependence is of order q and is achieved via a dependent beta process that links the branching probabilities of the trees. We study the prior properties and show how to obtain posterior inference. The model is tested under a simulation study and is illustrated with the analysis of the economic activity index of the 32 states of Mexico.",0
https://doi.org/10.1111/j.2517-6161.1970.tb00842.x,Asymptotic Properties of Conditional Maximum-Likelihood Estimators,The problem of obtaining consistent estimates for structural parameters in the presence of infinitely many incidental parameters was discussed first by Neyman and Scott (1948). In this paper a maximum-likelihood method based on the conditional distribution given minimal sufficient statistics for the incidental parameters is suggested. It is proved that conditional maximumlikelihood estimates in the regular case are consistent and asymptotically normally distributed with a simple asymptotic variance. The efficiency problem of this new estimator is discussed in particular with respect to some situations with ancillary information.,0
https://doi.org/10.1016/j.drugalcdep.2014.12.029,Effects of marijuana use on impulsivity and hostility in daily life,"Marijuana use is increasingly prevalent among young adults. While research has found adverse effects associated with marijuana use within experimentally controlled laboratory settings, it is unclear how recreational marijuana use affects day-to-day experiences in users. The present study sought to examine the effects of marijuana use on within-person changes in impulsivity and interpersonal hostility in daily life using smartphone administered assessments.Forty-three participants with no substance dependence reported on their alcohol consumption, tobacco use, recreational marijuana use, impulsivity, and interpersonal hostility over the course of 14 days. Responses were analyzed using multilevel modeling.Marijuana use was associated with increased impulsivity on the same day and the following day relative to days when marijuana was not used, independent of alcohol use. Marijuana was also associated with increased hostile behaviors and perceptions of hostility in others on the same day when compared to days when marijuana was not used. These effects were independent of frequency of marijuana use or alcohol use. There were no significant effects of alcohol consumption on impulsivity or interpersonal hostility.Marijuana use is associated with changes in impulse control and hostility in daily life. This may be one route by which deleterious effects of marijuana are observed for mental health and psychosocial functioning. Given the increasing prevalence of recreational marijuana use and the potential legalization in some states, further research on the potential consequences of marijuana use in young adults' day-to-day life is warranted.",0
https://doi.org/10.1207/s15328007sem0802_1,The Accuracy of Multilevel Structural Equation Modeling With Pseudobalanced Groups and Small Samples,"Hierarchical structured data cause problems in analysis, because the usual assumptions of independently and identically distributed variables are violated. Muthen (1989) described an estimation method for multilevel factor and path analysis with hierarchical data. This article assesses the robustness of the method with unequal groups, small sample sizes at both the individual and the group level, in the presence of a low or a high intraclass correlation (ICC). The within-groups part of the model poses no problems. The most important problem in the between-groups part of the model is the occurrence of inadmissible estimates, especially when group level sample size is small (50) while the intracluster correlation is low. This is partly compensated by using large group sizes. When an admissible solution is reached, the factor loadings are generally accurate. However, the residual variances are underestimated, and the standard errors are generally too small. Having more or larger groups or a higher ICC does n...",0
https://doi.org/10.1037/a0039540,Bayesian hierarchical grouping: Perceptual grouping as mixture estimation.,"We propose a novel framework for perceptual grouping based on the idea of mixture models, called Bayesian hierarchical grouping (BHG). In BHG, we assume that the configuration of image elements is generated by a mixture of distinct objects, each of which generates image elements according to some generative assumptions. Grouping, in this framework, means estimating the number and the parameters of the mixture components that generated the image, including estimating which image elements are ""owned"" by which objects. We present a tractable implementation of the framework, based on the hierarchical clustering approach of Heller and Ghahramani (2005). We illustrate it with examples drawn from a number of classical perceptual grouping problems, including dot clustering, contour integration, and part decomposition. Our approach yields an intuitive hierarchical representation of image elements, giving an explicit decomposition of the image into mixture components, along with estimates of the probability of various candidate decompositions. We show that BHG accounts well for a diverse range of empirical data drawn from the literature. Because BHG provides a principled quantification of the plausibility of grouping interpretations over a wide range of grouping problems, we argue that it provides an appealing unifying account of the elusive Gestalt notion of Prägnanz.",0
https://doi.org/10.1007/bf02294195,Test theory without an answer key,"A general model is presented for homogeneous, dichotomous items when the answer key is not known a priori. The model is structurally related to the two-class latent structure model with the roles of respondents and items interchanged. For very small sets of respondents, iterative maximum likelihood estimates of the parameters can be obtained by existing methods. For other situations, new estimation methods are developed and assessed with Monte Carlo data. The answer key can be accurately reconstructed with relatively small sets of respondents. The model is useful when a researcher wants to study objectively the knowledge possessed by members of a culturally coherent group that the researcher is not a member of. Â© 1987 The Psychometric Society.",0
https://doi.org/10.1080/00949650701519635,Parameter expansion for sampling a correlation matrix: an efficient GPX-RPMH algorithm,"Sampling the correlation matrix (R) plays an important role in statistical inference for correlated models. There are two main constraints on a correlation matrix: positive definiteness and fixed diagonal elements. These constraints make sampling R difficult. In this paper, an efficient generalized parameter expanded re-parametrization and Metropolis-Hastings (GPX-RPMH) algorithm for sampling a correlation matrix is proposed. Drawing all components of R simultaneously from its full conditional distribution is realized by first drawing a covariance matrix from the derived parameter expanded candidate density (PXCD), and then translating it back to a correlation matrix and accepting it according to a Metropolis-Hastings (M-H) acceptance rate. The mixing rate in the M-H step can be adjusted through a class of tuning parameters embedded in the generalized candidate prior (GCP), which is chosen for R to derive the PXCD. This algorithm is illustrated using multivariate regression (MVR) models and a simulation s...",0
https://doi.org/10.1007/bf00122574,Advances in prospect theory: Cumulative representation of uncertainty,"We develop a new version of prospect theory that employs cumulative rather than separable decision weights and extends the theory in several respects. This version, called cumulative prospect theory, applies to uncertain as well as to risky prospects with any number of outcomes, and it allows different weighting functions for gains and for losses. Two principles, diminishing sensitivity and loss aversion, are invoked to explain the characteristic curvature of the value function and the weighting functions. A review of the experimental evidence and the results of a new experiment confirm a distinctive fourfold pattern of risk attitudes: risk aversion for gains and risk seeking for losses of high probability; risk seeking for gains and risk aversion for losses of low probability. This article has benefited from discussions with Colin Camerer, Chew Soo-Hong, David Freedman, and David H. Krantz. We are especially grateful to Peter P. Wakker for his invaluable input and contribution to the axiomatic analysis. We are indebted to Richard Gonzalez and Amy Hayes for running the experiment and analyzing the data. This work was supported by Grants 89-0064 and 88-0206 from the Air Force Office of Scientific Research, by Grant SES-9109535 from the National Science Foundation, and by the Sloan Foundation. Â© 1992 Kluwer Academic Publishers.",0
https://doi.org/10.3102/0162373707299706,Intraclass Correlation Values for Planning Group-Randomized Trials in Education,"Experiments that assign intact groups to treatment conditions are increasingly common in social research. In educational research, the groups assigned are often schools. The design of group-randomized experiments requires knowledge of the intraclass correlation structure to compute statistical power and sample sizes required to achieve adequate power. This article provides a compilation of intraclass correlation values of academic achievement and related covariate effects that could be used for planning group-randomized experiments in education. It also provides variance component information that is useful in planning experiments involving covariates. The use of these values to compute the statistical power of group-randomized experiments is illustrated.",0
https://doi.org/10.1080/00273170802034844,The Performance of Methods to Test Upper-Level Mediation in the Presence of Nonnormal Data,"A Monte Carlo study compared the statistical performance of standard and robust multilevel mediation analysis methods to test indirect effects for a cluster randomized experimental design under various departures from normality. The performance of these methods was examined for an upper-level mediation process, where the indirect effect is a fixed effect and a group-implemented treatment is hypothesized to impact a person-level outcome via a person-level mediator. Two methods-the bias-corrected parametric percentile bootstrap and the empirical-M test-had the best overall performance. Methods designed for nonnormal score distributions exhibited elevated Type I error rates and poorer confidence interval coverage under some conditions. Although preliminary, the findings suggest that new mediation analysis methods may provide for robust tests of indirect effects.",0
https://doi.org/10.1016/j.neuroscience.2014.05.036,The place of human psychophysics in modern neuroscience,"Human psychophysics is the quantitative measurement of our own perceptions. In essence, it is simply a more sophisticated version of what humans have done since time immemorial: noticed and reflected upon what we can see, hear, and feel. In the 21st century, when hugely powerful techniques are available that enable us to probe the innermost structure and function of nervous systems, is human psychophysics still relevant? I argue that it is, and that in combination with other techniques, it will continue to be a key part of neuroscience for the foreseeable future. I discuss these points in detail using the example of binocular stereopsis, where human psychophysics in combination with physiology and computational vision, has made a substantial contribution.",0
https://doi.org/10.1007/bf02288892,A basis for analyzing test-retest reliability,"Three sources of variation in experimental results for a test are distinguished: trials, persons, and items. Unreliability is defined only in terms of variation over trials. This definition leads to a more complete analysis than does the conventional one; Spearman's contention is verified that the conventional approach-which was formulated by Yule-introduces unnecessary hypotheses. It is emphasized that at least two trials are necessary to estimate the reliability coefficient. This paper is devoted largely to developing lower bounds to the reliability coefficient that can be computed from but a single trial; these avoid the experimental difficulties of making two independent trials. Six different lower bounds are established, appropriate for different situations. Some of the bounds are easier to compute than are conventional formulas, and all the bounds assume less than do conventional formulas. The terminology used is that of psychological and sociological testing, but the discussion actually provides a general analysis of the reliability of the sum of n variables. Â© 1945 Psychometric Society.",0
https://doi.org/10.1207/s15328007sem0702_1,"Point Estimation, Hypothesis Testing, and Interval Estimation Using the RMSEA: Some Comments and a Reply to Hayduk and Glaser","Hayduk and Glaser (2000) asserted that the most commonly used point estimate of the Root Mean Square Error of Approximation index of fit (Steiger & Lind, 1980) has two significant problems: (a) The frequently cited target value of. 05 is not a stable target, but a sample size adjustment; and (b) the truncated point estimate Rt = max(R, 0) effectively throws away a substantial part of the sampling distribution of the test statistic with proper models, rendering it useless a substantial portion of the time. In this article, I demonstrate that both issues discussed by Hayduk and Glaser are actually not problems at all. The first derives from a false premise by Hayduk and Glaser that Steiger (1995) specifically warned about in an earlier publication. The second so-called problem results from the point estimate satisfying a fundamental property of a good estimator and can be shown to have virtually no negative implications for statistical practice.",0
https://doi.org/10.2165/00019053-200624010-00001,Bayesian Methods for Evidence Synthesis in Cost-Effectiveness Analysis,"Recently, health systems internationally have begun to use cost-effectiveness research as formal inputs into decisions about which interventions and programmes should be funded from collective resources. This process has raised some important methodological questions for this area of research. This paper considers one set of issues related to the synthesis of effectiveness evidence for use in decision-analytic cost-effectiveness (CE) models, namely the need for the synthesis of all sources of available evidence, although these may not 'fit neatly' into a CE model. Commonly encountered problems include the absence of head-to-head trial evidence comparing all options under comparison, the presence of multiple endpoints from trials and different follow-up periods. Full evidence synthesis for CE analysis also needs to consider treatment effects between patient subpopulations and the use of nonrandomised evidence. Bayesian statistical methods represent a valuable set of analytical tools to utilise indirect evidence and can make a powerful contribution to the decision-analytic approach to CE analysis. This paper provides a worked example and a general overview of these methods with particular emphasis on their use in economic evaluation. Ã‚Â© 2006 Adis Data Information BV. All rights reserved.",0
https://doi.org/10.1093/biostatistics/kxp053,Bayesian inference for generalized linear mixed models,"Generalized linear mixed models (GLMMs) continue to grow in popularity due to their ability to directly acknowledge multiple levels of dependency and model different data types. For small sample sizes especially, likelihood-based inference can be unreliable with variance components being particularly difficult to estimate. A Bayesian approach is appealing but has been hampered by the lack of a fast implementation, and the difficulty in specifying prior distributions with variance components again being particularly problematic. Here, we briefly review previous approaches to computation in Bayesian implementations of GLMMs and illustrate in detail, the use of integrated nested Laplace approximations in this context. We consider a number of examples, carefully specifying prior distributions on meaningful quantities in each case. The examples cover a wide range of data types including those requiring smoothing over time and a relatively complicated spline model for which we examine our prior specification in terms of the implied degrees of freedom. We conclude that Bayesian inference is now practically feasible for GLMMs and provides an attractive alternative to likelihood-based approaches such as penalized quasi-likelihood. As with likelihood-based approaches, great care is required in the analysis of clustered binary data since approximation strategies may be less accurate for such data.",0
https://doi.org/10.1186/s13075-015-0864-2,Determinants of patient preferences for total knee replacement: African-Americans and whites,"Patient preferences contribute to marked racial disparities in the utilization of total knee replacement (TKR). The objectives of this study were to identify the determinants of knee osteoarthritis (OA) patients' preferences regarding TKR by race and to identify the variables that may mediate racial differences in willingness to undergo TKR.Five hundred fourteen White (WH) and 285 African-American (AA) patients with chronic knee pain and radiographic evidence of OA participated in the study. Participants were recruited from the community, an academic medical center, and a Veterans Affairs hospital. Structured interviews were conducted to collect socio-demographics, disease severity, socio-cultural determinants, and treatment preferences. Logistic regression was performed, stratified by race, to identify determinants of preferences. Clinical and socio-cultural factors were entered simultaneously into the models. Stepwise selection identified factors for inclusion in the final models (p < 0.20).Compared to WHs, AAs were less willing to undergo TKR (80 % vs. 62 %, respectively). Better expectations regarding TKR surgery outcomes determined willingness to undergo surgery in both AAs (odds ratio (OR) 2.08, 95 % confidence interval (CI) 0.91-4.79 for 4(th) vs. 1(st) quartile) and WHs (OR 5.11, 95 % CI 2.31-11.30 for 4(th) vs. 1(st) quartile). Among AAs, better understanding of the procedure (OR 1.80, 95 % CI 0.97-3.35), perceiving a short hospital course (OR 0.81, 95 % CI 0.58-1.13), and believing in less post-surgical pain (OR 0.73, 95 % CI 0.39-1.35) and walking difficulties (OR 0.66, 95 % CI 0.37-1.16) also determined willingness. Among WHs, having surgical discussion with a physician (OR 1.96, 95 % CI 1.05-3.68), not ever receiving surgical referral (OR 0.56, 95 % CI 0.32-0.99), and higher trust in the healthcare system (OR 1.58, 95 % CI 0.75-3.31 for 4(th) vs. 1(st) quartile) additionally determined willingness. Among the variables considered, only knowledge-related matters pertaining to TKR attenuated the racial difference in knee OA patients' treatment preference.Expectations of surgical outcomes influence preference for TKR in all patients, but clinical and socio-cultural factors exist that shape marked racial differences in preferences for TKR. Interventions to reduce or eliminate racial disparities in the utilization of TKR should consider and target these factors.",0
,Estimation and Optimization of Functions,,0
https://doi.org/10.1006/jmps.1999.1268,Selective Influence and Response Time Cumulative Distribution Functions in Serial-Parallel Task Networks,"We analyze sets of mental processes, some of which are concurrent and some of which are sequential, under the assumption that the processes are partially ordered, that is, arranged in a directed acyclic network. Information about the process arrangement can be discovered by examining the effects on response time of selectively influencing process durations. Previous work has mainly focused on analyses of mean response times. Here we consider analyses based on cumulative distribution functions, for one of the major classes of directed acyclic networks, serial-parallel networks. When two processes are selectively influenced, patterns in the cumulative distribution functions can be used to test whether the processes are sequential or concurrent and whether the task network has AND gates or OR gates. Cumulative distribution functions are potentially more informative than means, and some previous results for means are shown to follow from our results for cumulative distribution functions. Copyright 2000 Academic Press.",0
https://doi.org/10.1006/jmps.1995.1033,"Spatio-temporal Properties of Elementary Perception: An Investigation of Parallel, Serial, and Coactive Theories","A mathematical theory and related experimental methodology are developed that permit strong, converging tests of parallel versus serial versus channel summation processing and of exhaustive versus self-terminating processing. An experimental design is used for two studies, in which the presence or absence of a target in each of two positions, is factorially combined with two levels of brightness (a version of the double factorial paradigm). When both targets are present (redundant target condition) the two levels of brightness permit factorial technology to determine mental architecture and stopping rules. Comparison of the single versus double target conditions allows capacity analyses that strongly reinforce the double target factorial phase of the investigation. The results provide decisive support for parallel channels with either a self-terminating stopping rule or a coactive summation of information. General serial models and any variety of exhaustive processing were conclusively falsified.",0
https://doi.org/10.1016/j.actpsy.2014.03.001,Properties of the size–weight illusion as shown by lines of subjective equality,"We studied the size-weight illusion through comparative judgments. The experiment had two direct aims: to verify whether the relative contribution of size to apparent heaviness can differ for different stimulus sets, and to verify whether that contribution can differ for different methods of comparing two objects (consecutive vs. simultaneous weighing). Thirty university students participated. Results show that the relative contribution of size depends on stimulus set, but is independent of the method used for comparing objects. The first finding implies that a linear model cannot describe the integration of size and weight information in the illusion; the second finding is evidence for the low-level character of the integration process.",0
https://doi.org/10.1007/bf00152012,New developments in LISREL: analysis of ordinal variables using polychoric correlations and weighted least squares,New developments in LISREL: : Analysis of ordinal variables using polychoric correlations and weighted least squares,0
https://doi.org/10.1037/met0000034,Fitting a linear–linear piecewise growth mixture model with unknown knots: A comparison of two common approaches to inference.,"A linear-linear piecewise growth mixture model (PGMM) is appropriate for analyzing segmented (disjointed) change in individual behavior over time, where the data come from a mixture of 2 or more latent classes, and the underlying growth trajectories in the different segments of the developmental process within each latent class are linear. A PGMM allows the knot (change point), the time of transition from 1 phase (segment) to another, to be estimated (when it is not known a priori) along with the other model parameters. To assist researchers in deciding which estimation method is most advantageous for analyzing this kind of mixture data, the current research compares 2 popular approaches to inference for PGMMs: maximum likelihood (ML) via an expectation-maximization (EM) algorithm, and Markov chain Monte Carlo (MCMC) for Bayesian inference. Monte Carlo simulations were carried out to investigate and compare the ability of the 2 approaches to recover the true parameters in linear-linear PGMMs with unknown knots. The results show that MCMC for Bayesian inference outperformed ML via EM in nearly every simulation scenario. Real data examples are also presented, and the corresponding computer codes for model fitting are provided in the Appendix to aid practitioners who wish to apply this class of models.",0
https://doi.org/10.1002/sim.1980,A three-level model for binary time-series data: the effects of air pollution on school absences in the Southern California Children's Health Study,"A three-level model is proposed to simultaneously examine the effects of daily exposure to air pollution and individual risk factors on health outcomes without aggregating over subjects or time. We used a logistic transition model with random effects to take into account heterogeneity and overdispersion of the observations. A distributed lag structure for pollution has been included, assuming that the event on day t for a subject depends on the levels of air pollution for several preceding days. We illustrate this proposed model via detailed analysis of the effect of air pollution on school absenteeism based on data from the Southern California Children's Health Study.",0
https://doi.org/10.1080/10705511.2014.936082,Diagnosing Global Case Influence on MAR Versus MNAR Model Comparisons,"When missingness is suspected to be not at random (MNAR) in longitudinal studies, researchers sometimes compare the fit of a target model that assumes missingness at random (here termed a MAR model) and a model that accommodates a hypothesized MNAR missingness mechanism (here termed a MNAR model). It is well known that such comparisons are only interpretable conditional on the validity of the chosen MNAR model’s assumptions about the missingness mechanism. For that reason, researchers often perform a sensitivity analysis comparing the MAR model to not one, but several, plausible alternative MNAR models. In the social sciences, it is not widely known that such model comparisons can be particularly sensitive to case influence, such that conclusions drawn could depend on a single case. This article describes two convenient diagnostics suited for detecting case influence on MAR–MNAR model comparisons. Both diagnostics require much less computational burden than global influence diagnostics that have been used...",0
https://doi.org/10.1177/1745691611406925,Bayesian Assessment of Null Values Via Parameter Estimation and Model Comparison,"Psychologists have been trained to do data analysis by asking whether null values can be rejected. Is the difference between groups nonzero? Is choice accuracy not at chance level? These questions have been traditionally addressed by null hypothesis significance testing (NHST). NHST has deep problems that are solved by Bayesian data analysis. As psychologists transition to Bayesian data analysis, it is natural to ask how Bayesian analysis assesses null values. The article explains and evaluates two different Bayesian approaches. One method involves Bayesian model comparison (and uses Bayes factors). The second method involves Bayesian parameter estimation and assesses whether the null value falls among the most credible values. Which method to use depends on the specific question that the analyst wants to answer, but typically the estimation approach (not using Bayes factors) provides richer information than the model comparison approach.",0
https://doi.org/10.1016/j.jlp.2011.11.014,Prediction of occupational accident statistics and work time loss distributions using Bayesian analysis,"This paper uses Bayesian analysis tools for the stochastic evaluation of work time losses due to occupational accidents in a workplace. Models are developed for accident frequencies, duration of recovery from an accident, and the worker unavailability. The unavailability statistics are hereby derived considering a two state stochastic model, to provide estimates for the expected work time losses over a base period of workplace operation. The above models are applied on real multiyear accident data collected from the Greek Petrochemical Industry.",0
https://doi.org/10.1037/1082-989x.12.2.121,Centering predictor variables in cross-sectional multilevel models: A new look at an old issue.,"Appropriately centering Level 1 predictors is vital to the interpretation of intercept and slope parameters in multilevel models (MLMs). The issue of centering has been discussed in the literature, but it is still widely misunderstood. The purpose of this article is to provide a detailed overview of grand mean centering and group mean centering in the context of 2-level MLMs. The authors begin with a basic overview of centering and explore the differences between grand and group mean centering in the context of some prototypical research questions. Empirical analyses of artificial data sets are used to illustrate key points throughout. The article provides a number of practical recommendations designed to facilitate centering decisions in MLM applications.",0
,Fixed effects models.,,0
https://doi.org/10.1007/s13253-013-0140-3,Modeling Complex Spatial Dependencies: Low-Rank Spatially Varying Cross-Covariances With Application to Soil Nutrient Data,"Advances in geo-spatial technologies have created data-rich environments which provide extraordinary opportunities to understand the complexity of large and spatially indexed data in ecology and the natural sciences. Our current application concerns analysis of soil nutrients data collected at La Selva Biological Station, Costa Rica, where inferential interest lies in capturing the spatially varying relationships among the nutrients. The objective here is to interpolate not just the nutrients across space, but also associations among the nutrients that are posited to vary spatially. This requires spatially varying cross-covariance models. Fully process-based specifications using matrix-variate processes are theoretically attractive but computationally prohibitive. Here we develop fully process-based low-rank but non-degenerate spatially varying cross-covariance processes that can effectively yield interpolate cross-covariances at arbitrary locations. We show how a particular low-rank process, the predictive process, which has been widely used to model large geostatistical datasets, can be effectively deployed to model non-degenerate cross-covariance processes. We produce substantive inferential tools such as maps of nonstationary cross-covariances that constitute the premise of further mechanistic modeling and have hitherto not been easily available for environmental scientists and ecologists. Ã‚Â© 2013 International Biometric Society.",0
https://doi.org/10.4992/jjpsy.85.13077,Unfolding item response model using best-worst scaling,"In attitude measurement and sensory tests, the unfolding model is typically used. In this model, response probability is formulated by the distance between the person and the stimulus. In this study, we proposed an unfolding item response model using best-worst scaling (BWU model), in which a person chooses the best and worst stimulus among repeatedly presented subsets of stimuli. We also formulated an unfolding model using best scaling (BU model), and compared the accuracy of estimates between the BU and BWU models. A simulation experiment showed that the BWU modell performed much better than the BU model in terms of bias and root mean square errors of estimates. With reference to Usami (2011), the proposed models were apllied to actual data to measure attitudes toward tardiness. Results indicated high similarity between stimuli estimates generated with the proposed models and those of Usami (2011).",0
https://doi.org/10.2307/1267913,Restricted Maximum Likelihood (REML) Estimation of Variance Components in the Mixed Model,"The maximum likelihood (ML) procedure of Hartley aud Rao [2] is modified by adapting a transformation from Pattersou and Thompson [7] which partitions the likelihood render normality into two parts, one being free of the fixed effects. Maximizing this part yields what are called restricted maximum likelihood (REML) estimators. As well as retaining the property of invariance under translation that ML estimators have, the REML estimators have the additional property of reducing to the analysis variance (ANOVA) estimators for many, if not all, cases of balanced data (equal subclass numbers). A computing algorithm is developed, adapting a transformation from Hemmerle and Hartley [6], which reduces computing requirements to dealing with matrices having order equal to the dimension of the parameter space rather than that of the sample space. These same matrices also occur in the asymptotic sampling variances of the estimators.",0
https://doi.org/10.1002/hec.1496,"Bayesian methods in cost-effectiveness studies: objectivity, computation and other relevant aspects","In a probabilistic sensitivity analysis (PSA) of a cost–effectiveness (CE) study, the unknown parameters are considered as random variables. A crucial question is what probabilistic distribution is suitable for synthesizing the available information (mainly data from clinical trials) about these parameters. In this context, the important role of Bayesian methodology has been recognized, where the parameters are of a random nature. We explore, in the context of CE analyses, how formal objective Bayesian methods can be implemented. We fully illustrate the methodology using two CE problems that frequently appear in the CE literature. The results are compared with those obtained with other popular approaches to PSA. We find that the discrepancies can be quite marked, specially when the number of patients enrolled in the simulated cohort under study is large. Finally, we describe in detail the numerical methods that need to be used to obtain the results. Copyright © 2009 John Wiley & Sons, Ltd.",0
https://doi.org/10.4324/978131589416-11,Multilevel models of burnout: separating group level and individual level effects in burnout research,"For nearly two decades, there has been a growing trend toward a multilevel perspective of individual behavior in organizations (Klein et al., 1994; Kozlowski & Klein, 2000). More recently, there has been an increasing emphasis on multilevel explanations within the stress literature, both from psychological and public health researchers (Bliese & Jex, 2002). Burnout research has followed a similar pattern to many organizational behavior constructs. Throughout the years, burnout has fairly consistently been defined as an individual phenomenon (Halbesleben & Buckley, 2004; Maslach et al., 2001). However, throughout that same time, there have been suggestions that burnout has a collective or shared element to it (Edelwich & Brodsky, 1980; Gonzalez-Morales et al., 2012). For example, Gonzalez-Morales et al. (2012) defined collective burnout in terms of the shared experience of burnout among coworkers in the same work environment. They argued that the contextual antecedents and experiences of burnout (e.g., Bakker & Schaufeli, 2000; Bakker et al., 2009) suggest that burnout can be conceptualized as an individual phenomenon or a socially constructed experience.",0
https://doi.org/10.1146/annurev-statistics-022513-115701,"Breaking Bad: Two Decades of Life-Course Data Analysis in Criminology, Developmental Psychology, and Beyond","Studies of human development require longitudinal data analysis methods that describe within- and between-individual variation in developmental and behavioral trajectories. This article reviews life-course data analysis methods for modeling these trajectories, as well as their application in studies of antisocial behavior and of crime in childhood, in adolescence, and throughout life. We set the stage by introducing growth curve (hierarchical linear) models. We focus our review on finite mixture models for life-course data, known as group-based trajectory and growth mixture models. We then discuss how these models are applied within criminology and developmental psychology, recent controversies over their substantive use and interpretation, and important issues of statistical practice and the challenges they raise. Building on the critical literature, we offer several recommendations for the applied users of the models. Finally, we present the most recent method of examining behavioral trajectories in criminology, the unimodal curve registration (UCR) approach. We briefly contrast the UCR model with growth curve and finite mixture models for life-course data analysis.",0
https://doi.org/10.1097/01.aud.0000440715.85844.b8,Listening Effort and Perceived Clarity for Normal-Hearing Children With the Use of Digital Noise Reduction,"Objectives The goal of this study was to evaluate how digital noise reduction (DNR) impacts listening effort and judgment of sound clarity in children with normal hearing. It was hypothesized that when two DNR algorithms differing in signal-to-noise ratio (SNR) output are compared, the algorithm that provides the greatest improvement in overall output SNR will reduce listening effort and receive a better clarity rating from child listeners. A secondary goal was to evaluate the relation between the inversion method measurements and listening effort with DNR processing. Design Twenty-four children with normal hearing (ages 7 to 12 years) participated in a speech recognition task in which consonant-vowel-consonant nonwords were presented in broadband background noise. Test stimuli were recorded through two hearing aids with DNR off and DNR on at 0 dB and +5 dB input SNR. Stimuli were presented to listeners and verbal response time (VRT) and phoneme recognition scores were measured. The underlying assumption was that an increase in VRT reflects an increase in listening effort. Children rated the sound clarity for each condition. The two commercially available HAs were chosen based on: (1) an inversion technique, which was used to quantify the magnitude of change in SNR with the activation of DNR, and (2) a measure of magnitude-squared coherence, which was used to ensure that DNR in both devices preserved the spectrum. Results One device provided a greater improvement in overall output SNR than the other. Both DNR algorithms resulted in minimal spectral distortion as measured using coherence. For both devices, VRT decreased for the DNR-on condition, suggesting that listening effort decreased with DNR in both devices. Clarity ratings were also better in the DNR-on condition for both devices. The device showing the greatest improvement in output SNR with DNR engaged improved phoneme recognition scores. The magnitude of this improved phoneme recognition was not accurately predicted with measurements of output SNR. Measured output SNR varied in the ability to predict other outcomes. Conclusions Overall, results suggest that DNR effectively reduces listening effort and improves subjective clarity ratings in children with normal hearing but that these improvements are not necessarily related to the output SNR improvements or preserved speech spectra provided by the DNR.",0
https://doi.org/10.1002/sim.4780142408,Bayesian approaches to random-effects meta-analysis: A comparative study,"Current methods for meta-analysis still leave a number of unresolved issues, such as the choice between fixed- and random-effects models, the choice of population distribution in a random-effects analysis, the treatment of small studies and extreme results, and incorporation of study-specific covariates. We describe how a full Bayesian analysis can deal with these and other issues in a natural way, illustrated by a recent published example that displays a number of problems. Such analyses are now generally available using the BUGS implementation of Markov chain Monte Carlo numerical integration techniques. Appropriate proper prior distributions are derived, and sensitivity analysis to a variety of prior assumptions carried out. Current methods are briefly summarized and compared to the full Bayes analysis.",0
,Estimation of the standard error and confidence interval of the indirect effect in multiple mediator models,,0
,Model Likelihoods and Bayes Factors for Switching and Mixture Models,"In the present paper we discuss the problem of estimating model likelihoods from the MCMC output for a general mixture and switching model. Estimation is based on the method of bridge sampling (Meng and Wong, 1996), where the MCMC sample is combined with an iid sample from an importance density. The importance density is constructed in an unsupervised manner from the MCMC output using a mixture of complete data posteriors. Whereas the importance sampling estimator as well as the reciprocal importance sampling estimator are sensitive to the tail behaviour of the importance density, we demonstrate that the bridge sampling estimator is far more robust in this concern. Our case studies range from computing marginal likelihoods for a mixture of multivariate normal distributions, testing for the inhomogeneity of a discrete time Poisson process, to testing for the presence of Markov switching and order selection in the MSAR model. (author's abstract)",0
https://doi.org/10.1037/0278-7393.10.4.598,Analysis of response time distributions in the study of cognitive processes.,"Examined the convolution analysis of RT distributions in 3 experiments employing 4 cognitive tasks with 14 college students. In Exp I (n = 6), a visual search task was contrasted with a short-term memory search task. In Exp II (n = 4) and Exp III (n = 4), a relative judgment of recency task was contrasted with a 2-alternative, forced-choice recognition task. Taken together, the experiments demonstrate that the convolution analysis provides a good description of RT distributions in a variety of tasks and that the parameters of the convolution analysis can behave differentially in different tasks. It is argued that the parameters of the convolution analysis can play an important role in discriminating between models and in critically evaluating models that may be otherwise acceptable. (26 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1984 American Psychological Association.",0
https://doi.org/10.1037/met0000094,Maximum likelihood versus multiple imputation for missing data in small longitudinal samples with nonnormality.,"The study examined the performance of maximum likelihood (ML) and multiple imputation (MI) procedures for missing data in longitudinal research when fitting latent growth models. A Monte Carlo simulation study was conducted with conditions of small sample size, intermittent missing data, and nonnormality. The results indicated that ML tended to display slightly smaller degrees of bias than MI across missing completely at random (MCAR) and missing at random (MAR) conditions. Although specification of prior information in the MI imputation-posterior (I-P) phase influenced the performance of MI, especially with nonnormal small samples and missing not at random (MNAR), the impact of this tight specification was not dramatic. Several corrected ML test statistics showed proper rejections rates across research designs, whereas posterior predictive p values for MI methods were more likely to be influenced by distribution shape and yielded higher rejection rates in MCAR and MAR than in MNAR. In conclusion, ML appears to be preferable to MI in research conditions with small missing samples and multivariate nonnormality whether or not strong prior information for the I-P phase of MI analysis is available. (PsycINFO Database Record",0
https://doi.org/10.1111/j.1468-2389.2006.00331.x,"From the Work One Knows the Worker: A Systematic Review of the Challenges, Solutions, and Steps to Creating Synthetic Validity","Synthetic validity has been promised as the future for selection, providing an inexpensive, fast, high-quality, legally defensible, and easily administered process. Despite 50 years of development, this promise has yet to be realized. However, recent advances in areas such as validity generalization indicate that synthetic validity is technically feasible and practically achievable. Consolidating new and previous work carried out on two synthetic validity strategies, the job-requirement matrix and job component validity, we review the methodological steps required to build them and provide working examples. Although the resources required for full realization of synthetic validity are large, similar, although larger, projects have been undertaken in the past and in the present, and there is increasing infrastructure to facilitate them in the future.",0
https://doi.org/10.3389/fpsyg.2013.00918,Release the BEESTS: Bayesian Estimation of Ex-Gaussian STop-Signal reaction time distributions,"The stop-signal paradigm is frequently used to study response inhibition. In this paradigm, participants perform a two-choice response time (RT) task where the primary task is occasionally interrupted by a stop-signal that prompts participants to withhold their response. The primary goal is to estimate the latency of the unobservable stop response (stop signal reaction time or SSRT). Recently, Matzke et al. (2013) have developed a Bayesian parametric approach (BPA) that allows for the estimation of the entire distribution of SSRTs. The BPA assumes that SSRTs are ex-Gaussian distributed and uses Markov chain Monte Carlo sampling to estimate the parameters of the SSRT distribution. Here we present an efficient and user-friendly software implementation of the BPA-BEESTS-that can be applied to individual as well as hierarchical stop-signal data. BEESTS comes with an easy-to-use graphical user interface and provides users with summary statistics of the posterior distribution of the parameters as well various diagnostic tools to assess the quality of the parameter estimates. The software is open source and runs on Windows and OS X operating systems. In sum, BEESTS allows experimental and clinical psychologists to estimate entire distributions of SSRTs and hence facilitates the more rigorous analysis of stop-signal data.",0
https://doi.org/10.1207/s15327906mbr3904_4,Evaluation of the Bayesian and Maximum Likelihood Approaches in Analyzing Structural Equation Models with Small Sample Sizes,"The main objective of this article is to investigate the empirical performances of the Bayesian approach in analyzing structural equation models with small sample sizes. The traditional maximum likelihood (ML) is also included for comparison. In the context of a confirmatory factor analysis model and a structural equation model, simulation studies are conducted with the different magnitudes of parameters and sample sizes n = da, where d = 2, 3, 4 and 5, and a is the number of unknown parameters. The performances are evaluated in terms of the goodness-of-fit statistics, and various measures on the accuracy of the estimates. The conclusion is: for data that are normally distributed, the Bayesian approach can be used with small sample sizes, whilst ML cannot.",1
https://doi.org/10.1080/01621459.1995.10476572,Bayes Factors,"In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: â€¢ From Jeffreysâ€™ Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. â€¢ Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. â€¢ Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. â€¢ Bayes factors are very general and do not require alternative models to be nested. â€¢ Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. â€¢ In â€œnonstandardâ€  statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance tests. â€¢ The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. â€¢ When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. â€¢ Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. â€¢ Bayes factors are useful for guiding an evolutionary model-building process. â€¢ It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used. Â© 1995 Taylor & Francis Group, LLC.",0
https://doi.org/10.1177/0146167215573212,The Girls Set the Tone,"In a four-wave longitudinal study with N = 1,321 adolescents in Germany, we examined the impact of class-level normative beliefs about aggression on aggressive norms and behavior at the individual level over the course of 3 years. At each data wave, participants indicated their normative acceptance of aggressive behavior and provided self-reports of physical and relational aggression. Multilevel analyses revealed significant cross-level interactions between class-level and individual-level normative beliefs at T1 on individual differences in physical aggression at T2, and the indirect interactive effects were significant up to T4. Normative approval of aggression at the class level, especially girls’ normative beliefs, defined the boundary conditions for the expression of individual differences in aggressive norms and their impact on physically and relationally aggressive behavior for both girls and boys. The findings demonstrate the moderating effect of social norms on the pathways from individual normative beliefs to aggressive behavior in adolescence.",0
https://doi.org/10.3982/ecta6474,On the Failure of the Bootstrap for Matching Estimators,"Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romano (1994).",0
https://doi.org/10.1111/j.1469-7998.2010.00770.x,Starting size and tadpole performance in the frog Rana latastei,"Egg provisioning is a major maternal effect in amphibians. We evaluated the relationship between starting body size (a proxy of egg provisioning) and multiple measures of larval performance in the Italian agile frog Rana latastei; we analysed within-clutch variation, to remove co-variation between provisioning and genetic maternal effect. We reared tadpoles from multiple clutches in a common environment under two food treatments (high- and low-protein content), and measured the mortality, tadpole size during development and development rate. We used a Bayesian framework (Markov chain Monte Carlo generalized linear mixed models) for the multivariate analysis of performance measures. Tadpoles with a large starting size remained the largest ones through the entire larval development, and attained metamorphosis earlier. Food with a high-protein content reduced mortality and increased the growth and development rate; the choice of food may be important in captive-breeding/headstarting programmes. We did not detect effects of the interaction between provisioning and type of food on tadpole performance. Our study confirms the importance of egg provisioning in amphibians, showing that it can affect multiple traits, and that their effects can last through the entire larval development.",0
https://doi.org/10.1037/0021-9010.66.3.274,Testing the constancy of validity with computer-generated sampling distributions of the multiplicative model variance estimate: Results for petroleum industry validation research.,"A comparison of computer-generated sampling distributions showed that increases in total N resulting from greater sample size/validity study reduced the sampling variation of multiplicative model estimates of true validity variance more than did increases in total N resulting from a larger number of studies. Validity coefficients, range restriction, and criterion reliability data for 5 tests predicting job performance and training performance were obtained from studies conducted at petroleum industry plants. Those tests included the Richardson-Bellows-Henry Ability, Arithmetic Reasoning, and Chemical Comprehension Tests; the Bennett Mechanical Comprehension Tests; and the Mathematics scale of the California Achievement Tests. Studies for 3 job categories (operation, maintenance, and laboratory) were combined for analysis to maximize sensitivity to job and plant differences. Sampling distributions of true validity variance estimates were generated for each test-criterion type combination. A comparison of actual variance estimates with critical values indicated that 3 of the 5 tests did not vary significantly in true validities for job performance or for training criteria. Results for 2 arithmetic reasoning tests support the existence of variation in true validities for job performance and for training criteria. (19 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Ã‚Â© 1981 American Psychological Association.",0
https://doi.org/10.1186/2046-4053-3-110,A Microsoft-Excel-based tool for running and critically appraising network meta-analyses—an overview and application of NetMetaXL,"The use of network meta-analysis has increased dramatically in recent years. WinBUGS, a freely available Bayesian software package, has been the most widely used software package to conduct network meta-analyses. However, the learning curve for WinBUGS can be daunting, especially for new users. Furthermore, critical appraisal of network meta-analyses conducted in WinBUGS can be challenging given its limited data manipulation capabilities and the fact that generation of graphical output from network meta-analyses often relies on different software packages than the analyses themselves.We developed a freely available Microsoft-Excel-based tool called NetMetaXL, programmed in Visual Basic for Applications, which provides an interface for conducting a Bayesian network meta-analysis using WinBUGS from within Microsoft Excel. . This tool allows the user to easily prepare and enter data, set model assumptions, and run the network meta-analysis, with results being automatically displayed in an Excel spreadsheet. It also contains macros that use NetMetaXL's interface to generate evidence network diagrams, forest plots, league tables of pairwise comparisons, probability plots (rankograms), and inconsistency plots within Microsoft Excel. All figures generated are publication quality, thereby increasing the efficiency of knowledge transfer and manuscript preparation.We demonstrate the application of NetMetaXL using data from a network meta-analysis published previously which compares combined resynchronization and implantable defibrillator therapy in left ventricular dysfunction. We replicate results from the previous publication while demonstrating result summaries generated by the software.Use of the freely available NetMetaXL successfully demonstrated its ability to make running network meta-analyses more accessible to novice WinBUGS users by allowing analyses to be conducted entirely within Microsoft Excel. NetMetaXL also allows for more efficient and transparent critical appraisal of network meta-analyses, enhanced standardization of reporting, and integration with health economic evaluations which are frequently Excel-based.",0
https://doi.org/10.1111/jcom.12115,"Deliberative Performance of Television News in Three Types of Democracy: Insights from the United States, Germany, and Russia","We show that television news is considerably more deliberative in established (United States, Germany) than in defective democracies (Russia) and slightly more deliberative in a power-sharing political system (Germany) than in a power-concentrating system (United States). We further demonstrate that public-service channels, nonpartisan programs, and in-depth news shows make stronger overall contributions toward deliberativeness than their respective counterparts. While national news cultures produce distinct national styles of mediated deliberation, individual channels in the United States (Fox, CNN) and Russia (REN) cut across these national patterns. The significance of deliberative media content for citizens and political elites is discussed.",0
https://doi.org/10.1016/b978-044452044-9/50011-2,Bayesian Structural Equation Modeling,"Abstract Structural equation models (SEMs) with latent variables are routinely used in social science research, and are of increasing importance in biomedical applications. Standard practice in implementing SEMs relies on frequentist methods. A simple and concise description of an alternative Bayesian approach is developed. Furthermore, a brief overview of the literature, a description of Bayesian specification of SEMs, and an outline of a Gibbs sampling strategy for model fitting is provided. Bayesian inferences are illustrated through an industrialization and democratization case study from the literature. The Bayesian approach has some distinct advantages, due to the availability of samples from the joint posterior distribution of the model parameters and latent variables, that are highlighted. These posterior samples provide important information not contained in the measurement and structural parameters. As is illustrated using the case study, this information can often provide valuable insight into structural relationships.",0
https://doi.org/10.3945/jn.113.184317,"Dietary Carotenoids Are Associated with Cardiovascular Disease Risk Biomarkers Mediated by Serum Carotenoid Concentrations ,","Hyperlipidemia and elevated circulating C-reactive protein (CRP) and total homocysteine (tHcy) concentrations are cardiovascular disease (CVD) risk factors. Previous studies indicated that higher serum carotenoid concentrations were inversely associated with some of these biomarkers. However, whether dietary carotenoid intake is inversely associated with these CVD risk biomarkers is not well known. We assessed the associations between individual dietary carotenoid intake and CVD risk biomarkers and tested whether the serum carotenoid concentrations explain (mediate) or influence the strength of (moderate) the associations, if any association exists. Dietary data collected from 2 24-h dietary recalls and serum measurements in adult men (n = 1312) and women (n = 1544) from the NHANES 2003-2006 were used. Regression models designed for survey analysis were used to examine the associations between individual dietary carotenoids and log-transformed blood cholesterol, CRP, and tHcy. The corresponding individual serum carotenoid concentration was considered as mediator (and moderator if applicable). After adjustment for covariates, significant inverse associations with LDL cholesterol were observed for dietary β-carotene (P < 0.05) and lutein + zeaxanthin (P < 0.001), and with tHcy for dietary β-carotene (P < 0.05), lycopene (P < 0.05), and total carotenoids (P < 0.05). Dietary lutein + zeaxanthin intake was also positively associated with HDL cholesterol concentrations (P < 0.01). Most of these associations were null after additional adjustment for corresponding serum carotenoid concentrations, indicating the complete mediation effects of serum carotenoids. Serum β-carotene significantly moderated the associations between dietary β-carotene and CRP (P-interaction < 0.05), and quartile 4 of dietary β-carotene was associated with lower CRP concentrations only among participants with serum β-carotene > 0.43 μmol/L. In this population-based cross-sectional study, serum carotenoids were mediators of dietary carotenoids and CVD risk biomarker associations. Serum β-carotene was also a moderator of the dietary β-carotene and CRP association. These findings may help in the design of future intervention studies on dietary carotenoids in the prevention of CVD.",0
https://doi.org/10.1080/00273171.2012.715560,A Bayesian Model For The Estimation Of Latent Interaction And Quadratic Effects When Latent Variables Are Non-Normally Distributed,"Structural equation models with interaction and quadratic effects have become a standard tool for testing nonlinear hypotheses in the social sciences. Most of the current approaches assume normally distributed latent predictor variables. In this article, we present a Bayesian model for the estimation of latent nonlinear effects when the latent predictor variables are nonnormally distributed. The nonnormal predictor distribution is approximated by a finite mixture distribution. We conduct a simulation study that demonstrates the advantages of the proposed Bayesian model over contemporary approaches (Latent Moderated Structural Equations [LMS], Quasi-Maximum-Likelihood [QML], and the extended unconstrained approach) when the latent predictor variables follow a nonnormal distribution. The conventional approaches show biased estimates of the nonlinear effects; the proposed Bayesian model provides unbiased estimates. We present an empirical example from work and stress research and provide syntax for substantive researchers. Advantages and limitations of the new model are discussed.",0
https://doi.org/10.1016/j.socnet.2009.02.004,Introduction to stochastic actor-based models for network dynamics,"Abstract Stochastic actor-based models are models for network dynamics that can represent a wide variety of influences on network change, and allow to estimate parameters expressing such influences, and test corresponding hypotheses. The nodes in the network represent social actors, and the collection of ties represents a social relation. The assumptions posit that the network evolves as a stochastic process ‘driven by the actors’, i.e., the model lends itself especially for representing theories about how actors change their outgoing ties. The probabilities of tie changes are in part endogenously determined, i.e., as a function of the current network structure itself, and in part exogenously, as a function of characteristics of the nodes (‘actor covariates’) and of characteristics of pairs of nodes (‘dyadic covariates’). In an extended form, stochastic actor-based models can be used to analyze longitudinal data on social networks jointly with changing attributes of the actors: dynamics of networks and behavior. This paper gives an introduction to stochastic actor-based models for dynamics of directed networks, using only a minimum of mathematics. The focus is on understanding the basic principles of the model, understanding the results, and on sensible rules for model selection.",0
https://doi.org/10.1177/01466210022031705,A Comparison of Item Selection Rules at the Early Stages of Computerized Adaptive Testing,"The effects of five item selection rules—Fisher information (FI), Fisher interval information (FII), Fisher information with a posterior distribution (FIP), Kullback-Leibler information (KL), and Kullback-Leibler information with a posterior distribution (KLP)—were compared with respect to the efficiency and precision of trait (Θ) estimation at the early stages of computerized adaptive testing (CAT). FII, FIP, KL, and KLP performed marginally better than FI at the early stages of CAT for Θ= -3 and -2. For tests longer than 10 items,there appeared to be no precision advantage for any of the selection rules.",0
https://doi.org/10.1198/016214501750333063,Markov chain Monte Carlo Estimation of Classical and Dynamic Switching and Mixture Models,"Bayesian estimation of a very general model class, where the distribution of the observations depends on a latent process taking values in a discrete state space, is discussed in this article. This model class covers finite mixture modeling, Markov switching autoregressive modeling, and dynamic linear models with switching. The consequences the unidentifiability of this type of model has on Markov chain Monte Carlo (MCMC) estimation are explicitly dealt with. Joint Bayesian estimation of all latent variables, model parameters, and parameters that determine the probability law of the latent process is carried out by a new MCMC method called permutation sampling. The permutation sampler first samples from the unconstrained posterior–which often can be done in a convenient multimove manner–and then applies a permutation of the current labeling of the states of the latent process. In a first run, the random permutation sampler used selected the permutation randomly. The MCMC output of the random permutation s...",0
https://doi.org/10.1016/s0001-4575(01)00086-0,The use of multilevel models for the prediction of road accident outcomes,"An important problem in road traffic accident research is the resolution of the magnitude by which individual accident characteristics affect the risk of fatality for each person involved. This article introduces the potential of a recently developed form of regression models, known as multilevel models, for quantifying the various influences on casualty outcomes. The application of multilevel models is illustrated by the analysis of the predictors of outcome amongst over 16,000 fatally and seriously injured casualties involved in accidents between 1985 and 1996 in Norway. Risk of fatality was found to be associated with casualty age and sex, as well as the type of vehicles involved, the characteristics of the impact, the attributes of the road section on which it took place, the time of day, and whether alcohol was suspected. After accounting for these factors, the multilevel analysis showed that 16% of unexplained variation in casualty outcomes was between accidents, whilst approximately 1% was associated with the area of Norway in which each incident occurred. The benefits of using multilevel models to analyse accident data are discussed along with the limitations of traditional regression modelling approaches.",0
https://doi.org/10.1007/bf01404679,Derivative free analogues of the Levenberg-Marquardt and Gauss algorithms for nonlinear least squares approximation,"In this paper we give two derivative-free computational algorithms for nonlinear least squares approximation. The algorithms are finite difference analogues of the Levenberg-Marquardt and Gauss methods. Local convergence theorems for the algorithms are proven. In the special case when the residuals are zero at the minimum, we show that certain computationally simple choices of the parameters lead to quadratic convergence. Numerical examples are included. Â© 1971 Springer-Verlag.",0
https://doi.org/10.2307/2532086,Some Covariance Models for Longitudinal Count Data with Overdispersion,"A family of covariance models for longitudinal counts with predictive covariates is presented. These models account for overdispersion, heteroscedasticity, and dependence among repeated observations. The approach is a quasi-likelihood regression similar to the formulation given by Liang and Zeger (1986, Biometrika 73, 13-22). Generalized estimating equations for both the covariate parameters and the variance-covariance parameters are presented. Large-sample properties of the parameter estimates are derived. The proposed methods are illustrated by an analysis of epileptic seizure count data arising from a study of progabide as an adjuvant therapy for partial seizures.",0
https://doi.org/10.1016/s0149-2063(03)00084-9,Recent Advances in Causal Modeling Methods for Organizational and Management Research,"The purpose of this article is to review recent advanced applications of causal modeling methods in organizational and management research. Developments over the past 10 years involving research on measurement and structural components of causal models will be discussed. Specific topics to be addressed include reflective vs. formative measurement, multidimensional construct assessment, method variance, measurement invariance, latent growth modeling (LGM), moderated structural relationships, and analysis of latent variable means. For each of the areas mentioned above an overview of developments will be presented, and examples from organizational and management research will be provided.",0
https://doi.org/10.2427/8981,"Building reliable evidence from real-world data: methods, cautiousness and recommendations","Routinely stored information on healthcare utilisation in everyday clinical practice has proliferated over the past several decades. There is, however, some reluctance on the part of many health professionals to use observational data to support healthcare decisions, especially when data are derived from large databases. Challenges in conducting observational studies based on electronic databases include concern about the adequacy of study design and methods to minimise the effect of both misclassifications (in the absence of direct assessments of exposure and outcome validity) and confounding (in the absence of randomisation). This paper points out issues that may compromise the validity of such studies, and approaches to managing analytic challenges. First, strategies of sampling within a large cohort, as an alternative to analysing the full cohort, will be presented. Second, methods for controlling outcome and exposure misclassifications will be described. Third, several techniques that take into account both measured and unmeasured confounders will also be presented. Fourth, some considerations regarding random uncertainty in the framework of observational studies using healthcare utilisation data will be discussed. Finally, some recommendations for good research practice are listed in this paper. The aim is to provide researchers with a methodological framework, while commenting on the value of new techniques for more advanced users.",0
https://doi.org/10.1080/01621459.1994.10476768,"Missing Data, Imputation, and the Bootstrap","Missing data refers to a class of problems made difficult by the absence of some portions of a familiar data structure. For example, a regression problem might have some missing values in the predi...",0
https://doi.org/10.1136/bmjopen-2013-004475,Built environment and physical activity in New Zealand adolescents: a protocol for a cross-sectional study: Table 1,"Built-environment interventions have the potential to provide population-wide effects and the means for a sustained effect on behaviour change. Population-wide effects for adult physical activity have been shown with selected built environment attributes; however, the association between the built environment and adolescent health behaviours is less clear. This New Zealand study is part of an international project across 10 countries (International Physical Activity and the Environment Network-adolescents) that aims to characterise the links between built environment and adolescent health outcomes.An observational, cross-sectional study of the associations between measures of the built environment with physical activity, sedentary behaviour, body size and social connectedness in 1600 New Zealand adolescents aged 12-18 years will be conducted in 2013-2014. Walkability and neighbourhood destination accessibility indices will be objectively measured using Geographic Information Systems (GIS). Physical activity and sedentary behaviours will be objectively measured using accelerometers over seven consecutive days. Body mass index will be calculated as weight divided by squared height. Demographics, socioeconomic status, active commuting behaviours and perceived neighbourhood walkability will be assessed using the Neighbourhood Environment Walkability Scale for Youth and psychosocial indicators. A web-based computer-assisted personal interview tool Visualisation and Evaluation of Route Itineraries, Travel Destinations, and Activity Spaces (VERITAS) and Global Positioning System (GPS) receivers will be used in a subsample of 300 participants. A qualitative research component will explore barriers and facilitators for physical activity in adolescents with respect to the built and social environment in a subsample of 80 participants.The study received ethical approval from the Auckland University of Technology Ethics Committee (12/161). Data will be entered and stored into a secure (password protected) database. Only the named researchers will have access to the data. Data will be stored for 10 years and permanently destroyed thereafter. The results papers will be submitted for publication in peer-reviewed journals.",0
https://doi.org/10.1111/j.1541-0420.2012.01781.x,Bayesian Inference for the Causal Effect of Mediation,Summary We propose a nonparametric Bayesian approach to estimate the natural direct and indirect effects through a mediator in the setting of a continuous mediator and a binary response. Several conditional independence assumptions are introduced (with corresponding sensitivity parameters) to make these effects identifiable from the observed data. We suggest strategies for eliciting sensitivity parameters and conduct simulations to assess violations to the assumptions. This approach is used to assess mediation in a recent weight management clinical trial.,0
https://doi.org/10.1016/j.jvb.2015.12.004,State work engagement and state affect: Similar yet distinct concepts,"Abstract State work engagement (SWE), a multidimensional construct of work-related well-being, was originally conceptualized as a trait, but diary studies have revealed substantial within-person fluctuations. Given that SWE is conceptualized as a work-related affective-motivational construct, the question arises as to whether SWE can be differentiated from other affective constructs. Thus, the goal of the present study was to compare SWE and state affect with respect to their degree of within-person variability and to examine their distinct relationships with health and performance variables (i.e., sleep quality and job performance). Fifty-two employees (44% female) participated in the study, which included 3 assessments per day over the course of 2 weeks. Our results revealed that energetic arousal and tense arousal fluctuated more strongly within days than vigor and absorption. Multilevel analyses demonstrated that high sleep quality predicted higher state affect but not higher SWE. In addition, vigor exhibited an inverted U-shaped relation with performance. In sum, SWE as a time-varying construct showed some overlap with state affect but also demonstrated discriminant validity.",0
https://doi.org/10.1080/01621459.1969.10500979,Missing Observations in Multivariate Statistics III: Large Sample Analysis of Simple Linear Regression,"Abstract We derive the asymptotic distribution of several estimators for the parameters of the linear regression of y on x when some observations on y and on x are missing. Then, we compare estimators using a mean square error criterion. We find for example that a simple estimator of the linear regression function has asymptotic efficiency no worse than 0.95 compared with the maximum likelihood estimator (assuming bivariate normality) provided that no more than 30 per cent of the y's and 30 per cent of the x's are missing. This simple estimator is defined without assuming bivariate normality in Section 8.1.",0
https://doi.org/10.1177/014662168500900409,The Difficulty of Test Items That Measure More Than One Ability,Many test items require more than one ability to obtain a correct response. This article proposes a mul tidimensional index of item difficulty that can be used with items of this type. The proposed index describes multidimensional item difficulty as the direction in the multidimensional space in which the item provides the most information and the distance in that direction to the most informative point. The multidimensional dif ficulty is derived for a particular item response theory model and an example of its application is given using the ACT Mathematics Usage Test.,0
https://doi.org/10.1080/00220970903224685,Three-Level Models for Indirect Effects in School- and Class-Randomized Experiments in Education,"Due to the clustered nature of field data, multi-level modeling has become commonly used to analyze data arising from educational field experiments. While recent methodological literature has focused on multi-level mediation analysis, relatively little attention has been devoted to mediation analysis when three levels (e.g., student, class, school) are present in a study setting. This article presents analysis models that can be used to test indirect effects in experimental designs having three levels where random assignment is at the third (school) or second (class) level and where the indirect effect may be random. In the presentation, simulated datasets are used to illustrate model specification and results interpretation for hypothetical three-level educational experiments involving mediation and moderation of treatment effects.",0
https://doi.org/10.1007/bf03372669,Nutzenermittlung in wahlbasierter Conjoint-Analyse: Ein Vergleich von Latent-Class- und hierarchischem Bayes-Verfahren,"Two different concepts of disentangling noise from systematic deviations in Choice-Based Conjoint evaluations are compared: The Latent Class Technique and the Hierarchical Bayes procedure. In addition, a probabilistic interpretation of LC estimates is presented as an interims model. Conceptual differences between these models are discussed and hypotheses on resulting differences in estimates are derived. These are tested in a large-scale empirical study. The relative performance is evaluated in two distinct application areas: segment/level and individual/level estimates. The expected patterns are confirmed only partly by empirical evidence. It is shown that the structure of the underlying heterogeneity concept influences the achievable outcomes. Contrary to expectations it is shown that the segment-level estimates are highly stable across methods. While individual Hierarchical Bayes estimates are often of questionable quality, they are to be preferred against the Latent Class estimates, because they detect outliers reasonably well and provide more flexibility in the data evaluation.",0
https://doi.org/10.1016/j.neuroimage.2014.12.025,Comparison of variants of canonical correlation analysis and partial least squares for combined analysis of MRI and genetic data,"The standard analysis approach in neuroimaging genetics studies is the mass-univariate linear modeling (MULM) approach. From a statistical view, however, this approach is disadvantageous, as it is computationally intensive, cannot account for complex multivariate relationships, and has to be corrected for multiple testing. In contrast, multivariate methods offer the opportunity to include combined information from multiple variants to discover meaningful associations between genetic and brain imaging data. We assessed three multivariate techniques, partial least squares correlation (PLSC), sparse canonical correlation analysis (sparse CCA) and Bayesian inter-battery factor analysis (Bayesian IBFA), with respect to their ability to detect multivariate genotype-phenotype associations. Our goal was to systematically compare these three approaches with respect to their performance and to assess their suitability for high-dimensional and multi-collinearly dependent data as is the case in neuroimaging genetics studies. In a series of simulations using both linearly independent and multi-collinear data, we show that sparse CCA and PLSC are suitable even for very high-dimensional collinear imaging data sets. Among those two, the predictive power was higher for sparse CCA when voxel numbers were below 400 times sample size and candidate SNPs were considered. Accordingly, we recommend Sparse CCA for candidate phenotype, candidate SNP studies. When voxel numbers exceeded 500 times sample size, the predictive power was the highest for PLSC. Therefore, PLSC can be considered a promising technique for multivariate modeling of high-dimensional brain-SNP-associations. In contrast, Bayesian IBFA cannot be recommended, since additional post-processing steps were necessary to detect causal relations. To verify the applicability of sparse CCA and PLSC, we applied them to an experimental imaging genetics data set provided for us. Most importantly, application of both methods replicated the findings of this data set.",0
https://doi.org/10.1207/s15328007sem1401_2,"Performance of Factor Mixture Models as a Function of Model Size, Covariate Effects, and Class-Specific Parameters","Abstract Factor mixture models are designed for the analysis of multivariate data obtained from a population consisting of distinct latent classes. A common factor model is assumed to hold within each of the latent classes. Factor mixture modeling involves obtaining estimates of the model parameters, and may also be used to assign subjects to their most likely latent class. This simulation study investigates aspects of model performance such as parameter coverage and correct class membership assignment and focuses on covariate effects, model size, and class-specific versus class-invariant parameters. When fitting true models, parameter coverage is good for most parameters even for the smallest class separation investigated in this study (0.5 SD between 2 classes). The same holds for convergence rates. Correct class assignment is unsatisfactory for the small class separation without covariates, but improves dramatically with increasing separation, covariate effects, or both. Model performance is not influe...",0
https://doi.org/10.1016/0304-4076(83)90093-3,Latent variable structural equation modeling with categorical data,"Structural equation modeling with latent variables is overviewed for situations involving a mixture of dichotomous, ordered polytomous, and continuous indicators of latent variables. Special emphasis is placed on categorical variables. Models in psychometrics, econometrics and biometrics are interrelated via a general model due to Muthén. Limited information least squares estimators and full information estimation are discussed. An example is estimated with a model for a four-wave longitudinal data set, where dichotomous responses are related to each other and a set of independent variables via latent variables with a variance component structure.",0
https://doi.org/10.1177/0146621609349800,IRT Parameter Estimation With Response Times as Collateral Information,"Hierarchical modeling of responses and response times on test items facilitates the use of response times as collateral information in the estimation of the response parameters. In addition to the regular information in the response data, two sources of collateral information are identified: (a) the joint information in the responses and the response times summarized in the estimates of the second-level parameters and (b) the information in the posterior distribution of the response parameters given the response times. The latter is shown to be a natural empirical prior distribution for the estimation of the response parameters. Unlike traditional hierarchical item response theory (IRT) modeling, where the gain in estimation accuracy is typically paid for by an increase in bias, use of this posterior predictive distribution improves both the accuracy and the bias of IRT parameter estimates. In an empirical study, the improvements are demonstrated for the estimation of the person and item parameters in a three-parameter response model.",0
https://doi.org/10.1111/add.12278,Availability of convenience stores and adolescent alcohol use in Taiwan: a multi-level analysis of national surveys,"Aim To examine the association between alcohol in school environments and adolescent alcohol use over the previous 6 months. Design A multi-level logistic regression analysis was performed of cross-sectional surveys conducted in 2004, 2005 and 2006. Participants and setting A total of 52 214 students aged 11–19 years from 387 middle or high schools were selected from a nationally representative, multi-stage, stratified probability sampling across Taiwan. Measurements Information on socio-demographic features and substance use experiences was collected using self-administered questionnaires. The alcohol in the environment was measured using the availability of convenience stores surrounding the schools. Using geographical information systems, the weighted numbers of convenience stores within 1 km, a 12–15-minute walk, of a school were calculated. The schools were later categorized into three subgroups via the tertile of nearby convenience stores. Findings Considering the compositional characteristics, the availability of convenience stores was found to account for 1.5% of the school-level variance of youthful drinking. The odds ratios (95% confidence interval) of alcohol use over the previous 6 months among youth attending schools with medium and high availability were 1.04 (0.96–1.13) and 1.08 (1.00–1.17), respectively, with a P-value of 0.04 in the trend test. Conclusion The greater availability of convenience stores near a school is associated with an increased risk of alcohol use among adolescents over the previous 6 months.",0
https://doi.org/10.1191/1740774505cn072oa,"Prior distributions for the intracluster correlation coefficient, based on multiple previous estimates, and their application in cluster randomized trials","Numerous estimates for the intracluster correlation coefficient (ICC) are available in research databases and publications. When planning a cluster randomized trial, an anticipated value for the ICC is required; currently, researchers base their choice informally on the magnitude of previous ICC estimates. In this paper, we make use of the wealth of ICC information by formally constructing informative prior distributions, while acknowledging the varying relevance and precision of the estimates available. Typically, for a planned trial in a given clinical setting, multiple relevant ICC estimates are available from each of several completed studies. Our preferred model allows for the imprecision in each ICC estimate around its underlying true value and, separately, allows for the similarity of ICC values from the same study. The relevance of each previous estimate to the planned clinical setting is considered, and estimates corresponding to less relevant outcomes or population types are given less influence. We find that such downweighting can increase the precision of the anticipated ICC. In trial design, the prior distribution constructed allows uncertainty about the ICC to be acknowledged, and we describe how to choose a design that provides adequate power across the range of likely ICC values. Prior information on the ICC can also be incorporated in analysis of the trial data, when taking a Bayesian approach. The methods proposed enable available ICC information to be summarised appropriately by an informative prior distribution, which is of direct practical use in cluster randomized trials.",0
https://doi.org/10.1080/00273170802490673,Distinguishing Between Latent Classes and Continuous Factors with Categorical Outcomes: Class Invariance of Parameters of Factor Mixture Models,"Factor mixture models (FMM's) are latent variable models with categorical and continuous latent variables which can be used as a model-based approach to clustering. A previous paper covered the results of a simulation study showing that in the absence of model violations, it is usually possible to choose the correct model when fitting a series of models with different numbers of classes and factors within class. The response format in the first study was limited to normally distributed outcomes. The current paper has two main goals, firstly, to replicate parts of the first study with 5-point Likert scale and binary outcomes, and secondly, to address the issue of testing class invariance of thresholds and loadings. Testing for class invariance of parameters is important in the context of measurement invariance and when using mixture models to approximate non-normal distributions. Results show that it is possible to discriminate between latent class models and factor models even if responses are categorical. Comparing models with and without class-specific parameters can lead to incorrectly accepting parameter invariance if the compared models differ substantially with respect to the number of estimated parameters. The simulation study is complemented with an illustration of a factor mixture analysis of ten binary depression items obtained from a female subsample of the Virginia Twin Registry.",0
https://doi.org/10.1007/978-94-007-3024-3_5,Multilevel Modelling,"Data often form a complex hierarchy, where multiple observations are made from within the same cluster. This natural clustering or nesting of observations results in data that possesses a lack of independence, which violates an important assumption of conventional single-level analyses and poses an issue in its correct implementation. In contrast, multilevel modelling (MLM) exploits this lack of independence to its advantage. Through outlining the notation used to specify MLM formulation, the theory of MLM is introduced. The pitfalls of ignoring the hierarchy in data are outlined. The assumptions and limitations of MLM are outlined and contrasted with those of the single-level model. The initial MLM considers the variance components model, which is further developed to illustrate random intercepts, slopes, and complex variation at both levels 1 and 2. Markov Chain Monte Carlo (MCMC) methods appropriate for estimating MLMs are introduced. Finally, complex hierarchies, such as cross-classified and multiple-membership structures are considered. These concepts are illustrated using various clinical examples. Ã‚Â© Springer Science+Business Media Dordrecht 2012.",0
https://doi.org/10.1177/1094428107300342,The Relative Validity of Inferences About Mediation as a Function of Research Design Characteristics,"Tests of assumed mediation models are common in the organizational sciences. However, the validity of inferences about mediation is a function of experimental design and the setting of a study. Regrettably, most tests of mediation have relied on the application of so-called ``causal modeling'' techniques to data from nonexperimental studies. As we demonstrate, inferences about the validity of assumed mediation models are highly suspect when they are based on the findings of nonexperimental research. One of the many reasons for this is the failure of the model being tested to be consistent with reality. Valid research-based inferences about mediation are possible. However, inferences from such tests are most likely to be valid when they are based on research that uses randomized experimental designs. Strategies for conducting research using these and other designs are described. Finally, we offer a set of conclusions and recommendations that stem from our analysis.",0
https://doi.org/10.1007/bf02296651,Model comparison of nonlinear structural equation models with fixed covariates,"Recently, it has been recognized that the commonly used linear structural equation model is inadequate to deal with some complicated substantive theory. A new nonlinear structural equation model with fixed covariates is proposed in this article. A procedure, which utilizes the powerful path sampling for computing the Bayes factor, is developed for model comparison. In the implementation, the required random observations are simulated via a hybrid algorithm that combines the Gibbs sampler and the Metropolis-Hastings algorithm. It is shown that the proposed procedure is efficient and flexible; and it produces Bayesian estimates of the parameters, latent variables, and their highest posterior density intervals as by-products. Empirical performances of the proposed procedure such as sensitivity to prior inputs are illustrated by a simulation study and a real example.",0
https://doi.org/10.2165/11594990-000000000-00000,Cost Utility of Tumour Necrosis Factor-α Inhibitors for Rheumatoid Arthritis,"Rheumatoid arthritis (RA) is a chronic autoimmune disease that affects approximately 1.5 million people in the US. Tumour necrosis factor (TNF)ÃŽÂ±; inhibitors have been shown to effectively treat and maintain remission in patients with moderately to severely active RA compared with conventional agents. The high acquisition cost of TNFÃŽÂ±; inhibitors prohibits access, which mandates economic investigations into their affordability. The lack of head-to-head comparisons between these agents makes it difficult to determine which agent is the most cost effective. Objective: This study aimed to determine which TNFÃŽÂ±; inhibitor was the most cost-effective agent for the treatment of moderately to severely active RA from the US healthcare payer's perspective. Methods: A Markov model was constructed to analyse the cost utility of five TNFÃŽÂ±; inhibitors (in combination with methotrexate [+MTX]) versus MTX monotherapy using Bayesian methods for evidence synthesis. The model had a cycle length of 3 months and an overall time horizon of 5 years. Transition probabilities and utility scores were based on published studies. Total direct costs were adjusted to year 2009 US using the medical component of the Consumer Price Index. All costs and QALYs were discounted at a rate of 3% per year. Patient response to the different strategies was determined by the American College of Rheumatology (ACR)50 criteria. One-way and probabilistic sensitivity analyses (PSAs) were performed to test the robustness of the base-case scenario. The base-case scenario was changed to ACR20 criteria (scenario 1) and ACR70 criteria (scenario 2) to determine the model's robustness. Cost-effectiveness acceptability curves and cost-effectiveness frontiers were used to estimate the cost-effectiveness probability of each treatment strategy. A willingness-to-pay (WTP) threshold was defined as three times the US GDP per capita (US139 143 per additional QALY gained). Primary results were presented as incremental cost-effective ratios (ICERs).Results: Etanercept+MTX was the most cost-effective treatment strategy in the base-case scenario up to a WTP threshold of US546 449 per QALY gained. At a WTP threshold of greater than US546 499 per QALY gained, certolizumab+MTX was the most cost-effective treatment strategy. One-way analyses showed that the base-case scenario was sensitive to the probability of achieving ACR50 criteria for MTX and each TNFÃŽÂ±; inhibitor, and changes in the utility score for patients who achieved the ACR50 criteria. With the exception of infliximab, all of the TNFÃŽÂ±; inhibitors were sensitive to drug cost per cycle. In the scenario analyses, certolizumab+MTX was a dominant treatment strategy using ACR20 criteria, but etanercept+MTX was a dominant treatment strategy using ACR70 criteria. Conclusions: Etanercept+MTX was a cost-effective treatment strategy in the base-case scenario; however, the model was sensitive to parameter uncertainties and ACR response criteria. Although Bayesian methods were used to determine transition probabilities, future studies will need to focus on head-tohead comparisons of multiple TNFÃŽÂ±; inhibitors to provide valid comparisons. Adis Ã‚Â© 2012 Springer International Publishing AG.",0
https://doi.org/10.1198/106186007x208768,Spatially Adaptive Bayesian Penalized Splines With Heteroscedastic Errors,"Penalized splines have become an increasingly popular tool for nonparametric smoothing because of their use of low-rank spline bases, which makes computations tractable while maintaining accuracy as good as smoothing splines. This article extends penalized spline methodology by both modeling the variance function nonparametrically and using a spatially adaptive smoothing parameter. This combination is needed for satisfactory inference and can be implemented effectively by Bayesian MCMC. The variance process controlling the spatially adaptive shrinkage of the mean and the variance of the heteroscedastic error process are modeled as log-penalized splines. We discuss the choice of priors and extensions of the methodology, in particular, to multivariate smoothing. A fully Bayesian approach provides the joint posterior distribution of all parameters, in particular, of the error standard deviation and penalty functions. MATLAB, C, and FORTRAN programs implementing our methodology are publicly available.",0
https://doi.org/10.1186/1471-2288-3-5,Estimation of the correlation coefficient using the Bayesian Approach and its applications for epidemiologic research,"BackgroundThe Bayesian approach is one alternative for estimating correlation coefficients in which knowledge from previous studies is incorporated to improve estimation. The purpose of this paper is to illustrate the utility of the Bayesian approach for estimating correlations using prior knowledge.MethodsThe use of the hyperbolic tangent transformation (ρ = tanh ξ and r = tanh z) enables the investigator to take advantage of the conjugate properties of the normal distribution, which are expressed by combining correlation coefficients from different studies.ConclusionsOne of the strengths of the proposed method is that the calculations are simple but the accuracy is maintained. Like meta-analysis, it can be seen as a method to combine different correlations from different studies.",0
https://doi.org/10.1017/s1049096504004585,Does identity or economic rationality drive public opinion on European integration,How do citizens respond to the reallocation of authority across levels of government? This article investigates the relative importance of economic versus identity bases of citizen support for the most far-reaching example of authority migration—European integration.,0
https://doi.org/10.1017/s0033291713002134,Randomized controlled trial and uncontrolled 9-month follow-up of an adjunctive emotion regulation group therapy for deliberate self-harm among women with borderline personality disorder,"Background Despite the clinical importance of deliberate self-harm (DSH; also referred to as non-suicidal self-injury) within borderline personality disorder (BPD), empirically supported treatments for this behavior among individuals with BPD are difficult to implement in many clinical settings. To address this limitation, a 14-week, adjunctive emotion regulation group therapy (ERGT) for DSH among women with BPD was developed. The current study examined the efficacy of this ERGT in a randomized controlled trial (RCT) and the durability of treatment gains over a 9-month uncontrolled follow-up period. Method Female out-patients with BPD and recent recurrent DSH were randomly assigned to receive this ERGT in addition to their ongoing out-patient therapy immediately ( n = 31) or after 14 weeks ( n = 30). Measures of DSH and other self-destructive behaviors, psychiatric symptoms, adaptive functioning and the proposed mechanisms of change (emotion dysregulation/avoidance) were administered pre- and post-treatment or -waitlist (to assess treatment efficacy), and 3 and 9 months post-treatment (to assess durability of treatment gains). Results Intent-to-treat (ITT) analyses ( n = 61) revealed significant effects of this ERGT on DSH and other self-destructive behaviors, emotion dysregulation, BPD symptoms, depression and stress symptoms, and quality of life. Analyses of all participants who began ERGT (across treatment and waitlist conditions; n = 51) revealed significant improvements from pre- to post-treatment on all outcomes, additional significant improvements from post-treatment to 9-month follow-up for DSH, emotion dysregulation/avoidance, BPD symptoms and quality of life, and no significant changes from post-treatment to 9-month follow-up on the other measures. Conclusions The results support the efficacy of this ERGT and the durability of treatment gains.",0
https://doi.org/10.1016/j.displa.2015.02.002,Measurement of minimum angle of resolution (MAR) for the spatial grating consisting of lines of two colors,"Abstract Minimum angle of resolution (MAR) was measured for the grating which consisted of lines of two colors selected from Red, Green, Blue, White and Black. Method of two alternative forced choice (2AFC) was used where the participants were asked to answer the direction of the color grating of the horizontal or vertical directions. From the measured psychometric function of the ratio of the correct answers, MAR which corresponded to the threshold of 75% correct answer ratio was determined. MAR of the grating patches with more than one primary color was measured to be affected by the combination of colors and to be 10–30% larger than that of the grating patch of White–Black. While the resolving power for Blue pattern had been known to be worse than those for Green and Red patterns, MAR of the grating including Blue was not always the worst.",0
https://doi.org/10.1097/ede.0b013e3181ba41cc,Mediating Various Direct-effect Approaches,,0
https://doi.org/10.1093/biostatistics/kxg040,Hierarchical bivariate time series models: a combined analysis of the effects of particulate matter on morbidity and mortality,"In this paper we develop a hierarchical bivariate time series model to characterize the relationship between particulate matter less than 10 microns in aerodynamic diameter (PM10) and both mortality and hospital admissions for cardiovascular diseases. The model is applied to time series data on mortality and morbidity for 10 metropolitan areas in the United States from 1986 to 1993. We postulate that these time series should be related through a shared relationship with PM10. At the first stage of the hierarchy, we fit two seemingly unrelated Poisson regression models to produce city-specific estimates of the log relative rates of mortality and morbidity associated with exposure to PM10 within each location. The sample covariance matrix of the estimated log relative rates is obtained using a novel generalized estimating equation approach that takes into account the correlation between the mortality and morbidity time series. At the second stage, we combine information across locations to estimate overall log relative rates of mortality and morbidity and variation of the rates across cities. Using the combined information across the 10 locations we find that a 10 microg/m3 increase in average PM10 at the current day and previous day is associated with a 0.26% increase in mortality (95% posterior interval -0.37, 0.65), and a 0.71% increase in hospital admissions (95% posterior interval 0.35, 0.99). The log relative rates of mortality and morbidity have a similar degree of heterogeneity across cities: the posterior means of the between-city standard deviations of the mortality and morbidity air pollution effects are 0.42 (95% interval 0.05, 1.18), and 0.31 (95% interval 0.10, 0.89), respectively. The city-specific log relative rates of mortality and morbidity are estimated to have very low correlation, but the uncertainty in the correlation is very substantial (posterior mean = 0.20, 95% interval -0.89, 0.98). With the parameter estimates from the model, we can predict the hospitalization log relative rate for a new city for which hospitalization data are unavailable, using that city's estimated mortality relative rate. We illustrate this prediction using New York as an example.",0
https://doi.org/10.1207/s15328007sem1202_5,Embedding IRT in Structural Equation Models: A Comparison With Regression Based on IRT Scores,"This article reviews the problems associated with using item response theory (IRT)-based latent variable scores for analytical modeling, discusses the connection between IRT and structural equation modeling (SEM)-based latent regression modeling for discrete data, and compares regression parameter estimates obtained using predicted IRT scores and standardized number-right scores in Ordinary Least Squares (OLS) regression with regression estimates obtained using the combined IRT-SEM approach. The Monte Carlo results show the expected a posteriori (EA approach is insensitive to sample size as expected but leads to appreciable attenuation in regression parameter estimates. Standardized number-right estimates and EAP regression estimates were found to be highly comparable. On the other hand, the IRT-SEM method produced smaller finite sample bias, and as expected, generated consistent regression estimates for suitably large sample sizes.",0
https://doi.org/10.1002/sim.4475,Using R and WinBUGS to fit a generalized partial credit model for developing and evaluating patient-reported outcomes assessments,"The US Food and Drug Administration recently announced the final guidelines on the development and validation of patient-reported outcomes (PROs) assessments in drug labeling and clinical trials. This guidance paper may boost the demand for new PRO survey questionnaires. Henceforth, biostatisticians may encounter psychometric methods more frequently, particularly item response theory (IRT) models to guide the shortening of a PRO assessment instrument. This article aims to provide an introduction on the theory and practical analytic skills in fitting a generalized partial credit model (GPCM) in IRT. GPCM theory is explained first, with special attention to a clearer exposition of the formal mathematics than what is typically available in the psychometric literature. Then, a worked example is presented, using self-reported responses taken from the international personality item pool. The worked example contains step-by-step guides on using the statistical languages r and WinBUGS in fitting the GPCM. Finally, the Fisher information function of the GPCM model is derived and used to evaluate, as an illustrative example, the usefulness of assessment items by their information contents. This article aims to encourage biostatisticians to apply IRT models in the re-analysis of existing data and in future research.",0
https://doi.org/10.1177/0013164411431838,Balancing Flexible Constraints and Measurement Precision in Computerized Adaptive Testing,"Managing test specifications—both multiple nonstatistical constraints and flexibly defined constraints—has become an important part of designing item selection procedures for computerized adaptive tests (CATs) in achievement testing. This study compared the effectiveness of three procedures: constrained CAT, flexible modified constrained CAT, and the weighted penalty model in balancing multiple flexible constraints and maximizing measurement precision in a fixed-length CAT. The study also addressed the effect of two different test lengths—25 items and 50 items—and of including or excluding the randomesque item exposure control procedure with the three methods, all of which were found effective in selecting items that met flexible test constraints when used in the item selection process for longer tests. When the randomesque method was included to control for item exposure, the weighted penalty model and the flexible modified constrained CAT models performed better than did the constrained CAT procedure in maintaining measurement precision. When no item exposure control method was used in the item selection process, no practical difference was found in the measurement precision of each balancing method.",0
https://doi.org/10.1175/2010jcli3503.1,Climate Sensitivity Distributions Dependence on the Possibility that Models Share Biases,"Abstract Uncertainty about biases common across models and about unknown and unmodeled feedbacks is important for the tails of temperature change distributions and thus for climate risk assessments. This paper develops a hierarchical Bayes framework that explicitly represents these and other sources of uncertainty. It then uses models’ estimates of albedo, carbon cycle, cloud, and water vapor–lapse rate feedbacks to generate posterior probability distributions for feedback strength and equilibrium temperature change. The posterior distributions are especially sensitive to prior beliefs about models’ shared structural biases: nonzero probability of shared bias moves some probability mass toward lower values for climate sensitivity even as it thickens the distribution’s positive tail. Obtaining additional models of these feedbacks would not constrain the posterior distributions as much as narrowing prior beliefs about shared biases or, potentially, obtaining feedback estimates having biases uncorrelated with those impacting climate models. Carbon dioxide concentrations may need to fall below current levels to maintain only a 10% chance of exceeding official 2°C limits on global average temperature change.",0
https://doi.org/10.1186/s12913-015-0686-6,"The effect of patient, provider and financing regulations on the intensity of ambulatory physical therapy episodes: a multilevel analysis based on routinely available data","BackgroundMany studies have found considerable variations in the resource intensity of physical therapy episodes. Although they have identified several patient- and provider-related factors, few studies have examined their relative explanatory power. We sought to quantify the contribution of patients and providers to these differences and examine how effective Swiss regulations are (nine-session ceiling per prescription and bonus for first treatments).MethodsOur sample consisted of 87,866 first physical therapy episodes performed by 3,365 physiotherapists based on referrals by 6,131 physicians. We modeled the number of visits per episode using a multilevel log linear regression with crossed random effects for physiotherapists and physicians and with fixed effects for cantons. The three-level explanatory variables were patient, physiotherapist and physician characteristics.ResultsThe median number of sessions was nine (interquartile range 6–13). Physical therapy use increased with age, women, higher health care costs, lower deductibles, surgery and specific conditions. Use rose with the share of nine-session episodes among physiotherapists or physicians, but fell with the share of new treatments. Geographical area had no influence. Most of the variance was explained at the patient level, but the available factors explained only 4% thereof. Physiotherapists and physicians explained only 6% and 5% respectively of the variance, although the available factors explained most of this variance. Regulations were the most powerful factors.ConclusionAgainst the backdrop of abundant physical therapy supply, Swiss financial regulations did not restrict utilization. Given that patient-related factors explained most of the variance, this group should be subject to closer scrutiny. Moreover, further research is needed on the determinants of patient demand.",0
https://doi.org/10.1111/cdev.12169,A Gentle Introduction to Bayesian Analysis: Applications to Developmental Research,"Bayesian statistical methods are becoming ever more popular in applied and fundamental research. In this study a gentle introduction to Bayesian analysis is provided. It is shown under what circumstances it is attractive to use Bayesian estimation, and how to interpret properly the results. First, the ingredients underlying Bayesian methods are introduced using a simplified example. Thereafter, the advantages and pitfalls of the specification of prior knowledge are discussed. To illustrate Bayesian methods explained in this study, in a second example a series of studies that examine the theoretical framework of dynamic interactionism are considered. In the Discussion the advantages and disadvantages of using Bayesian statistics are reviewed, and guidelines on how to report on Bayesian statistics are provided.",0
https://doi.org/10.3102/0013189x031003025,What Future Quantitative Social Science Research Could Look Like: Confidence Intervals for Effect Sizes,"An improved quantitative science would emphasize the use of confidence intervals (CIs), and especially CIs for effect sizes. This article reviews some definitions and issues related to developing these intervals. Confidence intervals for effect sizes are especially valuable because they facilitate meta-analytic thinking and the interpretation of intervals via comparison with the effect intervals from related prior studies. Several recommendations for the thoughtful use of such CIs are presented.",0
https://doi.org/10.1214/06-ba117a,Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper),"Various prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-$t$ family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of noninformative prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-$t$ family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-$t$ family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.",0
https://doi.org/10.3102/10769986028003195,Using Data Augmentation and Markov Chain Monte Carlo for the Estimation of Unfolding Response Models,"Unfolding response models, a class of item response theory (IRT) models that assume a unimodal item response function (IRF), are often used for the measurement of attitudes. Verhelst and Verstralen (1993) and Andrich and Luo (1993) independently developed unfolding response models by relating the observed responses to a more common monotone IRT model using a latent response model (LRM; Maris, 1995 ). This article generalizes their approach, and suggests a data augmentation scheme for the estimation of any unfolding response model. The article introduces two Markov chain Monte Carlo (MCMC) estimation procedures for the Bayesian estimation of unfolding model parameters; one is a direct implementation of MCMC, and the second utilizes the data augmentation method. We use the estimation procedure to analyze three data sets, one simulated, and two from real attitudinal surveys.",0
https://doi.org/10.1007/bf02293983,Bayesian estimation of item response curves,"Item response curves for a set of binary responses are studied from a Bayesian viewpoint of estimating the item parameters. For the two-parameter logistic model with normally distributed ability, restricted bivariate beta priors are used to illustrate the computation of the posterior mode via the EM algorithm. The procedure is illustrated by data from a mathematics test. Â© 1986 The Psychometric Society.",0
https://doi.org/10.1207/s15328007sem0904_4,A Bayesian Approach for Multigroup Nonlinear Factor Analysis,"The main purpose of this article is to develop a Bayesian approach for a general multigroup nonlinear factor analysis model. Joint Bayesian estimates of the factor scores and the structural parameters subjected to some constraints across different groups are obtained simultaneously. A hybrid algorithm that combines the Metropolis-Hastings algorithm and the Gibbs sampler is implemented to produce these joint Bayesian estimates. It is shown that this algorithm is computationally efficient. The Bayes factor approach is introduced for comparing models under various degrees of invariance across groups. The Schwarz criterion (BIC), a simple and useful approximation of the Bayes factor, is calculated on the basis of simulated observations from the Gibbs sampler. Efficiency and flexibility of the proposed Bayesian procedure are illustrated by some simulation results and a real-life example.",0
https://doi.org/10.1080/10705511.2014.936096,Detecting Individual Differences in Change: Methods and Comparisons,"This study examined and compared various statistical methods for detecting individual differences in change. Considering 3 issues including test forms (specific vs. generalized), estimation procedures (constrained vs. unconstrained), and nonnormality, we evaluated 4 variance tests including the specific Wald variance test, the generalized Wald variance test, the specific likelihood ratio (LR) variance test, and the generalized LR variance test under both constrained and unconstrained estimation for both normal and nonnormal data. For the constrained estimation procedure, both the mixture distribution approach and the alpha correction approach were evaluated for their performance in dealing with the boundary problem. To deal with the nonnormality issue, we used the sandwich standard error (SE) estimator for the Wald tests and the Satorra–Bentler scaling correction for the LR tests. Simulation results revealed that testing a variance parameter and the associated covariances (generalized) had higher power th...",0
https://doi.org/10.1093/jjfinec/nbs009,Efficient Estimation of Covariance Matrices using Posterior Mode Multiple Shrinkage,"We propose an approach to the regularization of covariance matrices that can be applied to any model for which the likelihood is available in closed form. The approach is based on using mixtures of double exponential or normal distributions as priors for correlation parameters, and on maximizing the resulting log-posterior (or penalized likelihood) using a stochastic optimization algorithm. The mixture priors are capable of clustering the correlations in several groups, each with separate mean and variance, and can therefore capture a large variety of structures besides sparsity. We apply this approach to the normal linear multivariate regression model as well as several other models that are popular in the literature but have not been previously studied for the purpose of regularization, including multivariate t, normal and t copulas, and mixture of normal distributions. Simulation experiments show the potential for large efficiency gains in estimating the density of the observations in all these models. Sizable gains are also obtained in four real applications. Copyright The Author, 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org, Oxford University Press.",0
https://doi.org/10.1177/0013164410391575,Computerized Classification Testing Under the Generalized Graded Unfolding Model,"The generalized graded unfolding model (GGUM) has been recently developed to describe item responses to Likert items (agree—disagree) in attitude measurement. In this study, the authors (a) developed two item selection methods in computerized classification testing under the GGUM, the current estimate/ability confidence interval method and the cut score/sequential probability ratio test method and (b) evaluated their accuracy and efficiency in classification through simulations. The results indicated that both methods were very accurate and efficient. The more points each item had and the fewer the classification categories, the more accurate and efficient the classification would be. However, the latter method may yield a very low accuracy in dichotomous items with a short maximum test length. Thus, if it is to be used to classify examinees with dichotomous items, the maximum text length should be increased.",0
https://doi.org/10.1167/4.12.11,A detection theory account of change detection,"Previous studies have suggested that visual short-term memory (VSTM) has a storage limit of approximately four items. However, the type of high-threshold (HT) model used to derive this estimate is based on a number of assumptions that have been criticized in other experimental paradigms (e.g., visual search). Here we report findings from nine experiments in which VSTM for color, spatial frequency, and orientation was modeled using a signal detection theory (SDT) approach. In Experiments 1-6, two arrays composed of multiple stimulus elements were presented for 100 ms with a 1500 ms ISI. Observers were asked to report in a yes/no fashion whether there was any difference between the first and second arrays, and to rate their confidence in their response on a 1-4 scale. In Experiments 1-3, only one stimulus element difference could occur (T = 1) while set size was varied. In Experiments 4-6, set size was fixed while the number of stimuli that might change was varied (T = 1, 2, 3, and 4). Three general models were tested against the receiver operating characteristics generated by the six experiments. In addition to the HT model, two SDT models were tried: one assuming summation of signals prior to a decision, the other using a max rule. In Experiments 7-9, observers were asked to directly report the relevant feature attribute of a stimulus presented 1500 ms previously, from an array of varying set size. Overall, the results suggest that observers encode stimuli independently and in parallel, and that performance is limited by internal noise, which is a function of set size.",0
https://doi.org/10.1080/01621459.1982.10477876,Round Robin Analysis of Variance via Maximum Likelihood,"Abstract A class of two-way random-effects models is presented for analyzing data that arise in a large variety of round robin designs. Examples of such designs can be found in numerous games, tournaments, and social interactions. The proposed models provide information not only about individual differences but also about the mutual contingency of the behaviors of interaction dyads. The statistical inference of the linear effects is discussed and a converging algorithm based on the EM algorithm is proposed for obtaining the maximum likelihood estimates of the covariance components. A balanced data set is analyzed using the methodology developed.",0
https://doi.org/10.1111/ajps.12212,The Externalities of Inequality: Fear of Crime and Preferences for Redistribution in Western Europe,"Why is the difference in redistribution preferences between the rich and the poor high in some countries and low in others? In this article, we argue that it has a lot to do with the rich and very little to do with the poor. We contend that while there is a general relative income effect on redistribution preferences, the preferences of the rich are highly dependent on the macrolevel of inequality. The reason for this effect is not related to immediate tax and transfer considerations but to a negative externality of inequality: crime. We will show that the rich in more unequal regions in Western Europe are more supportive of redistribution than the rich in more equal regions because of their concern with crime. In making these distinctions between the poor and the rich, the arguments in this article challenge some influential approaches to the politics of inequality.",0
https://doi.org/10.1111/j.1467-842x.2004.00363.x,INFERENCE IN DIRICHLET PROCESS MIXED GENERALIZED LINEAR MODELS BY USING MONTE CARLO EM,"Summary  Generalized linear mixed models are widely used for describing overdispersed and correlated data. Such data arise frequently in studies involving clustered and hierarchical designs. A more flexible class of models has been developed here through the Dirichlet process mixture. An additional advantage of using such mixture models is that the observations can be grouped together on the basis of the overdispersion present in the data. This paper proposes a partial empirical Bayes method for estimating all the model parameters by adopting a version of the EM algorithm. An augmented model that helps to implement an efficient Gibbs sampling scheme, under the non-conjugate Dirichlet process generalized linear model, generates observations from the conditional predictive distribution of unobserved random effects and provides an estimate of the average number of mixing components in the Dirichlet process mixture. A simulation study has been carried out to demonstrate the consistency of the proposed method. The approach is also applied to a study on outdoor bacteria concentration in the air and to data from 14 retrospective lung-cancer studies.",0
https://doi.org/10.1016/j.ecolmodel.2011.01.007,Estimation methods for nonlinear state-space models in ecology,"The use of nonlinear state-space models for analyzing ecological systems is increasing. A wide range of estimation methods for such models are available to ecologists, however it is not always clear, which is the appropriate method to choose. To this end, three approaches to estimation in the theta logistic model for population dynamics were benchmarked by Wang (2007). Similarly, we examine and compare the estimation performance of three alternative methods using simulated data. The first approach is to partition the state-space into a finite number of states and formulate the problem as a hidden Markov model (HMM). The second method uses the mixed effects modeling and fast numerical integration framework of the AD Model Builder (ADMB) open-source software. The third alternative is to use the popular Bayesian framework of BUGS. The study showed that state and parameter estimation performance for all three methods was largely identical, however with BUGS providing overall wider credible intervals for parameters than HMM and ADMB confidence intervals. © 2011 Elsevier B.V. All rights reserved.",0
https://doi.org/10.1016/j.scitotenv.2009.07.039,Temperature-dependent bioaccumulation of copper in an estuarine oyster,"Bioaccumulation models are an important and widely-used tool for assessing ecosystem health with regards to heavy metal contamination. However, these models do not usually account for the potentially significant effect of temperature-dependency in metal uptake. In this study, we explored the role of temperature-dependency in heavy metal bioaccumulation by developing and comparing two kinetic-based copper bioaccumulation models for a common estuarine oyster (Saccostrea glomerata): (i) a standard first-order model that ignores temperature effects; and (ii) a modified first-order model that uses a standard temperature function to account for the temperature-dependency of the uptake rate constant. The models were calibrated within a Bayesian framework so that parameters could be treated as random variables and any uncertainty propagated through to the model output. A 12-month biomonitoring study was carried out within Moreton Bay, Queensland, Australia to provide time-series data for the modelling. Results of the modelling showed that the two bioaccumulation models provided comparable fits of the biomonitoring field data. However, dependent on the time of year and monitoring period selected, the copper uptake rate would vary dramatically due to temperature effects, which could result in an overestimation or underestimation of the copper uptake rate. Finally by calibrating the bioaccumulation models within a Bayesian framework, these models were able to utilize prior knowledge of the model parameters as part of the calibration process and also account for the uncertainty and variability in the bioaccumulation predictions. The ability to account for uncertainty and variability is an important consideration when undertaking environmental risk assessments especially in coastal waterways where there are strong seasonal variations.",0
https://doi.org/10.1016/j.csda.2006.05.021,Robustness of the linear mixed model to misspecified error distribution,"A simulation study is performed to investigate the robustness of the maximum likelihood estimator of fixed effects from a linear mixed model when the error distribution is misspecified. Inference for the fixed effects under the assumption of independent normally distributed errors with constant variance is shown to be robust when the errors are either non-gaussian or heteroscedastic, except when the error variance depends on a covariate included in the model with interaction with time. Inference is impaired when the errors are correlated. In the latter case, the model including a random slope in addition to the random intercept is more robust than the random intercept model. The use of Cholesky residuals and conditional residuals to evaluate the fit of a linear mixed model is also discussed.",0
https://doi.org/10.2307/2533160,Matching Using Estimated Propensity Scores: Relating Theory to Practice,"Matched sampling is a standard technique in the evaluation of treatments in observational studies. Matching on estimated propensity scores comprises an important class of procedures when there are numerous matching variables. Recent theoretical work (Rubin, D. B. and Thomas, N., 1992, The Annals of Statistics 20, 1079-1093) on affinely invariant matching methods with ellipsoidal distributions provides a general framework for evaluating the operating characteristics of such methods. Moreover, Rubin and Thomas (1992, Biometrika 79, 797-809) uses this framework to derive several analytic approximations under normality for the distribution of the first two moments of the matching variables in samples obtained by matching on estimated linear propensity scores. Here we provide a bridge between these theoretical approximations and actual practice. First, we complete and refine the normal- based analytic approximations, thereby making it possible to apply these results to practice. Second, we perform Monte Carlo evaluations of the analytic results under normal and nonnormal ellipsoidal distributions, which confirm the accuracy of the analytic approximations, and demonstrate the predictable ways in which the approximations deviate from simulation results when normal assumptions are violated within the ellipsoidal family. Third, we apply the analytic approximations to real data with clearly nonellipsoidal distributions, and show that the theoretical expressions, although derived under artificial distributional conditions, produce useful guidance for practice. Our results delineate the wide range of settings in which matching on estimated linear propensity scores performs well, thereby providing useful information for the design of matching studies. When matching with a particular data set, our theoretical approximations provide benchmarks for expected performance under favorable conditions, thereby identifying matching variables requiring special treatment. After matching is complete and data analysis is at hand, our results provide the variances required to compute valid standard errors for common estimators.",0
https://doi.org/10.1111/1468-2389.00149,Impact of Assessments of Validity Generalization and Situational Specificity on the Science and Practice of Personnel Selection,"The application of meta-analysis, in particular validity generalization (VG) analysis, to the cumulative literature on the validity of selection tests has fundamentally changed the science and practice of personnel selection. VG analyses suggest that the validities of standardized tests and other structured assessments are both higher and more consistent across jobs and organizations than was previously believed. As a result, selection researchers and practitioners can draw on the research literature to make reasonably accurate forecasts about the validity and usefulness of different tests in particular applications. Distinctions between tests of validity generalization and tests of situational specificity are described, and difficulties in demonstrating that validity is constant across the different settings where tests are used are outlined.",0
https://doi.org/10.1002/j.1662-6370.2007.tb00091.x,Mixing Habermas with Bayes: Methodological and Theoretical Advances in the Study of Deliberation,"Two challenges stand out in the study of deliberation: the development of appropriate methodological tools and the development of more unified analytical frameworks. On the one hand, analysing deliberative processes is demanding and time-consuming; hence we tend to have only few and non-randomly selected cases at the group or context level. In addition, the real world of deliberation presents us with a complex matrix of nested, cross-classified, and repeated speakers. This article shows that Bayesian multi-level modelling provides an elegant way to tackle these methodological problems. On the other hand, we attempt to enrich comparative institutionalism with individual characteristics and psychologically relevant variables (such as group composition). Focusing on Swiss and German parliamentary debates we show that institutional factors - in particular, consensus systems -, the gender composition of committees and plenary sessions, and age matter for the quality of deliberation. Furthermore, we also show that partisan affiliation - government or opposition status of MPs - affects deliberative quality and can refine institutional arguments. We conclude that a multi-level approach to deliberation focusing on contextual and actor-related characteristics and using Bayesian hierarchical modelling paves the way toward a more advanced understanding - and methodological handling - of deliberative processes.",0
https://doi.org/10.1177/1548051815614321,An Assessment of the Magnitude of Effect Sizes,"This study compiles information from more than 250 meta-analyses conducted over the past 30 years to assess the magnitude of reported effect sizes in the organizational behavior (OB)/human resources (HR) literatures. Our analysis revealed an average uncorrected effect of r = .227 and an average corrected effect of ρ = .278 ( SDρ = .140). Based on the distribution of effect sizes we report, Cohen’s effect size benchmarks are not appropriate for use in OB/HR research as they overestimate the actual breakpoints between small, medium, and large effects. We also assessed the average statistical power reported in meta-analytic conclusions and found substantial evidence that the majority of primary studies in the management literature are statistically underpowered. Finally, we investigated the impact of the file drawer problem in meta-analyses and our findings indicate that the file drawer problem is not a significant concern for meta-analysts. We conclude by discussing various implications of this study for OB/HR researchers.",0
https://doi.org/10.1080/10705511.2010.489003,Confirmatory Factor Analysis of Ordinal Variables With Misspecified Models,Ordinal variables are common in many empirical investigations in the social and behavioral sciences. Researchers often apply the maximum likelihood method to fit structural equation models to ordin ...,0
https://doi.org/10.1177/0146621604265938,Implementation and Measurement Efficiency of Multidimensional Computerized Adaptive Testing,"Multidimensional adaptive testing (MAT) procedures are proposed for the measurement of several latent traits by a single examination. Bayesian latent trait estimation and adaptive item selection are derived. Simulations were conducted to compare the measurement efficiency of MAT with those of unidimensional adaptive testing and random administration. The results showed that the higher the correlation between latent traits, the more latent traits there were, and the more scoring levels there were in the items, the more efficient MAT was than the other two procedures. For tests containing multidimensional items, only MAT is applicable, whereas unidimensional adaptive testing is not. Issues in implementing MAT are discussed.",0
https://doi.org/10.1080/10705510701758166,Resampling and Distribution of the Product Methods for Testing Indirect Effects in Complex Models,"Recent advances in testing mediation have found that certain resampling methods and tests based on the mathematical distribution of 2 normal random variables substantially outperform the traditional z test. However, these studies have primarily focused only on models with a single mediator and 2 component paths. To address this limitation, a simulation was conducted to evaluate these alternative methods in a more complex path model with multiple mediators and indirect paths with 2 and 3 paths. Methods for testing contrasts of 2 effects were evaluated also. The simulation included 1 exogenous independent variable, 3 mediators and 2 outcomes and varied sample size, number of paths in the mediated effects, test used to evaluate effects, effect sizes for each path, and the value of the contrast. Confidence intervals were used to evaluate the power and Type I error rate of each method, and were examined for coverage and bias. The bias-corrected bootstrap had the least biased confidence intervals, greatest power to detect nonzero effects and contrasts, and the most accurate overall Type I error. All tests had less power to detect 3-path effects and more inaccurate Type I error compared to 2-path effects. Confidence intervals were biased for mediated effects, as found in previous studies. Results for contrasts did not vary greatly by test, although resampling approaches had somewhat greater power and might be preferable because of ease of use and flexibility.",0
https://doi.org/10.1073/pnas.17.12.684,The Distribution of Chi-Square,,0
https://doi.org/10.1111/pere.12065,A tutorial on analyzing data from speed-dating studies with heterosexual dyads,"Speed-dating studies provide a useful venue for studying attraction and other relationship initiation processes. They provide researchers with a unique opportunity to assess how much a dyadic behavior (e.g., attraction) is due to some relationship-specific adjustment or the dispositional tendencies of the two people interacting. Researchers can also investigate the impact of individual difference variables on relationship initiation outcomes. This article shows researchers how to address such interesting questions by providing an extended treatment of how to apply the social relations model (D. A. Kenny & L. La Voie, 1984) to data from a typical speed-dating study with heterosexual dyads. The statistical program SPSS is used given its widespread use and accessibility.",0
,"A multilevel approach to theory and research in organizations: Contextual, temporal, and emergent processes.",,0
https://doi.org/10.3102/10769986029004461,Experiences With Markov Chain Monte Carlo Convergence Assessment in Two Psychometric Examples,"There is an increasing use of Markov chain Monte Carlo (MCMC) algorithms for fitting statistical models in psychometrics, especially in situations where the traditional estimation techniques are very difficult to apply. One of the disadvantages of using an MCMC algorithm is that it is not straightforward to determine the convergence of the algorithm. Using the output of an MCMC algorithm that has not converged may lead to incorrect inferences on the problem at hand. The convergence is not one to a point, but that of the distribution of a sequence of generated values to another distribution, and hence is not easy to assess; there is no guaranteed diagnostic tool to determine convergence of an MCMC algorithm in general. This article examines the convergence of MCMC algorithms using a number of convergence diagnostics for two real data examples from psychometrics. Findings from this research have the potential to be useful to researchers using the algorithms. For both the examples, the number of iterations required (suggested by the diagnostics) to be reasonably confident that the MCMC algorithm has converged may be larger than what many practitioners consider to be safe.",0
https://doi.org/10.1002/sim.4031,Linear mixed models for skew-normal/independent bivariate responses with an application to periodontal disease,"Bivariate clustered (correlated) data often encountered in epidemiological and clinical research are routinely analyzed under a linear mixed model (LMM) framework with underlying normality assumptions of the random effects and within-subject errors. However, such normality assumptions might be questionable if the data set particularly exhibits skewness and heavy tails. Using a Bayesian paradigm, we use the skew-normal/independent (SNI) distribution as a tool for modeling clustered data with bivariate non-normal responses in an LMM framework. The SNI distribution is an attractive class of asymmetric thick-tailed parametric structure which includes the skew-normal distribution as a special case. We assume that the random effects follow multivariate SNI distributions and the random errors follow SNI distributions which provides substantial robustness over the symmetric normal process in an LMM framework. Specific distributions obtained as special cases, viz. the skew-t, the skew-slash and the skew-contaminated normal distributions are compared, along with the default skew-normal density. The methodology is illustrated through an application to a real data which records the periodontal health status of an interesting population using periodontal pocket depth (PPD) and clinical attachment level (CAL).",0
https://doi.org/10.1007/bf02295598,Bayesian estimation in the three-parameter logistic model,"A joint Bayesian estimation procedure for the estimation of parameters in the three-parameter logistic model is developed in this paper. Procedures for specifying prior beliefs for the parameters are given. It is shown through simulation studies that the Bayesian procedure (i) ensures that the estimates stay in the parameter space, and (ii) produces better estimates than the joint maximum likelihood procedure as judged by such criteria as mean squared differences between estimates and true values. © 1986 The Psychometric Society.",0
https://doi.org/10.1007/s00221-004-1991-1,Transcranial magnetic stimulation in the visual system. I. The psychophysics of visual suppression,"When applied over the occipital pole, transcranial magnetic stimulation (TMS) disrupts visual perception and induces phosphenes. Both the underlying mechanisms and the brain structures involved are still unclear. The first part of the study characterizes the suppressive effect of TMS by psychophysical methods. Luminance increment thresholds for orientation discrimination were determined in four subjects using an adaptive staircase procedure. Coil position was controlled with a stereotactic positioning device. Threshold values were modulated by TMS, reaching a maximum effect at a stimulus onset asynchrony (SOA) of approx. 100 ms after visual target presentation. Stronger TMS pulses increased the maximum threshold while decreasing the SOA producing the maximum effect. Slopes of the psychometric function were flattened with TMS masking by a factor of 2, compared to control experiments in the absence of TMS. No change in steepness was observed in experiments using a light flash as the mask instead of TMS. Together with the finding that at higher TMS intensities, threshold elevation occurs even with shorter SOAs, this suggests lasting inhibitory processes as masking mechanisms, contradicting the assumption that the phosphene as excitatory equivalent causes masking. In the companion contribution to this one we present perimetric measurements and phosphene forms as a function of the stimulation site in the brain and discuss the putative generator structures. Â© Springer-Verlag 2004.",0
https://doi.org/10.1016/j.ecolmodel.2014.01.011,Modeling environmental factors affecting assimilation of bomb-produced Δ14C in the North Pacific Ocean: Implications for age validation studies,"Abstract The bomb-produced radiocarbon (14C) chronometer has become the gold standard for assessing the accuracy of otolith growth ring based fish age estimates. In the northeast Pacific Ocean, nearly a dozen age validation studies have been conducted, ranging from California to Alaska, most of which have relied on a single reference chronology from the Gulf of Alaska. We developed a Bayesian hierarchical model using data sets of bomb-produced radiocarbon in the northeast Pacific Ocean and investigated whether latitude and upwelling exerts an influence on the parameters that describe the rapid Δ14C increase in marine calcium carbonates. Models incorporating both latitude and upwelling as linear covariates of a 4-parameter logistic model were favored based on ΔDIC statistics. There was substantial evidence to support that the timing of the Δ14C pulse was advanced and that total Δ14C uptake increased with increasing latitude. In contrast, increased oceanographic upwelling resulted in lower total radiocarbon input as well as a delay in the timing of the pulse curve, as was demonstrated in the upwelling dominated California Current System. Within the observed latitudinal and upwelling range of the data sets examined in this study the predicted timing of the bomb pulse curve varied by as much as 3 years, which could be misinterpreted as aging error. Our results suggest that new reference chronologies may be needed for regions of the North Pacific Ocean differing in latitude, seasonal upwelling strength and other mixing factors that can potentially change the functional form of the Δ14C curve.",0
https://doi.org/10.1108/s1475-9144(2009)0000008008,A componential analysis of leadership using the social relations model,"The social relations model (SRM; Kenny, 1994) explicitly proposes that leadership simultaneously operates at three levels of analysis: group, dyad, and individual (perceiver and target). With this model, researchers can empirically determine the amount of variance at each level as well as those factors that explain variance at these different levels. This chapter shows how the SRM can be used to address many theoretically important questions in the study of leadership and can be used to advance both the theory of and research in leadership. First, based on analysis of leadership ratings from seven studies, we find that there is substantial agreement (i.e., target variance) about who in the group is the leader and little or no reciprocity in the perceptions of leadership. We then consider correlations of leadership perceptions. In one analysis, we examine the correlations between task-oriented and socioemotional leadership. In another analysis, we examine the effect of gender and gender composition on the perception of leadership. We also explore how self-ratings of leadership differ from member perceptions of leadership. Finally, we discuss how the model can be estimated using conventional software.",0
https://doi.org/10.1080/00949655.2014.935377,Bayesian estimation with integrated nested Laplace approximation for binary logit mixed models,"In multilevel models for binary responses, estimation is computationally challenging due to the need to evaluate intractable integrals. In this paper, we investigate the performance of integrated nested Laplace approximation (INLA), a fast deterministic method for Bayesian inference. In particular, we conduct an extensive simulation study to compare the results obtained with INLA to the results obtained with a traditional stochastic method for Bayesian inference (MCMC Gibbs sampling), and with maximum likelihood through adaptive quadrature. Particular attention is devoted to the case of small number of clusters. The specification of the prior distribution for the cluster variance plays a crucial role and it turns out to be more relevant than the choice of the estimation method. The simulations show that INLA has an excellent performance as it achieves good accuracy (similar to MCMC) with reduced computational times (similar to adaptive quadrature).",0
https://doi.org/10.1214/aos/1176344611,Conjugate Priors for Exponential Families,Let $X$ be a random vector distributed according to an exponential family with natural parameter $\theta \in \Theta$. We characterize conjugate prior measures on $\Theta$ through the property of linear posterior expectation of the mean parameter of $X : E\{E(X|\theta)|X = x\} = ax + b$. We also delineate which hyperparameters permit such conjugate priors to be proper.,0
https://doi.org/10.1016/j.lindif.2015.06.003,Examining factor structures on the Test of Early Mathematics Ability — 3: A longitudinal approach,"Assessment of early mathematics skills in young children is important both psychometrically and practically. The TEMA-3 (Ginsburg & Baroody, 2003) yields a total score (Math Ability Score) and provides useful information on children's overall mathematics performance, but not about performance in specific skill areas that are targeted by state early learning guidelines and by the National Council of Teachers of Mathematics (NCTM) and Common Core State Standards for Mathematics as important content for Understanding Number in early childhood. This study examined the factor structure of the TEMA-3 to examine whether items reflect the conceptual categories in the TEMA-3 Examiner's Manual (Ginsburg & Baroody, 2003) or a priori categories reflecting Understanding Number. Longitudinal data from 389 children (182 males), mean age 54.46 months at first assessment, were obtained in fall and spring of pre-kindergarten, spring of kindergarten, and spring of 1st grade. Changes in factor structure are examined across measurement points. The a priori factor structure was found by confirmatory factor analysis to better fit the data. The numbers of factors identified increased across time. Subscale scores reflecting children's knowledge of specific mathematics concepts related to number concepts could enable teachers and parents to select activities to strengthen children's mathematical knowledge in those skill areas.",0
https://doi.org/10.1016/j.indmarman.2015.02.042,Which types of multi-stage marketing increase direct customers' willingness-to-pay? Evidence from a scenario-based experiment in a B2B setting,"The present study investigates empirically which types of multi-stage marketing by a business-to-business supplier affect its direct customers' willingness-to-pay. We conceptually develop comprehensive and selective multi-stage marketing as well as multi-stage awareness as distinct types of this concept. Their properties lead to differentiated hypotheses concerning their effects on direct customers' relationship value perceptions and perceived price importance which in turn influence willingness-to-pay. The paper also demonstrates how the direct customers' power position toward their own customers affects the effectiveness of a supplier's multi-stage marketing endeavors. We conduct a scenario-based, experimental study among 103 knowledgeable purchasing managers in customer companies to the adhesives industry, measuring willingness-to-pay, perceived relationship value, and price importance with a limit conjoint analysis. Multi-level modeling is used to test our hypotheses. The results show that comprehensive multi-stage marketing significantly increases purchasing agents' willingness-to-pay, mostly through their relationship value perception, and especially when the customer company is in a less powerful position toward its own customers. For managers, our study highlights the benefits of comprehensive multi-stage marketing over the other multi-stage marketing types.",0
https://doi.org/10.1073/pnas.0407247101,A versatile statistical analysis algorithm to detect genome copy number variation,"We have developed a versatile statistical analysis algorithm for the detection of genomic aberrations in human cancer cell lines. The algorithm analyzes genomic data obtained from a variety of array technologies, such as oligonucleotide array, bacterial artificial chromosome array, or array-based comparative genomic hybridization, that operate by hybridizing with genomic material obtained from cancer and normal cells and allow detection of regions of the genome with altered copy number. The number of probes (i.e., resolution), the amount of uncharacterized noise per probe, and the severity of chromosomal aberrations per chromosomal region may vary with the underlying technology, biological sample, and sample preparation. Constrained by these uncertainties, our algorithm aims at robustness by using a priorless maximum a posteriori estimator and at efficiency by a dynamic programming implementation. We illustrate these characteristics of our algorithm by applying it to data obtained from representational oligonucleotide microarray analysis and array-based comparative genomic hybridization technology as well as to synthetic data obtained from an artificial model whose properties can be varied computationally. The algorithm can combine data from multiple sources and thus facilitate the discovery of genes and markers important in cancer, as well as the discovery of loci important in inherited genetic disease.",0
https://doi.org/10.1214/11-sts352,Is Bayes Posterior just Quick and Dirty Confidence?,"Bayes [Philos. Trans. R. Soc. Lond. 53 (1763) 370--418; 54 296--325] introduced the observed likelihood function to statistical inference and provided a weight function to calibrate the parameter; he also introduced a confidence distribution on the parameter space but did not provide present justifications. Of course the names likelihood and confidence did not appear until much later: Fisher [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 222 (1922) 309--368] for likelihood and Neyman [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 237 (1937) 333--380] for confidence. Lindley [J. Roy. Statist. Soc. Ser. B 20 (1958) 102--107] showed that the Bayes and the confidence results were different when the model was not location. This paper examines the occurrence of true statements from the Bayes approach and from the confidence approach, and shows that the proportion of true statements in the Bayes case depends critically on the presence of linearity in the model; and with departure from this linearity the Bayes approach can be a poor approximation and be seriously misleading. Bayesian integration of weighted likelihood thus provides a first-order linear approximation to confidence, but without linearity can give substantially incorrect results.",0
https://doi.org/10.1037/a0024376,A 2 × 2 taxonomy of multilevel latent contextual models: Accuracy–bias trade-offs in full and partial error correction models.,"In multilevel modeling, group-level variables (L2) for assessing contextual effects are frequently generated by aggregating variables from a lower level (L1). A major problem of contextual analyses in the social sciences is that there is no error-free measurement of constructs. In the present article, 2 types of error occurring in multilevel data when estimating contextual effects are distinguished: unreliability that is due to measurement error and unreliability that is due to sampling error. The fact that studies may or may not correct for these 2 types of error can be translated into a 2 × 2 taxonomy of multilevel latent contextual models comprising 4 approaches: an uncorrected approach, partial correction approaches correcting for either measurement or sampling error (but not both), and a full correction approach that adjusts for both sources of error. It is shown mathematically and with simulated data that the uncorrected and partial correction approaches can result in substantially biased estimates of contextual effects, depending on the number of L1 individuals per group, the number of groups, the intraclass correlation, the number of indicators, and the size of the factor loadings. However, the simulation study also shows that partial correction approaches can outperform full correction approaches when the data provide only limited information in terms of the L2 construct (i.e., small number of groups, low intraclass correlation). A real-data application from educational psychology is used to illustrate the different approaches.",0
https://doi.org/10.1111/j.1745-3984.2009.00080.x,Conceptual Issues in Response-Time Modeling,"Two different traditions of response-time (RT) modeling are reviewed: the tradition of distinct models for RTs and responses, and the tradition of model integration in which RTs are incorporated in response models or the other way around. Several conceptual issues underlying both traditions are made explicit and analyzed for their consequences. We then propose a hierarchical modeling framework consistent with the first tradition but with the integration of their parameter structures as a second level of modeling. Two examples of the framework are presented. Also, a fundamental equation is derived which relates the RTs on test items to the speed of the test taker and the time intensity of the items. The equation serves as the core of the RT model in the framework. Finally, empirical applications of the framework demonstrating its practical value are reviewed. Test theorists have always been intrigued by the relationship between responses to test items and the time used by a test taker to produce them. Both seem indicative of the same behavior on test items. Nevertheless, their relationship appears to be difficult to conceptualize, let alone represent coherently in a statistical model. Although the computerization of educational tests has been a major impetus to the current interest in response-time (RT) modeling, it would be wrong to ignore its historical origins. One early development that has left traces in our current thinking about RTs was Woodbury’s (1951, 1963) treatment of test scores as the result of a time-dependent stochastic response process. His theory, which is summarized in Lord and Novick (1968, chap. 5), has linear axioms and theorems that are entirely parallel to those of regular classical test theory. But, more important to the scope of this article, it also lent statistical sophistication to the intuitive idea that total time and numbers of items completed are equivalent measures of the test taker’s performance (see the example in Lord & Novick, pp. 104‐105). The same idea was present in Gulliksen’s (1950, chap. 17) treatment of speed and power tests. He defined a pure speed test as a test with an unlimited number of items that are easy enough to be answered correctly. Such tests can be scored in two different ways, as (a) the total time used to complete a fixed number of items, and (b) the number of items completed in a fixed time interval. On the other hand, a pure power test was defined by him as a test with unlimited time but a fixed number of items of varying difficulty. Such tests can be scored only by counting the number of correct responses. A fundamental problem exists with respect to the asymmetry between Gulliksen’s scoring rules for speed and power tests. At a practical level, the problem becomes manifest when a test taker produces an incorrect answer on a speed test, which is not very likely for high-ability test takers and easy items but certainly possible. How should we treat such responses? And would it be fair to treat their RTs as equivalent",0
https://doi.org/10.1037/1082-989x.4.2.139,"Analyzing developmental trajectories: A semiparametric, group-based approach.","Carnegie Mellon UniversityA developmental trajectory describes the course of a behavior over age or time. Agroup-based method for identifying distinctive groups of individual trajectorieswithin the population and for profiling the characteristics of group members isdemonstrated. Such clusters might include groups of increasers. decreasers,and no changers. Suitably defined probability distributions are used to handle 3data types—count, binary, and psychometric scale data. Four capabilities are dem-onstrated: (a) the capability to identify rather than assume distinctive groups oftrajectories, (b) the capability to estimate the proportion of the population followingeach such trajectory group, (c) the capability to relate group membership probabil-ity to individual characteristics and circumstances, and (d) the capability to use thegroup membership probabilities for various other purposes such as creating profilesof group members.",0
https://doi.org/10.1007/s11222-010-9210-3,Reversible jump methods for generalised linear models and generalised linear mixed models,"A reversible jump algorithm for Bayesian model determination among generalised linear models, under relatively diffuse prior distributions for the model parameters, is proposed. Orthogonal projections of the current linear predictor are used so that knowledge from the current model parameters is used to make effective proposals. This idea is generalised to moves of a reversible jump algorithm for model determination among generalised linear mixed models. Therefore, this algorithm exploits the full flexibility available in the reversible jump method. The algorithm is demonstrated via two examples and compared to existing methods. Ã‚Â© 2010 Springer Science+Business Media, LLC.",0
https://doi.org/10.1007/978-1-4419-0056-2_5,Cultural Consensus Theory: Aggregating Expert Judgments about Ties in a Social Network,"This paper describes an approach to information aggregation called Cultural Consensus Theory (CCT). CCT is a statistical modeling approach to pooling information from informants (experts, automated sources) who share a common culture or knowledge base. Each informant responds to the same set of questions, and the goal is to estimate the consensus knowledge of the informants. CCT has become a leading methodology for determining consensus beliefs of groups in the social sciences, especially cultural and medical anthropology. The paper illustrates CCT by providing a model for aggregating expert judgments about ties in a social network. Expert sources each provide a digraph on the same set of nodes, and the CCT model is used to estimate the most likely digraph to represent their shared knowledge. Â© Springer Science + Business Media, LLC 2009.",0
https://doi.org/10.1214/aos/1034276631,Bayesian inference for causal effects in randomized experiments with noncompliance,"For most of this century, randomization has been a cornerstone of scientific experimentation, especially when dealing with humans as experimental units. In practice, however, noncompliance is relatively common with human subjects, complicating traditional theories of inference that require adherence to the random treatment assignment. In this paper we present Bayesian inferential methods for causal estimands in the presence of noncompliance, when the binary treatment assignment is random and hence ignorable, but the binary treatment received is not ignorable. We assume that both the treatment assigned and the treatment received are observed. We describe posterior estimation using EM and data augmentation algorithms. Also, we investigate the role of two assumptions often made in econometric instrumental variables analyses, the exclusion restriction and the monotonicity assumption, without which the likelihood functions generally have substantial regions of maxima. We apply our procedures to real and artificial data, thereby demonstrating the technology and showing that our new methods can yield valid inferences that differ in practically important ways from those based on previous methods for analysis in the presence of noncompliance, including intention-to-treat analyses and analyses based on econometric instrumental variables techniques. Finally, we perform a simulation to investigate the operating characteristics of the competing procedures in a simple setting, which indicates relatively dramatic improvements in frequency operating characteristics attainable using our Bayesian procedures.",0
https://doi.org/10.1080/10705510903439003,A Comparison of Approaches for the Analysis of Interaction Effects Between Latent Variables Using Partial Least Squares Path Modeling,"In social and business sciences, the importance of the analysis of interaction effects between manifest as well as latent variables steadily increases. Researchers using partial least squares (PLS) to analyze interaction effects between latent variables need an overview of the available approaches as well as their suitability. This article presents 4 PLS-based approaches: a product indicator approach (Chin, Marcolin, & Newsted, 2003), a 2-stage approach (Chin et al., 2003; Henseler & Fassott, in press), a hybrid approach (Wold, 1982), and an orthogonalizing approach (Little, Bovaird, & Widaman, 2006), and contrasts them using data related to a technology acceptance model. By means of a more extensive Monte Carlo experiment, the different approaches are compared in terms of their point estimate accuracy, their statistical power, and their prediction accuracy. Based on the results of the experiment, the use of the orthogonalizing approach is recommendable under most circumstances. Only if the orthogonalizing approach does not find a significant interaction effect, the 2-stage approach should be additionally used for significance test, because it has a higher statistical power. For prediction accuracy, the orthogonalizing and the product indicator approach provide a significantly and substantially more accurate prediction than the other two approaches. Among these two, the orthogonalizing approach should be used in case of small sample size and few indicators per construct. If the sample size or the number of indicators per construct is medium to large, the product indicator approach should be used.",0
https://doi.org/10.4324/9780203403020,Contemporary Issues in Exploratory Data Mining in the Behavioral Sciences,,0
https://doi.org/10.1214/ss/1177011137,Practical Markov Chain Monte Carlo,"Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.",0
https://doi.org/10.1093/acprof:oso/9780195339888.001.0001,Confirmatory Factor Analysis,"Measures that are reliable, valid, and can be used across diverse populations are vital to social work research, but the development of new measures is an expensive and time-consuming process. An array of existing measures can provide a cost-effective alternative, but in order to take this expedient step with confidence, researchers must ensure that the existing measure is appropriate for the new study. Confirmatory factor analysis (CFA) is one of the ways to do so. CFA has four primary functions-psychometric evaluation of measures, construct validation, testing method effects, and testing measurement invariance. This book provides an overview of the method, step-by-step guides to creating a CFA model and assessing its fit, and explanations of the requirements for using CFA, as well the book underscores the issues that are necessary to consider when using multiple groups or equivalent and multilevel models. Real-world examples, screenshots from the Amos software program that can be used to conduct CFA, and reading suggestions for each chapter form part of the book. Â© Oxford University Press, 2013.",0
https://doi.org/10.1111/biom.12269,On Bayesian estimation of marginal structural models,"The purpose of inverse probability of treatment (IPT) weighting in estimation of marginal treatment effects is to construct a pseudo-population without imbalances in measured covariates, thus removing the effects of confounding and informative censoring when performing inference. In this article, we formalize the notion of such a pseudo-population as a data generating mechanism with particular characteristics, and show that this leads to a natural Bayesian interpretation of IPT weighted estimation. Using this interpretation, we are able to propose the first fully Bayesian procedure for estimating parameters of marginal structural models using an IPT weighting. Our approach suggests that the weights should be derived from the posterior predictive treatment assignment and censoring probabilities, answering the question of whether and how the uncertainty in the estimation of the weights should be incorporated in Bayesian inference of marginal treatment effects. The proposed approach is compared to existing methods in simulated data, and applied to an analysis of the Canadian Co-infection Cohort.",0
https://doi.org/10.1093/biomet/87.2.425,Maximum likelihood estimation of generalised linear models for multivariate normal covariance matrix,"The positive-definiteness constraint is the most awkward stumbling block in modelling the covariance matrix. Pourahmadi's (1999) unconstrained parameterisation models covariance using covariates in a similar manner to mean modelling in generalised linear models. The new covariance parameters have statistical interpretation as the regression coefficients and logarithms of prediction error variances corresponding to regressing a response on its predecessors. In this paper, the maximum likelihood estimators of the parameters of a generalised linear model for the covariance matrix, their consistency and their asymptotic normality are studied when the observations are normally distributed. These results along with the likelihood ratio test and penalised likelihood criteria such as BIC for model and variable selection are illustrated using a real dataset.",0
https://doi.org/10.1111/j.1745-3984.2010.00127.x,A Strategy for Developing a Common Metric in Item Response Theory When Parameter Posterior Distributions Are Known,"Growing interest in fully Bayesian item response models begs the question: To what extent can model parameter posterior draws enhance existing practices? One practice that has traditionally relied on model parameter point estimates but may be improved by using posterior draws is the development of a common metric for two independently calibrated test forms. Before parameter estimates from independently calibrated forms can be compared, at least one form's estimates must be adjusted such that both forms share a common metric. Because this adjustment is estimated, there is a propagation of error effect when it is applied. This effect is typically ignored, which leads to overconfidence in the adjusted estimates; yet, when model parameter posterior draws are available, it may be accounted for with a simple sampling strategy. In this paper, it is shown using simulated data that the proposed sampling strategy results in adjusted posteriors with superior coverage properties than those obtained using traditional point-estimate-based methods.",0
https://doi.org/10.1007/s12187-014-9285-z,"Family, School, and Community Correlates of Children’s Subjective Well-being: An International Comparative Study","The primary purposes of this study are twofold: to examine how family, school, and community factors are related to childrenÃ¢â‚¬â„¢s subjective well-being; and to examine the patterns of the relationships between family, school, and community variables and childrenÃ¢â‚¬â„¢s subjective well-being across nations. We use the data from the pilot study of the International Survey of ChildrenÃ¢â‚¬â„¢s Well-Being for our analysis. We use multiple regression and multilevel methods in the study. We find that family, school, and community lives all significantly affect the levels of childrenÃ¢â‚¬â„¢s subjective well-being. We also find that family, school, and community lives of children are important predictors of subjective well-being even after controlling for the country-specific cultural and contextual factors. We find that the economic variables of GDP and inequality are not significant factors predicting childrenÃ¢â‚¬â„¢s subjective well-being. Rather it is the nature of childrenÃ¢â‚¬â„¢s relationships with immediate surrounding environments, such as frequency of family activities, frequency of peer activities, and neighborhood safety, are most consistently related to the levels of childrenÃ¢â‚¬â„¢s subjective well-being across the nations. Ã‚Â© 2014, Springer Science+Business Media Dordrecht.",0
https://doi.org/10.1207/s15328007sem0801_3,A Note on Estimating the Jöreskog-Yang Model for Latent Variable Interaction Using LISREL 8.3,"Kenny and Judd (1984) developed a latent variable interaction model for observed variables centered around their population means. They estimated the model by using a covariance matrix calculated from sample-mean-centered variables and products of these variables. Subsequently,Joreskog and Yang (1996) identified the need to include intercepts for the measurement and structural equations and estimated the model by using a covariance matrix calculated from noncentered observed variables and products of these variables, and means of the observed variables and the products of noncentered variables. Evidence is presented that the Joreskog-Yang procedure for estimating the Kenny-Judd interaction model is subject to severe convergence problems when implemented in LISREL8.3 and means for the indicators of the latent exogenous variables are nonzero. An alternative procedure is presented that solves the convergence problem and provides consistent estimators of the parameters.",0
https://doi.org/10.4324/9781410606747-8,Analysis of Reading Skills Development from Kindergarten through First Grade: An Application of Growth Mixture Modeling to Sequential Processes,Methods for investigating the influence of an early developmental process on a later process are discussed. Conventional growth modeling is found inadequate but a general growth mixture model is sufficiently flexible. The growth mixture model allows prediction of the later process using different trajectory classes for the early process. The growth mixture model is applied to the study of progress in reading skills among first-grade students.,0
https://doi.org/10.1093/biomet/asn048,Small area estimation when auxiliary information is measured with error,"SUMMARY Small area estimation methods typically combine direct estimates from a survey with pre dictions from a model in order to obtain estimates of population quantities with reduced mean squared error. When the auxiliary information used in the model is measured with error, using a small area estimator such as the Fay-Herriot estimator while ignoring measurement error may be worse than simply using the direct estimator. We propose a new small area estimator that accounts for sampling variability in the auxiliary information, and derive its properties, in particular show ing that it is approximately unbiased. The estimator is applied to predict quantities measured in the U.S. National Health and Nutrition Examination Survey, with auxiliary information from the U.S. National Health Interview Survey.",0
https://doi.org/10.1186/1471-2288-10-64,The importance of adjusting for potential confounders in Bayesian hierarchical models synthesising evidence from randomised and non-randomised studies: an application comparing treatments for abdominal aortic aneurysms,"Informing health care decision making may necessitate the synthesis of evidence from different study designs (e.g., randomised controlled trials, non-randomised/observational studies). Methods for synthesising different types of studies have been proposed, but their routine use requires development of approaches to adjust for potential biases, especially among non-randomised studies. The objective of this study was to extend a published Bayesian hierarchical model to adjust for bias due to confounding in synthesising evidence from studies with different designs.In this new methodological approach, study estimates were adjusted for potential confounders using differences in patient characteristics (e.g., age) between study arms. The new model was applied to synthesise evidence from randomised and non-randomised studies from a published review comparing treatments for abdominal aortic aneurysms. We compared the results of the Bayesian hierarchical model adjusted for differences in study arms with: 1) unadjusted results, 2) results adjusted using aggregate study values and 3) two methods for downweighting the potentially biased non-randomised studies. Sensitivity of the results to alternative prior distributions and the inclusion of additional covariates were also assessed.In the base case analysis, the estimated odds ratio was 0.32 (0.13,0.76) for the randomised studies alone and 0.57 (0.41,0.82) for the non-randomised studies alone. The unadjusted result for the two types combined was 0.49 (0.21,0.98). Adjusted for differences between study arms, the estimated odds ratio was 0.37 (0.17,0.77), representing a shift towards the estimate for the randomised studies alone. Adjustment for aggregate values resulted in an estimate of 0.60 (0.28,1.20). The two methods used for downweighting gave odd ratios of 0.43 (0.18,0.89) and 0.35 (0.16,0.76), respectively. Point estimates were robust but credible intervals were wider when using vaguer priors.Covariate adjustment using aggregate study values does not account for covariate imbalances between treatment arms and downweighting may not eliminate bias. Adjustment using differences in patient characteristics between arms provides a systematic way of adjusting for bias due to confounding. Within the context of a Bayesian hierarchical model, such an approach could facilitate the use of all available evidence to inform health policy decisions.",0
https://doi.org/10.1002/wics.1308,Estimation of variances and covariances for high‐dimensional data: a selective review,"Estimation of variances and covariances is required for many statistical methods such as t-test, principal component analysis and linear discriminant analysis. High-dimensional data such as gene expression microarray data and financial data pose challenges to traditional statistical and computational methods. In this paper, we review some recent developments in the estimation of variances, covariance matrix, and precision matrix, with emphasis on the applications to microarray data analysis. WIREs Comput Stat 2014, 6:255–264. doi: 10.1002/wics.1308    For further resources related to this article, please visit the WIREs website.    Conflict of interest: The authors have declared no conflicts of interest for this article.",0
https://doi.org/10.1198/108571105x58199,A simulation study on tests of hypotheses and confidence intervals for fixed effects in mixed models for blocked experiments with missing data,"This article considers the analysis of experiments with missing data from various experimental designs frequently used in agricultural research (randomized complete blocks, split plots, strip plots). We investigate the small sample properties of REML-based Wald-type F tests using linear mixed models. Several methods for approximating the denominator degrees of freedom are employed, all of which are available with the MIXED procedure of the SAS System (8.02). The simulation results show that the Kenward-Roger method provides the best control of the Type I error rate and is not inferior to other methods in terms of power. Â© 2005 American Statistical Association and the International Biometric Society.",0
https://doi.org/10.1037/pas0000004,A test of the International Personality Item Pool representation of the Revised NEO Personality Inventory and development of a 120-item IPIP-based measure of the five-factor model.,"There has been a substantial increase in the use of personality assessment measures constructed using items from the International Personality Item Pool (IPIP) such as the 300-item IPIP-NEO (Goldberg, 1999), a representation of the Revised NEO Personality Inventory (NEO PI-R; Costa & McCrae, 1992). The IPIP-NEO is free to use and can be modified to accommodate its users' needs. Despite the substantial interest in this measure, there is still a dearth of data demonstrating its convergence with the NEO PI-R. The present study represents an investigation of the reliability and validity of scores on the IPIP-NEO. Additionally, we used item response theory (IRT) methodology to create a 120-item version of the IPIP-NEO. Using an undergraduate sample (n = 359), we examined the reliability, as well as the convergent and criterion validity, of scores from the 300-item IPIP-NEO, a previously constructed 120-item version of the IPIP-NEO (Johnson, 2011), and the newly created IRT-based IPIP-120 in comparison to the NEO PI-R across a range of outcomes. Scores from all 3 IPIP measures demonstrated strong reliability and convergence with the NEO PI-R and a high degree of similarity with regard to their correlational profiles across the criterion variables (rICC = .983, .972, and .976, respectively). The replicability of these findings was then tested in a community sample (n = 757), and the results closely mirrored the findings from Sample 1. These results provide support for the use of the IPIP-NEO and both 120-item IPIP-NEO measures as assessment tools for measurement of the five-factor model.",0
https://doi.org/10.1016/0749-5978(87)90045-8,A meta-analytic study of the effects of goal setting on task performance: 1966–1984,"A meta-analytic study was conducted involving primarily published research from 1966 to 1984 and focusing on the relationship between goal-setting variables and task performance. Two major sets of studies were analyzed, those contrasting hard goals (goal difficulty) versus easy goals, and those comparing specific hard goals (goal specificity/difficulty) versus general goals, “do best” instructions, or no goal. As expected, strong support was obtained for the goal difficulty and goal specificity/difficulty components of E. A. Locke's (1968a , Organizational Behavior and Human Performance , 3 , 157–189) theory. A two-stage approach was employed to identify potential moderators of the goal difficulty and goal specificity/difficulty—performance relationships. Setting (laboratory versus field) was identified as a moderator of the relationship between goal specificity/difficulty and task performance. Two supplemental meta-analyses yielded support for the efficacy of combining specific hard goals with feedback versus specific hard goals without feedback and for participatively set goals versus assigned goal setting (when goal level is held constant), although this latter finding was interpreted as inconclusive based on the limited studies available. Implications for future research are addressed.",0
https://doi.org/10.1016/j.bbi.2006.10.010,Interleukin-6 mediates low-threshold mechanical allodynia induced by intrathecal HIV-1 envelope glycoprotein gp120,"Spinal cord glia (microglia and astrocytes) contribute to enhanced pain states. One model that has been used to study this phenomenon is intrathecal (i.t.) administration of gp120, an envelope glycoprotein of HIV-1 known to activate spinal cord glia and thereby induce low-threshold mechanical allodynia, a pain symptom where normally innocuous (non-painful) stimuli are perceived as painful. Previous studies have shown that i.t. gp120-induced allodynia is mediated via the release of the glial pro-inflammatory cytokines, tumor necrosis factor-alpha (TNF), and interleukin-1beta (IL-1). As we have recently reported that i.t. gp120 induces the release of interleukin-6 (IL-6), in addition to IL-1 and TNF, the present study tested whether this IL-6 release in spinal cord contributes to gp120-induced mechanical allodynia and/or to gp120-induced increases in TNF and IL-1. An i.t. anti-rat IL-6 neutralizing antibody was used to block IL-6 actions upon its release by i.t. gp120. This IL-6 blockade abolished gp120-induced mechanical allodynia. While the literature predominantly documents the cascade of pro-inflammatory cytokines as beginning with TNF, followed by the stimulation of IL-1, and finally TNF plus IL-1 stimulating the release of IL-6, the present findings indicate that a blockade of IL-6 inhibits the gp120-induced elevations of TNF, IL-1, and IL-6 mRNA in dorsal spinal cord, elevation of IL-1 protein in lumbar dorsal spinal cord, and TNF and IL-1 protein release into the surrounding lumbosacral cerebrospinal fluid. These results would suggest that IL-6 induces pain facilitation, and may do so in part by stimulating the production and release of other pro-inflammatory cytokines.",0
https://doi.org/10.1016/s0042-6989(97)00340-4,Forced-choice staircases with fixed step sizes: asymptotic and small-sample properties,"Visual detection and discrimination thresholds are often measured using adaptive staircases, and most studies use transformed (or weighted) up/down methods with fixed step sizes—in the spirit of Wetherill and Levitt (Br J Mathemat Statist Psychol 1965;18:1–10) or Kaernbach (Percept Psychophys 1991;49:227–229)—instead of changing step size at each trial in accordance with best-placement rules—in the spirit of Watson and Pelli (Percept Psychophys 1983;47:87–91). It is generally assumed that a fixed-step-size (FSS) staircase converges on the stimulus level at which a correct response occurs with the probabilities derived by Wetherill and Levitt or Kaernbach, but this has never been proved rigorously. This work used simulation techniques to determine the asymptotic and small-sample convergence of FSS staircases as a function of such parameters as the up/down rule, the size of the steps up or down, the starting stimulus level, or the spread of the psychometric function. The results showed that the asymptotic convergence of FSS staircases depends much more on the sizes of the steps than it does on the up/down rule. Yet, if the size Δ + of a step up differs from the size Δ − of a step down in a way that the ratio Δ − /Δ + is constant at a specific value that changes with up/down rule, then convergence percent-correct is unaffected by the absolute sizes of the steps. For use with the popular one-, two-, three- and four-down/one-up rules, these ratios must respectively be set at 0.2845, 0.5488, 0.7393 and 0.8415, rendering staircases that converge on the 77.85%-, 80.35%-, 83.15%- and 85.84%-correct points. Wetherill and Levitt's transformed up/down rules—which require Δ − /Δ + =1—and the general version of Kaernbach's weighted up/down rule—which allows any Δ − /Δ + ratio—fail to reach their presumed targets. The small-sample study showed that, even with the optimal settings, short FSS staircases (up to 20 reversals in length) are subject to some bias, and their precision is less than reasonable, but their characteristics improve when the size Δ + of a step up is larger than half the spread of the psychometric function. Practical recommendations are given for the design of efficient and trustworthy FSS staircases.",0
https://doi.org/10.1016/s0047-259x(03)00096-4,A well-conditioned estimator for large-dimensional covariance matrices,"Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.",0
https://doi.org/10.1007/s10651-012-0209-0,Space-time stick-breaking processes for small area disease cluster estimation,"We propose a space-time stick-breaking process for the disease cluster estimation. The dependencies for spatial and temporal effects are introduced by using space-time covariate dependent kernel stick-breaking processes. We compared this model with the space-time standard random effect model by checking each model’s ability in terms of cluster detection of various shapes and sizes. This comparison was made for simulated data where the true risks were known. For the simulated data, we have observed that space-time stick-breaking process performs better in detecting medium- and high-risk clusters. For the real data, county specific low birth weight incidences for the state of South Carolina for the years 1997–2007, we have illustrated how the proposed model can be used to find grouping of counties of higher incidence rate.",0
https://doi.org/10.1037/1082-989x.11.2.142,Conceptualizing and testing random indirect effects and moderated mediation in multilevel models: New procedures and recommendations.,"The authors propose new procedures for evaluating direct, indirect, and total effects in multilevel models when all relevant variables are measured at Level 1 and all effects are random. Formulas are provided for the mean and variance of the indirect and total effects and for the sampling variances of the average indirect and total effects. Simulations show that the estimates are unbiased under most conditions. Confidence intervals based on a normal approximation or a simulated sampling distribution perform well when the random effects are normally distributed but less so when they are nonnormally distributed. These methods are further developed to address hypotheses of moderated mediation in the multilevel context. An example demonstrates the feasibility and usefulness of the proposed methods.",0
https://doi.org/10.1007/978-1-4614-1800-9_17,Bayesian Statistics,"Article Outline: Glossary Definition of the Subject and Introduction The Bayesian Statistical Paradigm Three Examples Comparison with the Frequentist Statistical Paradigm Future Directions Bibliography Ã‚Â© 2012 Springer Science+Business Media, LLC. All rights reserved.",0
https://doi.org/10.1198/1085711032697,Nonlinear models for repeated measurement data: An overview and update,"Nonlinear mixed effects models for data in the form of continuous, repeated measurements on each of a number of individuals, also known as hierarchical nonlinear models, are a popular platform for analysis when interest focuses on individual-specific characteristics. This framework first enjoyed widespread attention within the statistical research community in the late 1980s, and the 1990s saw vigorous development of new methodological and computational techniques for these models, the emergence of general-purpose software, and broad application of the models in numerous substantive fields. This article presents an overview of the formulation, interpretation, and implementation of nonlinear mixed effects models and surveys recent advances and applications. Â© 2003 American Statistical Association and the International Biometric Society.",0
https://doi.org/10.1111/j.1475-6811.2010.01274.x,The measurement of reliability of social relations components from round-robin designs,"The social relations model (SRM) is a useful tool for measuring relationship effects, defined as the unique perceptions or behaviors of 2 people. The sources of variance in SRM studies are persons (actors and partners), groups, and items; the relationship effect is defined as the actor–partner interaction. By removing variance because of persons and groups, a measure of a “pure” relationship effect is obtained. In this article, generalizability theory (G Theory) is applied to estimate the reliability of SRM components from round-robin data structures. Using G Theory, reliability formulas for actor, partner, group, and relationship are developed and interpretations for the reliability estimates are provided. The authors also discuss how these formulas can be used in both planning and interpreting results from relationship research.",0
https://doi.org/10.2202/1558-3708.1609,Discovering Hidden Structures Using Mixture Models: Application to Nonlinear Time Series Processes,"This paper discuses the application of mixture models for analyzing data with hidden structure. More specifically, we are interested in situations where the structure is due to some unobserved factors. Detecting the latent structures in data is not trivial. On the other hand, since such structures could obscure the effect of the observed variables, ignoring them (e.g., using simple linear models) could result in wrong inference. To address this issue, we propose to use Dirichlet process mixtures. In this approach, the mixture identifier (i.e., the latent variable that assigns each component to a data point) acts as a surrogate for possible missing factors and provides insight into possible hidden structure in the data. An illustrative example is provided to show the usefulness of this approach. We also discuss the application of our method for analyzing time series processes that are subject to regime changes where there is no specific economic theory about the structure of the model.",0
https://doi.org/10.1080/00273170701710205,Quasi-Maximum Likelihood Estimation of Structural Equation Models With Multiple Interaction and Quadratic Effects,"In this article, a nonlinear structural equation model is introduced and a quasi-maximum likelihood method for simultaneous estimation and testing of multiple nonlinear effects is developed. The fo...",0
https://doi.org/10.1016/j.nicl.2018.01.027,An fMRI investigation of empathic processing in boys with conduct problems and varying levels of callous-unemotional traits,"The ability to empathise relies in part on using one's own affective experience to simulate the affective experience of others. This process is supported by a number of brain areas including the anterior insula (AI), anterior cingulate cortex (ACC), medial prefrontal cortex (mPFC), and the amygdala. Children with conduct problems (CP), and in particular those with high levels of callous-unemotional traits (CP/HCU) present with less empathy than their peers. They also show reduced neural response in areas supporting empathic processing when viewing other people in distress. The current study focused on identifying brain areas co-activated during affective introspection of: i) One's own emotions ('Own emotion'); ii) Others' emotions ('Other emotion'); and iii) One's feelings about others' emotions ('Feel for other') during fearful vs neutral scenarios in typically developing boys (TD; n = 31), boys with CP/HCU (n = 31), and boys with CP and low levels of CU (CP/LCU; n = 33). The conjunction analysis across conditions within the TD group revealed significant clusters of activation in the AI, ACC/mPFC, and occipital cortex. Conjunction analyses across conditions in the CP/HCU and CP/LCU groups did not identify these areas as significantly activated. However, follow-up analyses were not able to confirm statistically significant differences between groups across the whole network, and Bayes-factor analyses did not provide substantial support for either the null or alternate hypotheses. Post-hoc comparisons indicated that the lack of conjunction effects in the CP/HCU group may reflect reduced affective introspection in the 'Other emotion' and 'Feel for other' conditions, and by reduced affective introspection in the 'Own emotion' condition in the CP/LCU group. These findings provide limited and ultimately equivocal evidence for altered affective introspection regarding others in CP/HCU, and altered affective introspection for own emotions in CP/LCU, and highlight the need for further research to systematically investigate the precise nature of empathy deficits in children with CP.",0
https://doi.org/10.1002/sim.2671,Random changepoint modelling of HIV immunologic responses,"We propose a changepoint model for the analysis of longitudinal CD4 T-cell counts for HIV infected subjects following highly active antiretroviral treatment. The profile of CD4 counts for each subject follows a simple, 'broken stick' changepoint model, with random subject-specific parameters, including the changepoint. The model accounts for baseline covariates. The longitudinal CD4 records are censored at the time of the subject going off-study-treatment. This is a potentially informative drop-out mechanism, which we address by modelling it jointly with the CD4 count outcome. The drop-out model incorporates terms from the CD4 model, including the changepoint. The estimation is done in a Bayesian framework, with implementation via Markov chain Monte Carlo methods in the WinBUGS software. Model selection using DIC indicates that the data support the complex random changepoint and informative censoring model.",0
,SIMULATING RATIOS OF NORMALIZING CONSTANTS VIA A SIMPLE IDENTITY: A THEORETICAL EXPLORATION,"Let pi(w) ,i =1 , 2, be two densities with common support where each density is known up to a normalizing constant: pi(w )= qi(w)/ci .W e have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a compu- tational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: c1 c2 = E2(q1(w)α(w)) E1(q2(w)α(w)) . Here Ei denotes the expectation with respect to pi (i =1 , 2), and α is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of α. Using a simple but informa- tive example, we demonstrate that with sensible (not necessarily optimal) choices of α, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to α =1 /q2. We also introduce several generalizations of this identity for handling more compli- cated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.",0
https://doi.org/10.1093/pan/mpi030,Applying a Two-Step Strategy to the Analysis of Cross-National Public Opinion Data,"In recent years, large sets of national surveys with shared content have increasingly been used for cross-national opinion research. But scholars have not yet settled on the most flexible and efficient models for utilizing such data. We present a two-step strategy for such analysis that takes advantage of the fact that in such datasets each “cluster” (i.e., country sample) is large enough to sustain separate analysis of its internal variances and covariances. We illustrate the method by examining a puzzle of comparative electoral behavior—why does turnout decline rather than increase with the number of parties competing in an election (Blais and Dobryzynska 1998, for example)? This discussion demonstrates the ease with which a two-step strategy incorporates confounding variables operating at different levels of analysis. Technical appendices demonstrate that the two-step strategy does not lose efficiency of estimation as compared with a pooling strategy.",0
https://doi.org/10.1080/10705511.2010.489006,Residual Structures in Latent Growth Curve Modeling,"Several alternatives are available for specifying the residual structure in latent growth curve modeling. Two specifications involve uncorrelated residuals and represent the most commonly used residual structures. The first, building on repeated measures analysis of variance and common specifications in multilevel models, forces residual variances to be invariant across measurement occasions. The second, generally arising from structural equation modeling perspectives, allows residual variances to be freely estimated across occasions. Usually an afterthought, alternate specifications of the residual structure can have sizable effects on variance and covariance estimates for the intercept and slope factors (Ferron, Dailey, & Yi, 2002; Kwok, West, & Green, 2007; Sivo, Fan, & Witta, 2005), the fit of the model, and model convergence. We propose additional residual structures in latent growth modeling arising from ideas regarding growth curve reliability. These structures allow residual variances to change ac...",0
https://doi.org/10.1007/s10461-013-0620-z,Assessing Appearance-Related Disturbances in HIV-Infected Men Who have Sex with Men (MSM): Psychometrics of the Body Change and Distress Questionnaire—Short Form (ABCD-SF),"Appearance-related disturbances are common among HIV-infected MSM; however, to date, there have been limited options in the valid assessment of this construct. The aim of the current study was to assess the structural, internal, and convergent validity of the assessment of body change distress questionnaire (ABCD) and its short version. Exploratory and confirmatory factor analyses indicated that both versions fit the data well. Four subfactors were revealed measuring the following body disturbance constructs: (1) negative affect about appearance, (2) HIV health-related outcomes and stigma, (3) eating and exercise confusion, and (4) ART non-adherence. The subfactors and total scores revealed bivariate associations with salient health outcomes, including depressive symptoms, HIV sexual transmission risk behaviors, and ART non-adherence. The ABCD and its short form, offer valid means to assess varied aspects of body image disturbance among HIV-infected MSM, and require modest participant burden.",0
https://doi.org/10.1207/s15324818ame1603_4,The Use of Multilevel Item Response Theory Modeling in Applied Research: An Illustration,"Embedding item response theory (IRT) models within a multilevel modeling framework has been shown by many authors to allow better estimation of the relationships between predictor variables and IRT latent traits (Adams, Wilson, & Wu,1997). A multilevel IRT model recently proposed by Kamata (1998, 2001) yields the additional benefit of being able to accommodate data that are collected in hierarchical settings. This expansion of multilevel IRT models to three levels allows not only the dependency typically found in hierarchical data to be accommodated, but also the estimation of (a) latent traits at different levels and (b) the relationships between predictor variables and latent traits at different levels. The purpose of this article is to provide both a description and application of Kamata's 3-level IRT model. The advantages and disadvantages of using multilevel IRT models in applied research are discussed and directions for future research are given.",0
https://doi.org/10.1080/1369183x.2014.996534,Political Competition and Attitudes towards Immigration in Africa,"This paper examines the political conditions under which individuals are more likely to oppose immigration. We focus on immigration attitudes in Africa, which has been overlooked in existing literature and where there is wide variation on political factors. Drawing on existing case study literature that links exclusionary politics in that region to on-going processes of political liberalisation, we hypothesise that political competition heightens opposition to immigration by raising the salience of the issue and legitimising hostile attitudes. Using multilevel mixed-effect ordered logistic regression analysis with survey data from African countries, we find that opposition to immigration is significantly higher among individuals in countries that are more democratic, that have dominant party systems, and when the survey is conducted shortly before or after a national election. Our analysis also shows that opposition to immigration is more likely in African countries with higher levels of ethnic diversity ...",0
https://doi.org/10.1080/00949659308811554,Generalized linear mixed models a pseudo-likelihood approach,"A useful extension of the generalized linear model involves the addition of random effects andlor correlated errors. A pseudo-likelihood estimation procedure is developed to fit this class of mixed models based on an approximate marginal model for the mean response. The procedure is implemented via iterated fitting of a weighted Gaussian linear mixed model to a modified dependent variable. The approach allows for flexible specification of covariance structures for both the random effects and the correlated errors. An estimate of an additional dispersion parameter for underlying exponential family distributions is optionally automatic. The method allows for subject-specific and population-averaged inference, and the Salamander data example from McCullagh and Nelder (1989) is used to illustrate both.",0
https://doi.org/10.1086/209506,Consumer Preference for a No‐Choice Option,"The traditional focus in the decision-making literature has been on understanding how consumers choose among a given set of alternatives. The notion that preference uncertainty may lead to choice deferral when no single alternative has a decisive advantage is tested in seven studies. Building on recent research, the article shows that the decision to defer choice is influenced by the absolute difference in attractiveness among the alternatives provided and is not consistent with trade-off difficulty or the theory of search. These findings are then extended to show that choice deferral can also be modified for the same alternatives by manipulations that make them appear more similar in attractiveness, or that decrease the need to differentiate among them. The results are consistent with the notion that preference uncertainty results in a hesitation to commit to any single action since small differences in attractiveness among the alternatives are potentially reversible. Consistent with this premise, the effect of attractiveness difference on choice deferral decreased significantly when subjects were first allowed to practice making monetary trade-offs among the available alternatives.",0
https://doi.org/10.1037/1082-989x.13.1.1,Cluster randomized trials with treatment noncompliance.,"Cluster randomized trials (CRTs) have been widely used in field experiments treating a cluster of individuals as the unit of randomization. This study focused particularly on situations where CRTs are accompanied by a common complication, namely, treatment noncompliance or, more generally, intervention nonadherence. In CRTs, compliance may be related not only to individual characteristics but also to the environment of clusters individuals belong to. Therefore, analyses ignoring the connection between compliance and clustering may not provide valid results. Although randomized field experiments often suffer from both noncompliance and clustering of the data, these features have been studied as separate rather than concurrent problems. On the basis of Monte Carlo simulations, this study demonstrated how clustering and noncompliance may affect statistical inferences and how these two complications can be accounted for simultaneously. In particular, the effect of the intervention on individuals who not only were assigned to active intervention but also abided by this intervention assignment (complier average causal effect) was the focus. For estimation of intervention effects considering noncompliance and data clustering, an ML-EM estimation method was employed.",0
https://doi.org/10.1016/j.obhdp.2005.09.002,"Relationships between leader reward and punishment behavior and subordinate attitudes, perceptions, and behaviors: A meta-analytic review of existing and new research","Despite decades of research on the relationships between leader reward and punishment behaviors and employee attitudes, perceptions, and performance, no comprehensive examination of these relationships has been reported in the literature. This paper reports the results of two studies that address this issue. In the Wrst study, data from 20 new samples were gathered on the relationships between leader reward and punishment behaviors and some criterion variables that have not been examined extensively in previous research. In the second study, a meta-analytic review was conducted incorporating both the new and existing research in order to provide estimates of the bivariate relationships between these leader behaviors and a variety of employee criterion variables across 78 studies containing 118 independent samples. Results of regression analyses designed to control for the eVects of the other leader behaviors showed that: (a) the relationships between leader reward and punishment behaviors and employee attitudes, perceptions, and behaviors were more functional when the rewards or punishments were administered contingently than when they were administered non-contingently, and (b) these leader reward and punishment behaviors were strongly related to two variables (employees’ perceptions of justice and role ambiguity) that were expected to be key mediators of the relationships between these leader behaviors and the employee criterion variables. In addition, meta-analytic evidence from longitudinal studies suggested that the same leader behavior can be a cause of some employee criterion variables, and a consequence of others. Implications of these Wndings for future research in the area are discussed.",0
https://doi.org/10.3389/fpsyg.2014.00078,"Analyzing indirect effects in cluster randomized trials. The effect of estimation method, number of groups and group sizes on accuracy and power","Cluster randomized trials assess the effect of an intervention that is carried out at the group or cluster level. Ajzen's theory of planned behavior is often used to model the effect of the intervention as an indirect effect mediated in turn by attitude, norms and behavioral intention. Structural equation modeling (SEM) is the technique of choice to estimate indirect effects and their significance. However, this is a large sample technique, and its application in a cluster randomized trial assumes a relatively large number of clusters. In practice, the number of clusters in these studies tends to be relatively small, e.g., much less than fifty. This study uses simulation methods to find the lowest number of clusters needed when multilevel SEM is used to estimate the indirect effect. Maximum likelihood estimation is compared to Bayesian analysis, with the central quality criteria being accuracy of the point estimate and the confidence interval. We also investigate the power of the test for the indirect effect. We conclude that Bayes estimation works well with much smaller cluster level sample sizes such as 20 cases than maximum likelihood estimation; although the bias is larger the coverage is much better. When only 5-10 clusters are available per treatment condition even with Bayesian estimation problems occur.",1
https://doi.org/10.3758/s13428-014-0469-8,Validity and reliability of an online visual–spatial working memory task for self-reliant administration in school-aged children,"Working memory is an important predictor of academic performance, and of math performance in particular. Most working memory tasks depend on one-to-one administration by a testing assistant, which makes the use of such tasks in large-scale studies time-consuming and costly. Therefore, an online, self-reliant visual-spatial working memory task (the Lion game) was developed for primary school children (6-12 years of age). In two studies, the validity and reliability of the Lion game were investigated. The results from Study 1 (n = 442) indicated satisfactory six-week test-retest reliability, excellent internal consistency, and good concurrent and predictive validity. The results from Study 2 (n = 5,059) confirmed the results on the internal consistency and predictive validity of the Lion game. In addition, multilevel analysis revealed that classroom membership influenced Lion game scores. We concluded that the Lion game is a valid and reliable instrument for the online computerized and self-reliant measurement of visual-spatial working memory (i.e., updating).",0
https://doi.org/10.1016/j.rssm.2015.09.005,"What makes education positional? Institutions, overeducation and the competition for jobs","We compare three theoretical models for the relationship between schooling and labor market outcomes. On the one hand, the job competition model, which views education as a positional good with relative value on the labor market; on the other hand, the human capital and the social closure models, which view the value of education as absolute but differ in their expectations about returns to years of education above what required for the job. We analyze European countries using data from the European Social Survey (2010), and investigate the incidence of overeducation and the returns to years of overeducation in order to distinguish between the three theoretical models. We then relate these theoretical perspectives to institutions of the education system and of labor market coordination. Our empirical results indicate that education is more likely to function as a positional good in countries with weakly developed vocational education systems, where individuals have an incentive to acquire higher levels of education in order to stay ahead of the labor queue. However, no convincing support was found for the relationship we hypothesized between wage coordination and returns to years of overeducation.",0
https://doi.org/10.3758/bf03200763,Analyzing the dynamics of free recall: An integrative review of the empirical literature,"Relatively few experiments have measured the time course of free recall from episodic or semantic memory. Of those that have, most report that cumulative recall is a negatively accelerated exponential (or hyperbolic) function that is characterized by two properties: asymptotic recall and rate of approach to asymptote. The most common measure of free recall performance (viz., the number of items recalled) provides a reasonably good estimate of asymptotic recall if a relatively long recall period is used (which is rare), but the effect of experimental manipulations on the rate of approach to asymptote cannot be determined without timing when recall responses occur. The research reviewed herein suggests that the rate of approach to asymptote may offer an estimate of the breadth of search through long-term memory. The search in question, unlike most of those investigated in the memory literature, is unique in that it requires minutes rather than milliseconds to complete.",0
https://doi.org/10.2980/20-3-3599,Topographic influences on the distribution of white pine blister rust in<i>Pinus albicaulis</i>treeline communities,"The exotic disease white pine blister rust (caused by Cronartium ribicola) damages and kills whitebark pine (Pinus albicaulis), even in the extreme environments of alpine treeline communities. We surveyed P. albicaulis trees and tree islands for blister rust in 2 distinct alpine treeline communities in Montana, USA, and examined meso- and microtopographic factors potentially related to the climatic requirements for blister rust infection. For each of 60 sampling plots, we created high-resolution digital elevation models, derived microtopography variables, and compared these and distance to water feature variables with blister rust occurrence and intensity (number of cankers per infected tree) for every sampled P. albicaulis tree. Infection rates were 19% (of 328 sampled trees) and 24% (of 585 sampled trees) at the 2 sites. Tree island P. albicaulis had higher infection percentages than solitary trees. Using Bayesian analysis and a zero-inflated Poisson regression model, we determined that solar radiation and moisture-related variables correlated with both presence and number of blister rust cankers on P. albicaulis. Site factors that influence moisture, such as local topography, hydrology, and climate, differed between the 2 treeline study areas, which may account for the model variability.",0
https://doi.org/10.1037/0021-9010.91.1.25,Examining assumptions about item responding in personality assessment: Should ideal point methods be considered for scale development and scoring?,"The present study investigated whether the assumptions of an ideal point response process, similar in spirit to Thurstone's work in the context of attitude measurement, can provide viable alternatives to the traditionally used dominance assumptions for personality item calibration and scoring. Item response theory methods were used to compare the fit of 2 ideal point and 2 dominance models with data from the 5th edition of the Sixteen Personality Factor Questionnaire (S. Conn & M. L. Rieke, 1994). The authors' results indicate that ideal point models can provide as good or better fit to personality items than do dominance models because they can fit monotonically increasing item response functions but do not require this property. Several implications of these findings for personality measurement and personnel selection are described.",0
https://doi.org/10.1146/annurev.fluid.010908.165248,Uncertainty Quantification and Polynomial Chaos Techniques in Computational Fluid Dynamics,"The quantification of uncertainty in computational fluid dynamics (CFD) predictions is both a significant challenge and an important goal. Probabilistic uncertainty quantification (UQ) methods have been used to propagate uncertainty from model inputs to outputs when input uncertainties are large and have been characterized probabilistically. Polynomial chaos (PC) methods have found increased use in probabilistic UQ over the past decade. This review describes the use of PC expansions for the representation of random variables/fields and discusses their utility for the propagation of uncertainty in computational models, focusing on CFD models. Many CFD applications are considered, including flow in porous media, incompressible and compressible flows, and thermofluid and reacting flows. The review examines each application area, focusing on the demonstrated use of PC UQ and the associated challenges. Cross-cutting challenges with time unsteadiness and long time horizons are also discussed.",0
,Measuring the Threshold of Audibility of Temporal Decays,"A listening test system designed to measure the threshold of audibility of the decay time of low frequency resonances is described. The system employs the Parameter Estimation by Sequential Testing (PEST) technique and the listening test is conducted on calibrated headphones to remove factors associated with the listening environment. Program signal, replay level, and resonance frequency are believed to influence decay time threshold. A trial listening test shows that the system reveals realistic results but the temporal resonance modeling filter requires some adjustment to remove audible non-modal cues. Transducer limitations still affect the test at low frequencies and high replay levels. Factors for future large-scale listening tests are refined. Early indications are that temporal decay thresholds rise with reduced frequency and SPL.",0
https://doi.org/10.1007/s00180-007-0100-x,Computation of reference Bayesian inference for variance components in longitudinal studies,"Generalized linear mixed models (GLMMs) have been applied widely in the analysis of longitudinal data. This model confers two important advantages, namely, the flexibility to include random effects and the ability to make inference about complex covariances. In practice, however, the inference of variance components can be a difficult task due to the complexity of the model itself and the dimensionality of the covariance matrix of random effects. Here we first discuss for GLMMs the relation between Bayesian posterior estimates and penalized quasi-likelihood (PQL) estimates, based on the generalization of Harville's result for general linear models. Next, we perform fully Bayesian analyses for the random covariance matrix using three different reference priors, two with Jeffreys' priors derived from approximate likelihoods and one with the approximate uniform shrinkage prior. Computations are carried out via the combination of asymptotic approximations and Markov chain Monte Carlo methods. Under the criterion of the squared Euclidean norm, we compare the performances of Bayesian estimates of variance components with that of PQL estimates when the responses are non-normal, and with that of the restricted maximum likelihood (REML) estimates when data are assumed normal. Three applications and simulations of binary, normal, and count responses with multiple random effects and of small sample sizes are illustrated. The analyses examine the differences in estimation performance when the covariance structure is complex, and demonstrate the equivalence between PQL and the posterior modes when the former can be derived. The results also show that the Bayesian approach, particularly under the approximate Jeffreys' priors, outperforms other procedures. Â© 2008 Springer-Verlag.",1
https://doi.org/10.1037/a0024756,Changing emotion dynamics: Individual differences in the effect of anticipatory social stress on emotional inertia.,"Emotional inertia-the degree to which people's feelings carry over from one moment to the next-is an important property of the temporal dynamics of emotions. Thus far, emotional inertia has only been examined as a stable, trait-like characteristic. However, internal or external events (e.g., stress) may trigger changes in people's emotion dynamics, particularly among individuals with heightened sensitivity to such events. The current study investigated how emotional inertia is influenced by the anticipation of social stress, and how this effect is moderated by individual differences in depression, self-esteem, and fear of negative evaluation. We measured participants' (n = 71) emotional inertia in daily life using experience sampling before and after experimentally manipulating anticipatory social stress. Consistent with previous research, psychological maladjustment was associated with higher emotional inertia during ""normal"" daily life. However, when anticipating a socially stressful event, levels of emotional inertia dropped, particularly among participants scoring high on depression and fear of negative evaluation and low on self-esteem. These results demonstrate that emotion dynamics can vary as a function of contextual factors and identify moderators of such variation.",0
https://doi.org/10.1177/014662169101500307,A Comparison of Two Area Measures for Detecting Differential Item Functioning,"The area between two item response functions is often used as a measure of differential item functioning under item response theory. This area can be measured over either an open interval (i.e., exact) or closed interval. Formulas are presented for com puting the closed-interval signed and unsigned areas. Exact and closed-interval measures were estimated on data from a test with embedded items intentionally constructed to favor one group over another. No real differences in detection of these items were found between exact and closed-interval methods.",0
https://doi.org/10.1016/j.intmar.2010.04.003,Context-general and Context-specific Determinants of Online Satisfaction and Loyalty for Commerce and Content Sites,"We use the context-general and context-specific factor approach to examine the generalizability of satisfaction and loyalty models across two disparate online contexts—online retailing and content site browsing. Our conceptual models include the moderating effects of user-characteristic Web expertise, besides main effects of Web site factors and Web expertise. Results indicate that satisfaction and loyalty judgments are sensitive to both context-general and context-specific determinants, as well as to some interactions between them. Among context-general determinants, ease of use and customer service are positively related to satisfaction, Web community to loyalty, and Web expertise to both satisfaction and loyalty. Flow, a context-specific determinant, has a significant positive effect on satisfaction alone; security affects loyalty alone; and fulfillment/reliability and information quality are significant predictors of both satisfaction and loyalty. The results show that Web expertise moderates the effect of ease of use on satisfaction. The study contributes to marketing theory and practice by identifying satisfaction and loyalty mechanisms that are potentially generalizable across the two online contexts and providing a guiding framework for simultaneous consideration of context-specific and context-general factors in future research.",0
https://doi.org/10.1016/j.drugalcdep.2014.07.032,Familiality of addiction and its developmental mechanisms in girls,"Drug use disorders (DUD) have been theorized to share sources of risk variation with other consummatory behaviors. We hypothesized that common mechanisms exist for familial risk for DUD, physiological maturation and nutritional status in girls. Whereas body fat content must exceed a threshold to enable adrenarche and gonadarche, nutritional status may also be a behavior risk indicator. Impaired psychological self-regulation associated with DUD risk may manifest in early overeating, which could in turn accelerate reproductive maturation, resulting in a greater likelihood of affiliation with deviant/older peers and drug use.The sample consisted of families ascertained through the father who either had (N=95) or did not have (N=130) a DUD, and who had a 10-12 year old daughter and her mother available for study. Correlation, survival and path analyses of three consecutive assessments evaluated the relationships between parental DUD (number of affected parents, NAP), nutritional status (NS, subscapular skinfold measurements and body mass index), sexual maturation (Tanner stage), peer delinquency, and the daughter's lifetime DUD diagnosis.NAP was positively related to the girls' nutritional status. Longitudinal path analysis indicated mediation of the relationship between NAP and peer delinquency by sexual maturation. The relationship between NAP and sexual maturation is mediated by NS. The effect of sexual maturation at age ∼11 on the girls' DUD risk is mediated by peer delinquency.The data are consistent with mediation of intergenerational transmission of DUD risk in females by elevated nutrition, leading to accelerated maturation, and affiliation with deviant peers.",0
,"Interpreting Biomedical Science: Experiment, Evidence, and Belief","Interpreting Biomedical Science: Experiment, Evidence, and Belief discusses what can go wrong in biological science, providing an unbiased view and cohesive understanding of scientific methods, statistics, data interpretation, and scientific ethics that are illustrated with practical examples and real-life applications. Casting a wide net, the reader is exposed to scientific problems and solutions through informed perspectives from history, philosophy, sociology, and the social psychology of science. The book shows the differences and similarities between disciplines and different eras and illustrates the concept that while sound methodology is necessary for the progress of science, we cannot succeed without a right culture of doing things.Features theoretical concepts accompanied by examples from biological literatureContains an introduction to various methods, with an emphasis on statistical hypothesis testingPresents a clear argument that ties the motivations and ethics of individual scientists to the success of their scienceProvides recommendations on how to safeguard against scientific misconduct, fraud, and retractionsArms young scientists with practical knowledge that they can use every day",0
https://doi.org/10.1037/a0033820,Robust mediation analysis based on median regression.,"Mediation analysis has many applications in psychology and the social sciences. The most prevalent methods typically assume that the error distribution is normal and homoscedastic. However, this assumption may rarely be met in practice, which can affect the validity of the mediation analysis. To address this problem, we propose robust mediation analysis based on median regression. Our approach is robust to various departures from the assumption of homoscedasticity and normality, including heavy-tailed, skewed, contaminated, and heteroscedastic distributions. Simulation studies show that under these circumstances, the proposed method is more efficient and powerful than standard mediation analysis. We further extend the proposed robust method to multilevel mediation analysis, and demonstrate through simulation studies that the new approach outperforms the standard multilevel mediation analysis. We illustrate the proposed method using data from a program designed to increase reemployment and enhance mental health of job seekers.",0
https://doi.org/10.1080/00273171.2014.950719,Handling Missing Covariates in Conditional Mixture Models Under Missing at Random Assumptions,"Mixture modeling is a popular method that accounts for unobserved population heterogeneity using multiple latent classes that differ in response patterns. Psychologists use conditional mixture models to incorporate covariates into between-class and/or within-class regressions. Although psychologists often have missing covariate data, conditional mixtures are currently fit with a conditional likelihood, treating covariates as fixed and fully observed. Under this exogenous-x approach, missing covariates are handled primarily via listwise deletion. This sacrifices efficiency and does not allow missingness to depend on observed outcomes. Here we describe a modified joint likelihood approach that (a) allows inference about parameters of the exogenous-x conditional mixture even with nonnormal covariates, unlike a conventional multivariate mixture; (b) retains all cases under missing at random assumptions; (c) yields lower bias and higher efficiency than the exogenous-x approach under a variety of conditions with missing covariates; and (d) is straightforward to implement in available commercial software. The proposed approach is illustrated with an empirical analysis predicting membership in latent classes of conduct problems. Recommendations for practice are discussed.",0
https://doi.org/10.1080/10705511.2014.936340,"Growth Mixture Models Outperform Simpler Clustering Algorithms When Detecting Longitudinal Heterogeneity, Even With Small Sample Sizes","Identifying subpopulations based on longitudinal trajectories can provide new avenues to answer theoretically interesting research questions. Although many techniques to accomplish this task exist, a common method used in psychology is the growth mixture model. Recent simulations have found that this analytic method shows a decline in performance for smaller sample sizes commonly found in psychological research (Kim, 2012; Peugh & Fan, 2012). This raises this question: Are there better methods available for smaller sample sizes? Monte Carlo simulations were used to explicitly compare growth mixture models with other clustering methods, ranging on a spectrum from not informed to very informed, across different simulation conditions. To compare results both between and within analytic method, Kullback–Leibler divergence is introduced as a measure of cluster solution misfit. Results show that despite this decreased performance for smaller sample sizes, growth mixture models still outperform simpler, more gen...",0
https://doi.org/10.1016/j.jmva.2005.06.001,Bayesian modeling of several covariance matrices and some results on propriety of the posterior for linear regression with correlated and/or heterogeneous errors,"We explore simultaneous modeling of several covariance matrices across groups using the spectral (eigenvalue) decomposition and modified Cholesky decomposition. We introduce several models for covariance matrices under different assumptions about the mean structure. We consider ‘dependence’ matrices, which tend to have many parameters, as constant across groups and/or parsimoniously modeled via a regression formulation. For ‘variances’, we consider both unrestricted across groups and more parsimoniously modeled via log-linear models. In all these models, we explore the propriety of the posterior when improper priors are used on the mean and ‘variance’ parameters (and in some cases, on components of the ‘dependence’ matrices). The models examined include several common Bayesian regression models, whose propriety has not been previously explored, as special cases. We propose a simple approach to weaken the assumption of constant dependence matrices in an automated fashion and describe how to compute Bayes factors to test the hypothesis of constant ‘dependence’ across groups. The models are applied to data from two longitudinal clinical studies.",0
https://doi.org/10.1007/bf00170144,Multilevel analysis of the changing relationship between class and party in Britain 1964?1992,"Previous analyses of the changing relationship between class and vote in Britain have assumed that the British Election Surveys constitute simple random samples. In fact, they are all clustered samples, and the number of sampling points has varied substantially over time. The paper uses the statistical technique of multi-level modelling to investigate the effects of this clustering and compares the results with those obtained with single-level logistic models. In general, the multilevel and single-level models lead to similar conclusions about the changing relation between class and vote; they both show evidence of a change in the class/vote relationship over time. However, the multilevel models also show that, while the clustering does not affect conclusions about the class dealignment debate, there are other important substantive findings which emerge from the multilevel approach. First, there is clear evidence of substantial constituency differences in the intercepts; that is, individuals had very different propensities to vote Conservative in different constituencies. Second, there were also significant constituency differences in class voting, that is, constituencies seemed to vary in their level of class polarization.",0
https://doi.org/10.1002/(sici)1097-0258(20000415)19:7<975::aid-sim381>3.0.co;2-9,A robust mixed linear model analysis for longitudinal data,"This paper describes robust procedures for estimating parameters of a mixed effects linear model as applied to longitudinal data. In addition to fixed regression parameters, the model incorporates random subject effects to accommodate between-subjects variability and autocorrelation for within-subject variability. Robust empirical Bayesian estimation of subject effects is briefly discussed. As an illustration, the procedures are applied to data from a multiple sclerosis clinical trial.",0
,Bayesian Analysis for Penalized Spline Regression Using Win BUGS,"Penalized splines can be viewed as BLUPs in a mixed model framework, which allows the use of mixed model software for smoothing. Thus, software originally developed for Bayesian analysis of mixed models can be used for penalized spline regression. Bayesian inference for nonparametric models enjoys the flexibility of nonparametric models and the exact inference provided by the Bayesian inferential machinery. This paper provides a simple, yet comprehensive, set of programs for the implementation of nonparametric Bayesian analysis in WinBUGS.",0
https://doi.org/10.1080/0013791x.2013.859336,Empirical Tests of Stochastic Dominance in Capital Investment Planning: A Spreadsheet Framework,"The empirical inadequacy of direct application of stochastic dominance rules and their variants has led to the development of statistical tests for making dominance inferences. Yet very little is known about their application in capital investment planning, and the algorithms for their implementation are not readily available to the analyst who uses spreadsheets for capital investment planning. Therefore, this article develops a spreadsheet framework for conducting empirical tests of stochastic dominance when comparing alternative capital investment plans under uncertainty. It uses bootstrap and simulation methodology to compute the p-values required for making first- and second-order dominance inferences. Results from numerical examples show that empirical tests yield robust inferences when the structure of risk profiles and their integrals is such that dominance inferences by visual inspection are difficult. Therefore, analysts should model and empirically test for these relationships if they want to ma...",0
https://doi.org/10.1159/000313852,Localizing Putative Markers in Genetic Association Studies by Incorporating Linkage Disequilibrium into Bayesian Hierarchical Models,"Numerous methods have been proposed to model the association between multiple single nucleotide polymorphisms (SNPs) and a phenotype. Often these methods do not explicitly model the information regarding the linkage disequilibrium (LD) between SNPs. Furthermore, many methods shrink the SNP effects towards zero, rather than to an unknown latent gene-level effect.We outline the use of bayesian hierarchical models for gene-level analysis that incorporates LD information. Four different bayesian models, with either the inclusion or exclusion of LD information and 'shrinkage' of SNP effects to a latent gene-level effect or zero, were applied to a pharmacogenomic study and simulation STUDY.We observed that the inclusion of LD information resulted in increased precision in the SNP parameter estimates. The simulation study also demonstrated that the bayesian models were able to determine the simulated 'causative' variant more often with less false-positive associations as compared to commonly used multi-SNP analysis methods.Incorporating LD information in the analysis of multiple SNPs results in more precise estimates of SNP effects. In addition, the bayesian models are able to isolate the simulated 'causative' variant more often than commonly used regression modeling methods.",0
,Racial differences in treatment preferences among lupus patients: a two-site study.,"To identify the demographic, clinical and psychosocial characteristics associated with racial differences in willingness to receive cyclophosphamide (CYC) or participate in a research clinical trial (RCT) among patients with systemic lupus erythematosus (SLE).Data from 163 African-American (AA) and 180 white (WH) SLE patients were evaluated. Structured interviews and chart reviews were conducted to determine treatment preferences in hypothetical situations and identify variables that may affect preferences. Logistic regression models were performed to evaluate the relationship between patient preferences and race, adjusted for patient characteristics.Among patients who had never received CYC (n=293), 62.9% AAs compared to 87.6% WHs were willing to receive the medication (p<0.001). This difference persisted (OR 0.37 [95% CI, 0.16-0.87]) after adjusting for socio-demographics, clinical characteristics, and perceptions about CYC and physicians. Income and higher perception of CYC effectiveness were other determinants of willingness to receive CYC. Among patients who had never participated in an RCT (n=326), 64.9% AAs compared to 84.3% WHs were willing to do so (p<0.001). This difference persisted (OR 0.41 [95% CI, 0.20-0.83]) after adjusting for socio-demographics, clinical context and patients' perceptions of physicians. SLE damage score, number of immunosuppressive medications and higher trust in physicians were also independently associated with willingness to participate in an RCT.Race remains an independent determinant of treatment preferences after adjustment for income, medications, medication efficacy expectations and trust in physicians. While some factors related to racial differences in preferences are relatively fixed, others that may alleviate these differences also exist, including medication beliefs and provider trust.",0
https://doi.org/10.1111/j.1460-9568.2005.04379.x,Involvement of spinal cord nuclear factor κB activation in rat models of proinflammatory cytokine-mediated pain facilitation,"Proinflammatory cytokines, such as interleukin-1beta and tumour necrosis factor-alpha, are released by activated glial cells in the spinal cord and play a major role in pain facilitation. These cytokines exert their actions, at least partially, through the activation of the transcription factor, nuclear factor kappaB (NF-kappaB). In turn, NF-kappaB regulates the transcription of many inflammatory mediators, including cytokines. We have previously shown that intrathecal injection of the human immunodeficiency virus-1 (HIV-1) envelope glycoprotein, gp120, induces mechanical allodynia via the release of proinflammatory cytokines. Here, we investigated whether NF-kappaB is involved in gp120-induced pain behaviour in Sprague-Dawley rats. Intrathecal administration of NF-kappaB inhibitors, pyrrolidinedithiocarbamate (PDTC) and SN50, prior to gp120 partially attenuated gp120-induced allodynia. In addition, PDTC delayed and reversed allodynia in a model of neuropathic pain induced by sciatic nerve inflammation. These observations suggest that intrathecal gp120 may lead to activation of NF-kappaB within the spinal cord. To reveal NF-kappaB activation, we assessed inhibitory factor kappaBalpha (IkappaBalpha) mRNA expression by in situ hybridization, as NF-kappaB activation up-regulates IkappaBalpha gene expression as part of an autoregulatory feedback loop. No or low levels of IkappaBalpha mRNA were detected in the lumbar spinal cord of vehicle-injected rats, whereas IkappaBalpha mRNA expression was markedly induced in the spinal cord following intrathecal gp120 in predominantly astrocytes and endothelial cells. Moreover, IkappaBalpha mRNA expression positively correlated with proinflammatory cytokine protein levels in lumbosacral cerebrospinal fluid. Together, these results demonstrate that spinal cord NF-kappaB activation is involved, at least in part, in exaggerated pain states.",0
https://doi.org/10.1002/sim.2328,A comparison of propensity score methods: a case-study estimating the effectiveness of post-AMI statin use,"There is an increasing interest in the use of propensity score methods to estimate causal effects in observational studies. However, recent systematic reviews have demonstrated that propensity score methods are inconsistently used and frequently poorly applied in the medical literature. In this study, we compared the following propensity score methods for estimating the reduction in all-cause mortality due to statin therapy for patients hospitalized with acute myocardial infarction: propensity-score matching, stratification using the propensity score, covariate adjustment using the propensity score, and weighting using the propensity score. We used propensity score methods to estimate both adjusted treated effects and the absolute and relative risk reduction in all-cause mortality. We also examined the use of statistical hypothesis testing, standardized differences, box plots, non-parametric density estimates, and quantile-quantile plots to assess residual confounding that remained after stratification or matching on the propensity score. Estimates of the absolute reduction in 3-year mortality ranged from 2.1 to 4.5 per cent, while estimates of the relative risk reduction ranged from 13.3 to 17.0 per cent. Adjusted estimates of the reduction in the odds of 3-year death varied from 15 to 24 per cent across the different propensity score methods.",0
https://doi.org/10.1177/0272989x07302132,Cost-Effectiveness Analysis Using Data from Multinational Trials: The Use of Bivariate Hierarchical Modeling,"Health care cost-effectiveness analysis (CEA) often uses individual patient data (IPD) from multinational randomized controlled trials. Although designed to account for between-patient sampling variability in the clinical and economic data, standard analytical approaches to CEA ignore the presence of between-location variability in the study results. This is a restrictive limitation given that countries often differ in factors that could affect the results of CEAs, such as the availability of health care resources, their unit costs, clinical practice, and patient case mix. The authors advocate the use of Bayesian bivariate hierarchical modeling to analyze multinational cost-effectiveness data. This analytical framework explicitly recognizes that patient-level costs and outcomes are nested within countries. Using real-life data, the authors illustrate how the proposed methods can be applied to obtain (a) more appropriate estimates of overall cost-effectiveness and associated measure of sampling uncertainty compared to standard CEA and (b) country-specific cost-effectiveness estimates that can be used to assess the between-location variability of the study results while controlling for differences in country-specific and patientspecific characteristics. It is demonstrated that results from standard CEA using IPD from multinational trials display a large degree of variability across the 17 countries included in the analysis, producing potentially misleading results. In contrast, ``shrinkage estimates'' obtained from the modeling approach proposed here facilitate the appropriate quantification of country-specific cost-effectiveness estimates while weighting the results based on the level of information available within each country. The authors suggest that the methods presented here represent a general framework for the analysis of economic data collected from different locations.",0
https://doi.org/10.1177/147078530704900306,Predicting Purchase Decisions with Different Conjoint Analysis Methods: A Monte Carlo Simulation,"To forecast purchase decisions, different conjoint-based approaches have been discussed. Nevertheless, there is no clear evidence on which variant performs best. This study uses a Monte Carlo simulation to systematically compare different choice-based models and different models of a modified traditional conjoint variant, namely limit conjoint analysis (LCA), which allows for integrating choice decisions. All models compared, except the aggregate logit model, are rather robust. However, the hierarchical Bayes approaches perform best with both choice-based and limit data. The limit models are more efficient than those based on choice data. Thus, to predict purchase decision in practice, the limit hierarchical Bayes model should be considered first. © 2007 The Market Research Society. https://www.mrs.org.uk/ijmr_article/article/85657",0
https://doi.org/10.4054/demres.2014.30.11,Another 'futile quest'? A simulation study of Yang and Land's Hierarchical Age-Period-Cohort model,"Background: Whilst some argue that a solution to the age-period-cohort (APC) 'identification problem' is impossible, numerous methodological solutions have been proposed, including Yang and Land's Hierarchical-APC (HAPC) model: a multilevel model considering periods and cohorts as cross-classified contexts in which individuals exist. Objective: To assess the assumptions made by the HAPC model, and the situations in which it does and does not work. Methods: Simulation study. Simulation scenarios assess the effect of (a) cohort trends in the Data Generating Process (DGP) (compared to only random variation), and (b) grouping cohorts (in both DGP and fitted model). Results: The model only works if either (a) we can assume that there are no linear (or non-linear) trends in periods or cohorts, (b) we control any cohort trend in the model's fixed part and assume there is no period trend, or (c) we group cohorts in such a way that they exactly match the groupings in the (unknown) DGP. Otherwise, the model can arbitrarily reapportion APC effects, radically impacting interpretation. Conclusions: Since the purpose of APC analysis is often to ascertain the presence of period and/or cohort trends, and since we rarely have solid (if any) theory regarding cohort groupings, there are few circumstances in which this model achieves what Yang and Land claim it can. The results bring into question findings of several published studies using the HAPC model. However, the structure of the model remains a conceptual advance that is useful when we can assume the DGP has no period trends.",0
https://doi.org/10.1002/sim.2704,Evidence-based sample size calculations based upon updated meta-analysis,"Meta-analyses of randomized controlled trials (RCTs) provide the highest level of evidence regarding the effectiveness of interventions and as such underpin much of evidence-based medicine. Despite this, meta-analyses are usually produced as observational by-products of the existing literature, with no formal consideration of future meta-analyses when individual trials are being designed. Basing the sample size of a new trial on the results of an updated meta-analysis which will include it, may sometimes make more sense than powering the trial in isolation. A framework for sample size calculation for a future RCT based on the results of a meta-analysis of the existing evidence is presented. Both fixed and random effect approaches are explored through an example. Bayesian Markov Chain Monte Carlo simulation modelling is used for the random effects model since it has computational advantages over the classical approach. Several criteria on which to base inference and hence power are considered. The prior expectation of the power is averaged over the prior distribution for the unknown true treatment effect. An extension to the framework allowing for consideration of the design for a series of new trials is also presented. Results suggest that power can be highly dependent on the statistical model used to meta-analyse the data and even very large studies may have little impact on a meta-analysis when there is considerable between study heterogeneity. This raises issues regarding the appropriateness of the use of random effect models when designing and drawing inferences across a series of studies.",0
https://doi.org/10.1207/s15327906mbr3701_04,Analytic Estimation of Standard Error and Confidence Interval for Scale Reliability,"An analytic approach to standard error and confidence interval estimation of scale reliability with fixed congeneric measures is proposed that is based on a generally applicable estimator stability evaluation procedure, the delta method (e.g., Ogasawara, 1999). The approach complements wide-spread point estimation of composite reliability in behavioral scale construction and development, and can be used to evaluate precision of estimates and plausible ranges for reliability of multiple-component instruments in studied populations. The method is illustrated by means of a numerical example.",0
https://doi.org/10.5705/ss.2009.319,Bayesian designs for hierarchical linear models,"Two Bayesian optimal design criteria for hierarchical linear models are discussed – the ?? criterion for the estimation of individual-level parameters ?, and the ?? criterion for the estimation of hyperparameters ?. We focus on a specific case in which all subjects receive the same set of treatments and in which the covariates are independent of treatments. We obtain the explicit structure of ??- and ??- optimal continuous (approximate) designs for the case of independent random effects, and for some special cases of correlated random effects. Through examples and simulations, we compare ??- and ??-optimal designs under more gen- eral scenarios of correlated random effects. While orthogonal designs are often ??-optimal even when the random effects are correlated, ??-optimal designs tend to be nonorthogonal and unbalanced. In our study of the robustness of ??- and ??-optimal designs, both types of designs are found to be insensitive to various specifications of the response errors and the vari- ances of the random effects, but sensitive to the specifications of the signs of the correlations of the random effects.",0
https://doi.org/10.1002/9780470575789,Surveying Cultures,"Surveying Cultures uniquely employs techniques rooted in survey methodology to discover cultural patterns in social science research. Examining both classical and emerging methods that are used to survey and assess differing norms among populations, the book successfully breaks new ground in the field, introducing a theory of measurement for ethnographic studies that employs the consensus-as-culture model. The book begins with a basic overview of cross-cultural measurement of sentiments and presents innovative and sophisticated analyses of measurement issues and of homogeneity among respondents. Subsequent chapters explore topics that are at the core of successful data collection and analysis in culture studies, including: The role of bipolar scales and Internet data collection in measuring sentiments. Key methodological variables that determine the quality of quantitative data, including measurement errors, validity, and reliability. New approaches to reliability and several new methods of assessing a respondent's degree of inculcation into group culture. Sampling, coverage, nonresponse, and measurement errors, with an in-depth discussion of their occurrence in culture surveys, their impact assessments, and how current measurement techniques are constructed to help prevent these kinds of errors. Common problems often encountered in the acquisition and communication of data, including identifying error variances, interpreting gender differences in responses, and defining the difference between cultures and subcultures. Throughout the book, each topic is accompanied by a review of related methodological literature. For many of the presented concepts, the author includes a formal analysis of the related issues in measuring cultural norms and reports on analyses. Each chapter concludes with an organized list of major findings as well as an insightful outline of specific recommendations regarding practical problems in culture studies. Surveying Cultures serves as a valuable supplemental book to courses on survey and research methods at the upper-undergraduate and graduate levels. It is also an excellent reference for researchers in the fields of sociology, anthropology, psychology, and political science. Â© 2010 John Wiley & Sons, Inc.",0
https://doi.org/10.1080/00273171.2017.1292893,"Bayesian Modal Estimation of the Four-Parameter Item Response Model in Real, Realistic, and Idealized Data Sets","In this study, we explored item and person parameter recovery of the four-parameter model (4PM) in over 24,000 real, realistic, and idealized data sets. In the first analyses, we fit the 4PM and three alternative models to data from three Minnesota Multiphasic Personality Inventory-Adolescent form factor scales using Bayesian modal estimation (BME). Our results indicated that the 4PM fits these scales better than simpler item Response Theory (IRT) models. Next, using the parameter estimates from these real data analyses, we estimated 4PM item parameters in 6,000 realistic data sets to establish minimum sample size requirements for accurate item and person parameter recovery. Using a factorial design that crossed discrete levels of item parameters, sample size, and test length, we also fit the 4PM to an additional 18,000 idealized data sets to extend our parameter recovery findings. Our combined results demonstrated that 4PM item parameters and parameter functions (e.g., item response functions) can be accurately estimated using BME in moderate to large samples (N ⩾ 5, 000) and person parameters can be accurately estimated in smaller samples (N ⩾ 1, 000). In the supplemental files, we report annotated [Formula: see text] code that shows how to estimate 4PM item and person parameters in [Formula: see text] (Chalmers, 2012 ).",0
https://doi.org/10.1186/s12868-015-0228-5,Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives,"In neuroscience, experimental designs in which multiple measurements are collected in the same research object or treatment facility are common. Such designs result in clustered or nested data. When clusters include measurements from different experimental conditions, both the mean of the dependent variable and the effect of the experimental manipulation may vary over clusters. In practice, this type of cluster-related variation is often overlooked. Not accommodating cluster-related variation can result in inferential errors concerning the overall experimental effect.The exact effect of ignoring the clustered nature of the data depends on the effect of clustering. Using simulation studies we show that cluster-related variation in the experimental effect, if ignored, results in a false positive rate (i.e., Type I error rate) that is appreciably higher (up to ~20-~50 %) than the chosen [Formula: see text]-level (e.g., [Formula: see text] = 0.05). If the effect of clustering is limited to the intercept, the failure to accommodate clustering can result in a loss of statistical power to detect the overall experimental effect. This effect is most pronounced when both the magnitude of the experimental effect and the sample size are small (e.g., ~25 % less power given an experimental effect with effect size d of 0.20, and a sample size of 10 clusters and 5 observations per experimental condition per cluster).When data is collected from a research design in which observations from the same cluster are obtained in different experimental conditions, multilevel analysis should be used to analyze the data. The use of multilevel analysis not only ensures correct statistical interpretation of the overall experimental effect, but also provides a valuable test of the generalizability of the experimental effect over (intrinsically) varying settings, and a means to reveal the cause of cluster-related variation in experimental effect.",0
https://doi.org/10.1002/env.613,Semiparametric spatio-temporal frailty modeling,"Recent developments in GIS have encouraged health science databases to incorporate geographical information about the subjects under study. Such databases have in turn generated interest among statisticians to develop and analyze models that account for spatial clustering and variation. In this article we develop a semiparametric (Cox) hierarchical Bayesian frailty model for capturing spatio-temporal heterogeneity in the survival patterns of women diagnosed with breast cancer in Iowa. In the absence of appropriate surrogates for standards of treatments and health care for cancer patients in the different counties, epidemiologists and health-care professionals are interested in discerning spatial patterns in survival from breast cancer that might be present among the counties. In addition, it is naturally of interest to see if the counties show discernible temporal trends over the years. The SEER (Surveillance Epidemiology and End Results) database from the National Cancer Institute (NCI) provides data on a cohort of breast cancer patients observed progressively through time, spanning 26 years. We implement our hierarchical spatio-temporal models on data extracted from the database for the 99 counties of Iowa. Our results suggest log-relative hazards that are generally flat initially, but steadily decrease for more recent years. Copyright © 2003 John Wiley & Sons, Ltd.",0
https://doi.org/10.1080/01621459.1991.10475080,"Some Bayesian and Non-Bayesian Procedures for the Analysis of Comparative Experiments and for Small-Area Estimation: Computational Aspects, Frequentist Properties, and Relationships","Abstract The estimation of a treatment contrast from experimental data and the estimation of a small-area mean are special cases of the prediction of the realization of a linear combination of fixed and random effects in a possibly unbalanced two-part mixed linear model. In this article a Bayesian approach to point and interval prediction is presented and its computational requirements are examined. Differences between the Bayesian approach and the traditional (classical) approach are discussed in general terms and, in addition, in terms of two examples taken from the literature: (1) the comparison of drug formulations in a bioavailability trial (Westlake) and (2) the estimation of corn-crop areas using satellite data (Battese, Harter, and Fuller). Some deficiencies in the classical approach are pointed out, and the Bayesian approach is considered from a frequentist perspective. It is shown, via a Monte Carlo study, that, for certain (noninformative) choices of the prior distribution, the frequentist prop...",0
https://doi.org/10.5465/amj.2006.0241,Trust in Typical and High-Reliability Contexts: Building and Reacting to Trust among Firefighters,We develop theory that distinguishes trust among employees in typical task contexts (marked by low levels of situational unpredictability and danger) from trust in “high-reliability” task contexts ...,0
https://doi.org/10.1348/000711004849295,Estimation of maximal reliability: A note on a covariance structure modelling approach,"A one-step covariance structure analysis procedure for estimation of maximal reliability of linear composites with congeneric measures is outlined. The approach is readily employed within a single modelling session using popular covariance structure analysis software, and permits simultaneous estimation of the optimal measure weights with standard errors. The method is illustrated by a numerical example.",0
https://doi.org/10.1080/17437199.2017.1343676,An introduction to Bayesian statistics in health psychology,"The aim of the current article is to provide a brief introduction to Bayesian statistics within the field of health psychology. Bayesian methods are increasing in prevalence in applied fields, and they have been shown in simulation research to improve the estimation accuracy of structural equation models, latent growth curve (and mixture) models, and hierarchical linear models. Likewise, Bayesian methods can be used with small sample sizes since they do not rely on large sample theory. In this article, we discuss several important components of Bayesian statistics as they relate to health-based inquiries. We discuss the incorporation and impact of prior knowledge into the estimation process and the different components of the analysis that should be reported in an article. We present an example implementing Bayesian estimation in the context of blood pressure changes after participants experienced an acute stressor. We conclude with final thoughts on the implementation of Bayesian statistics in health psychology, including suggestions for reviewing Bayesian manuscripts and grant proposals. We have also included an extensive amount of online supplementary material to complement the content presented here, including Bayesian examples using many different software programmes and an extensive sensitivity analysis examining the impact of priors.",0
https://doi.org/10.1037/0033-2909.86.3.446,Group reaction time distributions and an analysis of distribution statistics.,"A method of obtaining an average reaction time distribution for a group of subjects is described. The method is particularly useful for cases in which data from many subjects are available but there are only 10-20 reaction time observations per subject cell. Essentially, reaction times for each subject are organized in ascending order, and quantiles are calculated. The quantiles are then averaged over subjects to give group quantiles (cf. Vincent learning curves). From the group quantiles, a group reaction time distribution can be constructed. It is shown that this method of averaging is exact for certain distributions (i.e., the resulting distribution belongs to the same family as the individual distributions). Furthermore, Monte Carlo studies and application of the method to the combined data from three large experiments provide evidence that properties derived from the group reaction time distribution are much the same as average properties derived from the data of individual subjects. This article also examines how to quantitatively describe the shape of reaction time distributions. The use of moments and cumulants as sources of information about distribution shape is evaluated and rejected because of extreme dependence on long, outlier reaction times. As an alternative, the use of explicit distribution functions as approximations to reaction time distributions is considered.",0
https://doi.org/10.1016/j.jbtep.2011.12.002,An experimental investigation of the role of negative mood in worry: The role of appraisals that facilitate systematic information processing,"Negative mood is associated with increased worry levels, and also with deployment of a systematic information processing style. An experimental study assessed the potential role of systematic information processing in mediating the facilitative effect of negative mood on worry (e.g. Johnston & Davey, 1997).Participants underwent appropriate vignette-based mood inductions (negative, neutral, and cognitive priming). Participants completed visual analogue scales measuring variables that reflect a raised processing sufficiency threshold and are known to increase systematic processing (responsibility, accountability, desire for control, and need for cognition), a measure of 'as many as can' worry stop rule deployment, and two measures of worry (the catastrophising interview and the Penn State Worry Questionnaire, PSWQ, Meyer, Miller, Metzger, & Borkovec, 1990).Experimentally-induced negative mood facilitated the endorsement of cognitive appraisals known to increase systematic as opposed to heuristic information processing. In addition, a meditational analysis showed that the systematic processing facilitators measure together with a measure of 'as many as can' worry stop rule deployment fully mediated the relationship between negative mood and a measure of worry frequency (PSWQ).Future studies should develop and validate direct measures of systematic processing.Similarities and differences between systematic processing and chronic worrying as effortful forms of information processing are discussed, and a role for systematic processing as an information processing style relevant to understanding worrisome thought is described.",0
https://doi.org/10.1093/esr/jcq071,Education Policy and Educational Inequality--Evidence from the Swiss Laboratory,"This article examines how the different education policies in the subnational units of Switzerland - the cantons - affect educational inequality. The article builds on previous research arguing that in order to properly evaluate education policy and its outcomes in decentralized countries, regional disparities must been taken into account. Switzerland has one of the most decentralized education systems in the world and is thus an exemplary case for a subnational analysis. Applying multilevel models the article illustrates that small class size, public investments in education as well as the mobility between ability groups are related to a lower degree of educational inequality. In contrast, longer schooldays strengthen inequality in education, implying that school instruction in the Swiss cantons does not provide equal opportunities to pupils irrespective of social class. Ã‚Â© 2011 The Author. Published by Oxford University Press. All rights reserved.",0
https://doi.org/10.1007/bf02296397,Latent variable modeling in heterogeneous populations,"Common applications of latent variable analysis fail to recognize that data may be obtained from several populations with different sets of parameter values. This article describes the problem and gives an overview of methodology that can address heterogeneity. Artificial examples of mixtures are given, where if the mixture is not recognized, strongly distorted results occur. MIMIC structural modeling is shown to be a useful method for detecting and describing heterogeneity that cannot be handled in regular multiple-group analysis. Other useful methods instead take a random effects approach, describing heterogeneity in terms of random parameter variation across groups. These random effects models connect with emerging methodology for multilevel structural equation modeling of hierarchical data. Examples are drawn from educational achievement testing, psychopathology, and sociology of education. Estimation is carried out by the LISCOMP program.",0
https://doi.org/10.1577/t06-108.1,"Modeling Annual Growth Variation using a Hierarchical Bayesian Approach and the von Bertalanffy Growth Function, with Application to Lake Trout in Southern Lake Huron","We compared two models for time-varying growth using a hierarchical Bayesian approach to inference. Both models were derived from the same time-invariant von Bertalanffy growth function (VBGF), and our model comparisons were based on the deviance information criterion. We fit models to length and age data for 15,675 individual lake trout Salvelinus namaycush collected during annual spring gill-net surveys in southern Lake Huron from 1976 to 2004. We found that a model structured with both year and cohort effects outperformed a model that only used the same year-specific VBGF parameters for all age-groups. For the better model, the full version that allowed all VBGF parameters to vary over time also outperformed alternatives for which some parameters were constant. Length at age changed greatly over the 1976-2004 period, and in some years different ages changed in different directions. These complex patterns, which were due to the combination of cohort-specific growth and year-specific changes in growth environment, were well captured by our model. When we modeled growth as varying over time, inferences about VBGF parameters differed between the two models, and correlations among VBGF parameters also differed from the usually reported relations based on time-invariant models.",0
https://doi.org/10.1037/0022-0167.52.3.310,"Group Climate, Cohesion, Alliance, and Empathy in Group Psychotherapy: Multilevel Structural Equation Models.","This study examined the definitional and statistical overlap among 4 key group therapeutic relationship constructs—group climate, cohesion, alliance, and empathy—across member–member, member–group, and member–leader relationships. Three multilevel structural equation models were tested using selfreport measures completed by 662 participants from 111 counseling center and personal growth groups. As hypothesized, almost all measures of therapeutic relationship were significantly correlated. Hypothesized 1-factor, 2-factor (Working and Bonding factors), and 3-factor (Member, Leader, and Group factors) models did not fit the data adequately. An exploratory model with Bonding, Working, and Negative factors provided the best fit to the data. Group members distinguished among relationships primarily according to relationship quality rather than the status or role of others (i.e., leader, member, or whole group).",0
https://doi.org/10.1002/hec.1531,Improving costing methods in multicentre economic evaluation: the use of multiple imputation for unit costs,"Economic evaluations must use appropriate costing methods. However, in multicentre cost-effectiveness analyses (CEA) a fundamental issue of how best to measure and analyse unit costs has been neglected. Multicentre CEA commonly take the mean unit cost from a national database, such as NHS reference costs. This approach does not recognise that unit costs vary across centres and are unavailable in some centres. This paper proposes the use of multiple imputation (MI) to predict those centre-specific unit costs that are not available, while recognising the statistical uncertainty surrounding this imputation.We illustrate MI with a CEA of a multicentre randomised trial (1014 patients, 60 centres), implemented using multilevel modelling. We use MI to derive centre-specific unit costs, based on centre characteristics including average casemix, and compare this to using mean NHS reference costs. In this case study, using MI unit costs rather than mean reference costs led to less heterogeneity across centres, more precise estimates of incremental cost, but similar estimates of incremental cost-effectiveness.We conclude that using MI to predict unit costs can preserve correlations, maximise the use of available data, and, when combined with multilevel modelling is an appropriate method for recognising the statistical uncertainty in multicentre CEA.",0
https://doi.org/10.1016/j.jval.2010.12.006,"Cost-Effectiveness Analysis of Linezolid, Daptomycin, and Vancomycin in Methicillin-Resistant Staphylococcus aureus: Complicated Skin and Skin Structure Infection Using Bayesian Methods for Evidence Synthesis","Methicillin-resistant Staphylococcus aureus (MRSA) complicated skin and skin structure infection (cSSSI) is a prominent infection encountered in hospital and outpatient settings that is associated with high resource use for the health-care system.A decision analytic (DA) model was developed to evaluate the cost-effectiveness analysis (CEA) of linezolid, daptomycin, and vancomycin in MRSA cSSSI.Bayesian methods for evidence synthesis were used to generate efficacy and safety parameters for a DA model using published clinical trials. CEA was done from the US health-care perspective. Efficacy was defined as a successfully treated patient at the test of cure without any adverse reaction. Primary outcome was the incremental cost-effectiveness ratio between linezolid and vancomycin, daptomycin and vancomycin, and linezolid and daptomycin in MRSA cSSSI. Univariate and probabilistic sensitivity analyses were performed to test the robustness of the model.The total direct costs of linezolid, daptomycin, and vancomycin were $18,057, $20,698, and $23,671, respectively. The cost-effectiveness ratios for linezolid, daptomycin, and vancomycin were $37,604, $44,086, and $52,663 per successfully treated patient, respectively. Linezolid and daptomycin were dominant strategies compared to vancomycin. However, linezolid was dominant when compared to daptomycin. The model was sensitive to the duration of daptomycin and linezolid treatment.Linezolid and daptomycin are potentially cost-effective based on the assumptions of the DA model; however, linezolid appears to be more cost-effective compared to daptomycin and vancomycin for MRSA cSSSIs.",0
https://doi.org/10.1007/bf02294775,Bayesian item selection criteria for adaptive testing,Owen (1975) proposed an approximate empirical Bayes procedure for item selection in computerized adaptive testing (CAT). The procedure replaces the true posterior by a normal approximation with closed-form expressions for its first two moments. This approximation was necessary to minimize the computational complexity involved in a fully Bayesian approach but is no longer necessary given the computational power currently available for adaptive testing. This paper suggests several item selection criteria for adaptive testing which are all based on the use of the true posterior. Some of the statistical properties of the ability estimator produced by these criteria are discussed and empirically characterized.,0
https://doi.org/10.1046/j.0039-0402.2003.00252.x,Robustness issues in multilevel regression analysis,"A multilevel problem concerns a population with a hierarchical structure. A sample from such a population can be described as a multistage sample. First, a sample of higher level units is drawn (e.g. schools or organizations), and next a sample of the sub-units from the available units (e.g. pupils in schools or employees in organizations). In such samples, the individual observations are in general not completely independent. Multilevel analysis software accounts for this dependence and in recent years these programs have been widely accepted. Two problems that occur in the practice of multilevel modeling will be discussed. The first problem is the choice of the sample sizes at the different levels. What are sufficient sample sizes for accurate estimation? The second problem is the normality assumption of the level-2 error distribution. When one wants to conduct tests of significance, the errors need to be normally distributed. What happens when this is not the case? In this paper, simulation studies are used to answer both questions. With respect to the first question, the results show that a small sample size at level two (meaning a sample of 50 or less) leads to biased estimates of the second-level standard errors. The answer to the second question is that only the standard errors for the random effects at the second level are highly inaccurate if the distributional assumptions concerning the level-2 errors are not fulfilled. Robust standard errors turn out to be more reliable than the asymptotic standard errors based on maximum likelihood.",0
https://doi.org/10.1111/j.1745-3984.1998.tb00530.x,Properties of Ability Estimation Methods in Computerized Adaptive Testing,"Simulations of computerized adaptive tests (CATs) were used to evaluate results yielded by four commonly used ability estimation methods: maximum likelihood estimation (MLE) and three Bayesian approaches—Owen's method, expected a posteriori (EAP), and maximum a posteriori. In line with the theoretical nature of the ability estimates and previous empirical research, the results showed clear distinctions between MLE and the Bayesian methods, with MLE yielding lower bias, higher standard errors, higher root mean square errors, lower fidelity, and lower administrative efficiency. Standard errors for MLE based on test information underestimated actual standard errors, whereas standard errors for the Bayesian methods based on posterior distribution standard deviations accurately estimated actual standard errors. Among the Bayesian methods, Owen's provided the worst overall results, and EAP provided the best. Using a variable starting rule in which examinees were initially classified into three broad/ability groups greatly reduced the bias for the Bayesian methods, but had little effect on the results for MLE. On the basis of these results, guidelines are offered for selecting appropriate CAT ability estimation methods in different decision contexts.",0
https://doi.org/10.1214/088342306000000015,General Design Bayesian Generalized Linear Mixed Models,"Linear mixed models are able to handle an extraordinary range of complications in regression-type analyses. Their most common use is to account for within-subject correlation in longitudinal data analysis. They are also the standard vehicle for smoothing spatial count data. However, when treated in full generality, mixed models can also handle spline-type smoothing and closely approximate kriging. This allows for nonparametric regression models (e.g., additive models and varying coefficient models) to be handled within the mixed model framework. The key is to allow the random effects design matrix to have general structure; hence our label general design. For continuous response data, particularly when Gaussianity of the response is reasonably assumed, computation is now quite mature and supported by the R, SAS and S-PLUS packages. Such is not the case for binary and count responses, where generalized linear mixed models (GLMMs) are required, but are hindered by the presence of intractable multivariate integrals. Software known to us supports special cases of the GLMM (e.g., PROC NLMIXED in SAS or glmmML in R) or relies on the sometimes crude Laplace-type approximation of integrals (e.g., the SAS macro glimmix or glmmPQL in R). This paper describes the fitting of general design generalized linear mixed models. A Bayesian approach is taken and Markov chain Monte Carlo (MCMC) is used for estimation and inference. In this generalized setting, MCMC requires sampling from nonstandard distributions. In this article, we demonstrate that the MCMC package WinBUGS facilitates sound fitting of general design Bayesian generalized linear mixed models in practice.",0
https://doi.org/10.1002/2014wr016553,A regional estimate of postfire streamflow change in California,"The effect of fire on annual streamflow has been examined in numerous watershed studies, with some studies observing postfire increases in streamflow while other have observed no conclusive change. Despite this inherent variability in streamflow response, the management of water resources for flood protection, water supply, water quality, and the environment necessitates an understanding of postfire effects on streamflow at regional scales. In this study, the regional effect of wildfire on annual streamflow was investigated using 12 paired watersheds in central and southern California. A mixed model was used to pool and statistically examine the combined paired-watershed data, with emphasis on the effects of percentage area burned, postfire recovery of vegetation, and postfire wetness conditions on postfire streamflow change. At a regional scale, postfire annual streamflow increased 134% (82%–200%) during the first postfire year assuming 100% area burned and average annual wetness conditions. Postfire response decreased with lower percentages of percentage area burned and during subsequent years as vegetation recovered following fire. Annual streamflow response to fire was found to be sensitive to annual wetness conditions, with postfire response being smallest during dry years, greatest during wet years, and slowly decreasing during very wet years. These findings provide watershed managers with a first-order estimate for predicting postfire streamflow response in both gauged and ungauged watersheds.",0
https://doi.org/10.1111/spsr.12125,What Type of Resources? Household Effects and Female Electoral Participation,"This paper investigates the relationship between socioeconomic resources and women's political participation, more precisely whether the distinction between individual and household resources helps to identify different mechanisms behind a possible “resource effect”. Based on a Bayesian multilevel analysis using the Swiss Election Study 2011, we show that both individual and household resources matter for explaining female electoral participation. However, resource effects must not necessarily be positive: While higher education of women and their partners tend to increase female participation, a demanding job of either the woman and/or her partner rather decreases her propensity to vote. Moreover, the mechanisms behind partner effects are contingent on women's individual resource endowment. While low and medium educated women most strongly profit from higher educational partner resources, i.e. from a compensatory mechanism, the resource “time” seems to particularly confine political involvement of women with both high professional status or no employment.",0
https://doi.org/10.1214/aoms/1177693250,Formal Bayes Estimation with Application to a Random Effects Model,,0
https://doi.org/10.1002/sim.3460,Bayesian propensity score analysis for observational data,"In the analysis of observational data, stratifying patients on the estimated propensity scores reduces confounding from measured variables. Confidence intervals for the treatment effect are typically calculated without acknowledging uncertainty in the estimated propensity scores, and intuitively this may yield inferences, which are falsely precise. In this paper, we describe a Bayesian method that models the propensity score as a latent variable. We consider observational studies with a dichotomous treatment, dichotomous outcome, and measured confounders where the log odds ratio is the measure of effect. Markov chain Monte Carlo is used for posterior simulation. We study the impact of modelling uncertainty in the propensity scores in a case study investigating the effect of statin therapy on mortality in Ontario patients discharged from hospital following acute myocardial infarction. Our analysis reveals that the Bayesian credible interval for the treatment effect is 10 per cent wider compared with a conventional propensity score analysis. Using simulations, we show that when the association between treatment and confounders is weak, then this increases uncertainty in the estimated propensity scores. Bayesian interval estimates for the treatment effect are longer on average, though there is little improvement in coverage probability. A novel feature of the proposed method is that it fits models for the treatment and outcome simultaneously rather than one at a time. The method uses the outcome variable to inform the fit of the propensity model. We explore the performance of the estimated propensity scores using cross-validation. Copyright © 2008 John Wiley & Sons, Ltd.",0
https://doi.org/10.1088/0004-637x/811/2/78,ELEMENTAL DEPLETIONS IN THE MAGELLANIC CLOUDS AND THE EVOLUTION OF DEPLETIONS WITH METALLICITY,"We present a study of the composition of gas and dust in the Large and Small Magellanic Clouds (LMC and SMC, together -- the MCs) as measured by UV absorption spectroscopy. We have measured P II and Fe II along 85 sightlines toward the MCs using archival FUSE observations. For 16 of those sightlines, we have measured Si II, Cr II, and Zn II from new HST COS observations. We have combined these measurements with H I and H$_2$ column densities and reference stellar abundances from the literature to derive gas-phase abundances, depletions, and gas-to-dust ratios (GDRs). 80 of our 84 P measurements and 13 of our 16 Zn measurements are depleted by more than 0.1 decades, suggesting that P and Zn abundances are not accurate metallicity indicators at and above the metallicity of the SMC. The maximum P and Zn depletions are the same in the MW, LMC, and SMC. Si, Cr, and Fe are systematically less depleted in the SMC than in the MW or LMC. The minimum Si depletion in the SMC is consistent with zero. Our depletion-derived GDRs broadly agree with GDRs from the literature. The GDR varies from location to location within a galaxy by a factor of up to 2 in the LMC and up to 5 in the SMC. This variation is evidence of dust destruction and/or growth in the diffuse neutral phase of the interstellar medium.",0
https://doi.org/10.3758/bf03202110,Microcomputer-based estimation of psychophysical thresholds: The Best PEST,"A new, maximally efficient technique for measuring psychophysical thresholds (Pentland, 1980) has been implemented on the microcomputer. This PEST (parameter estimation by sequential testing) technique is the most efficient sequential parameter estimation technique possible, given that the form of the psychometric function is known. The technique is similar to but faster and more accurate than other staircase procedures and may be applied whenever staircase techniques are applicable. The “Best PEST” is easily implemented on the micro-computer; a BASIC program for the Apple II which does so is presented. The Best PEST is compared with other staircase procedures, including one recently implemented on a micro-computer (Corwin, Kintz, & Beaty, 1979).",0
https://doi.org/10.1214/aoms/1177704731,The Problem of Negative Estimates of Variance Components,"The usefulness of variance component techniques is frequently limited by the occurrence of negative estimates of essentially positive parameters. This paper uses a restricted maximum likelihood principle to remove this objectionable characteristic for certain experimental models. Section 2 discusses certain necessary results from the theory of non-linear programming. Section 3 derives specific formulae for estimating the variance components of the random one-way and two-way classification models. The problem of determining the precision of instruments in the two instrument case is dealt with in section 4, and a surprising though not unreasonable answer is obtained. The remaining sections provide an algorithm for solving the problem of negative estimates of variance components for all random effects models whose expected mean square column may be thought of as forming a mathematical tree in a certain sense. The algorithm is as follows: Consider the minimum mean square in the entire array; if this mean square is the root of the tree then equate it to its expectation. If the minimum mean square is not the root then pool it with its predecessor. In either case the problem is reduced to an identical one having one less variable, and hence in a finite number of steps the process will yield estimates of the variance components. These estimates are non-negative and have a maximum likelihood property.",0
https://doi.org/10.18187/pjsor.v12i1.952,Bayesian Analysis of Linear and Nonlinear Latent Variable Models with Fixed Covariate and Ordered Categorical Data,"In this paper, ordered categorical variables are used to compare between linear and nonlinear interactions of fixed covariate and latent variables Bayesian structural equation models. Gibbs sampling method is applied for estimation and model comparison. Hidden continuous normal distribution (censored normal distribution) is used to handle the problem of ordered categorical data. Statistical inferences, which involve estimation of parameters and their standard deviations, and residuals analyses for testing the selected model, are discussed. The proposed procedure is illustrated by a simulation data obtained from R program. Analysis are done by using OpenBUGS program.",0
https://doi.org/10.1177/0962280206075309,Bayesian methods for latent trait modelling of longitudinal data,"Latent trait models have long been used in the social science literature for studying variables that can only be measured indirectly through multiple items. However, such models are also very useful in accounting for correlation in multivariate and longitudinal data, particularly when outcomes have mixed measurement scales. Bayesian methods implemented with Markov chain Monte Carlo provide a flexible framework for routine fitting of a broad class of latent variable (LV) models, including very general structural equation models. However, in considering LV models, a number of challenging issues arise, including identifiability, confounding between the mean and variance, uncertainty in different aspects of the model, and difficulty in computation. Motivated by the problem of modelling multidimensional longitudinal data, this article reviews the recent literature, provides some recommendations and highlights areas in need of additional research, focusing on methods for model uncertainty.",0
,The Beast of Aggregating Cognitive Load Measures in Technology-Based Learning.,"An increasing part of cognitive load research in technology-based learning includes a component of repeated measurements, that is: participants are measured two or more times on the same performance, mental effort or other variable of interest. In many cases, researchers aggregate scores obtained from repeated measurements to one single sum or average score per participant and use these aggregated scores in subsequent analysis. This paper demonstrates some dangers of this commonly encountered aggregation approach and presents two comprehensive alternatives: Split-plot analysis of variance (ANOVA) and more flexible two-level regression analysis. The core message of this paper is that the application of the aggregation approach can seriously distort our view of effects and relations of interest and should therefore not be used in cognitive load research. Multilevel analysis of repeated measurements data can account for various features of the data and constitutes a best practice.",0
https://doi.org/10.1080/10705511.2014.882688,An Empirical Evaluation of Mediation Effect Analysis With Manifest and Latent Variables Using Markov Chain Monte Carlo and Alternative Estimation Methods,"Recently, the Markov chain Monte Carlo (MCMC) estimation method has become explosively popular in a variety of quantitative research methods. In mediation effect analysis (MEA), the MCMC estimation methods can be a promising tool and an important alternative as compared with traditional methods (e.g., the z test using the delta method and the bias-corrected bootstrapping method) in addressing issues such as nonconvergence and complex modeling. In this article, a subject-level MCMC approach for the single MEA is empirically evaluated and compared with traditional methods through Monte Carlo simulation. The evaluation covers point and interval estimates of both manifest and latent variables across conditions including sample size, effect size, and magnitude of factor loadings. BUGS codes for MEA with both manifest and latent variables are provided that can be easily adapted to fit various MEA models in practice.",1
,A MULTIVARIATE PROBIT LATENT VARIABLE MODEL FOR ANALYZING DICHOTOMOUS RESPONSES,"We propose a multivariate probit model that is dened by a conrmatory factor analysis model with covariates for analyzing dichotomous data in medical re- search. Our proposal is a generalization of several useful multivariate probit models, and provides a exible framework for practical applications. We implement a Monte Carlo EM algorithm for maximum likelihood estimation of the model, and develop a path sampling procedure to compute the observed-data log-likelihood for evalu- ating the Bayesian Information Criterion for model comparison. Our methodology is illustrated by analyzing two data sets in medical research.",0
https://doi.org/10.1111/j.0269-283x.2004.00511.x,Exposure of sheep to mosquito bites: possible consequences for the transmission risk of Rift Valley Fever in Senegal,"Abstract. Rift Valley Fever (RVF) is a growing health problem in West Africa. In northern Senegal, the candidate vectors of this arbovirosis are Aedes (Aedimorphus) vexans Meigen and Culex (Culex) poicilipes Theobald (Diptera: Culicidae). Domestic ruminants are the reservoirs of the virus. A study was undertaken during the 2002 rainy season to assess spatial and temporal variations in exposure to mosquito bites in sheep herds, and to evaluate the possible consequences on the risk of RVF transmission to sheep. Mosquitoes were collected with sheep-baited traps. The number of Ae. vexans females (the predominant species during the 2002 rainy season) trapped per trap-night was the dependent variable in statistical analyses. The trapping periods were divided into six series of two to five consecutive days, from July to November 2002. Three temporary ponds were selected according to their ecological features: depth, bank slope, size and vegetation cover. Traps were laid on the pond bank and in the nearest available compound, close to the sheep night pen. Data were analysed using mixed-effects Poisson models. The explanatory variables were the trapping period, the pond, and the capture site. The exposure to mosquito bites varied according to the pond type, suggesting that the risk of transmission was spatially heterogeneous. However, there was no obvious trend in transmission risk due to the effect of the distance from the compound to the pond. The period with the highest exposure was in October, i.e. when transhumant herds left the Ferlo to relocate to their dry-season settlement. It is thus hypothesized that transhumance, the seasonal movements of herds, plays a significant role in the dissemination of RVF virus in the region.",0
https://doi.org/10.1198/016214503000071,Principal Stratification Approach to Broken Randomized Experiments,"The precarious state of the educational system in the inner cities of the United States, as well as its potential causes and solutions, have been popular topics of debate in recent years. Part of the difficulty in resolving this debate is the lack of solid empirical evidence regarding the true impact of educational initiatives. The efficacy of so-called “school choice” programs has been a particularly contentious issue. A current multimillion dollar program, the School Choice Scholarship Foundation Program in New York, randomized the distribution of vouchers in an attempt to shed some light on this issue. This is an important time for school choice, because on June 27, 2002 the U.S. Supreme Court upheld the constitutionality of a voucher program in Cleveland that provides scholarships both to secular and religious private schools. Although this study benefits immensely from a randomized design, it suffers from complications common to such research with human subjects: noncompliance with assigned “treatmen...",0
https://doi.org/10.1177/0049124194022003006,Multilevel Covariance Structure Analysis,"This article gives an introduction to some new techniques for multilevel covariance structure modeling with latent variables. Although these techniques only incorporate a subset of models that are relevant to multilevel data, the techniques do provide a large set of new analysis possibilities and have the advantage that they only require conventional structural equation modeling software. The presentation draws on methodology presented in earlier works by the author.",0
https://doi.org/10.1093/aje/kwq328,Invited Commentary: Pushing the Mediation Envelope,"The very insightful and clear paper by VanderWeele and Vansteelandt in this issue of the Journal (Am J Epidemiol. 2010;172(12):1339-1348) bridges the gap between biostatistics methodologists focusing on causal methods for mediation analyses and the practitioners of mediational analyses to the benefit of both groups. In an effort to continue the bridging of this gap, this invited commentary relates the important issue of ""natural direct effects"" to the well-known epidemiologic method of direct standardization. Additionally, attention is paid to the importance of temporal sequencing to help substantiate the mediation relations among the exposure, mediation, and outcome. A crucial mathematical distortion under the logistics model, called ""absence of collapsibility,"" is noted in motivating VanderWeele and Vansteelandt's use of the log-linear model for comparing the effect of exposure adjusted for the mediator with the effect of exposure unadjusted for the mediator. It is also noted that this issue applies to one approach to assessing confounding. Finally, some issues are raised for consideration when testing the interaction between the exposure and mediator before assessing mediation.",0
https://doi.org/10.1080/01621459.1972.10481215,Limiting the Risk of Bayes and Empirical Bayes Estimators—Part II: The Empirical Bayes Case,Abstract We discuss compromises between Stein's estimator and the MLE which limit the risk to individual components of the estimation problem while sacrificing only a small fraction of the savings in total squared error loss given by Stein's rule. The compromise estimators “limit translation” away from the MLE. The calculations are pursued in an empirical Bayesian manner by considering their performance against an entire family of prior distributions on the unknown parameters.,0
https://doi.org/10.3758/bf03212980,A comparison of two response time models applied to perceptual matching,"Two models, a Poisson race model and a diffusion model, are fit to data from a perceptual matching task. In each model, information about the similarity or the difference between two stimuli accumulates toward thresholds for either response. Stimulus variables are assumed to influence the rate at which information accumulates, and response variables are assumed to influence the level of the response thresholds. Three experiments were conducted to assess the performance of each model. In Experiment 1, observers performed under different response deadlines; in Experiment 2, response bias was manipulated by changing the relative frequency of same and different stimuli. In Experiment 3, stimulus pairs were presented at three eccentricities: foveal, parafoveal, and peripheral. We examined whether the race and diffusion models could fit the response time and accuracy data through changes only in response parameters (for Experiments 1 and 2) or stimulus parameters (for Experiment 3). Comparisons between the two models suggest that the race model, which has not been studied extensively, can account for perceptual matching data at least as well as the diffusion model. Furthermore, without the constraints on the parameters provided by the experimental conditions, the diffusion and the race models are indistinguishable. This finding emphasizes the importance of fitting models across several conditions and imposing logical psychological constraints on the parameters of models.",0
https://doi.org/10.1007/s11336-011-9211-y,A Joint Modeling Approach for Reaction Time and Accuracy in Psycholinguistic Experiments,"In the psycholinguistic literature, reaction times and accuracy can be analyzed separately using mixed (logistic) effects models with crossed random effects for item and subject. Given the potential correlation between these two outcomes, a joint model for the reaction time and accuracy may provide further insight. In this paper, a Bayesian hierarchical framework is proposed that allows estimation of the correlation between time intensity and difficulty at the item level, and between speed and ability at the subject level. The framework is shown to be flexible in that reaction times can follow a (log-) normal or (shifted) Weibull distribution. A simulation study reveals the reduction in bias gains possible when using joint models, and an analysis of an example from a Dutch–English word recognition study illustrates the proposed method.",0
https://doi.org/10.1007/s11336-009-9136-x,High-dimensional Exploratory Item Factor Analysis by A Metropolis–Hastings Robbins–Monro Algorithm,"A Metropolis–Hastings Robbins–Monro (MH-RM) algorithm for high-dimensional maximum marginal likelihood exploratory item factor analysis is proposed. The sequence of estimates from the MH-RM algorithm converges with probability one to the maximum likelihood solution. Details on the computer implementation of this algorithm are provided. The accuracy of the proposed algorithm is demonstrated with simulations. As an illustration, the proposed algorithm is applied to explore the factor structure underlying a new quality of life scale for children. It is shown that when the dimensionality is high, MH-RM has advantages over existing methods such as numerical quadrature based EM algorithm. Extensions of the algorithm to other modeling frameworks are discussed.",0
https://doi.org/10.1080/00220973.2014.919569,Standardized Effect Size Measures for Mediation Analysis in Cluster-Randomized Trials,"This article presents 3 standardized effect size measures to use when sharing results of an analysis of mediation of treatment effects for cluster-randomized trials. The authors discuss 3 examples of mediation analysis (upper-level mediation, cross-level mediation, and cross-level mediation with a contextual effect) with demonstration of the calculation and interpretation of the effect size measures using a simulated dataset and an empirical dataset from a cluster-randomized trial of peer tutoring. SAS syntax is provided for parametric percentile bootstrapped confidence intervals of the effect sizes. The use of any of the 3 standardized effect size measures depends on the nature of the inference the researcher wishes to make within a single site, across the broad population, or at the site level.",0
https://doi.org/10.1111/1467-9868.00220,Latent variable models with mixed continuous and polytomous data,"Owing to the nature of the problems and the design of questionnaires, discrete polytomous data are very common in behavioural, medical and social research. Analysing the relationships between the manifest and the latent variables based on mixed polytomous and continuous data has proven to be difficult. A general structural equation model is investigated for these mixed outcomes. Maximum likelihood (ML) estimates of the unknown thresholds and the structural parameters in the covariance structure are obtained. A Monte Carlo-EM algorithm is implemented to produce the ML estimates. It is shown that closed form solutions can be obtained for the M-step, and estimates of the latent variables are produced as a by-product of the analysis. The method is illustrated with a real example.",0
https://doi.org/10.1093/ije/dyi069,Covariance components models for longitudinal family data,"A longitudinal family study is an epidemiological design that involves repeated measurements over time in a sample that includes families. Such studies, that may also include relative pairs and unrelated individuals, allow closer investigation of not only the factors that cause a disease to arise, but also the genetic and environmental determinants that modulate the subsequent progression of that disease. Knowledge of such determinants may pay high dividends in terms of prognostic assessment and in the development of new treatments that may be tailored to the prognostic profile of individual patients. Unfortunately longitudinal family studies are difficult to analyse. They conflate the complex within-family correlation structure of a cross-sectional family study with the correlation over time that is intrinsic to longitudinal repeated measures. Here we describe an approach to analysis that is relatively straightforward to implement, yet is flexible in its application. It represents a natural extension of a Gibbs-sampling-based approach to the analysis of cross-sectional family studies that we have described previously. The approach can be applied to pedigrees of arbitrary complexity. It is applicable to continuous traits, repeated binary disease states, and repeated counts or rates with a Poisson distribution. It not only supports the analysis of observed determinants, including measured genotypes, but also allows decomposition of the correlation structure, thereby permitting conclusions to be drawn about the effect of unobserved genes and environment on key features of disease progression, and hence to estimate the heritability of these features. We demonstrate the efficacy of our methods using a range of simulated data analyses, and illustrate its practical application to longitudinal blood pressure data measured in families from the Framingham Heart Study.",0
https://doi.org/10.1002/sim.782,A closer look at combining data among a small number of binomial experiments,"In a regulatory environment, the regulators and the regulated may not be able to agree on the use of subjective prior information for a clinical trial. The use of a data-based prior offers a greater possibility for agreement, however, the degree of importance given to the prior data may still be contentious. The use of a hierarchical model to link the prior data and the current trial is shown to provide a relatively objective method for assigning weight to the prior data. Using a series of examples combining two binomial experiments, the effect of a hierarchical model on estimating rates, on the degree to which data is combined and on hypothesis testing is illustrated. In addition, the phenomenon in which combining data reduces the precision is explained. Simpler models based on finite mixtures of beta distributions are shown to work as well as the more computationally intensive, continuous mixtures. Lastly, an example combining three concurrent studies is illustrated. Published in 2001 by John Wiley & Sons, Ltd.",0
https://doi.org/10.1016/j.jkss.2015.01.002,A test for the increasing convex order based on the cumulative residual entropy,"The complete cumulative residual entropy can be generalized to the incomplete cumulative residual entropy (ICRE). In this paper, we introduce a partial ordering in terms of ICRE. The relationship between this ordering and some important orderings of lifetime distributions are investigated. We use this order to establish a test statistic for testing the stochastic equality against increasing convex order alternative. The performance of the test statistic is evaluated using a simulation study. Finally, a numerical example illustrating the theory is also given. Â© 2015 The Korean Statistical Society.",0
https://doi.org/10.1007/bf02293813,Contributions to factor analysis of dichotomous variables,"A new method is proposed for the factor analysis of dichotomous variables. Similar to the method of Christoffersson this uses information from the first and second order proportions to fit a multiple factor model. Through a transformation into a new set of sample characteristics, the estimation is considerably simplified. A generalized least-squares estimator is proposed, which asymptotically is as efficient as the corresponding estimator of Christoffersson, but which demands less computing time. Â© 1978 Psychometric Society.",0
https://doi.org/10.2202/1544-6115.1175,A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics,"Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.",0
https://doi.org/10.1080/10705511.2014.882686,The Impact of Inaccurate “Informative” Priors for Growth Parameters in Bayesian Growth Mixture Modeling,"Within Bayesian estimation, prior distributions are placed on model parameters and these distributions can take on many different levels of informativeness. Although much of the research conducted within this estimation framework uses what are called diffuse (or noninformative) priors, there are certain models and modeling circumstances where it is more optimal to use what are referred to as informative priors. This study focuses on the latter situation and examines the effects of inaccurate informative priors on the growth parameters within the context of growth mixture modeling. Overall, results indicated that growth mixture modeling is relatively robust to the use of inaccurate mean hyperparameters for the growth parameters, as long as the variance hyperparameters are somewhat large.",0
https://doi.org/10.1177/0956797610372634,Emotional Inertia and Psychological Maladjustment,"In this article, we examine the concept of emotional inertia as a fundamental property of the emotion dynamics that characterize psychological maladjustment. Emotional inertia refers to the degree to which emotional states are resistant to change. Because psychological maladjustment has been associated with both emotional underreactivity and ineffective emotion-regulation skills, we hypothesized that its overall emotion dynamics would be characterized by high levels of inertia. We provide evidence from two naturalistic studies that, using different methods, showed that the emotional fluctuations of individuals who exhibited low self-esteem (Study 1) and depression (Study 2) were characterized by higher levels of inertia in both positive and negative emotions than the emotional fluctuations of people who did not exhibit low self-esteem and depression. We also discuss the usefulness of the concept of emotional inertia as a hallmark of maladaptive emotion dynamics.",0
https://doi.org/10.1017/s0003055408080398,The Impartiality of International Judges: Evidence from the European Court of Human Rights,"Can international judges be relied upon to resolve disputes impartially? If not, what are the sources of their biases? Answers to these questions are critically important for the functioning of an emerging international judiciary, yet we know remarkably little about international judicial behavior. An analysis of a new dataset of dissents in the European Court of Human Rights (ECtHR) yields a mixed set of answers. On the bright side, there is no evidence that judges systematically employ cultural or geopolitical biases in their rulings. There is some evidence that career insecurities make judges more likely to favor their national government when it is a party to a dispute. Most strongly, the evidence suggests that international judges are policy seekers. Judges vary in their inclination to defer to member states in the implementation of human rights. Moreover, judges from former socialist countries are more likely to find violations against their own government and against other former socialist governments, suggesting that they are motivated by rectifying a particular set of injustices. I conclude that the overall picture is mostly positive for the possibility of impartial review of government behavior by judges on an international court. Like judges on domestic review courts, ECtHR judges are politically motivated actors in the sense that they have policy preferences on how to best apply abstract human rights in concrete cases, not in the sense that they are using their judicial power to settle geopolitical scores.",0
https://doi.org/10.1207/s15327906mbr3804_5,Have Multilevel Models Been Structural Equation Models All Along?,"A core assumption of the standard multiple regression model is independence of residuals, the violation of which results in biased standard errors and test statistics. The structural equation model (SEM) generalizes the regression model in several key ways, but the SEM also assumes independence of residuals. The multilevel model (MLM) was developed to extend the regression model to dependent data structures. Attempts have been made to extend the SEM in similar ways, but several complications currently limit the general application of these techniques in practice. Interestingly, it is well known that under a broad set of conditions SEM and MLM longitudinal ""growth curve"" models are analytically and empirically identical. This is intriguing given the clear violation of independence in growth modeling that does not detrimentally affect the standard SEM. Better understanding the source and potential implications of this isomorphism is my focus here. I begin by exploring why SEM and MLM are analytically equivalent methods in the presence of nesting due to repeated observations over time. I then capitalize on this equivalency to allow for the extension of SEMs to a general class of nested data structures. I conclude with a description of potential opportunities for multilevel SEMs and directions for future developments.",0
https://doi.org/10.1002/sim.6573,A Bayesian approach to estimating causal vaccine effects on binary post-infection outcomes,"To estimate causal effects of vaccine on post-infection outcomes, Hudgens and Halloran (2006) defined a post-infection causal vaccine efficacy estimand VEI based on the principal stratification framework. They also derived closed forms for the maximum likelihood estimators of the causal estimand under some assumptions. Extending their research, we propose a Bayesian approach to estimating the causal vaccine effects on binary post-infection outcomes. The identifiability of the causal vaccine effect VEI is discussed under different assumptions on selection bias. The performance of the proposed Bayesian method is compared with the maximum likelihood method through simulation studies and two case studies - a clinical trial of a rotavirus vaccine candidate and a field study of pertussis vaccination. For both case studies, the Bayesian approach provided similar inference as the frequentist analysis. However, simulation studies with small sample sizes suggest that the Bayesian approach provides smaller bias and shorter confidence interval length.",0
https://doi.org/10.1198/000313001300339950,Calibration of<i>ρ</i>Values for Testing Precise Null Hypotheses,"P values are the most commonly used tool to measure evidence against a hypothesis or hypothesized model. Unfortunately, they are often incorrectly viewed as an error probability for rejection of the hypothesis or, even worse, as the posterior probability that the hypothesis is true. The fact that these interpretations can be completely misleading when testing precise hypotheses is first reviewed, through consideration of two revealing simulations. Then two calibrations of a ρ value are developed, the first being interpretable as odds and the second as either a (conditional) frequentist error probability or as the posterior probability of the hypothesis.",0
https://doi.org/10.3758/bf03201182,Similarity comparisons with remembered and perceived magnitudes: Memory psychophysics and fundamental measurement,"At the outset, subjects learned to associate a label with each element in a set of perceptual magnitudes (visual extents), using traditional paired-associate learning methods. Subsequently, on some trials, subjects indicated which pair of two pairs of labels corresponded to the more similar perceptual referents, and, on other trials, they selected the more dissimilar pair. It is shown that these similarity comparisons satisfy the axioms (transitivity and intradimensional subtractivity) necessary to conclude that they are based on computation of the difference of the differences of analogue-based interval scale representations. The findings also permitted refutation of the idea that memory for elementary percepts arises from their reperception. Notably, the memory exponent was 0.697, but the perception exponent was 0.546, and the reperception idea requires that the memory exponent be the square of the perception exponent (0.546(2) = 0.298). Symbolic distance effects and enhanced response time-based semantic congruity effects, typically found with binary comparisons, extend the range of commonalties found between perceptual and memory psychophysics.",0
https://doi.org/10.4135/9781446247600.n4,Bayesian Multilevel Models,,0
https://doi.org/10.21236/ada640705,How Many Iterations in the Gibbs Sampler?,"Abstract : When the Gibbs sampler is used to estimate posterior distributions (Gelfand and Smith, 1990) the question of how many iterations are required is central to its implementation. When interest focuses on quantiles of functionals of the posterior distribution, we describe an easily-implemented method for determining the total number of iterations required, and also the number of initial iterations that should be discarded to allow for burn-in. The method uses only the Gibbs iterates themselves, and does not, for example, require external specification of characteristics of the posterior density. Here the method is described for the situation where one long run is generated, but it can also be easily applied if there are several runs from different starting points. It also applies more generally to Markov chain Monte Carlo schemes other than the quantities of interest are probabilities rather than full posterior distributions, and when the draws from the posterior distribution are required to be approximately independent. The method is applied to several different posterior distributions. These include a multivariate normal posterior distribution with independent parameters, a bimodal distribution, a cigar-shaped multivariate normal distribution in ten dimensions, and a highly complex 190-dimensional posterior distribution arising in spatial statistics. In each case the method appears to give satisfactory results.",0
https://doi.org/10.1007/bf02294828,A general model for two-level data with responses missing at random,"A general model for two-level multivariate data, with responses possibly missing at random, is described. The model combines regressions on fixed explanatory variables with structured residual covariance matrices. The likelihood function is reduced to a form enabling computational methods for estimating the model to be devised. Ã‚Â© 1993 The Psychometric Society.",0
https://doi.org/10.1080/10543406.2013.863780,A Comparison of Confidence Interval Methods for the Concordance Correlation Coefficient and Intraclass Correlation Coefficient with Small Number of Raters,"The intraclass correlation coefficient (ICC) with fixed raters or, equivalently, the concordance correlation coefficient (CCC) for continuous outcomes is a widely accepted aggregate index of agreement in settings with small number of raters. Quantifying the precision of the CCC by constructing its confidence interval (CI) is important in early drug development applications, in particular in qualification of biomarker platforms. In recent years, there have been several new methods proposed for construction of CIs for the CCC, but their comprehensive comparison has not been attempted. The methods consisted of the delta method and jackknifing with and without Fisher's Z-transformation, respectively, and Bayesian methods with vague priors. In this study, we carried out a simulation study, with data simulated from multivariate normal as well as heavier tailed distribution (t-distribution with 5 degrees of freedom), to compare the state-of-the-art methods for assigning CI to the CCC. When the data are normally distributed, the jackknifing with Fisher's Z-transformation (JZ) tended to provide superior coverage and the difference between it and the closest competitor, the Bayesian method with the Jeffreys prior was in general minimal. For the nonnormal data, the jackknife methods, especially the JZ method, provided the coverage probabilities closest to the nominal in contrast to the others which yielded overly liberal coverage. Approaches based upon the delta method and Bayesian method with conjugate prior generally provided slightly narrower intervals and larger lower bounds than others, though this was offset by their poor coverage. Finally, we illustrated the utility of the CIs for the CCC in an example of a wake after sleep onset (WASO) biomarker, which is frequently used in clinical sleep studies of drugs for treatment of insomnia.",0
https://doi.org/10.1016/s0065-2660(07)00413-0,Meta‐Analysis Methods,"Meta-analysis is the quantitative synthesis of information from several studies. It is applicable to a variety of study designs in genetics, from family-based linkage studies and population-based association studies to genome-wide scans and genome-wide association studies. By combining relevant evidence from many studies, statistical power is increased and more precise estimates may be obtained. Most importantly, meta-analysis provides a framework for the appreciation and assessment of between-study heterogeneity, that is, the methodological, epidemiological, clinical, and biological dissimilarity across the various studies. Being a retrospective research design in most cases, meta-analysis is subject to a variety of selection biases that may undermine its validity. A major challenge is to differentiate genuine between-study heterogeneity from systematic errors and biases.",0
https://doi.org/10.1080/01621459.1994.10476828,Toward a Reconciliation of the Bayesian and Frequentist Approaches to Point Estimation,"Abstract The Bayesian and frequentist approaches to point estimation are reviewed. The status of the debate regarding the use of one approach over the other is discussed, and its inconclusive character is noted. A criterion for comparing Bayesian and frequentist estimators within a given experimental framework is proposed. The competition between a Bayesian and a frequentist is viewed as a contest with the following components: a random observable, a true prior distribution unknown to both statisticians, an operational prior used by the Bayesian, a fixed frequentist rule used by the frequentist, and a fixed loss criterion. This competition is studied in the context of exponential families, conjugate priors, and squared error loss. The class of operational priors that yield Bayes estimators superior to the “best” frequentist estimator is characterized. The implications of the existence of a threshold separating the space of operational priors into good and bad priors are explored, and their relevance in ar...",0
https://doi.org/10.1002/9780470024737,Structural Equation Modeling,"Structural equation modeling , Structural equation modeling , کتابخانه دیجیتال جندی شاپور اهواز",0
https://doi.org/10.1016/j.tranpol.2015.07.001,Gender differences in activity and travel behavior in the Arab world,"Abstract The purpose of this study is to extend the research on gendered differences in travel patterns in the Arab world by an in-depth study of the interrelationship of travel-related activities and various socio-economic and demographic characteristics. This study is based on a unique data set that includes activity and travel diaries collected from three Arab communities in the Galilee region of Israel. Through descriptive statistics and nonlinear structural equations modeling, we found that gender plays an important role in both activity participation and travel behavior in these communities. Women tend to travel less than men in terms of both number of tours, defined as chain of trip segments that start and end at home, trips, and total time spent traveling. Women tend to work more within their communities and to conduct more of their activities by walking; they are also the ones who make more child-serving stops, which affect their travel patterns. Women tend to travel by car more as passengers, whereas men tend to be drivers. Those who made more tours also tended to make more complex tours, with more stops per tour, although, in general, complex tours are not substituted for making additional tours. People who work outside the community and make complex tours are more likely to drive, as the car is needed for these types of trips, which men make more than women. From a policy perspective, these findings suggest that public transportation services are needed to help overcome gender differences in travel behavior. Improving transit service for school trips and improving urban design through a friendlier environment, especially for children, will beneficially affect the complexity of women’s daily activity patterns and their quality of life.",0
https://doi.org/10.1287/opre.31.6.1109,Simulation Run Length Control in the Presence of an Initial Transient,"This paper studies the estimation of the steady state mean of an output sequence from a discrete event simulation. It considers the problem of the automatic generation of a confidence interval of prespecified width when there is an initial transient present. It explores a procedure based on Schruben's Brownian bridge model for the detection of nonstationarity and a spectral method for estimating the variance of the sample mean. The procedure is evaluated empirically for a variety of output sequences. The performance measures considered are bias, confidence interval coverage, mean confidence interval width, mean run length, and mean amount of deleted data. If the output sequence contains a strong transient, then inclusion of a test for stationarity in the run length control procedure results in point estimates with lower bias, narrower confidence intervals, and shorter run lengths than when no check for stationarity is performed. If the output sequence contains no initial transient, then the performance measures of the procedure with a stationarity test are only slightly degraded from those of the procedure without such a test. If the run length is short relative to the extent of the initial transient, the stationarity tests may not be powerful enough to detect the transient, resulting in a procedure with unreliable point and interval estimates.",0
https://doi.org/10.1080/00273171.2015.1093459,Moving in Parallel Toward a Modern Modeling Epistemology: Bayes Factors and Frequentist Modeling Methods,"The Bayesian-frequentist debate typically portrays these statistical perspectives as opposing views. However, both Bayesian and frequentist statisticians have expanded their epistemological basis away from a singular focus on the null hypothesis, to a broader perspective involving the development and comparison of competing statistical/mathematical models. For frequentists, statistical developments such as structural equation modeling and multilevel modeling have facilitated this transition. For Bayesians, the Bayes factor has facilitated this transition. The Bayes factor is treated in articles within this issue of Multivariate Behavioral Research. The current presentation provides brief commentary on those articles and more extended discussion of the transition toward a modern modeling epistemology. In certain respects, Bayesians and frequentists share common goals.",0
https://doi.org/10.1016/j.sste.2011.07.001,Space-time confounding adjusted determinants of child HIV/TB mortality for large zero-inflated data in rural South Africa,"South Africa is experiencing a major burden of HIV/TB. We used longitudinal data from the Agincourt sub-district in rural northeast South Africa over the years 2000 to 2005. A total of 187 HIV/TB deaths were observed among 16,844 children aged 1-5 years coming from 8,863 households. In this paper we used Bayesian models to assess risk factors for child HIV/TB mortality taking into account the presence of spatial correlation. Bayesian zero inflated spatiotemporal models were able to detect hidden patterns within the data. Our main finding was that maternal orphans experienced a threefold greater risk of HIV/TB death compared to those with living mothers (AHR=2.93, 95% CI[1.29;6.93]). Risk factor analyses which adjust for person, place and time provide evidence for policy makers that includes a spatial distribution of risk. Child survival is dependent on the mother's survival; hence programs that promote maternal survival are critical.",0
https://doi.org/10.1177/1043463114523715,Reciprocity and volunteering,"This paper evaluates whether volunteering is imbued with altruistic or strategic reciprocity. Although scholars have intensively studied the motivations and social norms to volunteer, to date there is no agreement why human beings perform activities in which time is freely given up in order to benefit another person, group or organization. We argue that attitudes towards reciprocity and volunteering are related, but that this relationship becomes only visible if we refine the conceptual framework for both concepts. Using data from the Swiss Volunteering Survey 2009, the empirical results of our Bayesian multilevel models show the following: firstly, individuals exhibiting high levels of altruistic reciprocity are more likely to engage in informal volunteering; secondly, we find a negative relationship between altruist reciprocity and the individual likeliness to do voluntary work within non-solidary associations; thirdly, once individuals opted to engage in formal volunteering, we find that strategic reciprocity is clearly related to voluntary engagement in non-solidary associations. Overall, our conceptual foundation provides a more appropriate model to explain the formation of volunteering.",0
,Estimating population kinetics.,"This paper will review the methods that have been advanced for the estimation of parameters of models quantifying the population characteristics of the kinetic behavior of endogenous and exogenous substances in individuals of the population. Such methods are used frequently, for example, in pharmacokinetic studies. In certain populations, especially biological ones, considerable kinetic variability between population members is present. The models with which we are concerned describe this variability. Some interindividual kinetic variability may be explainable on the basis of measureable concomitant variables, but much of it may remain unexplainable on such a basis. We give this matter particular attention. The assumptions underlying the models are critically discussed.",0
https://doi.org/10.1002/bimj.200900130,A Hierarchical Model to Estimate Fish Abundance in Alpine Streams by using Removal Sampling Data from Multiple Locations,"The author compares 12 hierarchical models in the aim of estimating the abundance of fish in alpine streams by using removal sampling data collected at multiple locations. The most expanded model accounts for (i) variability of the abundance among locations, (ii) variability of the catchability among locations, and (iii) residual variability of the catchability among fish. Eleven model reductions are considered depending which variability is included in the model. The more restrictive model considers none of the aforementioned variabilities. Computations of the latter model can be achieved by using the algorithm presented by Carle and Strub (Biometrics 1978, 34, 621–630). Maximum a posteriori and interval estimates of the parameters as well as the Akaike and the Bayesian information criterions of model fit are computed by using samples simulated by a Markov chain Monte Carlo method. The models are compared by using a trout (Salmo trutta fario) parr (0+) removal sampling data set collected at three locations in the Pyrénées mountain range (Haute-Garonne, France) in July 2006. Results suggest that, in this case study, variability of the catchability is not significant, either among fish or locations. Variability of the abundance among locations is significant. 95% interval estimates of the abundances at the three locations are [0.15, 0.24], [0.26, 0.36], and [0.45, 0.58] parrs per m2. Such differences are likely the consequence of habitat variability.",0
https://doi.org/10.3758/brm.42.3.884,Bayesian inference using WBDev: A tutorial for social scientists,"Over the last decade, the popularity of Bayesian data analysis in the empirical sciences has greatly increased. This is partly due to the availability of WinBUGS, a free and flexible statistical software package that comes with an array of predefined functions and distributions, allowing users to build complex models with ease. For many applications in the psychological sciences, however, it is highly desirable to be able to define one's own distributions and functions. This functionality is available through the WinBUGS Development Interface (WBDev). This tutorial illustrates the use of WBDev by means of concrete examples, featuring the expectancy-valence model for risky behavior in decision making, and the shifted Wald distribution of response times in speeded choice.",0
https://doi.org/10.2307/2531734,Models for Longitudinal Data: A Generalized Estimating Equation Approach,"This article discusses extensions of generalized linear models for the analysis of longitudinal data. Two approaches are considered: subject-specific (SS) models in which heterogeneity in regression parameters is explicitly modelled; and population-averaged (PA) models in which the aggregate response for the population is the focus. We use a generalized estimating equation approach to fit both classes of models for discrete and continuous outcomes. When the subject-specific parameters are assumed to follow a Gaussian distribution, simple relationships between the PA and SS parameters are available. The methods are illustrated with an analysis of data on mother's smoking and children's respiratory disease.",0
https://doi.org/10.1177/00131649921969811,Validity Issues in the Likert and Thurstone Approaches to Attitude Measurement,"This article highlights the theoretical differences between the Likert and Thurstone approaches to attitude measurement and demonstrates how such differences can lead to discrepant attitude estimates for individuals with the most extreme opinions. Both simulated data and real data on attitude toward abortion are used to demonstrate this discrepancy. The results suggest that attitude researchers should, at the very least, devote more attention to the empirical response characteristics of items on a Likert attitude questionnaire. At most, these results suggest that other methods, such as the Thurstone technique or one of its recently developed item response theory counterparts, should be used to derive attitude estimates from disagree-agree responses.",0
https://doi.org/10.1007/s00221-015-4501-8,"Determining thresholds using adaptive procedures and psychometric fits: evaluating efficiency using theory, simulations, and human experiments","When measuring thresholds, careful selection of stimulus amplitude can increase efficiency by increasing the precision of psychometric fit parameters (e.g., decreasing the fit parameter error bars). To find efficient adaptive algorithms for psychometric threshold (""sigma"") estimation, we combined analytic approaches, Monte Carlo simulations, and human experiments for a one-interval, binary forced-choice, direction-recognition task. To our knowledge, this is the first time analytic results have been combined and compared with either simulation or human results. Human performance was consistent with theory and not significantly different from simulation predictions. Our analytic approach provides a bound on efficiency, which we compared against the efficiency of standard staircase algorithms, a modified staircase algorithm with asymmetric step sizes, and a maximum likelihood estimation (MLE) procedure. Simulation results suggest that optimal efficiency at determining threshold is provided by the MLE procedure targeting a fraction correct level of 0.92, an asymmetric 4-down, 1-up staircase targeting between 0.86 and 0.92 or a standard 6-down, 1-up staircase. Psychometric test efficiency, computed by comparing simulation and analytic results, was between 41 and 58% for 50 trials for these three algorithms, reaching up to 84% for 200 trials. These approaches were 13-21% more efficient than the commonly used 3-down, 1-up symmetric staircase. We also applied recent advances to reduce accuracy errors using a bias-reduced fitting approach. Taken together, the results lend confidence that the assumptions underlying each approach are reasonable and that human threshold forced-choice decision making is modeled well by detection theory models and mimics simulations based on detection theory models.",0
https://doi.org/10.1016/j.jempfin.2014.09.006,An empirical Bayesian approach to stein-optimal covariance matrix estimation,"This paper proposes a conjugate Bayesian regression model to estimate the covariance matrix of a large number of securities. Characterizing the return generating process with an unrestricted factor model, prior beliefs impose structure while preserving estimator consistency. This framework accommodates economically-motivated prior beliefs and nests shrinkage covariance matrix estimators, providing a common model for their interpretation. Minimizing posterior finite-sample square error delivers a fully-automated covariance matrix estimator with beliefs that become diffuse as the sample grows relative to the dimension of the problem. In application, this Stein-optimal posterior covariance matrix performs well in a large set of simulation experiments.",0
https://doi.org/10.1109/jbhi.2016.2532354,Driver Fatigue Classification With Independent Component by Entropy Rate Bound Minimization Analysis in an EEG-Based System,"This paper presents a two-class electroencephal-ography-based classification for classifying of driver fatigue (fatigue state versus alert state) from 43 healthy participants. The system uses independent component by entropy rate bound minimization analysis (ERBM-ICA) for the source separation, autoregressive (AR) modeling for the features extraction, and Bayesian neural network for the classification algorithm. The classification results demonstrate a sensitivity of 89.7%, a specificity of 86.8%, and an accuracy of 88.2%. The combination of ERBM-ICA (source separator), AR (feature extractor), and Bayesian neural network (classifier) provides the best outcome with a p-value < 0.05 with the highest value of area under the receiver operating curve (AUC-ROC = 0.93) against other methods such as power spectral density as feature extractor (AUC-ROC = 0.81). The results of this study suggest the method could be utilized effectively for a countermeasure device for driver fatigue identification and other adverse event applications.",0
https://doi.org/10.1080/01621459.1975.10479864,Data Analysis Using Stein's Estimator and its Generalizations,"Abstract In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution having uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein's rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson's chi-square test with results from a computer simulation. In each of these examples, the mean square error of these rules is less than half that of the sample mean.",0
https://doi.org/10.1093/esr/jcs059,Positive or Negative Policy Feedbacks? Explaining Popular Attitudes Towards Pragmatic Pension Policy Reforms,"Recent decades have seen increased interest in public attitudes towards public pension policies. Most previous research, however, relies heavily on dependent variables that fail to reflect the effective alternatives being discussed in most affluent democracies. This article seeks to improve our understanding of public attitudes towards pragmatic welfare policy options by examining cross-national differences in attitudes towards (i) cuts in old-age pension benefits, (ii) increases in social security contributions, and (iii) increases in the statutory retirement age. We test predictions of the dominant positive policy feedback theory and the alternative negative policy feedback theory. These approaches argue that policies induce consequences and attitudes that reinforce (positive feedback) or undermine (negative feedback) past policymaking trajectories. Empirical results obtained by multilevel analyses from a sample of 27 European countries are consistent mainly with the negative feedback approach. In countries with higher statutory retirement ages, citizens are more likely to support a postponement of retirement. However, in countries with higher elderly poverty, citizens are less likely to support cuts in pension benefits. In countries with higher social security contributions, citizens are less likely to support further increases in these contributions. Ã‚Â© The Author 2012.",0
https://doi.org/10.1186/1471-2458-13-300,Weight gain prevention in young adults: design of the study of novel approaches to weight gain prevention (SNAP) randomized controlled trial,"Weight gain during young adulthood is common and is associated with increased cardiovascular risk. Preventing this weight gain from occurring may be critical to improving long-term health. Few studies have focused on weight gain prevention, and these studies have had limited success. SNAP (Study of Novel Approaches to Weight Gain Prevention) is an NIH-funded randomized clinical trial examining the efficacy of two novel self-regulation approaches to weight gain prevention in young adults compared to a minimal treatment control. The interventions focus on either small, consistent changes in eating and exercise behaviors, or larger, periodic changes to buffer against expected weight gains.SNAP targets recruitment of six hundred young adults (18-35 years) with a body mass index between 21.0-30.0 kg/m2, who will be randomly assigned with equal probability to: (1) minimal intervention control; (2) self-regulation with Small Changes; or (3) self-regulation with Large Changes. Both interventions receive 8 weekly face-to-face group sessions, followed by 2 monthly sessions, with two 4-week refresher courses in each of subsequent years. Participants are instructed to report weight via web at least monthly thereafter, and receive monthly email feedback. Participants in Small Changes are taught to make small daily changes (~100 calorie changes) in how much or what they eat and to accumulate 2000 additional steps per day. Participants in Large Changes are taught to create a weight loss buffer of 5-10 pounds once per year to protect against anticipated weight gains. Both groups are encouraged to self-weigh daily and taught a self-regulation color zone system that specifies action depending on weight gain prevention success. Individualized treatment contact is offered to participants who report weight gains. Participants are assessed at baseline, 4 months, and then annually. The primary outcome is weight gain over an average of 3 years of follow-up; secondary outcomes include diet and physical activity behaviors, psychosocial measures, and cardiovascular disease risk factors.SNAP is unique in its focus on weight gain prevention in young adulthood. The trial will provide important information about whether either or both of these novel interventions are effective in preventing weight gain.ClinicalTrials.gov, NCT01183689.",0
https://doi.org/10.1177/0272989x08324036,Integration of Meta-analysis and Economic Decision Modeling for Evaluating Diagnostic Tests,"Meta-analysis of diagnostic test accuracy data is more difficult than of effectiveness data because of 1) statistical challenges of dealing with multiple measures of accuracy (e.g., sensitivity and specificity) simultaneously and 2) incorporating threshold effects. A number of meta-analysis models are in use, ranging from naïve synthesis of independent sensitivity and specificity to optimization of a hierarchical summary receiver operating characteristic (SROC) curve. Little work has been done on how such analyses should inform decision models. This article aims to present a unified framework for the synthesis of primary data and economic evaluation of alternative diagnostic testing strategies using Bayesian Markov Chain Monte Carlo simulation methods. The authors extend this previous work by using systematic review to derive model parameters, fully allowing for uncertainty in their estimation, and formally incorporating variability between study results into the decision analysis. Using a simple decision model comparing alternative testing strategies for suspected deep vein thrombosis as an example, the authors consider how to use outputs of different alternative meta-analysis models in decision models. They also explore the limitations of diagnostic test studies, particularly when there is no obvious threshold value. To correct some of the limitations of diagnostic test studies, they propose that tests with implicit and explicit thresholds should be studied using distinctly different frameworks. Specifically, when a threshold exists, quantitative threshold information should be included in meta-analysis models to aid interpretation of SROCs. Setting policy to relate to a specific point may be much more difficult for studies with implicit thresholds.",0
https://doi.org/10.1080/10543406.2011.607736,The Bootstrap and Markov-Chain Monte Carlo,"This note concerns the use of parametric bootstrap sampling to carry out Bayesian inference calculations. This is only possible in a subset of those problems amenable to Markov-Chain Monte Carlo (MCMC) analysis, but when feasible the bootstrap approach offers both computational and theoretical advantages. The discussion here is in terms of a simple example, with no attempt at a general analysis.",0
https://doi.org/10.1080/10705511.2012.634703,Using SEM to Analyze Complex Survey Data: A Comparison between Design-Based Single-Level and Model-Based Multilevel Approaches,"Both ad-hoc robust sandwich standard error estimators (design-based approach) and multilevel analysis (model-based approach) are commonly used for analyzing complex survey data with nonindependent observations. Although these 2 approaches perform equally well on analyzing complex survey data with equal between- and within-level model structures (B. O. Muthen & Satorra, 1995), the performances of these 2 approaches for analyzing multilevel data with unequal between- and within-level structures have not yet been systematically examined. In this study, we extended B. O. Muthen and Satorra's (1995) study by comparing these 2 approaches and an additional model-based maximum model for analyzing multilevel data considering number of clusters, cluster size, intraclass correlation, and the equality of different level structures. The simulation results showed the model-based maximum model generally performed well across conditions. This model is also recommended as an alternative for analyzing nonindependent survey...",0
https://doi.org/10.1080/00273171.2012.640596,Explanation of Two Anomalous Results in Statistical Mediation Analysis,"Previous studies of different methods of testing mediation models have consistently found two anomalous results. The first result is elevated Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap tests not found in nonresampling tests or in resampling tests that did not include a bias correction. This is of special concern as the bias-corrected bootstrap is often recommended and used due to its higher statistical power compared with other tests. The second result is statistical power reaching an asymptote far below 1.0 and in some conditions even declining slightly as the size of the relationship between X and M, a, increased. Two computer simulations were conducted to examine these findings in greater detail. Results from the first simulation found that the increased Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap are a function of an interaction between the size of the individual paths making up the mediated effect and the sample size, such that elevated Type I error rates occur when the sample size is small and the effect size of the nonzero path is medium or larger. Results from the second simulation found that stagnation and decreases in statistical power as a function of the effect size of the a path occurred primarily when the path between M and Y, b, was small. Two empirical mediation examples are provided using data from a steroid prevention and health promotion program aimed at high school football players (Athletes Training and Learning to Avoid Steroids; Goldberg et al., 1996), one to illustrate a possible Type I error for the bias-corrected bootstrap test and a second to illustrate a loss in power related to the size of a. Implications of these findings are discussed.",0
https://doi.org/10.1177/014662169301700307,A Hyperbolic Cosine Latent Trait Model For Unfolding Dichotomous Single-Stimulus Responses,"Social-psychological variables are typically measured using either cumulative or unfolding response processes. In the former, the greater the location of a person relative to the location of a stimulus on the continuum, the greater the proba bility of a positive response; in the latter, the closer the location of the person to the location of the statement, irrespective of direction, the greater the probability of a positive response. Formal probability models for these processes are, respec tively, monotonically increasing and single-peaked as a function of the location of the person relative to the location of the statement. In general, these models have been considered to be independent of each other. However, if statements constructed on the basis of a cumulative model have three ordered response categories, the response function within the statement for the middle category is in fact single-peaked. Using this observation, a unidimen sional model for responses to statements that have an unfolding structure was constructed from the cumulative Rasch model for ordered response categories. A location and unit of measurement parameter exist for each statement. A joint maxi mum likelihood estimation procedure was inves tigated. Analysis of a small simulation study and a small real dataset showed that the model is readily applicable.",0
https://doi.org/10.3758/bf03192841,Analyzing individual differences in sentence processing performance using multilevel models,"The use of multilevel models is increasingly common in the behavioral sciences for analyzing hierarchically structured data, including repeated measures data. These models are flexible and easily implemented via a variety of commercially available statistical software programs. We consider their application in the context of an eye-movement experiment testing readers' responses to a semantic plausibility manipulation in temporarily ambiguous sentences. Multilevel models were used to study the relationship between working memory capacity and the extent to which readers were disrupted by syntactic misanalysis. This represented a cross-level interaction between an individual difference measure and a sentence-level characteristic. We compare a multilevel modeling approach to a standard approach based on ANOVA.",0
https://doi.org/10.1348/000711005x38951,Multilevel IRT using dichotomous and polytomous response data,"A structural multilevel model is presented where some of the variables cannot be observed directly but are measured using tests or questionnaires. Observed dichotomous or ordinal polytomous response data serve to measure the latent variables using an item response theory model. The latent variables can be defined at any level of the multilevel model. A Bayesian procedure Markov chain Monte Carlo (MCMC), to estimate all parameters simultaneously is presented. It is shown that certain model checks and model comparisons can be done using the MCMC output. The techniques are illustrated using a simulation study and an application involving students' achievements on a mathematics test and test results regarding management characteristics of teachers and principles.",0
https://doi.org/10.1177/1094428108327450,Testing Multilevel Mediation Using Hierarchical Linear Models,"Testing multilevel mediation using hierarchical linear modeling (HLM) has gained tremendous popularity in recent years. However, potential confounding in multilevel mediation effect estimates can arise in these models when within-group effects differ from between-group effects. This study summarizes three types of HLM-based multilevel mediation models, and then explains that in two types of these models confounding can be produced and erroneous conclusions may be derived when using popularly recommended procedures. A Monte Carlo simulation study illustrates that these procedures can underestimate or overestimate true mediation effects. Recommendations are provided for appropriately testing multilevel mediation and for differentiating within-group versus between-group effects in multilevel settings.",0
https://doi.org/10.1111/rssc.12096,A Bayesian approach for the analysis of triadic data in cognitive social structures,Summary The paper proposes a fully Bayesian approach for the analysis of triadic data in social networks. Inference is based on Markov chain Monte Carlo methods as implemented in the software package WinBUGS. We apply the methodology to two data sets to highlight the ease with which cognitive social structures can be analysed.,0
https://doi.org/10.1016/j.neurobiolaging.2008.04.004,Establishing short-term prognosis in Frontotemporal Lobar Degeneration spectrum: Role of genetic background and clinical phenotype,"Establishing the short-term prognosis in Frontotemporal Lobar Degeneration (FTLD) is a clinical challenge for defining disease outcomes and monitoring therapeutic interventions. No reliable neuropsychological assessment balancing all FTLD aspects is available yet, thus no clear-cut follow-up study has been performed.To evaluate the rate of progression and the predictors of worsening in FTLD patients.One-hundred twenty-seven FTLD patients entered the study and were re-evaluated at 1-year follow-up. A statistical driven approach on wide neuropsychological, behavioral, and functional data was applied to identify homogeneous groups both at baseline and at follow-up within FTLD. Three set of predictors on disease progression were considered: (i) the demographic characteristics, (ii) the genetic background, i.e. Apolipoprotein E (APOE) genotype, Tau haplotype, and functional polymorphisms affecting serotonin and dopamine pathways, and (iii) the clinical phenotype.Among FTLD, two groups of patients were recognized on the basis of the overall assessment, thus termed for different disease severity as ""good performers"" and ""bad performers"". At 1-year follow-up, almost 30% of FTLD patients progressed from ""good"" to ""bad"" performances, whilst 70% maintained stable ""good"" performances. APOE varepsilon4 allele, Tau H2 haplotype and behavioral variant FTD phenotype were associated with worse prognosis over time.This preliminary study proposed genetic and clinical predictors in FTLD progression. The identification of disease-modifying predictors of prognosis opens a new avenue in studying FTLD, and may contribute to define outcomes and to monitor pharmacological targets.",0
https://doi.org/10.1177/0146621603027004004,Further Investigation of the Performance of S - X2: An Item Fit Index for Use With Dichotomous Item Response Theory Models,"This study presents new findings on the utility of S - X 2 as an item fit index for dichotomous item response theory models. Results are based on a simulation study in which item responses were generated and calibrated for 100 tests under each of 27 conditions. The item fit indices S - X 2 and Q 1 - X 2 were calculated for each item. ROC curves were constructed based on the hit and false alarm rates of the two indices. Examination of these curves indicated that in general, the performance of S - X 2 improved with test length and sample size. The performance of S - X 2 was superior to that of Q 1 - X 2 under most but not all conditions. Results from this study imply that S - X 2 may be a useful tool in detecting the misfit of one item contained in an otherwise well-fitted test, lending additional support to the utility of the index for use with dichotomous item response theory models. Index Terms: item response theory, S - X 2 , Q 1 - X, model = data fit, item fit index.",0
https://doi.org/10.1016/j.adolescence.2015.08.009,Does body satisfaction influence self‐esteem in adolescents' daily lives? An experience sampling study,"This study examined, within the context of the Contingencies of Self-Worth model, state-based associations between self-esteem and body satisfaction using the experience sampling method. One hundred and forty-four adolescent girls (mean age = 14.28 years) completed up to 6 assessments per day for one week using Palm Digital Assistants, in addition to baseline measures of trait body satisfaction and self-esteem. Results showed considerable variation in both state-based constructs within days, and evidence of effects of body satisfaction on self-esteem, but not vice versa. Although these state-based associations were small in size and weakened as the time lag between assessments increased for the sample as a whole, individual differences in the magnitude of these effects were observed and predicted by trait self-esteem and body satisfaction. Collectively, these findings offer support for key tenets of the Contingencies of Self-Worth model.",0
https://doi.org/10.1093/europace/euq450,"Mixed treatment comparison of dronedarone, amiodarone, sotalol, flecainide, and propafenone, for the management of atrial fibrillation","Mixed treatment comparisons (MTC) were performed to assess the relative efficacy and tolerability of the main anti-arrhythmic drugs used for the treatment of atrial fibrillation (AF)/flutter.Electronic databases were systematically searched to identify randomized controlled trials (RCTs) examining amiodarone, dronedarone, flecainide, propafenone, sotalol, or placebo for the treatment of AF. Thirty-nine RCTs met inclusion criteria and were combined using MTC models to provide direct and indirect comparisons in a single analysis. Results are presented vs. placebo. Amiodarone had the largest effect in reducing AF recurrence (OR 0.22, 95% CI 0.16-0.29). Amiodarone was associated with the highest rate of patients experiencing at least one serious adverse event (OR 2.41, 95% CI 0.96-6.06) and treatment withdrawals due to adverse events (OR 2.91, 95% CI 1.66-5.11). Dronedarone was associated with the lowest rate of proarrhythmic events including bradycardia (OR 1.45, 95% CI 1.02-2.08). Dronedarone significantly reduced the risk of stroke (OR 0.69, 95% CI 0.57-0.84). Trends towards increased mortality for sotalol (OR 3.44, 95% CI 1.02-11.59) and amiodarone (OR 2.17, 95% CI 0.63-7.51) were found, which were stronger when small studies randomizing <100 subjects per group were excluded.Amiodarone has been demonstrated to be the most effective drug in maintaining sinus rhythm. Differences in outcomes between the anti-antiarrhythmic drugs were reported, with sotalol and possibly amiodarone increasing mortality and dronedarone possibly decreasing the incidence of serious adverse events and proarrhythmia.",0
https://doi.org/10.1198/000313005x20727,Randomization of Clusters Versus Randomization of Persons Within Clusters,"Many experiments aim at populations with persons nested within clusters. Randomization to treatment conditions can be done at the cluster level or at the person level within each cluster. The latter may result in control group contamination, and cluster randomization is therefore often preferred in practice. This article models the control group contamination, calculates the required sample sizes for both levels of randomization, and gives the degree of contamination for which cluster randomization is preferable above randomization of persons within clusters. Moreover, it provides examples of situations where one has to make a choice between both levels of randomization.",0
https://doi.org/10.1016/s0042-6989(02)00118-9,Recognizing spatial patterns: a noisy exemplar approach,"Models of categorization typically rely on the use of stimuli composed of well-defined dimensions (e.g., Ashby & Maddox (1998) in Choice , decision , and measurement: Essays in honor of R. Duncan Luce , p. 251–301, Mahwah, NJ: Erlbaum). We apply a similar approach to the analysis of recognition memory. Using a version of short-term recognition paradigm (Sternberg, Science 153 (1966) 652), we asked whether NEMO Sternberg's, a noisy exemplar summed-similarity model, could account for variation in mean performance on individual trials. NEMO provided a very good overall fit to recognition data from three experiments. However, its failure to fit data for certain lists of stimuli suggested a revision of the summed-similarity assumption. Our model-based analysis showed that subjects used interitem similarity, in addition to probe-item similarity, as the basis for their decisions. This represents a major departure from existing recognition models that assume subjects' judgments depend exclusively on the summed similarity of the probe to the study items.",0
https://doi.org/10.1214/ss/1009212673,Power prior distributions for regression models,"We propose a general class of prior distributions for arbitrary regression models. We discuss parametric and semiparametric models. The prior specification for the regression coefficients focuses on observ- able quantities in that the elicitation is based on the availability of his- torical data Do and a scalar quantity ao quantifying the uncertainty in Do. Then Do and ao are used to specify a prior for the regression coeffi- cients in a semiautomatic fashion. The most natural specification of Do arises when the raw data from a similar previous study are available. The availability of historical data is quite common in clinical trials, car- cinogenicity studies, and environmental studies, where large data bases are available from similar previous studies. Although the methodology we present here is quite general, we will focus only on using historical data from similar previous studies to construct the prior distributions. The prior distributions are based on the idea of raising the likelihood function of the historical data to the power ao, where 0 < ao < 1. We call such prior distributions power prior distributions. We examine the power prior for four commonly used classes of regression models. These include generalized linear models, generalized linear mixed models, semipara- metric proportional hazards models, and cure rate models for survival data. For these classes of models, we discuss the construction of the power prior, prior elicitation issues, propriety conditions, model selec- tion, and several other properties. For each class of models, we present real data sets to demonstrate the proposed methodology.",0
https://doi.org/10.1037/1082-989x.12.3.317,Mixed-effects logistic regression for estimating transitional probabilities in sequentially coded observational data.,"This article demonstrates the use of mixed-effects logistic regression (MLR) for conducting sequential analyses of binary observational data. MLR is a special case of the mixed-effects logit modeling framework, which may be applied to multicategorical observational data. The MLR approach is motivated in part by G. A. Dagne, G. W. Howe, C. H. Brown, & B. O. Muthén (2002) advances in general linear mixed models for sequential analyses of observational data in the form of contingency table frequency counts. The advantage of the MLR approach is that it circumvents obstacles in the estimation of random sampling error encountered using Dagne and colleagues' approach. This article demonstrates the MLR model in an analysis of observed sequences of communication in a sample of young adult same-sex peer dyads. The results obtained using MLR are compared with those of a parallel analysis using Dagne and colleagues' linear mixed model for binary observational data in the form of log odds ratios. Similarities and differences between the results of the 2 approaches are discussed. Implications for the use of linear mixed models versus mixed-effects logit models for sequential analyses are considered.",0
https://doi.org/10.1007/bf02296272,A rasch model for partial credit scoring,"A unidimensional latent trait model for responses scored in two or more ordered categories is developed. This ""Partial Credit"" model is a member of the family of latent trait models which share the property of parameter separability and so permit ""specifically objective"" comparisons of persons and items. The model can be viewed as an extension of Andrich's Rating Scale model to situations in which ordered response alternatives are free to vary in number and structure from item to item. The difference between the parameters in this model and the ""category boundaries"" in Samejima's Graded Response model is demonstrated. An unconditional maximum likelihood procedure for estimating the model parameters is developed. Â© 1982 The Psychometric Society.",0
https://doi.org/10.3402/ejpt.v4i0.21892,Longitudinal course of physical and psychological symptoms after a natural disaster,"After disaster, physical symptoms are common although seldom recognized due to lack of knowledge of the course of symptoms and relation to more studied psychological symptoms.This study aimed to investigate the change in the reporting of different physical symptoms after a disaster, including possible factors for change, and whether psychological symptoms predict physical symptoms reporting at a later point in time.A longitudinal study of citizens of Stockholm who survived the 2004 Indian Ocean tsunami. A total of 1,101 participants completed questionnaires on somatic symptoms, general distress, posttraumatic stress, exposure, and demographic details 14 months and 3 years after the disaster. Physical symptoms occurring daily or weekly during the last year were investigated in four symptom indices: neurological, cardiorespiratory, gastrointestinal, and musculoskeletal. We used generalized estimating equations (GEE) analysis to determine odds ratios for a change in symptoms, and pathway analysis to predict the influence of psychological symptoms on physical symptoms.There was a general decrease of reporting in all physical symptom indices except the musculoskeletal symptom index. The change in the neurological symptom index showed the strongest association with exposure, and for women. General distress and posttraumatic stress at 14 months postdisaster predicted physical symptoms at 3 years.Physical symptoms were predicted by psychological symptoms at an earlier time point, but in a considerable proportion of respondents, physical symptoms existed independently from psychological symptoms. Physicians should be observant on the possible connection of particular pseudoneurological symptoms with prior adversities.",0
https://doi.org/10.1186/1476-072x-13-36,Comparing multilevel and Bayesian spatial random effects survival models to assess geographical inequalities in colorectal cancer survival: a case study,"Multilevel and spatial models are being increasingly used to obtain substantive information on area-level inequalities in cancer survival. Multilevel models assume independent geographical areas, whereas spatial models explicitly incorporate geographical correlation, often via a conditional autoregressive prior. However the relative merits of these methods for large population-based studies have not been explored. Using a case-study approach, we report on the implications of using multilevel and spatial survival models to study geographical inequalities in all-cause survival.Multilevel discrete-time and Bayesian spatial survival models were used to study geographical inequalities in all-cause survival for a population-based colorectal cancer cohort of 22,727 cases aged 20-84 years diagnosed during 1997-2007 from Queensland, Australia.Both approaches were viable on this large dataset, and produced similar estimates of the fixed effects. After adding area-level covariates, the between-area variability in survival using multilevel discrete-time models was no longer significant. Spatial inequalities in survival were also markedly reduced after adjusting for aggregated area-level covariates. Only the multilevel approach however, provided an estimation of the contribution of geographical variation to the total variation in survival between individual patients.With little difference observed between the two approaches in the estimation of fixed effects, multilevel models should be favored if there is a clear hierarchical data structure and measuring the independent impact of individual- and area-level effects on survival differences is of primary interest. Bayesian spatial analyses may be preferred if spatial correlation between areas is important and if the priority is to assess small-area variations in survival and map spatial patterns. Both approaches can be readily fitted to geographically enabled survival data from international settings.",0
https://doi.org/10.1002/sim.6631,The choice of prior distribution for a covariance matrix in multivariate meta-analysis: a simulation study,"Bayesian meta-analysis is an increasingly important component of clinical research, with multivariate meta-analysis a promising tool for studies with multiple endpoints. Model assumptions, including the choice of priors, are crucial aspects of multivariate Bayesian meta-analysis (MBMA) models. In a given model, two different prior distributions can lead to different inferences about a particular parameter. A simulation study was performed in which the impact of families of prior distributions for the covariance matrix of a multivariate normal random effects MBMA model was analyzed. Inferences about effect sizes were not particularly sensitive to prior choice, but the related covariance estimates were. A few families of prior distributions with small relative biases, tight mean squared errors, and close to nominal coverage for the effect size estimates were identified. Our results demonstrate the need for sensitivity analysis and suggest some guidelines for choosing prior distributions in this class of problems. The MBMA models proposed here are illustrated in a small meta-analysis example from the periodontal field and a medium meta-analysis from the study of stroke. Copyright © 2015 John Wiley & Sons, Ltd.",0
https://doi.org/10.1080/03055698.2015.955750,The relationship between ethnic diversity and classroom disruption in the context of migration policies,"This paper studies the relationship between ethnic school composition and classroom disruption in secondary education in the context of migration policies. We measured classroom disruption using students’ reports from 3533 schools in 20 countries provided by cross-national PISA (Programme for International Student Assessment) 2009 data. We employ the migrant share and the ethnic diversity net of the native share as indicators of the ethnic composition of a school. The MIPEX (Immigrant Integration Policy indEX) is used as an indicator of migration policies. Our results show a positive association between ethnic school diversity net of the migrant share and classroom disruption. Furthermore, we show a negative interaction term of the migration policy and ethnic diversity. Consequently, our results indicate that students in countries with a more inclusive migration policy are at least less harmed by influence of ethnic school diversity regarding classroom disruption. Findings partly support the “contact hypo...",0
https://doi.org/10.1207/s15324818ame0204_6,Procedures for Selecting Items for Computerized Adaptive Tests,"Many procedures have been developed for selecting the best items for a computerized adaptive test. There is a trend toward the use of adaptive testing in applied settings such as licensure tests, program entrance tests, and educational tests. It is useful to consider procedures for item selection and the special needs of applied testing settings to facilitate test design. The current study reviews several classical approaches and alternative approaches to item selection and discusses their relative merit. This study also describes procedures for constrained computerized adaptive testing (C-CAT) that may be added to classical item selection approaches to allow them to be used for applied testing, while maintaining the high measurement precision and short test length that made adaptive testing attractive to practitioners initially.",0
https://doi.org/10.1016/j.humov.2007.07.005,A new methodology providing evidence of two distinct processes in the production of hand/foot simultaneous responses,"To get simultaneous responses of the hand and the foot, it is mandatory to compensate for the longer peripheral motor conduction delay of the foot. According to the reactive-projective model [Paillard, J. (1948). Quelques données psychophysiologiques relatives au déclenchement de la commande motrice (Some psychophysiological data in relation to the releasing of the motor commands). Année Psychologique, 28-47; Paillard, J. (1990). Réactif et prédictif: deux modes de gestion du geste de la motricité. In V. Nougier, & J. Blanchi (Eds.), Pratiques sportives et modélisation du geste (Sport activity and gesture modeling) (pp. 13-56). Grenoble: Université Joseph-Fourier.] no compensation occurs in a reaction time situation; the hand responds before the foot, which indicates a single motor command released for both effectors. However, in a self-initiated condition, the foot tends to precede the hand suggesting that two distinct motor commands are issued, with the foot command first. Fully self-initiated movements are not usual. It is more usual to prepare a response in anticipation of the time occurrence of a stimulus (e.g., a musician following a conductor, synchronized swimmers emerging together with the music). Therefore, we developed a methodology to test whether the model holds in an anticipation coincidence task. In Experiment 1, the participants were asked to initiate a synchronized hand/foot response when the continuous visual stimulus (constant speed) reaches a target. The results fitted the model. In Experiment 2, anticipation coincidence tasks were performed in three conditions: using the foot (1) or the hand (2) alone, and using the hand and the foot simultaneously (3). Following a constant stimulus protocol, short tones were randomly produced, prior the stimulus, to indicate the participants to inhibit their response. As expected, the frequencies of correct inhibition in each preset period followed a sigmoid curve. The command release is assumed to occur at the biserial point (50% of inhibition). The results confirmed that the motor command of the foot is released sooner than the command of the hand. The hand/foot delay is lower in the simultaneous condition, because the command of the hand is released 40 ms earlier; while the foot command is 10 ms earlier. These data confirm and extend the projective-reactive model to a new category of coordination behavior.",0
https://doi.org/10.1198/jasa.2009.tm08564,Generalized Multilevel Functional Regression,"We introduce Generalized Multilevel Functional Linear Models (GMFLMs), a novel statistical framework for regression models where exposure has a multilevel functional structure. We show that GMFLMs are, in fact, generalized multilevel mixed models (GLMMs). Thus, GMFLMs can be analyzed using the mixed effects inferential machinery and can be generalized within a well researched statistical framework. We propose and compare two methods for inference: 1) a two-stage frequentist approach; and 2) a joint Bayesian analysis. Our methods are motivated by and applied to the Sleep Heart Health Study (SHHS), the largest community cohort study of sleep. However, our methods are general and easy to apply to a wide spectrum of emerging biological and medical data sets. Supplemental materials for this article are available online.",0
https://doi.org/10.1002/9781119961154.ch19,Multilevel Models for Ordinal Data,"This chapter is concerned with regression models for ordinal responses, with special emphasis on random effects models for multilevel or clustered data. After a brief discussion on ordinal variables, it reviews the most common regression models for ordinal responses, focusing on cumulative models, namely models based on cumulative probabilities. The chapter then deals with random effects cumulative models for multilevel data, discussing several issues peculiar to the random effects extension such as the distinction between marginal and conditional effects, the measures of unobserved cluster-level heterogeneity, the consequences of adding covariates, and the main types of predicted probabilities. It also deals with estimation, inference and prediction, with a brief look on available software. Finally, it presents an application of random effects cumulative models to the analysis of student ratings of university courses. Ã‚Â© 2012 John Wiley & Sons, Ltd. All rights reserved.",0
https://doi.org/10.15446/rce.v37n2spe.47940,Hierarchical Graphical Bayesian Models in Psychology,The improvement of graphical methods in psychological research can promote their use and a better comprehension of their expressive power. The application of hierarchical Bayesian graphical models has recently become more frequent in psychological research. The aim of this contribution is to introduce suggestions for the improvement of hierarchical Bayesian graphical models in psychology. This novel set of suggestions stems from the description and comparison between two main approaches concerned with the use of plate notation and distribution pictograms. It is concluded that the combination of relevant aspects of both models might improve the use of powerful hierarchical Bayesian graphical models in psychology.,0
https://doi.org/10.1214/aoms/1177693528,Proper Bayes Minimax Estimators of the Multivariate Normal Mean,,0
https://doi.org/10.1037/a0028087,"Testing the significance of a correlation with nonnormal data: Comparison of Pearson, Spearman, transformation, and resampling approaches.","It is well known that when data are nonnormally distributed, a test of the significance of Pearson's r may inflate Type I error rates and reduce power. Statistics textbooks and the simulation literature provide several alternatives to Pearson's correlation. However, the relative performance of these alternatives has been unclear. Two simulation studies were conducted to compare 12 methods, including Pearson, Spearman's rank-order, transformation, and resampling approaches. With most sample sizes (n ≥ 20), Type I and Type II error rates were minimized by transforming the data to a normal shape prior to assessing the Pearson correlation. Among transformation approaches, a general purpose rank-based inverse normal transformation (i.e., transformation to rankit scores) was most beneficial. However, when samples were both small (n ≤ 10) and extremely nonnormal, the permutation test often outperformed other alternatives, including various bootstrap tests.",0
https://doi.org/10.1177/01421602026002002,Multidimensional Constrained Test Assembly,"Two mathematical programming approaches are presented for the assembly of ability tests from item pools calibrated under a multidimensional item response theory model. Item selection is based on Fisher information matrix. Several criteria can be used to optimize this matrix. In this article, the A-criterion and the D-criterion are applied. Empirical examples for a two-dimensional mathematics item pool illustrate the methods. Both criteria provided good results. Recommendations are provided about when to apply either approach.",0
https://doi.org/10.1037/a0014268,Average causal effects from nonrandomized studies: A practical guide and simulated example.,"In a well-designed experiment, random assignment of participants to treatments makes causal inference straightforward. However, if participants are not randomized (as in observational study, quasi-experiment, or nonequivalent control-group designs), group comparisons may be biased by confounders that influence both the outcome and the alleged cause. Traditional analysis of covariance, which includes confounders as predictors in a regression model, often fails to eliminate this bias. In this article, the authors review Rubin's definition of an average causal effect (ACE) as the average difference between potential outcomes under different treatments. The authors distinguish an ACE and a regression coefficient. The authors review 9 strategies for estimating ACEs on the basis of regression, propensity scores, and doubly robust methods, providing formulas for standard errors not given elsewhere. To illustrate the methods, the authors simulate an observational study to assess the effects of dieting on emotional distress. Drawing repeated samples from a simulated population of adolescent girls, the authors assess each method in terms of bias, efficiency, and interval coverage. Throughout the article, the authors offer insights and practical guidance for researchers who attempt causal inference with observational data.",0
https://doi.org/10.1002/9781119030638.ch12,RMPW extensions to alternative designs and measurement,,0
https://doi.org/10.1177/0962280211432219,A practical introduction to multivariate meta-analysis,"Multivariate meta-analysis is becoming increasingly popular and official routines or self-programmed functions have been included in many statistical software. In this article, we review the statistical methods and the related software for multivariate meta-analysis. Emphasis is placed on Bayesian methods using Markov chain Monte Carlo, and codes in WinBUGS are provided. The various model-fitting options are illustrated in two examples and specific guidance is provided on how to run a multivariate meta-analysis using various software packages.",0
https://doi.org/10.1016/s0885-2006(03)00027-9,Evaluating academic outcomes of Head Start: an application of general growth mixture modeling,"Abstract This study intends to illustrate the utility of general growth mixture modeling (GGMM) for evaluation of early childhood education programs, using a sample of children with Head Start experience. In the first analysis of this study growth mixture modeling (GMM) found that children with Head Start experience had two distinct growth patterns. In the second analysis of this study general growth mixture modeling found that children with two or more years of program participation did not have faster achievement growth, on average, than children with only one year of program participation. This study also found that a gender gap in mathematics and an income gap in reading and mathematics were exclusively exhibited by the children with no preschool experience. Therefore, it was concluded that the Head Start program may be reducing both a gender gap in mathematics and an income gap in reading and mathematics.",0
https://doi.org/10.1111/1467-9868.00070,"Updating Schemes, Correlation Structure, Blocking and Parameterization for the Gibbs Sampler",In this paper many convergence issues concerning the implementation of the Gibbs sampler are investigated. Exact computable rates of convergence for Gaussian target distributions are obtained. Different random and non-random updating strategies and blocking combinations are compared using the rates. The effect of dimensionality and correlation structure on the convergence rates are studied. Some examples are considered to demonstrate the results. For a Gaussian image analysis problem several updating strategies are described and compared. For problems in Bayesian linear models several possible parameterizations are analysed in terms of their convergence rates characterizing the optimal choice.,0
https://doi.org/10.1371/journal.pone.0148171,Use of Wishart Prior and Simple Extensions for Sparse Precision Matrix Estimation,"A conjugate Wishart prior is used to present a simple and rapid procedure for computing the analytic posterior (mode and uncertainty) of the precision matrix elements of a Gaussian distribution. An interpretation of covariance estimates in terms of eigenvalues is presented, along with a simple decision-rule step to improve the performance of the estimation of sparse precision matrices and associated graphs. In this, elements of the estimated precision matrix that are zero or near zero can be detected and shrunk to zero. Simulated data sets are used to compare posterior estimation with decision-rule with two other Wishart-based approaches and with graphical lasso. Furthermore, an empirical Bayes procedure is used to select prior hyperparameters in high dimensional cases with extension to sparsity.",0
https://doi.org/10.1111/j.1467-985x.2010.00639.x,Estimation and adjustment of bias in randomized evidence by using mixed treatment comparison meta-analysis,"Summary.  There is good empirical evidence that specific flaws in the conduct of randomized controlled trials are associated with exaggeration of treatment effect estimates. Mixed treatment comparison meta-analysis, which combines data from trials on several treatments that form a network of comparisons, has the potential both to estimate bias parameters within the synthesis and to produce bias-adjusted estimates of treatment effects. We present a hierarchical model for bias with common mean across treatment comparisons of active treatment versus control. It is often unclear, from the information that is reported, whether a study is at risk of bias or not. We extend our model to estimate the probability that a particular study is biased, where the probabilities for the ‘unclear’ studies are drawn from a common beta distribution. We illustrate these methods with a synthesis of 130 trials on four fluoride treatments and two control interventions for the prevention of dental caries in children. Whether there is adequate allocation concealment and/or blinding are considered as indicators of whether a study is at risk of bias. Bias adjustment reduces the estimated relative efficacy of the treatments and the extent of between-trial heterogeneity.",0
https://doi.org/10.1111/j.2044-8317.1970.tb00432.x,"THE THEORETICAL FOUNDATIONS OF PRINCIPAL FACTOR ANALYSIS, CANONICAL FACTOR ANALYSIS, AND ALPHA FACTOR ANALYSIS","It is shown that PFA, CFA and AFA are particular cases of a scale-invariant factoring procedure based on variance ratios of certain weighted combinations of variables. Standard derivations in the literature are shown, in contrast, to have unsatisfactory features. It is suggested that the choice between PFA, CFA and AFA involves relatively independent choices of features of each, and that in most cases CFA is to be preferred.",0
https://doi.org/10.1155/2015/592626,Assessing Traffic Accident Occurrence of Road Segments through an Optimized Decision Rule,"Statistical models for estimating the safety status of transportation facilities have received great attention in the last two decades. These models also perform an important role in transportation safety planning as well as diagnoses of locations with high accident risks. However, the current methods largely rely on regression analyses and therefore they could ignore the multicollinearity characteristics of factors, which may provide additional information for enhancing the performance of forecasting models. This study seeks to develop more precise models for forecasting safety status as well as addressing the issue of multicollinearity of dataset. The proposed mathematical approach is indeed a discriminant analysis with respect to the goal of minimizing Bayes risks given multivariate distributions of factors. Based on this model, numerical analyses also perform with the application of a simulated dataset and an empirically observed dataset of traffic accidents in road segments. These examples essentially illustrate the process of Bayes risk minimization on predicating the safety status of road segments toward the objective of smallest misclassification rate. The paper finally concludes with a discussion of this methodology and several important avenues for future studies are also provided.",0
https://doi.org/10.1016/j.ijresmar.2007.10.001,"Estimating the SCAN⁎PRO model of store sales: HB, FM or just OLS?","In this paper we investigate whether consideration of store-level heterogeneity in marketing mix effects improves the accuracy of the marketing mix elasticities, fit, and forecasting accuracy of the widely-applied SCAN*PRO model of store sales. Models with continuous and discrete representations of heterogeneity, estimated using hierarchical Bayes (HB) and finite mixture (FM) techniques, respectively, are empirically compared to the original model, which does not account for store-level heterogeneity in marketing mix effects, and is estimated using ordinary least squares (OLS). The empirical comparisons are conducted in two contexts: Dutch store-level scanner data for the shampoo product category, and an extensive simulation experiment. The simulation investigates how between- and within-segment variance in marketing mix effects, error variance, the number of weeks of data, and the number of stores impact the accuracy of marketing mix elasticities, model fit, and forecasting accuracy. Contrary to expectations, accommodating store-level heterogeneity does not improve the accuracy of marketing mix elasticities relative to the homogeneous SCAN*PRO model, suggesting that little may be lost by employing the original homogeneous SCAN*PRO model estimated using ordinary least squares. Improvements in fit and forecasting accuracy are also fairly modest. We pursue an explanation for this result since research in other contexts has shown clear advantages from assuming some type of heterogeneity in market response models. In an Afterthought section, we comment on the controversial nature of our result, distinguishing factors inherent to household-level data and associated models vs. general store-level data and associated models vs. the unique SCAN*PRO model specification.",0
https://doi.org/10.1037/pspp0000043,Consequences of interpersonal spin on couple-relevant goal progress and relationship satisfaction in romantic relationships.,"Large fluctuations in a person's interpersonal behavior across situations and over time are thought to be associated with poor personal and interpersonal outcomes. This study examined 2 outcomes, relationship satisfaction and goal progress, that could be associated with individual differences in dispersion of interpersonal behavior (interpersonal spin) in romantic relationships. Need satisfaction and perceived autonomy support for goal pursuit from the partner were examined as mediator variables. Spin was measured using an event-contingent recording (ECR) methodology with a sample of 93 cohabiting couples who reported their interpersonal behavior in interactions with each other during a 20-day period. Relationship satisfaction and goal completion were measured at the end of the ECR procedure (T2) and approximately 7 months after the ECR (T3). Need satisfaction and perceived autonomy support were measured at T2. In both genders, higher spin was associated with lower T2 relationship satisfaction. There was also a decline in relationship satisfaction from T2 to T3 among men with high spin partners. In both genders, higher spin was associated with lower need satisfaction, and lower need satisfaction was associated with a decline in relationship satisfaction from T2 to T3. In both genders, higher spin was associated with lower perceived autonomy support, and lower support was associated with decreased progress in goal completion from T2 to T3. The effects of spin were independent of the effects of mean levels of behavior. These findings extend the understanding of the detrimental consequences of dispersion in interpersonal behavior to the disruption of the person's romantic relationships.",0
https://doi.org/10.4324/9780203843147-9,Modeling age-based turning points in longitudinal life-span growth curves of cognition,,0
https://doi.org/10.1111/1467-985x.0asp2,Some Practical Issues in the Evaluation of Heterogeneous Labour Market Programmes by Matching Methods,"Summary Recently several studies have analysed active labour market policies by using a recently proposed matching estimator for multiple programmes. Since there is only very limited practical experience with this estimator, this paper checks its sensitivity with respect to issues that are of practical importance in this kind of evaluation study. The estimator turns out to be fairly robust to several features that concern its implementation. Furthermore, the paper demonstrates that the matching approach per se is no panacea for solving all the problems of evaluation studies, but that its success depends critically on the information that is available in the data. Finally, a comparison with a bootstrap distribution provides some justification for using a simplified approximation of the distribution of the estimator that ignores its sequential nature.",0
https://doi.org/10.2307/2986186,Algorithm AS 287: Adaptive Rejection Sampling from Log-Concave Density Functions,,0
https://doi.org/10.1111/j.1468-5884.2011.00476.x,Generalized graded unfolding model with structural equation for subject parameters,"The generalized graded unfolding model (GGUM) is capable of analyzing polytomous scored, unfolding data such as agree-disagree responses to attitude statements. In the present study, we proposed a GGUM with structural equation for subject parameters, which enabled us to evaluate the relation between subject parameters and covariates and/or latent variables simultaneously, in order to avoid the influence of attenuation. Additionally, an algorithm for parameter estimation is newly implemented via the Markov Chain Monte Carlo (MCMC) method, based on Bayesian statistics. In the simulation, we compared the accuracy of estimates of regression coefficients between the proposed model and a conventional method using a GGUM (where regression coefficients are estimated using estimates of θ). As a result, the proposed model performed much better than the conventional method in terms of bias and root mean squared errors of estimates of regression coefficients. The study concluded by verifying the efficacy of the proposed model, using an actual data example of attitude measurement.",0
https://doi.org/10.1198/016214504000001015,Bilinear Mixed-Effects Models for Dyadic Data,"This article discusses the use of a symmetric multiplicative interaction effect to capture certain types of third-order dependence patterns often present in social networks and other dyadic datasets. Such an effect, along with standard linear fixed and random effects, is incorporated into a generalized linear model, and a Markov chain Monte Carlo algorithm is provided for Bayesian estimation and inference. In an example analysis of international relations data, accounting for such patterns improves model fit and predictive performance.",0
https://doi.org/10.1037/0022-3514.77.6.1200,Intergroup perception in naturally occurring groups of differential status: A social relations perspective.,"This study investigated intergroup perception in well-acquainted groups. Also of interest were the effects of a naturally occurring status differential on these perceptions. The study is framed within the social relations model, which provides a measure of in-group bias as well as 3 innovative measures of out-group homogeneity. Results indicated that low-status groups consistently displayed out-group favoritism. High-status groups displayed in-group bias, but only on ratings of leadership ability. The results also provided consistent evidence of out-group homogeneity. In instances when group status moderated out-group homogeneity effects, members of the high-status groups perceived their in-group as more variable than the out-group, whereas members of the low-status groups tended to see the in-group and out-group as equally variable.",0
https://doi.org/10.1186/s12886-015-0085-0,Outcome measure for the treatment of cone photoreceptor diseases: orientation to a scene with cone-only contrast,"Inherited retinal degenerations (IRDs) preferentially affecting cone photoreceptor function are being considered for treatment trials aiming to improve day vision. The purpose of the current work was to develop cone-specific visual orientation outcomes that can differentiate day vision improvement in the presence of retained night vision.A lighted wall (1.4 m wide, 2 m high) resembling a beaded curtain was formed with 900 individually addressable red, blue and green LED triplets placed in 15 vertical strips hanging 0.1 m apart. Under computer control, different combination of colors and intensities were used to produce the appearance of a door on the wall. Scotopically-matched trials were designed to be perceptible to the cone-, but not rod-, photoreceptor based visual systems. Unmatched control trials were interleaved at each luminance level to determine the existence of any vision available for orientation. Testing started with dark-adapted eyes and a scene luminance attenuated 8 log units from the maximum attainable, and continued with progressively increasing levels of luminance. Testing was performed with a three-alternative forced choice method in healthy subjects and patients with Leber congenital amaurosis (LCA) caused by mutations in GUCY2D, the gene that encodes retinal guanylate cyclase-1.Normal subjects could perform the orientation task using cone vision at 5 log attenuation and brighter luminance levels. Most GUCY2D-LCA patients failed to perform the orientation task with scotopically-matched test trials at any luminance level even though they were able to perform correctly with unmatched control trials. These results were consistent with a lack of cone system vision and use of the rod system under ambient conditions normally associated with cone system activity. Two GUCY2D-LCA patients demonstrated remnant cone vision but at a luminance level 2 log brighter than normal.The newly developed device can probe the existence or emergence of cone-based vision in patients for an orientation task involving the identification of a door on the wall under free-viewing conditions. This key advance represents progress toward developing an appropriate outcome measure for a clinical trial to treat currently incurable eye diseases severely affecting cone vision despite retained rod vision.",0
https://doi.org/10.1207/s15328007sem1303_1,A Maximum Likelihood Approach for Multisample Nonlinear Structural Equation Models With Missing Continuous and Dichotomous Data,"Structural equation models are widely appreciated in social-psychological research and other behavioral research to model relations between latent constructs and manifest variables and to control for measurement error. Most applications of SEMs are based on fully observed continuous normal data and models with a linear structural equation. However, discrete nonnormal data and missing data are rather common, and sometimes it is necessary to incorporate nonlinear structural equations for assessing the impact of nonlinear terms of the exogenous latent variables to the endogenous latent variables. Moreover, to study the behaviors of different populations, it is necessary to extend from a single sample model to a multisample model. In this article, a maximum likelihood (ML) approach is developed for analyzing a multisample nonlinear structural equation model, in the context of mixed continuous and dichotomous data that involve data that are missing at random. The article demonstrates the newly developed method...",0
https://doi.org/10.1007/978-3-642-83943-6_15,New Results in Test Theory Without an Answer Key,"This paper discusses some recent developments in a new methodology for the social sciences being developed jointly by the authors. The methodology is called cultural consensus analysis, and it is designed to aggregate informant responses to test items about their common culture. The object is to determine correct knowledge about the nature of that culture. Our methods estimate informant competency parameters and “correct” answers to each item from the informant response data. The methodology is designed for anthropological field studies and related research designs. This paper studies the consequences of relaxing the axioms of our model for dichotomous items, and it presents new models for other testing formats.",0
https://doi.org/10.1016/j.pain.2003.12.023,"Snake venom phospholipase A2s (Asp49 and Lys49) induce mechanical allodynia upon peri-sciatic administration: involvement of spinal cord glia, proinflammatory cytokines and nitric oxide","Snakebites constitute a serious public health problem in Central and South America, where species of the lancehead pit vipers (genus Bothrops) cause the majority of accidents. Bothrops envenomations are very painful, and this effect is not neutralized by antivenom treatment. Two variants of secretory phospholipases A2 (sPLA2), corresponding to Asp49 and Lys49 PLA2s, have been isolated from Bothrops asper venom. These sPLA2s induce hyperalgesia in rats following subcutaneous injection. However, venom in natural Bothrops bites is frequently delivered intramuscularly, thereby potentially reaching peripheral nerve bundles. Thus, the present series of experiments tested whether these sPLA2s could exert pain-enhancing effects following administration around healthy sciatic nerve. Both were found to produce mechanical allodynia ipsilateral to the injection site; no thermal hyperalgesia was observed. As no prior study has examined potential spinal mechanisms underlying sPLA2 actions, a series of anatomical and pharmacological studies were performed. These demonstrated that both sPLA2s produce activation of dorsal horn astrocytes and microglia that is more prominent ipsilateral to the site of injection. As proinflammatory cytokines and nitric oxide have each been previously implicated in spinally mediated pain facilitation, the effect of pharmacological blockade of these substances was tested. The results demonstrate that mechanical allodynia induced by both sPLA2s is blocked by interleukin-1 receptor antagonist, anti-rat interleukin-6 neutralizing antibody, the anti-inflammatory cytokine interleukin-10, and a nitric oxide synthesis inhibitor (L-NAME). As a variety of immune cells also produce and release sPLA2s during inflammatory states, the data may have general implications for the understanding of inflammatory pain.",0
https://doi.org/10.1080/00273171.2014.931798,Structural Equation Models in a Redundancy Analysis Framework With Covariates,"A recent method to specify and fit structural equation modeling in the Redundancy Analysis framework based on so-called Extended Redundancy Analysis (ERA) has been proposed in the literature. In this approach, the relationships between the observed exogenous variables and the observed endogenous variables are moderated by the presence of unobservable composites, estimated as linear combinations of exogenous variables. However, in the presence of direct effects linking exogenous and endogenous variables, or concomitant indicators, the composite scores are estimated by ignoring the presence of the specified direct effects. To fit structural equation models, we propose a new specification and estimation method, called Generalized Redundancy Analysis (GRA), allowing us to specify and fit a variety of relationships among composites, endogenous variables, and external covariates. The proposed methodology extends the ERA method, using a more suitable specification and estimation algorithm, by allowing for covariates that affect endogenous indicators indirectly through the composites and/or directly. To illustrate the advantages of GRA over ERA we propose a simulation study of small samples. Moreover, we propose an application aimed at estimating the impact of formal human capital on the initial earnings of graduates of an Italian university, utilizing a structural model consistent with well-established economic theory.",0
https://doi.org/10.1198/106186008x287337,Using Redundant Parameterizations to Fit Hierarchical Models,"Hierarchical linear and generalized linear models can be fit using Gibbs samplers and Metropolis algorithms; these models, however, often have many parameters, and convergence of the seemingly most natural Gibbs and Metropolis algorithms can sometimes be slow. We examine solutions that involve reparameterization and over-parameterization. We begin with parameter expansion using working parameters, a strategy developed for the EM algorithm. This strategy can lead to algorithms that are much less susceptible to becoming stuck near zero values of the variance parameters than are more standard algorithms. Second, we consider a simple rotation of the regression coefficients based on an estimate of their posterior covariance matrix. This leads to a Gibbs algorithm based on updating the transformed parameters one at a time or a Metropolis algorithm with vector jumps; either of these algorithms can perform much better (in terms of total CPU time) than the two standard algorithms: one-at-a-time updating of untrans...",0
https://doi.org/10.1007/bf02294494,Kernel smoothing approaches to nonparametric item characteristic curve estimation,"The option characteristic curve, the relation between ability and probability of choosing a particular option for a test item, can be estimated by nonparametric smoothing techniques. What is smoothed is the relation between some function of estimated examinee ability rankings and the binary variable indicating whether or not the option was chosen. This paper explores the use of kernel smoothing, which is particularly well suited to this application. Examples show that, with some help from the fast Fourier transform, estimates can be computed about 500 times as rapidly as when using commonly used parametric approaches such as maximum marginal likelihood estimation using the three-parameter logistic distribution. Simulations suggest that there is no loss of efficiency even when the population curves are three-parameter logistic. The approach lends itself to several interesting extensions. Â© 1991 The Psychometric Society.",0
,The Infinite Gaussian Mixture Model,In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the right number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.,0
https://doi.org/10.1002/1098-2272(200009)19:2<127::aid-gepi2>3.0.co;2-s,Variance components analysis for pedigree-based censored survival data using generalized linear mixed models (GLMMs) and Gibbs sampling in BUGS,"Complex human diseases are an increasingly important focus of genetic research. Many of the determinants of these diseases are unknown and there is often a strong residual covariance between relatives even when all known genetic and environmental factors have been taken into account. This must be modeled correctly whether scientific interest is focused on fixed effects, as in an association analysis, or on the covariance structure itself. Analysis is straightforward for multivariate normally distributed traits, but difficulties arise with other types of trait. Generalized linear mixed models (GLMMs) offer a potentially unifying approach to analysis for many classes of phenotype including right censored survival times. This includes age-at-onset and age-at-death data and a variety of other censored traits. Markov chain Monte Carlo (MCMC) methods, including Gibbs sampling, provide a convenient framework within which such GLMMs may be fitted. In this paper, we use BUGS (“Bayesian inference using Gibbs sampling”: a readily available, generic Gibbs sampler) to fit GLMMs for right-censored survival times in nuclear and extended families. We discuss parameter interpretation and statistical inference, and show how to circumvent a number of important theoretical and practical problems. Using simulated data, we show that model parameters are consistent. We further illustrate our methods using data from an ongoing cohort study. Finally, we propose that the random effects associated with a genetic component of variance (e.g., σ2A) in a GLMM may be regarded as an adjusted “phenotype” and used as input to a conventional model-based or model-free linkage analysis. This provides a simple way to conduct a linkage analysis for a trait reflected in a right-censored survival time while comprehensively adjusting for observed confounders at the level of the individual and latent environmental effects shared across families. Genet. Epidemiol. 19:127–148, 2000. © 2000 Wiley-Liss, Inc.",0
https://doi.org/10.1577/t07-012.1,Modeling Variation in Mass-Length Relations and Condition Indices of Lake Trout and Chinook Salmon in Lake Huron: A Hierarchical Bayesian Approach,"Abstract Commonly used approaches to studying mass-length relations and condition indices often do not adequately address covariance between mass-length parameters, usually ignore heterogeneity in individual variance for body mass at a given length, and assume that length distributions of fish samples are similar across regions and years. We used body mass at selected lengths as condition indices based on statistical modeling and a hierarchical Bayesian approach to inferences, and our approach allowed us to avoid using restrictive assumptions. We estimated spatial and annual variation in mass-length relations, where the process errors in parameters are drawn from a multivariate distribution. We also estimated region-, year-, and size-group-specific variance for individual variation in mass at given lengths. We applied our approach to study mass-length relations of lake trout Salvelinus namaycush (1977-2005) and Chinook salmon Oncorhynchus tshawytscha (1983-2004) in U.S. waters of Lake Huron. We found that...",0
https://doi.org/10.1016/j.jbi.2007.01.003,Estimation of epistasis among finite polygenic loci for complex traits with a mixed model using Gibbs sampling,"Epistasis among loci is important factor behind the expression of many complex traits, but many analyses have ruled out its possibility. A method to estimate epistasis was introduced with a mixed model using Gibbs sampling (MMGS). The posterior mean estimate for every possible genotype combined from multiple loci was calculated as the mean of the conditional expected values of the parameters in post warming-up rounds from Gibbs sampling. A simulation study was performed to compare MMGS with restricted partition method (RPM). Mean square prediction error (MSPE) using MMGS was smaller than that using RPM ( P < 0.05), which might be due to information loss introduced by grouping of genotypes in RPM. This was also supported by the result that MSPE increased as the number of merged groups decreased. The simulation study implied that MMGS was more plausible in estimating epistatic effects than the RPM.",0
https://doi.org/10.1080/00273171.2014.928492,Bayesian Model Averaging for Propensity Score Analysis,"This article considers Bayesian model averaging as a means of addressing uncertainty in the selection of variables in the propensity score equation. We investigate an approximate Bayesian model averaging approach based on the model-averaged propensity score estimates produced by the R package BMA but that ignores uncertainty in the propensity score. We also provide a fully Bayesian model averaging approach via Markov chain Monte Carlo sampling (MCMC) to account for uncertainty in both parameters and models. A detailed study of our approach examines the differences in the causal estimate when incorporating noninformative versus informative priors in the model averaging stage. We examine these approaches under common methods of propensity score implementation. In addition, we evaluate the impact of changing the size of Occam's window used to narrow down the range of possible models. We also assess the predictive performance of both Bayesian model averaging propensity score approaches and compare it with the case without Bayesian model averaging. Overall, results show that both Bayesian model averaging propensity score approaches recover the treatment effect estimates well and generally provide larger uncertainty estimates, as expected. Both Bayesian model averaging approaches offer slightly better prediction of the propensity score compared with the Bayesian approach with a single propensity score equation. Covariate balance checks for the case study show that both Bayesian model averaging approaches offer good balance. The fully Bayesian model averaging approach also provides posterior probability intervals of the balance indices.",0
https://doi.org/10.1037/1082-989x.6.4.371,"Evaluating statistical difference, equivalence, and indeterminacy using inferential confidence intervals: An integrated alternative method of conducting null hypothesis statistical tests.","Null hypothesis statistical testing (NHST) has been debated extensively but always successfully defended. The technical merits of NHST are not disputed in this article. The widespread misuse of NHST has created a human factors problem that this article intends to ameliorate. This article describes an integrated, alternative inferential confidence interval approach to testing for statistical difference, equivalence, and indeterminacy that is algebraically equivalent to standard NHST procedures and therefore exacts the same evidential standard. The combined numeric and graphic tests of statistical difference, equivalence, and indeterminacy are designed to avoid common interpretive problems associated with NHST procedures. Multiple comparisons, power, sample size, test reliability, effect size, and cause-effect ratio are discussed. A section on the proper interpretation of confidence intervals is followed by a decision rule summary and caveats.",0
https://doi.org/10.1016/j.peptides.2008.05.023,Antinociceptive effect of the C-terminus of murine S100A9 protein on experimental neuropathic pain,"The synthetic peptide identical to the C-terminus of murine S100A9 protein (mS100A9p) has antinociceptive effect on different acute inflammatory pain models. In this study, the effect of mS100A9p was investigated on neuropathic pain induced by chronic constriction injury (CCI) of the sciatic nerve in rats. Hyperalgesia, allodynia, and spontaneous pain were assessed to evaluate nociception. These three signs were detected as early as 2 days after sciatic nerve constriction and lasted for over 14 days after CCI. Rats were treated with different doses of mS100A9p by intraplantar, oral, or intrathecal routes on day 14 after CCI, and nociception was evaluated 1h later. These three routes of administration blocked hyperalgesia, allodynia and spontaneous pain. The duration of the effect of mS100A9p depends on the route used and phenomenon analyzed. Moreover, intraplantar injection of mS100A9p in the contralateral paw inhibited the hyperalgesia on day 14 days after CCI. The results obtained herein demonstrate the antinociceptive effect of the C-terminus of murine S100A9 protein on experimental neuropathic pain, suggesting a potential therapeutic use for it in persistent pain syndromes, assuming that tolerance does not develop to mS100A9p.",0
https://doi.org/10.1007/s11306-017-1164-4,Multilevel pharmacokinetics-driven modeling of metabolomics data,"Multilevel modeling is a quantitative statistical method to investigate variability and relationships between variables of interest, taking into account population structure and dependencies. It can be used for prediction, data reduction and causal inference from experiments and observational studies allowing for more efficient elucidation of knowledge.In this study we introduced the concept of multilevel pharmacokinetics (PK)-driven modelling for large-sample, unbalanced and unadjusted metabolomics data comprising nucleoside and creatinine concentration measurements in urine of healthy and cancer patients.A Bayesian multilevel model was proposed to describe the nucleoside and creatinine concentration ratio considering age, sex and health status as covariates. The predictive performance of the proposed model was summarized via area under the ROC, sensitivity and specificity using external validation.Cancer was associated with an increase in methylthioadenosine/creatinine excretion rate by a factor of 1.42 (1.09-2.03) which constituted the highest increase among all nucleosides. Age influenced nucleosides/creatinine excretion rates for all nucleosides in the same direction which was likely caused by a decrease in creatinine clearance with age. There was a small evidence of sex-related differences for methylthioadenosine. The individual a posteriori prediction of patient classification as area under the ROC with 5th and 95th percentile was 0.57(0.5-0.67) with sensitivity and specificity of 0.59(0.42-0.76) and 0.57(0.45-0.7), respectively suggesting limited usefulness of 13 nucleosides/creatinine urine concentration measurements in predicting disease in this population.Bayesian multilevel pharmacokinetics-driven modeling in metabolomics may be useful in understanding the data and may constitute a new tool for searching towards potential candidates of disease indicators.",0
https://doi.org/10.1002/sim.4780100604,Using empirical bayes methods in biopharmaceutical research,"A compound sampling model, where a unit-specific parameter is sampled from a prior distribution and then observed are generated by a sampling distribution depending on the parameter, underlies a wide variety of biopharmaceutical data. For example, in a multi-centre clinical trial the true treatment effect varies from centre to centre. Observed treatment effects deviate from these true effects through sampling variation. Knowledge of the prior distribution allows use of Bayesian analysis to compute the posterior distribution of clinic-specific treatment effects (frequently summarized by the posterior mean and variance). More commonly, with the prior not completely specified, observed data can be used to estimate the prior and use it to produce the posterior distribution: an empirical Bayes (or variance component) analysis. In the empirical Bayes model the estimated prior mean gives the typical treatment effect and the estimated prior standard deviation indicates the heterogeneity of treatment effects. In both the Bayes and empirical Bayes approaches, estimated clinic effects are shrunken towards a common value from estimates based on single clinics. This shrinkage produces more efficient estimates. In addition, the compound model helps structure approaches to ranking and selection, provides adjustments for multiplicity, allows estimation of the histogram of clinic-specific effects, and structures incorporation of external information. This paper outlines the empirical Bayes approach. Coverage will include development and comparison of approaches based on parametric priors (for example, a Gaussian prior with unknown mean and variance) and non-parametric priors, discussion of the importance of accounting for uncertainty in the estimated prior, comparison of the output and interpretation of fixed and random effects approaches to estimating population values, estimating histograms, and identification of key considerations in the use and interpretation of empirical Bayes methods.",0
https://doi.org/10.1111/1467-9248.12162,When the Going Gets Tough: The Differential Impact of National Unemployment on the Perceived Threats of Immigration,"Economic competition theory predicts that anti-immigration sentiments will increase in periods with high unem-ployment, in particular among low-skilled workers. Using five rounds of cross-sectional data from the European Social Survey and utilising the rise in unemployment in many European countries due to the financial crisis, this article provides a more effective empirical test of interest-based theories than previous studies. It employs hierarchical, two-stage regression techniques to estimate the relationship between aggregate unemployment rates and immigration opinion, and explores whether the relationship differs according to respondent’s level of education. It is found that high unemployment rates are associated with a high level of economic concern over immigration – particularly if the size of the foreign-born population is large. The relationship is stronger among the low skilled, implying a tendency for polarisation of opinions about immigration in economic recessions. Finally, it is discovered that the general level of cultural concern over immigration is unrelated to variation in unemployment.",0
https://doi.org/10.3102/1076998609332756,Sample Size Estimation in Cluster Randomized Educational Trials: An Empirical Bayes Approach,"The educational field has now accumulated an extensive literature reporting on values of the intraclass correlation coefficient, a parameter essential to determining the required size of a planned cluster randomized trial. We propose here a simple simulation-based approach including all relevant information that can facilitate this task. An example and corresponding computer code is attached.",0
https://doi.org/10.1177/0013164409332222,A Model Fit Statistic for Generalized Partial Credit Model,"Investigating the fit of a parametric model is an important part of the measurement process when implementing item response theory (IRT), but research examining it is limited. A general nonparametric approach for detecting model misfit, introduced by J. Douglas and A. S. Cohen (2001), has exhibited promising results for the two-parameter logistic model and Samejima s graded response model. This study extends this approach to test the fit of generalized partial credit model (GPCM). The empirical Type I error rate and power of the proposed method are assessed for various test lengths, sample sizes, and type of assessment. Overall, the proposed fit statistic performed well under the studied conditions in that the Type I error rate was not inflated and the power was acceptable, especially for moderate to large sample sizes. A further advantage of the nonparametric approach is that it provides a convenient graphical display of possible misfit.",0
,USAGE OF DIFFERENT PRIOR DISTRIBUTIONS IN BAYESIAN VECTOR AUTOREGRESSIVE MODELS,"In Bayesian vector autoregressive models, the Litterman or Minnesota Prior is widely used. However, in some cases, the Minnesota prior is not the best prior distribution that can be used. Thus, other prior dis- tributions can also be applied. In this paper, as well as the Minnesota prior, four other prior distributions have been studied. Based on these prior distributions, five different Bayesian vector autoregressive models have been built to forecast the Turkish unemployment rate and the in- dustrial production index for the two periods of the year 2008. Finally, the five priors have been compared with each other according to the forecasting performances of the models that they are used in.",0
https://doi.org/10.1080/10705511.2010.488997,An Alternative Approach for Nonlinear Latent Variable Models,"In the last decades there has been an increasing interest in nonlinear latent variable models. Since the seminal paper of Kenny and Judd, several methods have been proposed for dealing with these kinds of models. This article introduces an alternative approach. The methodology involves fitting some third-order moments in addition to the means and covariances. This article discusses how the model equations can be formulated and how several standard tests, like the model fit and Lagrange multiplier tests, can be performed. The new method compares favorably with the maximum likelihood method in several studies and can provide evidence of interaction that earlier approaches might ignore.",0
https://doi.org/10.1016/j.jmp.2009.06.006,Systems Factorial Technology provides new insights on global–local information processing in autism spectrum disorders,"Previous studies of global-local processing in autism spectrum disorders (ASDs) have indicated mixed findings, with some evidence of a local processing bias, or preference for detail-level information, and other results suggesting typical global advantage, or preference for the whole or gestalt. Findings resulting from this paradigm have been used to argue for or against a detail focused processing bias in ASDs, and thus have important theoretical implications. We applied Systems Factorial Technology, and the associated Double Factorial Paradigm (both defined in the text), to examine information processing characteristics during a divided attention global-local task in high-functioning individuals with an ASD and typically developing controls. Group data revealed global advantage for both groups, contrary to some current theories of ASDs. Information processing models applied to each participant revealed that task performance, although showing no differences at the group level, was supported by different cognitive mechanisms in ASD participants compared to controls. All control participants demonstrated inhibitory parallel processing and the majority demonstrated a minimum-time stopping rule. In contrast, ASD participants showed exhaustive parallel processing with mild facilitatory interactions between global and local information. Thus our results indicate fundamental differences in the stopping rules and channel dependencies in individuals with an ASD.",0
https://doi.org/10.1523/jneurosci.4097-14.2015,Neural Population Coding of Multiple Stimuli,"In natural scenes, objects generally appear together with other objects. Yet, theoretical studies of neural population coding typically focus on the encoding of single objects in isolation. Experimental studies suggest that neural responses to multiple objects are well described by linear or nonlinear combinations of the responses to constituent objects, a phenomenon we call stimulus mixing. Here, we present a theoretical analysis of the consequences of common forms of stimulus mixing observed in cortical responses. We show that some of these mixing rules can severely compromise the brain's ability to decode the individual objects. This cost is usually greater than the cost incurred by even large reductions in the gain or large increases in neural variability, explaining why the benefits of attention can be understood primarily in terms of a stimulus selection, or demixing, mechanism rather than purely as a gain increase or noise reduction mechanism. The cost of stimulus mixing becomes even higher when the number of encoded objects increases, suggesting a novel mechanism that might contribute to set size effects observed in myriad psychophysical tasks. We further show that a specific form of neural correlation and heterogeneity in stimulus mixing among the neurons can partially alleviate the harmful effects of stimulus mixing. Finally, we derive simple conditions that must be satisfied for unharmful mixing of stimuli.",0
https://doi.org/10.1080/07474938.2012.690650,Introduction to Robustness in Multidimensional Wellbeing Analysis,"The multidimensional nature of wellbeing is now the widely accepted approach in frontier research on poverty, inequality and policy analysis. However, significant challenges and disagreement remain...",0
https://doi.org/10.1177/0143624414566245,Is CO<sub>2</sub> a good proxy for indoor air quality in classrooms? Part 2: Health outcomes and perceived indoor air quality in relation to classroom exposure and building characteristics,"The aim of this paper is to investigate whether keeping indoor thermal conditions and carbon dioxide (CO 2 ) levels within the current guideline values can provide a healthy and comfortable school environment. The study was organised as a longitudinal investigation over an academic year using a cohort of 376 students aged 9 to 11 (response rate: 87%) attending 15 classrooms in five London primary schools. The prevalence of asthmatic symptoms and asthma attacks was significantly higher among children attending urban schools (10.2%) than suburban schools (1.5%), and was significantly related to exposure to higher nitrogen dioxide (NO 2 ) concentrations (odds ratio: 1.11, 95% confidence interval: 1.00–1.19). Self-reported dermal, mucosal, respiratory and general symptoms were 18.5%, 60.7%, 28.2% and 43.6% respectively in the heating season, and decreased in the non-heating season. Infiltration rates were negatively associated with prevalence and incidence of all sick building syndrome symptoms. Exposure to traffic-related pollutants, such NO 2 , ozone (O 3 ) and tetrachloroethylene (T4CE), associated with mucosal symptoms, also increased dissatisfaction with indoor air quality (IAQ) and, therefore, perceived IAQ might be a first indication of exposure. Among targeted microbial counts, only Trichoderma viride remained significant predictors of satisfaction with IAQ even at low concentrations. The study provides evidence that simultaneous provision for limiting indoor CO 2 levels and thermal conditions below current guidelines (e.g. below 1000 ppm and 26℃ or 22℃ depending on season) may improve perceived IAQ. This paper stresses the need to go beyond current regulations to investigate concentrations of specific pollutants to ensure a healthy school environment, and closes with a section on the practical implications on the UK policy and the building design industry. Practical application: The findings highlight the role and responsibility of stakeholders, from regulators to designers and school authorities, to account for the external environment and take the steps needed to ensure that schools provide a healthy indoor environment for their students. The recommendations focus on the need to decrease outdoor pollution levels in the school vicinity, thus improving health of the students and reducing the prevalence of respiratory illness. Building designers and engineers shall adopt an integrated approach for the simultaneous provision of adequate thermal conditions and IAQ in classrooms.",0
https://doi.org/10.1016/j.jclinepi.2010.08.010,Conducting quantitative synthesis when comparing medical interventions: AHRQ and the Effective Health Care Program,"<h2>Abstract</h2><h3>Objective</h3> This article is to establish recommendations for conducting quantitative synthesis, or meta-analysis, using study-level data in comparative effectiveness reviews (CERs) for the Evidence-based Practice Center (EPC) program of the Agency for Healthcare Research and Quality. <h3>Study Design and Setting</h3> We focused on recurrent issues in the EPC program and the recommendations were developed using group discussion and consensus based on current knowledge in the literature. <h3>Results</h3> We first discussed considerations for deciding whether to combine studies, followed by discussions on indirect comparison and incorporation of indirect evidence. Then, we described our recommendations on choosing effect measures and statistical models, giving special attention to combining studies with rare events; and on testing and exploring heterogeneity. Finally, we briefly presented recommendations on combining studies of mixed design and on sensitivity analysis. <h3>Conclusion</h3> Quantitative synthesis should be conducted in a transparent and consistent way. Inclusion of multiple alternative interventions in CERs increases the complexity of quantitative synthesis, whereas the basic issues in quantitative synthesis remain crucial considerations in quantitative synthesis for a CER. We will cover more issues in future versions and update and improve recommendations with the accumulation of new research to advance the goal for transparency and consistency.",0
https://doi.org/10.1177/1742715014543579,Thought self-leadership and effectiveness in self-management teams,"This study empirically examines the multilevel nature of thought self-leadership at work. Furthermore, this study tests the relationship between team level thought self-leadership and team effectiveness (i.e. performance and viability) through collective efficacy. A total of 103 self-management teams (453 individuals), enrolled in a five-week management competition participated in the study. The results from multilevel confirmatory factor analysis suggest that thought self-leadership is functionally equivalent across levels of analysis (i.e. individuals and teams). In addition, we found an indirect effect of team level thought self-leadership on team effectiveness criteria, through collective efficacy. These findings extend previous work on thought self-leadership and team effectiveness, and open new roads for research in self-managing work teams. Finally, this study also provides guidelines for organizations in case they wish to foster team performance and viability in their work force.",0
https://doi.org/10.1080/01621459.1995.10476487,Estimating Unknown Transition Times Using a Piecewise Nonlinear Mixed-Effects Model in Men with Prostate Cancer,Abstract It may be clinically useful to know when prostate-specific antigen (PSA) levels first begin to rise rapidly and to determine if the natural history of PSA progression is different in men with locally confined prostate cancers compared to men with metastatic tumors. This article uses a nonlinear mixed-effects model to describe longitudinal changes in PSA in men before their prostate cancers were detected clinically. Repeated measurements of PSA are available for 18 subjects with a diagnosis of prostate cancer based on prostate biopsy. PSA measurements were determined on repeated frozen serum samples collected from subjects with at least 10.0 years and up to 25.6 years of observation before the cancer was detected. A piecewise model is used to describe this data. The model is linear long before the cancer was detected and exponential nearer the time the cancer was detected. The time at which the PSA levels change from linear to exponential PSA progression is unknown but can be estimated by includin...,0
https://doi.org/10.1007/bf02294856,A Bayesian approach to nonlinear latent variable models using the Gibbs sampler and the metropolis-hastings algorithm,"Nonlinear latent variable models are specified that include quadratic forms and interactions of latent regressor variables as special cases. To estimate the parameters, the models are put in a Bayesian framework with conjugate priors for the parameters. The posterior distributions of the parameters and the latent variables are estimated using Markov chain Monte Carlo methods such as the Gibbs sampler and the Metropolis-Hastings algorithm. The proposed estimation methods are illustrated by two simulation studies and by the estimation of a non-linear model for the dependence of performance on task complexity and goal specificity using empirical data.",0
https://doi.org/10.1007/bf02293811,A method for simulating non-normal distributions,"A method of introducing a controlled degree of skew and kurtosis for Monte Carlo studies was derived. The form of such a transformation on normal deviates [X â‰ˆN(0, 1)] is Y =a +bX +cX2 +dX3. Analytic and empirical validation of the method is demonstrated. Â© 1978 Psychometric Society.",0
https://doi.org/10.1177/1094428114555993,Developing Ideal Intermediate Personality Items for the Ideal Point Model,"The importance of intermediate items has been overlooked since the emergence of dominance-based Likert-type scales. The current study aims to advance our understanding of the psychometric properties of intermediate items by showing that they can be successfully calibrated by an ideal point model. A student sample and an MTurk sample were selected to answer personality scales with intermediate items included. Results showed that personality scales with intermediate items demonstrated satisfactory model fits to the ideal point model, but not to the dominance model. When analyzed with the ideal point model, intermediate items provided more information than extreme items for respondents with high latent trait levels. Among four proposed domains of intermediate items (Frequency, Average, Condition, and Transition, “FACT”), the Average domain was consistently found to exhibit the best properties. The proportion of intermediate items in the scale also affected the model fit and the validity of the scale.",0
https://doi.org/10.1027/1614-2241.4.3.97,Nonlinear Change Models in Populations with Unobserved Heterogeneity,"When unobserved heterogeneity exists in populations where the phenomenon of interest is governed by a functional form of change linear in its parameters, the growth mixture model (GMM) is useful for modeling change conditional on latent class. However, when the functional form of interest is nonlinear in its parameters, the GMM is not very useful because it is based on a system of equations linear in its parameters. The nonlinear change mixture model (NCMM) is proposed, which explicitly addresses unobserved heterogeneity in situations where change follows a nonlinear functional form. Due to the integration of nonlinear multilevel models and finite mixture models, neither of which generally have closed form solutions, analytic solutions do not generally exist for the NCMM. Five methods of parameter estimation are developed and evaluated with a comprehensive Monte Carlo simulation study. The simulation showed that the parameters of the NCMM can be accurately estimated with several of the proposed methods, and that the method of choice depends on the precise question of interest.",0
https://doi.org/10.1037/xlm0000075,No evidence for a fixed object limit in working memory: Spatial ensemble representations inflate estimates of working memory capacity for complex objects.,"A central question for models of visual working memory is whether the number of objects people can remember depends on object complexity. Some influential ""slot"" models of working memory capacity suggest that people always represent 3-4 objects and that only the fidelity with which these objects are represented is affected by object complexity. The primary evidence supporting this claim is the finding that people can detect large changes to complex objects (consistent with remembering at least 4 individual objects), but that small changes cannot be detected (consistent with low-resolution representations). Here we show that change detection with large changes greatly overestimates individual item capacity when people can use global representations of the display to detect such changes. When the ability to use such global ensemble or texture representations is reduced, people remember individual information about only 1-2 complex objects. This finding challenges models that propose people always remember a fixed number of objects, regardless of complexity, and supports a more flexible model with an important role for spatial ensemble representations.",0
https://doi.org/10.1002/sim.5795,Propensity scores used for analysis of cluster randomized trials with selection bias: a simulation study,"Cluster randomized trials (CRTs) are often prone to selection bias despite randomization. Using a simulation study, we investigated the use of propensity score (PS) based methods in estimating treatment effects in CRTs with selection bias when the outcome is quantitative. Of four PS-based methods (adjustment on PS, inverse weighting, stratification, and optimal full matching method), three successfully corrected the bias, as did an approach using classical multivariable regression. However, they showed poorer statistical efficiency than classical methods, with higher standard error for the treatment effect, and type I error much smaller than the 5% nominal level.",0
https://doi.org/10.1080/10705511.2011.534695,Addressing the Problem of Switched Class Labels in Latent Variable Mixture Model Simulation Studies,"The discrimination between alternative models and the detection of latent classes in the context of latent variable mixture modeling depends on sample size, class separation, and other aspects that are related to power. Prior to a mixture analysis it is useful to investigate model performance in a simulation study that reflects the research settings. Multiple data sets are generated under 1 or more models, and alternative models are fitted to the data. The aggregation of results over multiple data sets is complicated by the fact that mixture models are only identified up to a permutation of the class labels. Estimated class labels are arbitrary, with the effect that the estimated parameters for Class 1 could be incorrectly labeled as Class 2, Class 3, and so forth, relative to their data generating labels. In a simulation study, the detection of switched labels needs to be automated. Switched class labels are not necessarily simple to detect. This article describes different possible scenarios of switched...",0
https://doi.org/10.1080/10705511.2014.882660,A Simulation Study Comparing Recent Approaches for the Estimation of Nonlinear Effects in SEM Under the Condition of Nonnormality,"In the past decade new approaches for the estimation of latent nonlinear interaction and quadratic effects in structural equation modeling have been proposed (Kelava & Brandt, 2009; Klein & Moosbrugger, 2000; Klein & Muthen, 2007; Marsh, Wen, & Hau, 2004; Mooijaart & Bentler, 2010; Wall & Amemiya, 2003). Most approaches have been developed for the analysis of normally distributed latent predictor variables. In this article, we investigate the performance of five recent approaches under the condition of nonnormally distributed data: the extended unconstrained approach (Kelava & Brandt, 2009), LMS (Klein & Moosbrugger, 2000), QML (Klein & Muthen, 2007), the 2SMM approach (Wall & Amemiya, 2003), and the method of moments approach by Mooijaart and Bentler (2010). Advantages and limitations of the approaches are discussed.",0
https://doi.org/10.1207/s15327906mbr3704_04,A Version of Quadratic Regression with Interpretable Parameters,"The quadratic regression model is popular and effective in describing a wide variety of data, but it is based on a function whose parameters are not easy to interpret. We suggest an alternative form of the quadratic model that has the same expectation function, but also has the useful feature that its parameters are interpretable. Examples are provided of a simple regression problem and also of a nonlinear mixed-effects model. The models can be estimated with available software.",0
https://doi.org/10.1037/0021-9010.69.2.307,"Mediators, moderators, and tests for mediation.","Abstract : The following points are developed. First, mediation relations are generally thought of in causal terms. Influences of an antecedent are transmitted to a consequence through an intervening mediator. Second, mediation relations may assume a number of functional forms, including nonadditive, nonlinear, and nonrecursive forms. Special attention is given to nonadditive forms, or moderated mediation, where it is shown that while mediation and moderation are distinguishable processes, a particular variable may be both a mediator and a moderator within a single set of functional relations. Third, current procedures for testing mediation relations in industrial and organizational psychology need to be updated because these procedures often involve a dubious interplay between exploratory (correlational) statistical tests and causal inference. It is suggested that no middle ground exists between exploratory and confirmatory (causal) analysis, and that attempts to explain how mediation processes occur require well-specified causal models. Given such models, confirmatory analytic techniques furnish the more informative tests of mediation. (Author)",0
https://doi.org/10.1111/ddi.12062,The value of a datum - how little data do we need for a quantitative risk analysis?,"Aim  Conservation managers are typically faced with limited resources, time and information. The philosophy underlying risk assessment should be robust to these limitations. While there is a broad support for the concept of risk assessments, there is a tendency to rely on expert opinion and exclude formal data analysis, possibly because available information is often scarce. When data analyses are conducted, often much simplified models are advocated, even though this means excluding processes believed by experts to be important. In this manuscript, we ask: should statistical analyses be conducted and decisions modified based on a single datum? How many data points are needed before predictions are meaningful? Given limited data, how complex should models be?    Location  World-wide.    Methods  We use simulation approaches with known ‘true’ values to assess which inferences are possible, given different amounts of information. We use two metrics of performance: the magnitude of uncertainty (using posterior mean squared error) and bias (using P–P plots). We assess six models of relevance to conservation ecologists.    Results  We show that the greatest reduction in uncertainty occurred at the smallest sample sizes for models examined, and much of parameter space could be excluded. Thus, analyses based on even a single datum potentially can be useful. Further, with only a few observations, the predicted distribution of outcomes matched the probabilities of actual occurrences, even for relatively complex state-space models with multiple sources of stochasticity.    Main conclusions  We highlight the utility of quantitative analyses even with severely limited data, given existing practices and arguments in the conservation literature. The purpose of our manuscript is in part a philosophical discourse, as modifications are needed to how conservation ecologists are often trained to think about problems and data, and in part a demonstration via simulation analysis.",0
https://doi.org/10.1080/1359432x.2014.983085,From state neuroticism to momentary task performance: A person × situation approach,"The goal of this article was to investigate the mechanisms through which personality relates to task performance. We argue that perceptions of work pressure and task complexity trigger momentary levels of neuroticism (i.e., state neuroticism) and that these momentary levels of neuroticism predict momentary task performance. Moreover, we hypothesized that the relationship between momentary job demands and state neuroticism is moderated by trait neuroticism. To test this model, we conducted an event reconstruction study and a day reconstruction study. The results revealed that trait neuroticism indeed moderated the momentary job demands–state neuroticism relationship, and in three out of four cases state neuroticism was found to mediate the relationship between momentary job demands and momentary task performance. From a practical point-of-view our results suggest that employees’ task performance can be improved by enhancing the way in which individuals perceive job demands. This strategy would be particula...",0
https://doi.org/10.18148/srm/2009.v3i1.666,A Monte Carlo sample size study: How many countries are needed for accurate multilevel SEM?,"Recently, there has been growing scientific interest for cross-national survey research. Various scholars have used multilevel techniques to link individual characteristics to aspects of the national context. At first sight, multilevel SEM seems to be a promising tool for this purpose, as it integrates multilevel modeling within a latent variable framework. However, due to the fact that the number of countries in most international surveys does not exceed 30, the application of multilevel SEM in cross-national research is problematic. Taking European Social Survey (ESS) data as a point of departure, this paper uses Monte Carlo studies to assess the estimation accuracy of multilevel SEM with small group sample sizes. The results indicate that a group sample size of 20 ‐ a situation common in cross-national research ‐ does not guarantee accurate estimation at all. Unacceptable amounts of parameter and standard error bias are present for the between-level estimates. Unless the standardized e ect is very large (0.75), statistical power for detecting a significant between-level structural e ect is seriously lacking. Required group sample sizes depend strongly on the specific interests of the researcher, the expected e ect sizes and the complexity of the model. If the between-level model is relatively simple and one is merely interested in the between-level factor structure, a group sample size of 40 could be su cient. To detect large (>0.50) structural e ects at the between level, at least 60 groups are required. To have an acceptable probability of detecting smaller e ects, more than 100 groups are needed. These guidelines are shown to be quite robust for varying cluster sizes and intra-class correlations (ICCs).",0
https://doi.org/10.1007/bf02294839,Bayesian estimation of a multilevel IRT model using gibbs sampling,"In this article, a two-level regression model is imposed on the ability parameters in an item response theory (IRT) model. The advantage of using latent rather than observed scores as dependent variables of a multilevel model is that it offers the possibility of separating the influence of item difficulty and ability level and modeling response variation and measurement error. Another advantage is that, contrary to observed scores, latent scores are test-independent, which offers the possibility of using results from different tests in one analysis where the parameters of the IRT model and the multilevel model can be concurrently estimated. The two-parameter normal ogive model is used for the IRT measurement model. It will be shown that the parameters of the two-parameter normal ogive model and the multilevel model can be estimated in a Bayesian framework using Gibbs sampling. Examples using simulated and real data are given.",0
https://doi.org/10.1086/288135,Theory-Testing in Psychology and Physics: A Methodological Paradox,"Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching ½ of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by “success” is very weak, and becomes weaker with increased precision. “Statistical significance” plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental “cuteness” and a free reliance upon ad hoc explanations to avoid refutation.",0
https://doi.org/10.1080/00220973.2014.907229,Examining the Rule of Thumb of Not Using Multilevel Modeling: The “Design Effect Smaller Than Two” Rule,"Educational researchers commonly use the rule of thumb of “design effect smaller than 2” as the justification of not accounting for the multilevel or clustered structure in their data. The rule, however, has not yet been systematically studied in previous research. In the present study, we generated data from three different models (which differ in the location of the clustering effect). With a 3 (design effect) × 5 (cluster size) × 4 (number of clusters) Monte Carlo simulation study we found that the rule should not be applied when researchers: (a) are interested in the effects of higher-level predictors, or (b) have a cluster size less than 10. Implications of the findings and limitations of the study are discussed.",0
https://doi.org/10.1016/j.jspi.2012.09.006,Estimation of covariance matrices based on hierarchical inverse-Wishart priors,"Abstract This paper focuses on Bayesian shrinkage methods for covariance matrix estimation. We examine posterior properties and frequentist risks of Bayesian estimators based on new hierarchical inverse-Wishart priors. More precisely, we give the conditions for the existence of the posterior distributions. Advantages in terms of numerical simulations of posteriors are shown. A simulation study illustrates the performance of the estimation procedures under three loss functions for relevant sample sizes and various covariance structures.",0
https://doi.org/10.1037/met0000112,An alternative to post hoc model modification in confirmatory factor analysis: The Bayesian lasso.,"As a commonly used tool for operationalizing measurement models, confirmatory factor analysis (CFA) requires strong assumptions that can lead to a poor fit of the model to real data. The post hoc modification model approach attempts to improve CFA fit through the use of modification indexes for identifying significant correlated residual error terms. We analyzed a 28-item emotion measure collected for n = 175 participants. The post hoc modification approach indicated that 90 item-pair errors were significantly correlated, which demonstrated the challenge in using a modification index, as the error terms must be individually modified as a sequence. Additionally, the post hoc modification approach cannot guarantee a positive definite covariance matrix for the error terms. We propose a method that enables the entire inverse residual covariance matrix to be modeled as a sparse positive definite matrix that contains only a few off-diagonal elements bounded away from zero. This method circumvents the problem of having to handle correlated residual terms sequentially. By assigning a Lasso prior to the inverse covariance matrix, this Bayesian method achieves model parsimony as well as an identifiable model. Both simulated and real data sets were analyzed to evaluate the validity, robustness, and practical usefulness of the proposed procedure. (PsycINFO Database Record",0
https://doi.org/10.1002/sim.6185,Propensity score methods for estimating relative risks in cluster randomized trials with low-incidence binary outcomes and selection bias,"Despite randomization, selection bias may occur in cluster randomized trials. Classical multivariable regression usually allows for adjusting treatment effect estimates with unbalanced covariates. However, for binary outcomes with low incidence, such a method may fail because of separation problems. This simulation study focused on the performance of propensity score (PS)-based methods to estimate relative risks from cluster randomized trials with binary outcomes with low incidence. The results suggested that among the different approaches used (multivariable regression, direct adjustment on PS, inverse weighting on PS, and stratification on PS), only direct adjustment on the PS fully corrected the bias and moreover had the best statistical properties.",0
https://doi.org/10.1177/1465116515618252,Going public against institutional constraints? Analyzing the online presence intensity of 2014 European Parliament election candidates,"Political parties and candidates have not been immune to the changes that the Internet and social media have introduced in electoral campaigns. Yet, as the use of digital media by political elites is becoming a norm in the United States, in Europe, the decision to develop an online presence depends on the cross-national differences regarding candidates’ constraints and incentives. European Parliament elections present an exceptional comparative opportunity to measure this potential diversity. Using an original database on the online presence of more than 5000 candidates competing under the label of incumbent parties in 2014, we demonstrate that there are two relevant groups of nonadopters, and that candidates’ online campaign intensity varies significantly depending on incumbency and the ballot structure in their countries.",0
https://doi.org/10.2165/00019053-200624050-00005,Comprehensive Decision-Analytic Model and Bayesian Value-of-Information Analysis,"Objective: To conduct a Bayesian value-of-information analysis of the cost effectiveness of pentoxifylline (vs placebo) as an adjunct to compression for venous leg ulcers. Methods: A probabilistic Markov model was developed to estimate mean clinical benefits and costs associated with oral pentoxifylline (400mg three times daily) and placebo. Clinical data were obtained from a systematic review and synthesised using Bayesian methods. The decision uncertainty associated with the adoption of pentoxifylline as well as the maximum value associated with further research were estimated before and after the completion of the largest 'definitive' treatment trial. Resource use was obtained from a UK national audit and unit costs applied (Ã‚Â£, 2004 values). Results: The prior and posterior analyses suggest that pentoxifylline is a dominant therapy versus placebo. In the prior analysis, patients in the pentoxifylline group healed an average of 8.28 weeks quicker than patients in the placebo group (95% credibility interval [CI] 1.89, 14.56), had a 0.02 gain in QALYs (95% CI -0.12, 0.17) and an average reduction in cost of Ã‚Â£153.4 (95% CI -53.11, 354.9). Estimates of the uncertainty surrounding the cost effectiveness of pentoxifylline and the value of perfect information in both analyses did not suggest further research was justified. In the prior analysis, for willingness-to-pay values of Ã‚Â£0, Ã‚Â£100 and Ã‚Â£500 per QALY gained, the estimated values of perfect information were Ã‚Â£128 200, Ã‚Â£127 100 and Ã‚Â£126 700, respectively. Incorporation of the information from the largest randomised controlled trial on pentoxifylline did improve the estimate of the clinical effect associated with this drug; however, the variation was not large enough to reverse either the decision regarding the dominance of pentoxifylline or the maximum value associated with further research. Conclusion: Bayesian value-of-information analysis represents a valuable tool for healthcare decision making. Had the results from this analysis been available before the largest trial was funded, a more efficient allocation of research and development resources could have been made. Ã‚Â© 2006 Adis Data Information BV. All rights reserved.",0
https://doi.org/10.1016/j.cyto.2003.08.001,Further characterization of high mobility group box 1 (HMGB1) as a proinflammatory cytokine: central nervous system effects,"High mobility group box 1 (HMGB1), an abundant, highly conserved cellular protein, is widely known as a nuclear DNA-binding protein. HMGB1 has been recently implicated as a proinflammatory cytokine because of its role as a late mediator of endotoxin lethality and ability to stimulate release of proinflammatory cytokines from monocytes. Production of central cytokines is a critical step in the pathway by which endotoxin and peripheral proinflammatory cytokines, including interleukin-1beta (IL-1) and tumor necrosis factor-alpha (TNF), produce sickness behaviors and fever. Intracerebroventricular (ICV) administration of HMGB1 has been shown to increase TNF expression in mouse brain and induce aphagia and taste aversion. Here we show that ICV injections of HMGB1 induce fever and hypothalamic IL-1 in rats. Furthermore, we show that intrathecal administration of HMGB1 produces mechanical allodynia (lowering of the response threshold to calibrated stimuli). Finally, while endotoxin (lipopolysaccharide, LPS) administration elevates IL-1 and TNF mRNA in various brain regions, HMGB1 mRNA is unchanged. It remains possible that HMGB1 protein is released in brain in response to LPS. Nonetheless, these data suggest that HMGB1 may play a role as an endogenous pyrogen and support the concept that HMGB1 has proinflammatory characteristics within the central nervous system.",0
https://doi.org/10.1037/a0020761,A general approach to causal mediation analysis.,"Traditionally in the social sciences, causal mediation analysis has been formulated, understood, and implemented within the framework of linear structural equation models. We argue and demonstrate that this is problematic for 3 reasons: the lack of a general definition of causal mediation effects independent of a particular statistical model, the inability to specify the key identification assumption, and the difficulty of extending the framework to nonlinear models. In this article, we propose an alternative approach that overcomes these limitations. Our approach is general because it offers the definition, identification, estimation, and sensitivity analysis of causal mediation effects without reference to any specific statistical model. Further, our approach explicitly links these 4 elements closely together within a single framework. As a result, the proposed framework can accommodate linear and nonlinear relationships, parametric and nonparametric models, continuous and discrete mediators, and various types of outcome variables. The general definition and identification result also allow us to develop sensitivity analysis in the context of commonly used models, which enables applied researchers to formally assess the robustness of their empirical conclusions to violations of the key assumption. We illustrate our approach by applying it to the Job Search Intervention Study. We also offer easy-to-use software that implements all our proposed methods.",0
https://doi.org/10.1007/s13595-015-0530-5,Improved models of harvest-induced bark damage,"We provided a precise quantitative analysis of the factors at the origin of bark damage during harvesting operations and developed a model able to predict them accurately. The major factors were the distance of trees to skid trails, the intensity of removals, the harvesting system as well as the interactions between the distance of trees to skid trails with harvesting systems, the average skidding distance, the tree species and tree height.  During timber harvesting, trees in the remaining stand may suffer bark damage resulting from tree-felling or log manipulation. Although a multitude of case studies and empirical observations provide qualitative and quantitative information with respect to the potential causal factors, the basic quantitative relationship between major factors of influence and the resulting degree of bark damage remains largely unclear. The objective was to provide a precise quantitative analysis of impact factors explaining the occurrence of bark damage during harvesting operations. Three different modelling approaches were tested: boosted regression tree (BRT), a generalised linear mixed effects model (GLMM) and Bayesian Markov chain Monte Carlo generalised linear mixed models (MCMCglmm). The major factors with a significant impact on the occurrence of bark damage were the distance of trees to skid trails, the intensity of removals, the harvesting system and the interaction term between the distance of trees to skid trails with harvesting systems, average skidding distance, tree species and tree height. The final model includes the relevant major factors impacting on the infliction of bark damage during practical harvesting operations. Furthermore, it discriminates well with respect to the occurrence of bark damage, and it provides managers with a rational and conclusive tool for optimising harvesting operations.",0
https://doi.org/10.1016/s0022-5371(80)90266-2,"Norms of 300 general-information questions: Accuracy of recall, latency of recall, and feeling-of-knowing ratings","Normative data were collected on 300 general-information questions from a wide variety of topics, including history, sports, art, geography, literature, and entertainment. Male and female undergraduates at two different universities made a one-word response to each question either in a response booklet or at a computer console. The reported data include the following for each question: (a) probability of recall for all 270 undergraduates, for males versus females, and for University of Washington subjects versus University of California, Irvine, subjects, (b) latency of correct recall, (c) latency of errors, and (d) feeling-of-knowing ratings for nonrecalled items. Correlations among these dependent variables, along with measures of reliability, are also reported.",0
https://doi.org/10.1046/j.0039-0402.2003.00256.x,Performance of empirical Bayes estimators of random coefficients in multilevel analysis: Some results for the random intercept-only model,"For a multilevel model with two levels and only a random intercept, the quality of different estimators of the random intercept is examined. Analytical results are given for the marginal model interpretation where negative estimates of the variance components are allowed for. Except for four or five level-2 units, the Empirical Bayes Estimator (EBE) has a lower average Bayes risk than the Ordinary Least Squares Estimator (OLSE). The EBEs based on restricted maximum likelihood (REML) estimators of the variance components have a lower Bayes risk than the EBEs based on maximum likelihood (ML) estimators. For the hierarchical model interpretation, where estimates of the variance components are restricted being positive, Monte Carlo simulations were done. In this case the EBE has a lower average Bayes risk than the OLSE, also for four or five level-2 units. For large numbers of level-1 (30) or level-2 units (100), the performances of REML-based and ML-based EBEs are comparable. For small numbers of level-1 (10) and level-2 units (25), the REML-based EBEs have a lower Bayes risk than ML-based EBEs only for high intraclass correlations (0.5).",0
https://doi.org/10.1177/0013164413498256,Mutual Information Item Selection Method in Cognitive Diagnostic Computerized Adaptive Testing With Short Test Length,"Cognitive diagnostic computerized adaptive testing (CD-CAT) purports to combine the strengths of both CAT and cognitive diagnosis. Cognitive diagnosis models aim at classifying examinees into the correct mastery profile group so as to pinpoint the strengths and weakness of each examinee whereas CAT algorithms choose items to determine those strengths and weakness as efficiently as possible. Most of the existing CD-CAT item selection algorithms are evaluated when test length is relatively long whereas several applications of CD-CAT, such as in interim assessment, require an item selection algorithm that is able to accurately recover examinees’ mastery profile with short test length. In this article, we introduce the mutual information item selection method in the context of CD-CAT and then provide a computationally easier formula to make the method more amenable in real time. Mutual information is then evaluated against common item selection methods, such as Kullback–Leibler information, posterior weighted Kullback–Leibler information, and Shannon entropy. Based on our simulations, mutual information consistently results in nearly the highest attribute and pattern recovery rate in more than half of the conditions. We conclude by discussing how the number of attributes, Q-matrix structure, correlations among the attributes, and item quality affect estimation accuracy.",0
https://doi.org/10.1016/j.fishres.2014.11.016,"Relative magnitude of cohort, age, and year effects on size at age of exploited marine fishes","Abstract Variation in individual growth rates contributes to changes over time in compensatory population growth and surplus production for marine fishes. However, there is little evidence regarding the prevalence and magnitude of time-varying growth for exploited marine fishes in general, whether it is best approximated using changes in length-at-age or weight-at-length parameters, or how it can be represented parsimoniously. We therefore use a database of average weight in each year and age for 91 marine fish stocks from 25 species, and fit models with random variation in length and weight parameters by year, age, or cohort (birth-year). Results show that year effects are more parsimonious than age or cohort effects and that variation in length and weight parameters provide roughly similar fit to average weight-at-age data, although length parameters show a greater magnitude of variability than weight parameters. Finally, the saturated model can explain nearly 2/3 of total variability, while a single time-varying factor can explain nearly 1/2 of variability in weight-at-age data. We conclude that time-varying growth can often be estimated parsimoniously using a single time-varying factor, either internally or prior to including ‘empirical’ weight at age in population dynamics models.",0
https://doi.org/10.1016/j.jsp.2013.05.005,Examining classroom influences on student perceptions of school climate: The role of classroom management and exclusionary discipline strategies,"There is growing emphasis on the use of positive behavior supports rather than exclusionary discipline strategies to promote a positive classroom environment. Yet, there has been limited research examining the association between these two different approaches to classroom management and students' perceptions of school climate. Data from 1902 students within 93 classrooms that were nested within 37 elementary schools were examined using multilevel structural equation modeling procedures to investigate the association between two different classroom management strategies (i.e., exclusionary discipline strategies and the use of positive behavior supports) and student ratings of school climate (i.e., fairness, order and discipline, student-teacher relationship, and academic motivation). The analyses indicated that greater use of exclusionary discipline strategies was associated with lower order and discipline scores, whereas greater use of classroom-based positive behavior supports was associated with higher scores on order and discipline, fairness, and student-teacher relationship. These findings suggest that pre-service training and professional development activities should promote teachers' use of positive behavior support strategies and encourage reduced reliance on exclusionary discipline strategies in order to enhance the school climate and conditions for learning.",0
https://doi.org/10.4324/9780203848852.ch9,Bootstrapping in Multilevel Models,"We note, however, that even as the number  of bootstrap replications tends to infinity, the  estimate of the population density function  that is used to generate the bootstrap samples is the empirical “plug in” one derived  from the actual sampled observations by  placing mass points (e.g., equal probabilities) at each one. In other words the sample  is assumed to be a reasonable representation  of the population. Thus, with nonparametric  bootstrapping, we do not have exact inference. This does not carry over to the parametric case that we describe below, where the  model-based (assumed) population distribution is used for sampling: we shall return  to this case later. In fact, in some situations  the nonparametric bootstrap can perform  very badly, for example, in small or moderate samples where the statistic of interest is  the smallest or largest value, say of a set of  higher level residuals in a multilevel model.",0
https://doi.org/10.1111/j.1745-9125.2008.00123.x,THE EFFECTS OF RESIDENTIAL TURNOVER ON HOUSEHOLD VICTIMIZATION*,"Americans move frequently, and moving alters their risks of victimization. This study uses unique longitudinal, multilevel data from the 1980–1985 National Crime Survey to examine the effects of residential turnover on household victimization. The two major findings of the study are as follows: First, housing turnover is a transition that independently increases the risk that a dwelling will experience a crime. This finding is true even controlling for persistent differences in crime vulnerability between dwellings. Second, changes in the composition and routine activities of households also alter the risks of victimization. These findings provide support for social disorganization and crime opportunity theories.",0
https://doi.org/10.1038/36846,The capacity of visual working memory for features and conjunctions,"Short-term memory storage can be divided into separate subsystems for verbal information and visual information, and recent studies have begun to delineate the neural substrates of these working-memory systems. Although the verbal storage system has been well characterized, the storage capacity of visual working memory has not yet been established for simple, suprathreshold features or for conjunctions of features. Here we demonstrate that it is possible to retain information about only four colours or orientations in visual working memory at one time. However, it is also possible to retain both the colour and the orientation of four objects, indicating that visual working memory stores integrated objects rather than individual features. Indeed, objects defined by a conjunction of four features can be retained in working memory just as well as single-feature objects, allowing sixteen individual features to be retained when distributed across four objects. Thus, the capacity of visual working memory must be understood in terms of integrated objects rather than individual features, which places significant constraints on cognitive and neurobiological models of the temporary storage of visual information.",0
https://doi.org/10.1026/0932-4089/a000165,Metaanalyse–praktische Schritte und Entscheidungen im Umsetzungsprozess,"In diesem Beitrag wird der Durchführungsprozess metaanalytischer Techniken nach Hunter und Schmidt (2004) Schritt für Schritt beschrieben. In Form eines Tutoriums geben wir evidenzbasierte Empfehlungen, verweisen auf relevante Quellen und Hilfsmittel und bewerten alternative Vorgehensweisen in Bezug auf ihre Güte und Akzeptanz. Wir wenden uns an Forschende, die eine Metaanalyse mit der Besonderheit der Artefaktkorrekturen nach Hunter und Schmidt durchführen wollen und geben Anregungen für die Verbreitung metaanalytischer Ergebnisse sowohl im Wissenschaftskontext als auch im Austausch mit Praktikern.",0
https://doi.org/10.1177/0146621613497532,A General Approach for Assessing Person Fit and Person Reliability in Typical-Response Measurement,"In constant-θ IRT models, person unreliability is regarded as a source of person misfit. In Variable-θ IRT models, on the other hand, it is regarded as a relevant individual characteristic, which is modeled as an additional person parameter. As in any IRT application, person fit must be assessed when a variable-θ model is fitted to real data. This assessment aims to detect inconsistent response patterns once person unreliability has been taken into account. The present paper makes a general proposal for assessing person fit based on variable-θ models that are intended to be used with personality and attitude measures. For the three types of variable-θ IRT models developed to date—binary, graded, and (approximately) continuous response models—(a) graphical procedures, (b) global person-fit indices, and (c) residual indices at the item level are proposed and discussed. A multistage approach for using the indices and procedures is also proposed and illustrated using an empirical example.",0
https://doi.org/10.1016/b978-012372560-8/50040-1,Multivariate autoregressive models,"Functional neuroimaging has been used to corroborate functional specialization as a principle of organization in the human brain. However, disparate regions of the brain do not operate in isolation and, more recently, neuroimaging has been used to characterize the network properties of the brain under speci.c cognitive states Buchel and Friston, 1997a  and  Buchel and Friston, 2000 . These studies address a complementary principle of organization, functional integration.",0
https://doi.org/10.3102/10769986028002111,Maximum Likelihood Estimation of Nonlinear Structural Equation Models with Ignorable Missing Data,"The existing maximum likelihood theory and its computer software in structural equation modeling are established on the basis of linear relationships among latent variables with fully observed data. However, in social and behavioral sciences, nonlinear relationships among the latent variables are important for establishing more meaningful models and it is very common to encounter missing data. In this article, an EM type algorithm is developed for maximum likelihood estimation of a general nonlinear structural equation model with ignorable missing data, which are missing at random with an ignorable mechanism. To avoid computation of the complicated multiple integrals involved in the conditional expectations, the E-step is completed by a hybrid algorithm that combines the Gibbs sampler and the Metropolis-Hastings algorithm; while the M-step is completed efficiently by conditional maximization. Standard errors of the maximum likelihood estimates are obtained via Louis’s formula. The methodology is illustrated with results obtained from a simulation study and a real data set with rather complicated missing patterns and a large number of missing entries.",0
https://doi.org/10.1016/s0167-9473(01)00058-5,Bayesian and likelihood methods for fitting multilevel models with complex level-1 variation,"In multilevel modelling it is common practice to assume constant variance at level 1 across individuals. In this paper we consider situations where the level-1 variance depends on predictor variables. We examine two cases using a dataset from educational research; in the first case the variance at level 1 of a test depends on a continuous intake score predictor, and in the second case the variance is assumed to differ according to gender. We contrast two maximum-likelihood methods based on iterative generalised least squares with two Markov chain Monte Carlo (MCMC) methods based on adaptive hybrid versions of the Metropolis-Hastings (MH) algorithm, and we use two simulation experiments to compare these four methods. We find that all four approaches have good repeated-sampling behaviour in the classes of models we simulate. We conclude by contrasting raw- and log-scale formulations of the level-1 variance function, and we find that adaptive MH sampling is considerably more efficient than adaptive rejection sampling when the heteroscedasticity is modelled polynomially on the log scale.",0
https://doi.org/10.1016/j.worlddev.2011.07.013,Small Farmers and Big Retail: Trade-offs of Supplying Supermarkets in Nicaragua,"Summary In Nicaragua and elsewhere in Central America, small-scale farmers are weighing the risks of entering into contracts with supermarket chains. We use unique data from cooperatives supplying supermarkets to study the effect of supply agreements on producers’ mean output prices and price stability. We find that prices paid by the domestic retail chain approximate the traditional market in mean and variance while mean prices paid by Walmart are significantly lower than the traditional market. However, the Walmart contract is found to systematically reduce price volatility. We find some evidence, however, that farmers may be paying too much for this contractual insurance against price variation.",0
,Data analysis in social psychology.,,0
https://doi.org/10.1016/j.neuropsychologia.2010.12.023,Storage and binding of object features in visual working memory,"An influential conception of visual working memory is of a small number of discrete memory ""slots"", each storing an integrated representation of a single visual object, including all its component features. When a scene contains more objects than there are slots, visual attention controls which objects gain access to memory. A key prediction of such a model is that the absolute error in recalling multiple features of the same object will be correlated, because features belonging to an attended object are all stored, bound together. Here, we tested participants' ability to reproduce from memory both the color and orientation of an object indicated by a location cue. We observed strong independence of errors between feature dimensions even for large memory arrays (6 items), inconsistent with an upper limit on the number of objects held in memory. Examining the pattern of responses in each dimension revealed a gaussian distribution of error centered on the target value that increased in width under higher memory loads. For large arrays, a subset of responses were not centered on the target but instead predominantly corresponded to mistakenly reproducing one of the other features held in memory. These misreporting responses again occurred independently in each feature dimension, consistent with 'misbinding' due to errors in maintaining the binding information that assigns features to objects. The results support a shared-resource model of working memory, in which increasing memory load incrementally degrades storage of visual information, reducing the fidelity with which both object features and feature bindings are maintained.",0
https://doi.org/10.1207/s15327906mbr4001_5,Fit Indices Versus Test Statistics,"Model evaluation is one of the most important aspects of structural equation modeling (SEM). Many model fit indices have been developed. It is not an exaggeration to say that nearly every publication using the SEM methodology has reported at least one fit index. Most fit indices are defined through test statistics. Studies and interpretation of fit indices commonly assume that the test statistics follow either a central chi-square distribution or a noncentral chi-square distribution. Because few statistics in practice follow a chi-square distribution, we study properties of the commonly used fit indices when dropping the chi-square distribution assumptions. The study identifies two sensible statistics for evaluating fit indices involving degrees of freedom. We also propose linearly approximating the distribution of a fit index/statistic by a known distribution or the distribution of the same fit index/statistic under a set of different conditions. The conditions include the sample size, the distribution of the data as well as the base-statistic. Results indicate that, for commonly used fit indices evaluated at sensible statistics, both the slope and the intercept in the linear relationship change substantially when conditions change. A fit index that changes the least might be due to an artificial factor. Thus, the value of a fit index is not just a measure of model fit but also of other uncontrollable factors. A discussion with conclusions is given on how to properly use fit indices.",0
https://doi.org/10.1002/sim.2666,Flexible random-effects models using Bayesian semi-parametric models: applications to institutional comparisons,"Random effects models are used in many applications in medical statistics, including meta-analysis, cluster randomized trials and comparisons of health care providers. This paper provides a tutorial on the practical implementation of a flexible random effects model based on methodology developed in Bayesian non-parametrics literature, and implemented in freely available software. The approach is applied to the problem of hospital comparisons using routine performance data, and among other benefits provides a diagnostic to detect clusters of providers with unusual results, thus avoiding problems caused by masking in traditional parametric approaches. By providing code for Winbugs we hope that the model can be used by applied statisticians working in a wide variety of applications.",0
https://doi.org/10.1002/0470023724.ch1b(ii),Hierarchical Modelling: Multilevel Modelling of Medical Data,"This tutorial presents an overview of multilevel or hierarchical data modelling and its applications in medicine. A description of the basic model for nested data is given and it is shown how this can be extended to fit flexible models for repeated measures data and more complex structures involving cross-classi cations and multiple membership patterns within the software package MLwiN. A variety of response types are covered and both frequentist and Bayesian estimation methods are described. Ã‚Â© 2004 John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester. All Rights Reserved.",0
https://doi.org/10.1016/s0169-2607(00)00096-1,A system for the assessment and training of temporal-order discrimination,"Two programs were developed for psychophysical assessment and training of temporal-order thresholds in the visual and auditory modalities. Order threshold is defined as the minimum onset interval between two sensory events (stimulus onset asynchrony, SOA) that must exist before an observer is able to indicate the correct order of the events. Brain-injured patients with aphasia and children with language-learning impairments, i.e. those who have been diagnosed as performing poorly on temporal-order tasks and in discriminating stop-consonant vowel syllables, can effectively be trained by a feedback training procedure in which the SOA is manipulated. The performance in the temporal-order task and the ability to discriminate phonemes improves with this procedure. In the diagnostic program, the SOA is changed by an adaptive procedure that generates a sequence of SOAs converging to the threshold and is driven by the responses of the subject. The feedback-training program begins with the presentation of SOAs, which are slightly above the individual order threshold; they are subsequently varied according to the responses of the subject.",0
https://doi.org/10.1063/1.1666011,Generalization of Euler Angles to N‐Dimensional Orthogonal Matrices,"An algorithm is presented whereby an N‐dimensional orthogonal matrix can be represented in terms of ½N(N − 1) independent parameters θk(ν)[ν=2,3,…,N;k=1,2,…,(ν−1)]. The parameters have the character of angles, whose compact domains are defined in a manner such that there exists a one‐to‐one correspondence between the points in the parameter space and the group of orthogonal matrices. Explicit formulas are given which express all matrix elements in terms of the angles, and formulas are given which express the angles in terms of the matrix elements. Special choices of angles give block‐diagonal matrices. For three‐dimensional matrices, the parametrization is equivalent to that of Euler.",0
https://doi.org/10.1016/j.pain.2004.06.001,Snake venom components enhance pain upon subcutaneous injection: an initial examination of spinal cord mediators,"Snakebites are a relevant public health problem in Central and South America. Snake bite envenomations cause intense pain, not relieved by anti-venom. The fangs of many species are short, causing subcutaneous injection. Fangs of larger species inflict subcutaneous or intramuscular envenomation. To understand pain induced by subcutaneous venom, this study examined spinal mechanisms involved in pain-enhancing effects of subcutaneous Lys49 and Asp49 secretory phospholipase-A(2) (sPLA2), two components of Bothrops asper snake venom showing highly different enzymatic activities. Unilateral intraplantar sPLA2-Lys49 (catalytically inactive) or sPLA2-Asp49 (catalytically active) into rat hindpaws each induced mechanical hyperalgesia (Randall-Selitto test), whereas only catalytically active sPLA2-Asp49 caused mechanical allodynia (von Frey test). Effects induced by both sPLA2s were inhibited by intrathecal fluorocitrate, a reversible glial metabolic inhibitor. In support, immunohistochemical analysis revealed activation of dorsal horn astrocytes and microglia after intraplantar injection of either sPLA2. Spinal proinflammatory cytokines, nitric oxide, and prostanoids each appear to be involved in the pain-enhancing effects of these sPLA2s. Blockade of interleukin-1 (IL1) inhibited hyperalgesia induced by both sPLA2s, while leaving allodynia unaffected. Blockade of tumor necrosis factor reduced responses to sPLA2-Asp49. An inhibitor of neuronal nitric oxide synthase, 7-nitroindazole (7-NI), inhibited hyperalgesia induced by both sPLA2s, without interfering with allodynia induced by sPLA2-Asp49. On the other hand, L-N(6)-(1-iminoethyl)lysine (L-NI), an inhibitor of the inducible nitric oxide synthase, did not alter any sPLA2-induced effect. Lastly, celecoxib, an inhibitor of cyclooxygenase-2, attenuated sPLA2 actions. These data provide the first evidence of spinal mediators involved in pain facilitation induced by subcutaneous venoms.",0
https://doi.org/10.1136/bmjopen-2014-005644,"Evidence for a persistent, major excess in all cause admissions to hospital in children with type-1 diabetes: results from a large Welsh national matched community cohort study","<h3>Objectives</h3> To estimate the excess in admissions associated with type1 diabetes in childhood. <h3>Design</h3> Matched-cohort study using anonymously linked hospital admission data. <h3>Setting</h3> Brecon Group Register of new cases of childhood diabetes in Wales linked to hospital admissions data within the Secure Anonymised Information Linkage Databank. <h3>Population</h3> 1577 Welsh children (aged between 0 and 15 years) from the Brecon Group Register with newly-diagnosed type-1 diabetes between 1999–2009 and 7800 population controls matched on age, sex, county, and deprivation, randomly selected from the local population. <h3>Main outcome measures</h3> Difference in all-cause hospital admission rates, 30-days post-diagnosis until 31 May 2012, between participants and controls. <h3>Results</h3> Children with type-1 diabetes were followed up for a total of 12 102 person years and were at 480% (incidence rate ratios, IRR 5.789, (95% CI 5.34 to 6.723), p&lt;0.0001) increased risk of hospital admission in comparison to matched controls. The highest absolute excess of admission was in the age group of 0–5 years, with a 15.4% (IRR 0.846, (95% CI 0.744 to 0.965), p=0.0061) reduction in hospital admissions for every 5-year increase in age at diagnosis. A trend of increasing admission rates in lower socioeconomic status groups was also observed, but there was no evidence of a differential rate of admissions between men and women when adjusted for background risk. Those receiving outpatient care at large centres had a 16.1% (IRR 0.839, (95% CI 0.709 to 0.990), p=0.0189) reduction in hospital admissions compared with those treated at small centres. <h3>Conclusions</h3> There is a large excess of hospital admissions in paediatric patients with type-1 diabetes. Rates are highest in the youngest children with low socioeconomic status. Factors influencing higher admission rates in smaller centres (eg, “out of hours resources”) need to be explored with the aim of targeting modifiable influences on admission rates.",0
https://doi.org/10.1016/j.jmva.2006.08.003,Estimation of high-dimensional prior and posterior covariance matrices in Kalman filter variants,"This work studies the effects of sampling variability in Monte Carlo-based methods to estimate very high-dimensional systems. Recent focus in the geosciences has been on representing the atmospheric state using a probability density function, and, for extremely high-dimensional systems, various sample-based Kalman filter techniques have been developed to address the problem of real-time assimilation of system information and observations. As the employed sample sizes are typically several orders of magnitude smaller than the system dimension, such sampling techniques inevitably induce considerable variability into the state estimate, primarily through prior and posterior sample covariance matrices. In this article, we quantify this variability with mean squared error measures for two Monte Carlo-based Kalman filter variants: the ensemble Kalman filter and the ensemble square-root Kalman filter. Expressions of the error measures are derived under weak assumptions and show that sample sizes need to grow proportionally to the square of the system dimension for bounded error growth. To reduce necessary ensemble size requirements and to address rank-deficient sample covariances, covariance-shrinking (tapering) based on the Schur product of the prior sample covariance and a positive definite function is demonstrated to be a simple, computationally feasible, and very effective technique. Rules for obtaining optimal taper functions for both stationary as well as non-stationary covariances are given, and optimal taper lengths are given in terms of the ensemble size and practical range of the forecast covariance. Results are also presented for optimal covariance inflation. The theory is verified and illustrated with extensive simulations.",0
https://doi.org/10.1198/016214502388618942,Parsimonious Covariance Matrix Estimation for Longitudinal Data,"This article proposes a data-driven method to identify parsimony in the covariance matrix of longitudinal data and to exploit any such parsimony to produce a statistically efficient estimator of the covariance matrix. The approach parameterizes the covariance matrix through the Cholesky decomposition of its inverse. For longitudinal data, this is a one-step-ahead predictive representation, and the Cholesky factor is likely to have off-diagonal elements that are zero or close to zero. A hierarchical Bayesian model is used to identify any such zeros in the Cholesky factor, similar to approaches that have been successful in Bayesian variable selection. The model is estimated using a Markov chain Monte Carlo sampling scheme that is computationally efficient and can be applied to covariance matrices of high dimension. It is demonstrated through simulations that the proposed method compares favorably in terms of statistical efficiency with a highly regarded competing approach. The estimator is applied to three ...",0
https://doi.org/10.1037/a0020141,A general multilevel SEM framework for assessing multilevel mediation.,"Several methods for testing mediation hypotheses with 2-level nested data have been proposed by researchers using a multilevel modeling (MLM) paradigm. However, these MLM approaches do not accommodate mediation pathways with Level-2 outcomes and may produce conflated estimates of between- and within-level components of indirect effects. Moreover, these methods have each appeared in isolation, so a unified framework that integrates the existing methods, as well as new multilevel mediation models, is lacking. Here we show that a multilevel structural equation modeling (MSEM) paradigm can overcome these 2 limitations of mediation analysis with MLM. We present an integrative 2-level MSEM mathematical framework that subsumes new and existing multilevel mediation approaches as special cases. We use several applied examples and accompanying software code to illustrate the flexibility of this framework and to show that different substantive conclusions can be drawn using MSEM versus MLM.",0
https://doi.org/10.1037/a0016972,Bayesian mediation analysis.,"In this article, we propose Bayesian analysis of mediation effects. Compared with conventional frequentist mediation analysis, the Bayesian approach has several advantages. First, it allows researchers to incorporate prior information into the mediation analysis, thus potentially improving the efficiency of estimates. Second, under the Bayesian mediation analysis, inference is straightforward and exact, which makes it appealing for studies with small samples. Third, the Bayesian approach is conceptually simpler for multilevel mediation analysis. Simulation studies and analysis of 2 data sets are used to illustrate the proposed methods. (PsycINFO Database Record (c) 2009 APA, all rights reserved).",1
https://doi.org/10.1080/10705511.2010.510052,Evaluation of Reliability Coefficients for Two-Level Models via Latent Variable Analysis,"A latent variable analysis procedure for evaluation of reliability coefficients for 2-level models is outlined. The method provides point and interval estimates of group means' reliability, overall reliability of means, and conditional reliability. In addition, the approach can be used to test simple hypotheses about these parameters. The procedure is applicable with unconditional models as well as with conditional models including higher level explanatory variables. The proposed method is illustrated with an empirical example.",0
https://doi.org/10.1016/s0304-3959(99)00302-4,An evaluation of homogeneity tests in meta-analyses in pain using simulations of individual patient data,"In this paper we consider the validity and power of some commonly used statistics for assessing the degree of homogeneity between trials in a meta-analysis. We show, using simulated individual patient data typical of that occurring in randomized controlled trials in pain, that the most commonly used statistics do not give the expected levels of statistical significance (i.e. the proportion of trials giving a significant result is not equal to the proportion expected due to random chance) when used with truly homogeneous data. In addition, all such statistics are shown to have extremely low power to detect true heterogeneity even when that heterogeneity is very large. Since, in most practical situations, failure to detect heterogeneity does not allow us to say with any helpful degree of certainty that the data is truly homogeneous, we advocate the quantitative combination of results only where the trials contained in a meta-analysis can be shown to be clinically homogeneous. We propose as a definition of clinical homogeneity that all trials have (i) fixed and clearly defined inclusion criteria and (ii) fixed and clearly defined outcomes or outcome measures. In pain relief, for example, the first of these would be satisfied by all patients having moderate or severe pain, whilst the second would be satisfied by using at least 50% pain relief as the successful outcome measure.",0
https://doi.org/10.1007/978-1-4939-2236-9_10,Constraining Cognitive Abstractions Through Bayesian Modeling,"There are many ways to combine neural and behavioral measures to study cognition. Someways are theoretical, and otherways are statistical. The predominant statistical approach treats both sources of data as independent and the relationship between the two measures is inferred by way of a (post hoc) regression analysis. Inthis chapter, we review an alternative approach that allows for flexible modeling of both measures simultaneously.We then explore and elaborate on several of the most important benefits of this modeling approach, and close with a model comparison of the Linear Ballistic Accumulator model and a drift diffusion model on neural and behavioral data. Â© Springer Science+Business Media, LLC 2015.",0
https://doi.org/10.1016/b978-0-12-724960-5.50010-x,Patterns of Change within Latent Variable Structural Equation Models,"This chapter demonstrates how selected hypotheses about patterns of change can be investigated using concepts derived from linear structural equation modeling. In Section 5.1 we discuss the concept of structuring correlations by structural equation methods. In Section 5.2 we describe some longitudinal data and illustrate technical aspects of various structural models for these summary statistics. Here we describe the relations between the algebraic and graphic representation of linear models and the nonlinear expectations they imply. In Section 5.3 four different models of change are presented and fitted to these longitudinal data using structural equation model algorithms. These models include an autoregressive change model, a difference components change model, a growth curve change model, and a factor analysis change model. In Section 5.4 some of these ideas are considered together in a multivariate framework. These examples demonstrate the merger of contemporary issues in developmental psychology and structural equation analysis.",0
https://doi.org/10.1598/rrq.40.2.3,Reinterpreting the development of reading skills,"Theories about reading have neglected basic differences in the developmental trajectories of skills related to reading. This essay proposes that some reading skills, such as learning the letters of the alphabet, are constrained to small sets of knowledge that are mastered in relatively brief periods of development. In contrast, other skills, such as vocabulary, are unconstrained by the knowledge to be acquired or the duration of learning. The conceptual, developmental, and methodological constraints on different reading skills are described in this essay that identifies various types of constraints on reading constructs and measures. Examples of reading research and assessment are discussed to illustrate (a) how the constraints can help to explain transitory correlational patterns among reading data, (b) how proxy effects surrounding constrained skills influence interpretations of reading development, (c) how prescriptions to teach constrained skills are causal misinterpretations of longitudinal correlations, and (d) why interventions on constrained skills usually lead only to temporary gains on skills aligned with the constrained skill. Because constrained skills are not normally distributed conceptually or empirically, except on special occasions, analyses based on parametric statistics are inappropriate. This essay describes implications for theories of reading development, research methods, and educational policies; the “extra” commentary linked to the online version of the article expands on this latter theme.  Las teorias acerca de la lectura han descuidado diferencias basicas en el camino evolutivo de las habilidades relacionadas con la lectura. Este ensayo propone que ciertas habilidades de lectura, como el aprendizaje de las letras del alfabeto, estan restringidas a pequenos conjuntos de conocimientos cuyo dominio se concreta en periodos relativamente breves. En contraste, otras habilidades tales como el vocabulario, no estan restringidas por el conocimiento que se adquirira ni por la duracion del aprendizaje. En este ensayo se describen las restricciones conceptuales, evolutivas y metodologicas en diferentes habilidades de lectura; el mismo identifica varios tipos de restricciones en las conceptualizaciones sobre la lectura y en las medidas de lectura. Se discuten ejemplos de investigacion y evaluacion en lectura para ilustrar: a) de que modo las restricciones pueden ayudar a explicar patrones correlacionales transitorios entre los datos de lectura, b) como efectos secundarios proximos a las habilidades restringidas influencian las interpretaciones del desarrollo en lectura, c) como ciertas propuestas para ensenar habilidades restringidas son malas interpretaciones causales de correlaciones longitudinales y d) por que las intervenciones sobre las habilidades restringidas generalmente conducen solo a logros temporarios en habilidades afines a la habilidad restringida. Debido a que las habilidades restringidas, excepto en ocasiones especiales, no estan normalmente distribuidas conceptual o empiricamente, son inapropiados los analisis basados en la estadistica parametrica. El ensayo describe implicancias para las teorias del desarrollo lector, los metodos de investigacion y las politicas educativas.  Theorien ubers Lesen haben die grundsatzlichen Unterschiede in den aufs Lesen bezogenen sich entwickelnden Leistungsbahnen vernachlassigt. Dieser Aufsatz vertritt die Ansicht, das einige Leseleistungen, wie beispielsweise das Erlernen der Buchstaben des Alphabets, auf eng begrenzte Erkenntniswerte beschrankt sind, die in relativ kurzen Entwicklungsperioden beherrscht werden. Im Gegensatz dazu sind andere Kenntnisse, wie das Vokabular, unbegrenzt im zu erlernenden Wissen oder in der Dauer beim Lernen. Die konzeptualen, entwicklungsbedingten und methodologischen Beschrankungen auf unterschiedliche Leseleistungen werden in diesem Aufsatz beschrieben, welcher verschiedene Typen von Einschrankungen im Lesen von Wortfugungen und Abschatzungen identifiziert. Beispiele aus der Leseforschung und Bewertung werden diskutiert, um zu illustrieren, (a) wie die Einschrankungen dazu verhelfen konnen, transitorische Korrelationsmuster zwischen den Lesedaten zu erklaren, (b) wie Vertretungsfunktionen umgebende Auswertungen von Leistungseinwirkungen der Leseentwicklung beeinflust, (c) wie Verordnungen zum Unterrichten eingegrenzter Kenntnisse kausale Fehlinterpretationen der Langenkorrelationen sind, und (d) warum Interventionen bei eingegrenzten Kenntnissen meistens nur zu zeitweisen Fortschritten der angeglichenen Leistungen eingeschrankter Fahigkeiten fuhren. Da eingeschrankte Befahigungen normalerweise nicht konzeptionell oder empirisch eingeteilt sind, auser bei speziellen Anlassen, sind die auf parametrische Statistiken basierte Analysen ungeeignet. Dieser Aufsatz beschreibt Implikationen fur Theorien zur Leseentwicklung, Forschungsmethoden und Unterrichtsverfahren.  Les theories de la lecture ont neglige des differences de base dans les trajectoires du developpement des competences relatives a la lecture. Cet essai enonce que certaines competences de lecture telles que les lettres de l'alphabet se reduisent a de petits ensembles de connaissances qui sont maitrisees au cours de periodes de developpement relativement breves. Par contre, d'autres competences telles que le vocabulaire ne sont pas limitees par les connaissances a acquerir ou par la duree de l'apprentissage. On decrit dans cet essai, qui identifie differents types de contraintes sur les constructs et les evaluations de la lecture, les contraintes conceptuelles, developpementales, et methodologiques de differentes competences de lecture. On discute des exemples de recherche et d'evaluation de la lecture afin de montrer a) comment les contraintes peuvent aider a expliquer certains patrons provisoires de correlations entre resultats de lecture, b) comment des effets voisins qui se trouvent autour des competences reduites jouent un role sur les interpretations du developpement de la lecture, c) comment les prescriptions d'enseignement de competences reduites sont responsables d'erreurs d'interpretation des correlations longitudinales, et d) pourquoi des interventions sur des competences reduites ne conduisent en general qu'a des gains temporaires sur des competences liees a celles-ci. Du fait que, sauf en de rares occasions, les competences reduites ne sont pas distribuees normalement, tant conceptuellement qu'empiriquement, les analyses reposant sur des statistiques parametriques sont inappropriees. Cet essai enonce enfin des implications sur les theories du developpement de la lecture, les methodes de recherche, et les politiques educatives.",0
https://doi.org/10.1037/a0023002,The PHLAME (Promoting Healthy Lifestyles: Alternative Models' Effects) firefighter study: Testing mediating mechanisms.,"This paper examines the mechanisms by which PHLAME (Promoting Healthy Lifestyles: Alternative Models' Effects), a health promotion intervention, improved healthy eating and exercise behavior among firefighters, a population at high risk for health problems due to occupational hazards. In a randomized trial, 397 firefighters participated in either the PHLAME team intervention with their work shift or a control condition. Intervention sessions taught benefits of a healthy diet and regular exercise, and sought to improve social norms and social support from coworkers for healthy behavior. At posttest, team intervention participants had increased their fruit and vegetable consumption as compared to control participants. An increase in knowledge of fruit and vegetable benefits and improved dietary coworker norms partially mediated these effects. Exercise habits and VO2 max were related to targeted mediators but were not significantly changed by the team intervention. Partial support was found for both the action and conceptual theories underlying the intervention. Our findings illustrate how an effective program's process can be deconstructed to understand the underpinnings of behavior change and refine interventions. Further, fire stations may improve the health of firefighters by emphasizing the benefits of healthy diet and exercise behaviors while also encouraging behavior change by coworkers as a whole.",0
https://doi.org/10.1007/s11222-006-7072-5,Variance component models for longitudinal count data with baseline information: epilepsy data revisited,"Random effect models have often been used in longitudinal data analysis since they allow for association among repeated measurements due to unobserved heterogeneity. Various approaches have been proposed to extend mixed models for repeated count data to include dependence on baseline counts. Dependence between baseline counts and individual-specific random effects result in a complex form of the (conditional) likelihood. An approximate solution can be achieved ignoring this dependence, but this approach could result in biased parameter estimates and in wrong inferences. We propose a computationally feasible approach to overcome this problem, leaving the random effect distribution unspecified. In this context, we show how the EM algorithm for nonparametric maximum likelihood (NPML) can be extended to deal with dependence of repeated measures on baseline counts. Â© Springer Science + Business Media, LLC 2006.",0
https://doi.org/10.1023/a:1020206907668,,"Markov chain Monte Carlo (MCMC) techniques have revolutionized the field of Bayesian statistics by enabling posterior inference for arbitrarily complex models. The now widely used WinBUGS software has, over the years, made the methodology accessible to a great many applied scientists, in all fields of research. Despite this, serious application of MCMC methods within the field of population PK/PD has been comparatively limited. We appreciate that for many applied pharmacokineticists the prospect of conducting a Bayesian analysis will require numerous alien concepts to be taken on board and it may be difficult to justify investing the time and effort required in order to understand them (especially since the approach is so computer-intensive). For this reason we provide here a thorough (but often informal) discussion of all aspects of Bayesian inference as they apply specifically to population PK/PD. We also acknowledge that while the WinBUGS software is general purpose, model specification for some types of problem, population PK/PD being a prime example, can be very difficult, to the extent that a specialized interface for describing the problem at hand is often a practical necessity. In the latter part of this paper we describe such an interface, namely PKBugs. A principal aim of the paper is to offer sufficient technical background, in an easy to follow format, that the reader may develop both the confidence and know-how to make appropriate use of the PKBugs/WinBUGS framework (or similar software) for their own data analysis needs, should they choose to adopt a Bayesian approach.",0
https://doi.org/10.1177/0013164406288164,Mutual Information Item Selection in Adaptive Classification Testing,"A general approach for item selection in adaptive multiple-category classification tests is provided. The approach uses mutual information (MI), a special case of the Kullback-Leibler distance, or relative entropy. MI works efficiently with the sequential probability ratio test and alleviates the difficulties encountered with using other local- and global-information measures in the multiple-category classification setting. Results from simulation studies using three item selection methods, Fisher information (FI), posterior-weighted FI (FIP), and MI, are provided for an adaptive four-category classification test. Both across and within the four classification categories, it is shown that in general, MI item selection classifies the highest proportion of examinees correctly and yields the shortest test lengths. The next best performance is observed for FIP item selection, followed by FI.",0
https://doi.org/10.1007/bf02296207,Maximum likelihood estimation of the polychoric correlation coefficient,"The polychoric correlation is discussed as a generalization of the tetrachoric correlation coefficient to more than two classes. Two estimation methods are discussed: Maximum likelihood estimation, and what may be called ""two-step maximum likelihood"" estimation. For the latter method, the thresholds are estimated in the first step. For both methods, asymptotic covariance matrices for estimates are derived, and the methods are illustrated and compared with artificial and real data. Ã‚Â© 1979 The Psychometric Society.",0
https://doi.org/10.1348/135910700168892,The Theory of Planned Behaviour and exercise: Evidence for the moderating role of past behaviour,"Objectives. This study reports an application of the Theory of Planned Behaviour (TPB) to the prediction of exercise intentions and behaviour over a 6-month period. The study also considers the moderating effect of past behaviour on the intention-behaviour and perceived behavioural control-behaviour relationships. Design and methods. A sample of 87 patients attending health promotions clinics in a primary care setting completed questionnaires on the TPB and exercise behaviour after their clinic appointment and were followed-up at 6 months. Results. The TPB was found to be predictive of initial exercise intentions and future exercise behaviour at 6-month follow-up, with the perceived behavioural control construct emerging as the sole independent predictor in both cases. Past behaviour was found to have a direct effect on future exercise behaviour over and above the influence of the TPB. In addition, past behaviour moderated the perceived behavioural control-behaviour relationship which was found to be significant when the frequency of past behaviour was moderate or high, but non-significant when the frequency of past behaviour was low. Conclusion. The results highlight (1) the importance of the perceived behavioural control construct of the TPB in the prediction of exercise intentions and behaviour, and (2) the need to consider the moderating effect of past behaviour on TPB-behaviour relations.",0
https://doi.org/10.1037/0022-006x.74.2.263,Dissonance and healthy weight eating disorder prevention programs: A randomized efficacy trial.,"In this trial, adolescent girls with body dissatisfaction (N = 481, M age = 17 years) were randomized to an eating disorder prevention program involving dissonance-inducing activities that reduce thin-ideal internalization, a prevention program promoting healthy weight management, an expressive writing control condition, or an assessment-only control condition. Dissonance participants showed significantly greater reductions in eating disorder risk factors and bulimic symptoms than healthy weight, expressive writing, and assessment-only participants, and healthy weight participants showed significantly greater reductions in risk factors and symptoms than expressive writing and assessment-only participants from pretest to posttest. Although these effects faded over 6-month and 12-month follow-ups, dissonance and healthy weight participants showed significantly lower binge eating and obesity onset and reduced service utilization through 12-month follow-up, suggesting that both interventions have public health potential.",0
https://doi.org/10.1080/00273171.2013.802978,Accuracy and Precision of an Effect Size and Its Variance From a Multilevel Model for Cluster Randomized Trials: A Simulation Study,"This article investigates an effect size (MLM ES) and its variance for cluster randomized trials based on parameter estimates from multilevel modeling analysis. Accuracy and precision of MLM ES were evaluated using Monte Carlo simulation methods and compared with the performance of an effect size, computed from summary statistics, proposed by Hedges (2007; Hedges' dB ). Simulation results indicated that MLM ES had acceptable accuracy in all conditions, also demonstrating efficiency and consistency. With small sample sizes, MLM ES did not suffer from the same negative bias as Hedges' dB due to overestimation of between-cluster variance. With large sample sizes, MLM ES and Hedges' dB were comparable for accuracy and efficiency. Both MLM ES and Hedges' dB showed considerable bias in some conditions when cluster sizes were unequal. An illustrative example using real data was provided.",0
https://doi.org/10.1016/s0065-2601(08)60144-6,The Social Relations Model,"Publisher Summary The Social Relations Model represents one method of studying two-person relationships. It attempts to separate the effects of persons and dyad. The Social Relations Model has three potential contributions to the study of dyads. First, it provides a purely methodological-statistical solution to the analysis of dyadic data. The Social Relations Model represents a new approach to the analysis of dyadic data structures. Second, the model can provide social psychology with better procedures to resolve the theoretical issues of the discipline. Third, the model is useful because it looks at social behavior as simultaneously operating at multiple levels. Very different principles operate at these different levels and only by simultaneously examining social behavior at different levels, the complexity and simplicity of social life can be fully appreciated.",0
https://doi.org/10.1037/0021-9010.88.4.694,Ethnic group differences in measures of job performance: A new meta-analysis.,"The authors conducted a new meta-analysis of ethnic group differences in job performance. Given a substantially increased set of data as compared with earlier analyses, the authors were able to conduct analyses of Black-White differences within more homogeneous categories of job performance and to reexamine findings on objective versus subjective measurement. Contrary to one perspective sometimes adopted in the field, objective measures are associated with very similar, if not somewhat larger, standardized ethnic group differences (ds) than subjective measures across a variety of indicators. This trend was consistent across quality, quantity, and absenteeism measures. Further, work samples and job knowledge tests are associated with larger ds than performance ratings or measures of absenteeism. Analysis of Hispanic-White standardized differences shows that they are generally lower than Black-White differences in several categories.",0
https://doi.org/10.1016/j.ssresearch.2015.12.006,Using geocoded survey data to improve the accuracy of multilevel small area synthetic estimates,"This paper examines the secondary data requirements for multilevel small area synthetic estimation (ML-SASE). This research method uses secondary survey data sets as source data for statistical models. The parameters of these models are used to generate data for small areas. The paper assesses the impact of knowing the geographical location of survey respondents on the accuracy of estimates, moving beyond debating the generic merits of geocoded social survey datasets to examine quantitatively the hypothesis that knowing the approximate location of respondents can improve the accuracy of the resultant estimates. Four sets of synthetic estimates are generated to predict expected levels of limiting long term illnesses using different levels of knowledge about respondent location. The estimates were compared to comprehensive census data on limiting long term illness (LLTI). Estimates based on fully geocoded data were more accurate than estimates based on data that did not include geocodes.",0
https://doi.org/10.1097/jnr.0000000000000042,Factors Affecting Perceptions of Family Function in Caregivers of Children With Attention Deficit Hyperactivity Disorders,"Attention deficit and hyperactivity disorder (ADHD) is the most common neurobehavioral disorder of childhood. ADHD has been shown to persist into adulthood in 30%-70% of cases. The long-term and escalating nature of ADHD creates an increasing burden on families because of the influence of hyperactivity and impulsivity on academic achievement and social interaction. There is a lack of information on factors influencing function in the families of children with ADHD.The purpose of this study was to test theoretically derived relationships among family demographic characteristics; family factors such as support, hardiness, and caregiver health; and family-functioning outcomes.This study used a cross-sectional study and structural equation modeling approach. A self-report questionnaire collected information from 122 caregivers on demographics, income, employment, and marital status data as well as on personal health, family support, family hardiness, and family function statuses as determined, respectively, using the Duke Health Profile, Family APGAR score, Family Hardiness Index, and Family Assessment Device.Structural equation modeling provided a reasonable fit to the data using AMOS (χ = .249, df = 1, p = .613, minimum discrepancy C = .249), goodness-of-fit index (.999), adjusted goodness of fit index (.990), normed fit index (.999), comparative fit index (1.0), and root mean square error of approximation (.000). Results indicated a 55.6% probability of becoming the construct model, with family hardiness and family support directly affecting family function and caregiver health. Family support functioned as a mediator in the relationship between family hardiness and family function.The findings of this study help nurses improve professional assessments and interventions for families of children with ADHD by highlighting the importance of increased family support, promoting family hardiness, and promoting caregivers' health to improved family function.",0
https://doi.org/10.3102/10769986030001026,Maximum Likelihood Analysis of a Two-Level Nonlinear Structural Equation Model With Fixed Covariates,"In this article, a maximum likelihood (ML) approach for analyzing a rather general two-level structural equation model is developed for hierarchically structured data that are very common in educational and/or behavioral research. The proposed two-level model can accommodate nonlinear causal relations among latent variables as well as effects of fixed covariate in its various components. Methods for computing the ML estimates, and the Bayesian information criterion (BIC) for model comparison are established on the basis of powerful tools in statistical computing such as the Monte Carlo EM algorithm, Gibbs sampler, Metropolis–Hastings algorithm, conditional maximization, bridge sampling, and path sampling. The newly developed procedures are illustrated by results obtained from a simulation study and analysis of a real data set in education.",0
https://doi.org/10.1177/0146621612446806,"Evaluating EIV, OLS, and SEM Estimators of Group Slope Differences in the Presence of Measurement Error","Measurement error significantly biases interaction effects and distorts researchers’ inferences regarding interactive hypotheses. This article focuses on the single-indicator case and shows how to accurately estimate group slope differences by disattenuating interaction effects with errors-in-variables (EIV) regression. New analytic findings were presented along with simulation results to compare the relative bias, power, and Type I error rates of EIV, ordinary least squares (OLS), and sparse (i.e., single indicator) multigroup structural equation model (SEM) estimators of interaction effects in the presence of measurement error. The results suggest that EIV was less biased than were OLS and sparse SEM. Furthermore, OLS and sparse SEM were unable to control the Type I error rate for tests of slope differences in circumstances where groups differ in predictor reliability. Additional derivations examined the impact of using Cronbach’s alpha, which is typically a lower bound for reliability, with EIV. The results provided evidence that using alpha does result in overcorrected EIV estimates and the bias in EIV estimates associated with using Cronbach’s alpha increases with variability in item loadings and bias decreases as either test length or the average loading increases. The bias in EIV estimates when using alpha is not larger than the bias produced by using OLS or sparse SEM. In summary, the results provide compelling evidence that researchers should use EIV instead of OLS and sparse SEM to estimate group slope differences in the presence of measurement error.",0
https://doi.org/10.1016/0022-2496(80)90003-6,On appropriate procedures for combining probability distributions within the same family,"Abstract This article considers procedures for combining individual probability distributions that belong to some “family” into a “group” probability distribution that belongs to the same family. The procedures considered are Vincentizing, in which quantiles are averaged across distributions; generalized Vincentizing, in which the quantiles are transformed before averaging; and pooling based on the distribution function or the probability density function. Some of these results are applied to models of reaction time in psychological experiments.",0
https://doi.org/10.1201/b14835-13,Inference and monitoring convergence,,0
https://doi.org/10.1002/sim.1020,Sample size considerations in observational health care quality studies,"A common objective in health care quality studies involves measuring and comparing the quality of care delivered to cohorts of patients by different health care providers. The data used for inference involve observations on units grouped within clusters, such as patients treated within hospitals. Unlike cluster randomization trials where often clusters are randomized to interventions to learn about individuals, the target of inference in health quality studies is the cluster. Furthermore, randomization is often not performed and the resulting biases may invalidate standard tests. In this paper, we discuss approaches to sample size determination in the design of observational health quality studies when the outcome is binary. Methods for calculating sample size using marginal models are briefly reviewed, but the focus is on hierarchical binomial models. Sample size in unbalanced clusters and stratified designs are characterized. We draw upon the experiences that have arisen from a study funded by the Agency for Healthcare Research and Quality involving assessment of quality of care for patients with cardiovascular disease. If researchers are interested in comparing clusters, hierarchical models are preferred. Copyright © 2002 John Wiley & Sons, Ltd.",0
https://doi.org/10.1016/j.addbeh.2012.03.031,Personality mediators of psychopathy and substance dependence in male offenders,"Psychopathy and substance dependence (SUD) is highly prevalent in incarcerated populations and tends to co-occur in the same individuals. The factors underlying this relationship are not clearly understood. The primary purpose of this study was to investigate whether two personality models mediate the relationship between psychopathy and substance misuse in male offenders. Ninety-two inmates in provincial correctional centers in New Brunswick completed questionnaires, including the Sensitivity to Reward Sensitivity to Punishment Questionnaire to measure behavioral activation and behavioral inhibition, the Substance Use Risk Profile Scale to measure anxiety sensitivity, introversion/hopelessness, sensation seeking and impulsivity, and the Psychopathic Personality Inventory—Revised to assess psychopathy levels. Results revealed that high impulsivity indirectly mediated the relationship between psychopathy and stimulant dependence. In addition, low anxiety sensitivity indirectly mediated the relationship between psychopathy and opioid dependence. Finally, impulsivity indirectly and inconsistently mediated the relationship between psychopathy and alcohol dependence. These results suggest that individuals with psychopathic traits are at increased risk of misusing certain drugs due to underlying personality-based differences. ► Psychopathy traits/substance misuse are highly prevalent and comorbid in offenders. ► We recruited inmates and measured psychopathy, personality, and substance dependence. ► High impulsivity was an indirect mediator of psychopathy and stimulant/alcohol SUD. ► Low anxiety sensitivity was an indirect mediator of psychopathy and opioid SUD. ► Psychopathic traits are distinctly related with SUDS due to personality differences.",0
https://doi.org/10.1007/s11336-007-9008-1,Multilevel Modeling with Correlated Effects,"When there exist omitted effects, measurement error, and/or simultaneity in multilevel models, explanatory variables may be correlated with random components, and standard estimation methods do not provide consistent estimates of model parameters. This paper introduces estimators that are consistent under such conditions. By employing generalized method of moments (GMM) estimation techniques in multilevel modeling, the authors present a series of estimators along a robust to efficient continuum. This continuum depends on the assumptions that the analyst makes regarding the extent of the correlated effects. It is shown that the GMM approach provides an overarching framework that encompasses well-known estimators such as fixed and random effects estimators and also provides more options. These GMM estimators can be expressed as instrumental variable (IV) estimators which enhances their interpretability. Moreover, by exploiting the hierarchical structure of the data, the current technique does not require additional variables unlike traditional IV methods. Further, statistical tests are developed to compare the different estimators. A simulation study examines the finite sample properties of the estimators and tests and confirms the theoretical order of the estimators with respect to their robustness and efficiency. It further shows that not only are regression coefficients biased, but variance components may be severely underestimated in the presence of correlated effects. Empirical standard errors are employed as they are less sensitive to correlated effects when compared to model-based standard errors. An example using student achievement data shows that GMM estimators can be effectively used in a search for the most efficient among unbiased estimators. Ã‚Â© 2007 The Psychometric Society.",0
https://doi.org/10.1016/j.csda.2006.10.005,Comparison of PQL and Laplace 6 estimates of hierarchical linear models when comparing groups of small incident rates in cluster randomised trials,"The variances of the random components in hierarchical generalised linear models (HGLMs) with binary outcomes have been reported to have a considerable downward bias when estimated with the commonly used penalised quasilikelihood (PQL) technique. The more recently proposed Laplace 6 approximation promises to reduce this bias. This study compares the performance of these two techniques when estimating the parameters of a particular HGLM. This comparison is performed via Monte Carlo simulations in which the difference between two groups of proportions, modelled after those appearing in many epidemiological cluster randomised interventions, are tested using this model. The Laplace 6 approximation does reduce the bias mentioned above, but at the price of a higher mean square error. The results of this study suggest that the optimal solution involves using a combination of these two techniques. This combination is illustrated by analysing a data set from a real cluster randomised intervention.",0
https://doi.org/10.3102/1076998609332750,A Nonlinear Mixed Effects Model for Latent Variables,"The nonlinear mixed effects model for continuous repeated measures data has become an increasingly popular and versatile tool for investigating nonlinear longitudinal change in observed variables. In practice, for each individual subject, multiple measurements are obtained on a single response variable over time or condition. This structure can be adapted to examine the change in latent variables rather than modeling change in manifest variables. This article considers a nonlinear mixed effects model for describing nonlinear change of a latent construct over time, where the latent construct of interest is measured by multiple indicators gathered at each measurement occasion. To accomplish this, the nonlinear mixed effects model is modified to include a measurement model that explicitly expresses the relationship of the observed variables to the latent constructs. A method for marginal maximum likelihood estimation of this model is presented and discussed. An example using education data is provided to illustrate the utility of the model.",0
,Double pulse resolution in the visual field: the influence of temporal stimulus characteristics,"I. It has been suggested that measuring double-pulse resolution in the visual field is more useful than performing flicker perimetry. Yet it is difficult to assess the diagnostic potentials of this technique unless a number of methodological difficulties are overcome. 2. We succeeded to show that double-pulse resolution can be measured efficiently and reliably by varying pulse durations over a wide temporal range, by employing a nine-alternative forced-choice paradigm with nine locations in the visual field, and by employing a maximum likelihood estimate of the threshold parameter. Our results, which have been obtained at the central fovea and at eight locations on the principal meridians with 3.4° eccentricity, reveal three main properties of visual performance. 3. Temporal resolution is worst (i.e. 50-70 ms) at a duration of the leading pulse of 20 ms. It monotonically improves to assume an asymptotic value of about 20 ms beyond pulse durations of say 150 ms. Resolution may also improve if the pulse duration is as brief as 10 ms. 4. The prolongation of the trailing pulse has virtually no effect on double-pulse resolution. 5. Double-pulse resolution in the central fovea is, almost independently of the pulse duration, 10-20 ms better than in the peripheral visual field.",0
https://doi.org/10.1177/0963721414529144,The Adaptive Nature of Visual Working Memory,"A growing body of scientific evidence suggests that visual working memory and statistical learning are intrinsically linked. Although visual working memory is severely resource limited, in many cases, it makes efficient use of its available resources by adapting to statistical regularities in the visual environment. However, experimental evidence also suggests that there are clear limits and biases in statistical learning. This raises the intriguing possibility that performance limitations observed in visual working memory tasks can to some degree be explained in terms of limits and biases in statistical-learning ability, rather than limits in memory capacity.",0
https://doi.org/10.1002/sim.1903,Stratification and weighting via the propensity score in estimation of causal treatment effects: a comparative study,"Estimation of treatment effects with causal interpretation from observational data is complicated because exposure to treatment may be confounded with subject characteristics. The propensity score, the probability of treatment exposure conditional on covariates, is the basis for two approaches to adjusting for confounding: methods based on stratification of observations by quantiles of estimated propensity scores and methods based on weighting observations by the inverse of estimated propensity scores. We review popular versions of these approaches and related methods offering improved precision, describe theoretical properties and highlight their implications for practice, and present extensive comparisons of performance that provide guidance for practical use. Copyright © 2004 John Wiley & Sons, Ltd.",0
https://doi.org/10.1177/01466216970212006,Estimation of Composite Reliability for Congeneric Measures,"A structural equation model is described that permits estimation of the reliability index and coefficient of a composite test for congeneric measures. The method is also helpful in exploring the factorial structure of an item set, and its use in scale reliability estimation and development is illustrated. The modeling. estimator of composite reliability it yields does not possess the general underestimation property of Cronbach's coefficient a.",0
https://doi.org/10.1016/j.bbi.2008.05.004,Proinflammatory cytokines oppose opioid-induced acute and chronic analgesia,"Spinal proinflammatory cytokines are powerful pain-enhancing signals that contribute to pain following peripheral nerve injury (neuropathic pain). Recently, one proinflammatory cytokine, interleukin-1, was also implicated in the loss of analgesia upon repeated morphine exposure (tolerance). In contrast to prior literature, we demonstrate that the action of several spinal proinflammatory cytokines oppose systemic and intrathecal opioid analgesia, causing reduced pain suppression. In vitro morphine exposure of lumbar dorsal spinal cord caused significant increases in proinflammatory cytokine and chemokine release. Opposition of analgesia by proinflammatory cytokines is rapid, occurring < or =5 min after intrathecal (perispinal) opioid administration. We document that opposition of analgesia by proinflammatory cytokines cannot be accounted for by an alteration in spinal morphine concentrations. The acute anti-analgesic effects of proinflammatory cytokines occur in a p38 mitogen-activated protein kinase and nitric oxide dependent fashion. Chronic intrathecal morphine or methadone significantly increased spinal glial activation (toll-like receptor 4 mRNA and protein) and the expression of multiple chemokines and cytokines, combined with development of analgesic tolerance and pain enhancement (hyperalgesia, allodynia). Statistical analysis demonstrated that a cluster of cytokines and chemokines was linked with pain-related behavioral changes. Moreover, blockade of spinal proinflammatory cytokines during a stringent morphine regimen previously associated with altered neuronal function also attenuated enhanced pain, supportive that proinflammatory cytokines are importantly involved in tolerance induced by such regimens. These data implicate multiple opioid-induced spinal proinflammatory cytokines in opposing both acute and chronic opioid analgesia, and provide a novel mechanism for the opposition of acute opioid analgesia.",0
https://doi.org/10.1046/j.1529-8027.2001.006001111.x,Sciatic inflammatory neuritis (SIN): Behavioral allodynia is paralleled by peri‐sciatic proinflammatory cytokine and superoxide production,"We have recently developed a model of sciatic inflammatory neuritis (SIN) to assess how immune activation near peripheral nerves influences somatosensory processing. Administration of zymosan (yeast cell walls) around a single sciatic nerve produces dose-dependent low-threshold mechanical allodynia without thermal hyperalgesia. Low (4 microg) doses produce both territorial and extraterritorial allodynia restricted to the injected hindleg. In contrast, higher (40 microg) doses produce territorial and extraterritorial allodynias of both hindlegs, an effect not accounted for by systemic spread of the zymosan. The aim of these experiments was to determine whether these behavioral allodynias were correlated with immunological and/or anatomical changes in or around the sciatic nerve. These experiments reveal that zymosan-induced bilateral allodynia was associated with the following: (a) increased release of both interleukin-1beta and tumor necrosis factor-alpha from peri-sciatic immune cells; (b) increased release of reactive oxygen species from perisciatic immune cells; (c) no change in circulating levels of proinflammatory cytokine; (d) no apparent zymosan-induced influx of immune cells into the sciatic nerve from the endoneurial blood vessels; (e) mild edema of the sciatic, which was predominantly restricted to superficial regions closest to the peri-sciatic immune cells; and (f) no anatomic evidence of changes in either the ipsilateral saphenous nerve or contralateral sciatic nerve that could account for the appearance of extraterritorial or contralateral (""mirror"") allodynia, respectively. No reliable differences were found when the low-dose zymosan was compared with vehicle controls. Taken together, these data suggest that substances released by peri-sciatic immune cells may induce changes in the sciatic nerve, leading to the appearance of bilateral allodynia.",0
https://doi.org/10.1111/j.1559-1816.1998.tb01685.x,Extending the Theory of Planned Behavior: A Review and Avenues for Further Research,"This paper describes and reviews the theory of planned behavior (TPB). The focus is on evidence supporting the further extension of the TPB in various ways. Empirical and theoretical evidence to support the addition of 6 variables to the TPB is reviewed: belief salience measures, past behaviodhabit, perceived behavioral control (PBC) vs. selfefficacy, moral norms, self-identity, and affective beliefs. In each case there appears to be growing empirical evidence to support their addition to the TPB and some understanding of the processes by which they may be related to other TPB variables, intentions, and behavior. Two avenues for expansion of the TPB are presented. First, the possibility of incorporating the TPB into a dual-process model of attitude-behavior relationships is reviewed. Second, the expansion of the TPB to include consideration of the volitional processes determining how goal intentions may lead to goal achievement is discussed.",0
https://doi.org/10.1093/mnras/stv2501,Hierarchical cosmic shear power spectrum inference,"We develop a Bayesian hierarchical modelling approach for cosmic shear power spectrum inference, jointly sampling from the posterior distribution of the cosmic shear field and its (tomographic) power spectra. Inference of the shear power spectrum is a powerful intermediate product for a cosmic shear analysis, since it requires very few model assumptions and can be used to perform inference on a wide range of cosmological models \emph{a posteriori} without loss of information. We show that joint posterior for the shear map and power spectrum can be sampled effectively by Gibbs sampling, iteratively drawing samples from the map and power spectrum, each conditional on the other. This approach neatly circumvents difficulties associated with complicated survey geometry and masks that plague frequentist power spectrum estimators, since the power spectrum inference provides prior information about the field in masked regions at every sampling step. We demonstrate this approach for inference of tomographic shear $E$-mode, $B$-mode and $EB$-cross power spectra from a simulated galaxy shear catalogue with a number of important features; galaxies distributed on the sky and in redshift with photometric redshift uncertainties, realistic random ellipticity noise for every galaxy and a complicated survey mask. The obtained posterior distributions for the tomographic power spectrum coefficients recover the underlying simulated power spectra for both $E$- and $B$-modes.",0
https://doi.org/10.3758/pbr.17.1.59,Homogeneity computation: How interitem similarity in visual short-term memory alters recognition,"Visual short-term recognition memory for multiple stimuli is strongly influenced by the study items’ similarity to one another—that is, by their homogeneity. However, the mechanism responsible for this homogeneity effect has remained unclear. We evaluated competing explanations of this effect, using controlled sets of Gabor patches as study items and probe stimuli. Our results, based on recognition memory for spatial frequency, rule out the possibility that the homogeneity effect arises because similar study items are encoded and/or maintained with higher fidelity in memory than dissimilar study items are. Instead, our results support the hypothesis that the homogeneity effect reflects trial-by-trial comparisons of study items, which generate a homogeneity signal. This homogeneity signal modulates recognition performance through an adjustment of the subject’s decision criterion. Additionally, it seems the homogeneity signal is computed prior to the presentation of the probe stimulus, by evaluating the familiarity of each new stimulus with respect to the items already in memory. This suggests that recognition-like processes operate not only on the probe stimulus, but on study items as well.",0
https://doi.org/10.1007/s12561-014-9124-2,Bayesian Two-Stage Biomarker-Based Adaptive Design for Targeted Therapy Development,"We propose a Bayesian two-stage biomarker-based adaptive randomization (AR) design for the development of targeted agents. The design has three main goals: (1) to test the treatment efficacy, (2) to identify prognostic and predictive markers for the targeted agents, and (3) to provide better treatment for patients enrolled in the trial. To treat patients better, both stages are guided by the Bayesian AR based on the individual patient's biomarker profiles. The AR in the first stage is based on a known marker. A Go/No-Go decision can be made in the first stage by testing the overall treatment effects. If a Go decision is made at the end of the first stage, a two-step Bayesian lasso strategy will be implemented to select additional prognostic or predictive biomarkers to refine the AR in the second stage. We use simulations to demonstrate the good operating characteristics of the design, including the control of per-comparison type I and type II errors, high probability in selecting important markers, and treating more patients with more effective treatments. Bayesian adaptive designs allow for continuous learning. The designs are particularly suitable for the development of multiple targeted agents in the quest of personalized medicine. By estimating treatment effects and identifying relevant biomarkers, the information acquired from the interim data can be used to guide the choice of treatment for each individual patient enrolled in the trial in real time to achieve a better outcome. The design is being implemented in the BATTLE-2 trial in lung cancer at the MD Anderson Cancer Center.",0
https://doi.org/10.1215/03616878-3523946,Path Dependency and the Politics of Socialized Health Care,"Abstract Rich democracies exhibit vast cross-national and historical variation in the socialization of health care. Yet, cross-national analyses remain relatively rare in the health policy literature, and health care remains relatively neglected in the welfare state literature. We analyze pooled time series models of the public share of total health spending for eighteen rich democracies from 1960 to 2010. Building on path dependency theory, we present a strategy for modeling the relationship between the initial 1960 public share and the current public share. We also examine two contrasting accounts for how the 1960 public share interacts with conventional welfare state predictors: the self-reinforcing hypothesis expecting positive feedbacks and the counteracting hypothesis expecting negative feedbacks. We demonstrate that most of the variation from 1960 to 2010 in the public share can be explained by a country's initial value in 1960. This 1960 value has a large significant effect in models of 1961–2010, and including the 1960 value alters the coefficients of conventional welfare state predictors. To investigate the mechanism whereby prior social policy influences public opinion about current social policy, we use the 2006 International Social Survey Programme (ISSP). This analysis confirms that the 1960 values predict individual preferences for government spending on health. Returning to the pooled time series, we demonstrate that the 1960 values interact significantly with several conventional welfare state predictors. Some interactions support the self-reinforcing hypothesis, while others support the counteracting hypothesis. Ultimately, this study illustrates how historical legacies of social policy exert substantial influence on the subsequent politics of social policy.",0
https://doi.org/10.1111/j.1467-9469.2006.00492.x,Empirical and Hierarchical Bayesian Estimation in Finite Population Sampling under Structural Measurement Error Models,".  This paper considers simultaneous estimation of means from several strata. A model-based approach is taken, where the covariates in the superpopulation model are subject to measurement errors. Empirical Bayes (EB) and Hierarchical Bayes estimators of the strata means are developed and asymptotic optimality of EB estimators is proved. Their performances are examined and compared with that of the sample mean in a simulation study as well as in data analysis.",0
https://doi.org/10.1080/00273171.2015.1022639,A Heterogeneous Growth Curve Model for Nonnormal Data,"The heterogeneous growth curve model (HGM; Klein & Muthén, 2006 ) is a method for modeling heterogeneity of growth rates with a heteroscedastic residual structure for the slope factor. It has been developed as an extension of a conventional growth curve model and a complementary tool to growth curve mixture models. In this article, a robust version of the heterogeneous growth curve model (HGM-R) is presented that extends the original HGM with a mixture model to allow for an unbiased parameter estimation under the condition of nonnormal data. In two simulation studies, the performance of the method is examined under the condition of nonnormality and a misspecified heteroscedastic residual structure. The results of the simulation studies suggest an unbiased estimation of the heterogeneity by the HGM-R when sample size was large enough and a good approximation of the heteroscedastic residual structure even when the functional form of the heteroscedasticity was misspecified. The practical application of the approach is demonstrated for a data set from HIV-infected patients.",0
https://doi.org/10.1080/00220973.2013.813360,Tests of Mediation: Paradoxical Decline in Statistical Power as a Function of Mediator Collinearity,"Increasing the correlation between the independent variable and the mediator (a coefficient) increases the effect size (ab) for mediation analysis; however, increasing a by definition increases collinearity in mediation models. As a result, the standard error of product tests increase. The variance inflation due to increases in a at some point outweighs the increase of the effect size (ab) and results in a loss of statistical power. This phenomenon also occurs with nonparametric bootstrapping approaches because the variance of the bootstrap distribution of ab approximates the variance expected from normal theory. Both variances increase dramatically when a exceeds the b coefficient, thus explaining the power decline with increases in a. Implications for statistical analysis and applied researchers are discussed.",0
https://doi.org/10.1037/a0023464,Evaluating models for partially clustered designs.,"Partially clustered designs, where clustering occurs in some conditions and not others, are common in psychology, particularly in prevention and intervention trials. This article reports results from a simulation comparing 5 approaches to analyzing partially clustered data, including Type I errors, parameter bias, efficiency, and power. Results indicate that multilevel models adapted for partially clustered data are relatively unbiased and efficient and consistently maintain the nominal Type I error rate when using appropriate degrees of freedom. To attain sufficient power in partially clustered designs, researchers should attend primarily to the number of clusters in the study. An illustration using data from a partially clustered eating disorder prevention trial is provided.",0
https://doi.org/10.1111/j.2517-6161.1963.tb00481.x,Sequential Estimation of Quantal Response Curves,,0
https://doi.org/10.1093/acprof:oso/9780195173444.001.0001,Models for Intensive Longitudinal Data,A new class of longitudinal data has emerged with the use of technological devices for scientific data collection. This class of data is called intensive longitudinal data (ILD). This volume features state-of-the-art applied statistical modelling strategies developed by leading statisticians and methodologists working in conjunction with behavioural scientists.,0
https://doi.org/10.1111/1467-9884.00117,Markov chain Monte Carlo method and its application,"The Markov chain Monte Carlo (MCMC) method, as a computer-intensive statistical tool, has enjoyed an enormous upsurge in interest over the last few years. This paper provides a simple, comprehensive and tutorial review of some of the most common areas of research in this field. We begin by discussing how MCMC algorithms can be constructed from standard building-blocks to produce Markov chains with the desired stationary distribution. We also motivate and discuss more complex ideas that have been proposed in the literature, such as continuous time and dimension jumping methods. We discuss some implementational issues associated with MCMC methods. We take a look at the arguments for and against multiple replications, consider how long chains should be run for and how to determine suitable starting points. We also take a look at graphical models and how graphical approaches can be used to simplify MCMC implementation. Finally, we present a couple of examples, which we use as case-studies to highlight some of the points made earlier in the text. In particular, we use a simple changepoint model to illustrate how to tackle a typical Bayesian modelling problem via the MCMC method, before using mixture model problems to provide illustrations of good sampler output and of the implementation of a reversible jump MCMC algorithm",0
https://doi.org/10.1287/mnsc.38.4.555,Stochastic Dominance and Expected Utility: Survey and Analysis,"While Stochastic Dominance has been employed in various forms as early as 1932, it has only been since 1969–1970 that the notion has been developed and extensively employed in the area of economics, finance, agriculture, statistics, marketing and operations research. In this survey, the first-, second- and third-order stochastic dominance rules are discussed with an emphasis on the development in the area since the 1980s.",0
https://doi.org/10.1111/j.1470-6431.2011.01067.x,Consumer willingness to pay for food safety in Tanzania: an incentive-aligned conjoint analysis,"In this paper, we present results from a consumer experiment in Tanzania focusing on food safety. We elicit consumers' willingness to pay (WTP) a premium for tomatoes that have been inspected by health officials to meet the standards set by the Tanzania Bureau of Standards. We also elicit consumers' WTP for tomato attributes that can be associated with different food safety standards: conventional vs. organically produced and various origins. Two hundred sixty-nine urban consumers from Morogoro, Tanzania took part in the experiment where they evaluated tomatoes using the Becker–deGroot–Marschak mechanism. The results show that on average, consumers in Tanzania are willing to pay a premium for inspected and organically produced tomatoes. Consumers have a strong preference for tomatoes produced in Tanzania and do not discount tomatoes produced in areas associated with poor agricultural practices. However, consumers do significantly discount tomatoes imported from South Africa.",0
https://doi.org/10.3310/hta16350,Influence of reported study design characteristics on intervention effect estimates from randomised controlled trials: combined analysis of meta-epidemiological studies.,"Background The design of randomised controlled trials (RCTs) should incorporate characteristics (such as concealment of randomised allocation and blinding of participants and personnel) that avoid biases resulting from lack of comparability of the intervention and control groups. Empirical evidence suggests that the absence of such characteristics leads to biased intervention effect estimates, but the findings of different studies are not consistent. Objectives To examine the influence of unclear or inadequate random sequence generation and allocation concealment, and unclear or absent double blinding, on intervention effect estimates and between-trial heterogeneity, and whether or not these influences vary with type of clinical area, intervention, comparison and outcome measure. Data sources and methods Data were combined from seven contributing meta-epidemiological studies (collections of meta-analyses in which trial characteristics are assessed and results recorded). The resulting database was used to identify and remove overlapping meta-analyses. Outcomes were coded such that odds ratios < 1 correspond to beneficial intervention effects. Outcome measures were classified as mortality, other objective or subjective. We examined agreement between assessments of trial characteristics in trials assessed in more than one contributing study. We used hierarchical Bayesian bias models to estimate the effect of trial characteristics on average bias [quantified as ratios of odds ratios (RORs) with 95% credible intervals (CrIs) comparing trials with and without a characteristic] and in increasing between-trial heterogeneity. Results The analysis data set contained 1973 trials included in 234 meta-analyses. Median kappa statistics for agreement between assessments of trial characteristics were: sequence generation 0.60, allocation concealment 0.58 and blinding 0.87. Intervention effect estimates were exaggerated by an average 11% in trials with inadequate or unclear (compared with adequate) sequence generation (ROR 0.89, 95% CrI 0.82 to 0.96); between-trial heterogeneity was higher among such trials. Bias associated with inadequate or unclear sequence generation was greatest for subjective outcomes (ROR 0.83, 95% CrI 0.74 to 0.94) and the increase in heterogeneity was greatest for such outcomes [standard deviation (SD) 0.20, 95% CrI 0.03 to 0.32]. The effect of inadequate or unclear (compared with adequate) allocation concealment was greatest among meta-analyses with a subjectively assessed outcome intervention effect (ROR 0.85, 95% CrI 0.75 to 0.95), and the increase in between-trial heterogeneity was also greatest for such outcomes (SD 0.20, 95% CrI 0.02 to 0.33). Lack of, or unclear, double blinding (compared with double blinding) was associated with an average 13% exaggeration of intervention effects (ROR 0.87, 95% CrI 0.79 to 0.96), and between-trial heterogeneity was increased for such studies (SD 0.14, 95% CrI 0.02 to 0.30). Average bias (ROR 0.78, 95% CrI 0.65 to 0.92) and between-trial heterogeneity (SD 0.37, 95% CrI 0.19 to 0.53) were greatest for meta-analyses assessing subjective outcomes. Among meta-analyses with subjectively assessed outcomes, the effect of lack of blinding appeared greater than the effect of inadequate or unclear sequence generation or allocation concealment. Conclusions Bias associated with specific reported study design characteristics leads to exaggeration of beneficial intervention effect estimates and increases in between-trial heterogeneity. For each of the three characteristics assessed, these effects were greatest for subjectively assessed outcomes. Assessments of the risk of bias in RCTs should account for these findings. Further research is needed to understand the effects of attrition bias, as well as the relative importance of blinding of patients, care-givers and outcome assessors, and thus separate the effects of performance and detection bias. Funding National Institute for Health Research Health Technology Assessment programme.",0
https://doi.org/10.1023/b:hsor.0000031402.52155.62,Hierarchical Generalised Linear Models with Time-Dependent Clustering: Assessing the Effect of Health Sector Reform on Patient Outcomes in New Zealand,"New Zealand has one of the most reformed health systems in the world. This paper is primarily concerned with modelling the impact on hospital outcomes of the reforms of the early 1990s, when as part of a major, health sector wide reform process, the administration of public hospitals passed from elected Area Health Boards (AHBs) to Crown Health Enterprises (CHEs) operating under a competitive model of health care provision dominated by the funder/purchaser/provider split. The impact of reform processes on public hospitals is of particular interest since they consume 40%-50% of public expenditure on health, and have been repeatedly restructured in an attempt to contain the ever-expanding cost of health care. There is concern among both health professionals and the general public that these restructurings are reducing the quality of hospital services, and therefore negatively effecting patient outcomes. Using data from a study of 34 New Zealand public hospitals, we discuss the application of Bayesian hierarchical generalised linear models to the analysis of trends in patient outcomes over the period 1988-2001. The time-varying nature of the grouping of hospitals within larger health authorities complicates the application of HGLMs because the cluster structure of the data changes over the study period. An approach to dealing with such ""time-dependent clustering"" by introducing period-specific authority level effects is developed. The analysis does not support the proposition that higher level authorities had an effect on outcome trends, or that the administrative changeover from AHBs to CHEs impacted on 60-day post-admission mortality. Ã‚Â© 2004 Kluwer Academic Publishers.",0
https://doi.org/10.1080/1369183x.2013.830496,Do Integration Policies Affect Immigrants' Voluntary Engagement? An Exploration at Switzerland's Subnational Level,"This paper investigates whether integration policies influence immigrants' propensity to volunteer, the latter being an important element of immigrants' integration into the host society. By distinguishing different categories of integration policies at Switzerland's subnational level and applying a Bayesian multilevel approach, our results suggest varying policy effects: while policies fostering socio-structural rights enhance immigrants' propensity to volunteer, we observe a negative curvilinear relationship between cultural rights and obligations and immigrants' volunteerism implying that a combination of cultural entitlements and obligations is most conducive to immigrants' civic engagement.",0
https://doi.org/10.1177/0146621612459552,Computerized Adaptive Testing Using a Class of High-Order Item Response Theory Models,"In the human sciences, a common assumption is that latent traits have a hierarchical structure. Higher order item response theory models have been developed to account for this hierarchy. In this study, computerized adaptive testing (CAT) algorithms based on these kinds of models were implemented, and their performance under a variety of situations was examined using simulations. The results showed that the CAT algorithms were very effective. The progressive method for item selection, the Sympson and Hetter method with online and freeze procedure for item exposure control, and the multinomial model for content balancing can simultaneously maintain good measurement precision, item exposure control, content balance, test security, and pool usage.",0
,Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R,"Matching is an R package which provides functions for multivariate and propensity score matching and for finding optimal covariate balance based on a genetic search algorithm. A variety of univariate and multivariate metrics to determine if balance actually has been obtained are provided. The underlying matching algorithm is written in C++, makes extensive use of system BLAS and scales efficiently with dataset size. The genetic algorithm which finds optimal balance is parallelized and can make use of multiple CPUs or a cluster of computers. A large number of options are provided which control exactly how the matching is conducted and how balance is evaluated.",0
https://doi.org/10.1177/0013164405278558,Using the SPSS Mixed Procedure to Fit Cross-Sectional and Longitudinal Multilevel Models,"Beginning with Version 11, SPSS implemented the MIXED procedure, which is capable of performing many common hierarchical linear model analyses. The purpose of this article was to provide a tutorial for performing cross-sectional and longitudinal analyses using this popular software platform. In doing so, the authors borrowed heavily from Singer’s overview of SAS PROC MIXED, duplicating her analyses using the SPSS MIXED procedure.",0
https://doi.org/10.1111/j.0006-341x.2004.00224.x,Bayesian Multivariate Logistic Regression,"Bayesian analyses of multivariate binary or categorical outcomes typically rely on probit or mixed effects logistic regression models that do not have a marginal logistic structure for the individual outcomes. In addition, difficulties arise when simple noninformative priors are chosen for the covariance parameters. Motivated by these problems, we propose a new type of multivariate logistic distribution that can be used to construct a likelihood for multivariate logistic regression analysis of binary and categorical data. The model for individual outcomes has a marginal logistic structure, simplifying interpretation. We follow a Bayesian approach to estimation and inference, developing an efficient data augmentation algorithm for posterior computation. The method is illustrated with application to a neurotoxicology study.",0
https://doi.org/10.1504/ijids.2013.058286,Consumer-oriented new product development: a review of recent developments,"Firms need to continuously develop new products or redesign their existing ones, due to the intense competition they are facing, as well as the rapidly changing economical and sociopolitical environment. In this context, consumer behaviour modelling has become an important and inextricable part of successful new product development during the last decades. This paper constitutes a survey of the literature in consumer-oriented new product development. We review a total of 60 research propositions in consumer behaviour modelling as a part of the product development procedure. The findings indicate a trend for integrative methodologies that approach the new product development process from both a marketing and an engineering perspective. The incorporation of dynamic consumer behaviour models into the product development methodologies seems to be the most promising area for future research.",0
https://doi.org/10.1037//1082-989x.7.4.422,Mediation in experimental and nonexperimental studies: New procedures and recommendations.,"Mediation is said to occur when a causal effect of some variable X on an outcome Y is explained by some intervening variable M. The authors recommend that with small to moderate samples, bootstrap methods (B. Efron & R. Tibshirani. 1993) be used to assess mediation. Bootstrap tests are powerful because they detect that the sampling distribution of the mediated effect is skewed away from 0. They argue that R. M. Baron and D. A. Kenny's (1986) recommendation of first testing the X â†’ Y association for statistical significance should not be a requirement when there is a priori belief that the effect size is small or suppression is a possibility. Empirical examples and computer setups for bootstrap analyses are provided.",0
https://doi.org/10.1016/j.ssresearch.2015.08.003,"The mental health consequences of the economic crisis in Europe among the employed, the unemployed, and the non-employed","Applying a multi-level framework to the data from the European Social Survey's Round 3 (2006) and Round 6 (2012), we assessed the crisis by increases in rates of unemployment, while also controlling for countries' pre-crisis economic conditions. We found a positive relationship between depression and an increase in national unemployment rates. This relationship can be only partly ascribed to an increase in the number of unemployed and those employed in nonstandard job conditions-with the exception of the self-employed and women working part-time. The crisis effect is more pronounced among men and those between 35 and 49years of age. Moreover, in strongly effected countries, the crisis has changed the relationship between part-time work and depression, between depression and certain subcategories of the unemployed (looking for a job or not looking), and between depression and the non-employed.",0
https://doi.org/10.1007/bf02291552,Bayesian estimation in unrestricted factor analysis: A treatment for heywood cases,"A Bayesian procedure is given for estimation in unrestricted common factor analysis. A choice of the form of the prior distribution is justified. It is shown empirically that the procedure achieves its objective of avoiding inadmissible estimates of unique variances, and is reasonably insensitive to certain variations in the shape of the prior distribution. Ã‚Â© 1975 Psychometric Society.",0
https://doi.org/10.1080/00273171.2014.962683,An Index and Test of Linear Moderated Mediation,"I describe a test of linear moderated mediation in path analysis based on an interval estimate of the parameter of a function linking the indirect effect to values of a moderator-a parameter that I call the index of moderated mediation. This test can be used for models that integrate moderation and mediation in which the relationship between the indirect effect and the moderator is estimated as linear, including many of the models described by Edwards and Lambert ( 2007 ) and Preacher, Rucker, and Hayes ( 2007 ) as well as extensions of these models to processes involving multiple mediators operating in parallel or in serial. Generalization of the method to latent variable models is straightforward. Three empirical examples describe the computation of the index and the test, and its implementation is illustrated using Mplus and the PROCESS macro for SPSS and SAS.",0
https://doi.org/10.3758/pbr.15.1.1,Three case studies in the Bayesian analysis of cognitive models,"Bayesian statistical inference offers a principled and comprehensive approach for relating psychological models to data. This article presents Bayesian analyses of three influential psychological models: multidimensional scaling models of stimulus representation, the generalized context model of category learning, and a signal detection theory model of decision making. In each case, the model is recast as a probabilistic graphical model and is evaluated in relation to a previously considered data set. In each case, it is shown that Bayesian inference is able to provide answers to important theoretical and empirical questions easily and coherently. The generality of the Bayesian approach and its potential for the understanding of models and data in psychology are discussed.",0
https://doi.org/10.1037/a0035628,Analyzing multiple outcomes in clinical research using multivariate multilevel models.,"Multilevel models have become a standard data analysis approach in intervention research. Although the vast majority of intervention studies involve multiple outcome measures, few studies use multivariate analysis methods. The authors discuss multivariate extensions to the multilevel model that can be used by psychotherapy researchers.Using simulated longitudinal treatment data, the authors show how multivariate models extend common univariate growth models and how the multivariate model can be used to examine multivariate hypotheses involving fixed effects (e.g., does the size of the treatment effect differ across outcomes?) and random effects (e.g., is change in one outcome related to change in the other?). An online supplemental appendix provides annotated computer code and simulated example data for implementing a multivariate model.Multivariate multilevel models are flexible, powerful models that can enhance clinical research.",0
https://doi.org/10.1371/journal.pone.0060650,Individual Participant Data Meta-Analysis for a Binary Outcome: One-Stage or Two-Stage?,"A fundamental aspect of epidemiological studies concerns the estimation of factor-outcome associations to identify risk factors, prognostic factors and potential causal factors. Because reliable estimates for these associations are important, there is a growing interest in methods for combining the results from multiple studies in individual participant data meta-analyses (IPD-MA). When there is substantial heterogeneity across studies, various random-effects meta-analysis models are possible that employ a one-stage or two-stage method. These are generally thought to produce similar results, but empirical comparisons are few.We describe and compare several one- and two-stage random-effects IPD-MA methods for estimating factor-outcome associations from multiple risk-factor or predictor finding studies with a binary outcome. One-stage methods use the IPD of each study and meta-analyse using the exact binomial distribution, whereas two-stage methods reduce evidence to the aggregated level (e.g. odds ratios) and then meta-analyse assuming approximate normality. We compare the methods in an empirical dataset for unadjusted and adjusted risk-factor estimates.Though often similar, on occasion the one-stage and two-stage methods provide different parameter estimates and different conclusions. For example, the effect of erythema and its statistical significance was different for a one-stage (OR = 1.35, [Formula: see text]) and univariate two-stage (OR = 1.55, [Formula: see text]). Estimation issues can also arise: two-stage models suffer unstable estimates when zero cell counts occur and one-stage models do not always converge.When planning an IPD-MA, the choice and implementation (e.g. univariate or multivariate) of a one-stage or two-stage method should be prespecified in the protocol as occasionally they lead to different conclusions about which factors are associated with outcome. Though both approaches can suffer from estimation challenges, we recommend employing the one-stage method, as it uses a more exact statistical approach and accounts for parameter correlation.",0
https://doi.org/10.1177/0149206314546750,Institutionalizing Bayesianism Within the Organizational Sciences,"Bayesian estimation and inference remain infrequently used in organizational science research. Despite innumerable warnings regarding the entrenched frequentist paradigm, our field has yet to embrace the Bayesian “revolution” that seems to be sweeping through so many other disciplines. With this context as a backdrop, we address a simple yet difficult question: What is the likelihood that Bayesian methodologies eventually will supplement or even supplant traditional frequentist methodologies in the organizational science community? We draw on institutional theory to address this question, highlighting the cultural-cognitive, normative, and regulative forces that play important roles. As novel contributions to the discussion, we go beyond our own ideas and previously published opinions on the subject to report the opinions of 26 institutional elites (current and former officers of academic associations, editors, and editorial board members). These leading scholars help us shed light not only on the likelihood that Bayesianism will take root in the field but also on practical steps that could be taken to assist in this process. In some ways, we build Bayesian priors about Bayesian analysis, where those priors will be qualified on the basis of future events and outcomes.",0
https://doi.org/10.3758/bf03196470,Perceptual organization influences visual working memory,"Previous studies have demonstrated that top-down factors can bias the storage of information in visual working memory. However, relatively little is known about the role that bottom-up stimulus characteristics play in visual working memory storage. In the present study, subjects performed a change detection task in which the to-be-remembered objects were organized in accordance with Gestalt grouping principles. When an attention-capturing cue was presented at the location of one object, other objects that were perceptually grouped with the cued object were more likely to be stored in working memory than were objects that were not grouped with the cued object. Thus, objects that are grouped together tend to be stored together, indicating that bottom-up perceptual organization influences the storage of information in visual working memory.",0
https://doi.org/10.1109/jstsp.2015.2407855,Bayesian Fusion of Multi-Band Images,"In this paper, a Bayesian fusion technique for remotely sensed multi-band images is presented. The observed images are related to the high spectral and high spatial resolution image to be recovered through physical degradations, e.g., spatial and spectral blurring and/or subsampling defined by the sensor characteristics. The fusion problem is formulated within a Bayesian estimation framework. An appropriate prior distribution exploiting geometrical consideration is introduced. To compute the Bayesian estimator of the scene of interest from its posterior distribution, a Markov chain Monte Carlo algorithm is designed to generate samples asymptotically distributed according to the target distribution. To efficiently sample from this high-dimension distribution, a Hamiltonian Monte Carlo step is introduced in the Gibbs sampling strategy. The efficiency of the proposed fusion method is evaluated with respect to several state-of-the-art fusion techniques. In particular, low spatial resolution hyperspectral and multispectral images are fused to produce a high spatial resolution hyperspectral image.",0
https://doi.org/10.1016/c2009-0-30639-x,Introduction to WinBUGS for Ecologists,"Bayesian statistics has exploded into biology and its sub-disciplines such as ecology over the past decade. The free software program WinBUGS and its open-source sister OpenBugs is currently the only flexible and general-purpose program available with which the average ecologist can conduct their own standard and non-standard Bayesian statistics. Introduction to WINBUGS for Ecologists goes right to the heart of the matter by providing ecologists with a comprehensive, yet concise, guide to applying WinBUGS to the types of models that they use most often: linear (LM), generalized linear (GLM), linear mixed (LMM) and generalized linear mixed models (GLMM).Introduction to WinBUGS for Ecologists combines the use of simulated data sets ""paired"" analyses using WinBUGS (in a Bayesian framework for analysis) and in R (in a frequentist mode of inference) and uses a very detailed step-by-step tutorial presentation style that really lets the reader repeat every step of the application of a given mode in their own research.- Introduction to the essential theories of key models used by ecologists- Complete juxtaposition of classical analyses in R and Bayesian Analysis of the same models in WinBUGS- Provides every detail of R and WinBUGS code required to conduct all analyses- Written with ecological language and ecological examples- Companion Web Appendix that contains all code contained in the book, additional material (including more code and solutions to exercises)- Tutorial approach shows ecologists how to implement Bayesian analysis in practical problems that they face. Â© 2010 Elsevier Inc. All rights reserved.",0
https://doi.org/10.1186/1471-2288-12-34,Individual patient data meta-analysis of survival data using Poisson regression models,"Abstract Background An Individual Patient Data (IPD) meta-analysis is often considered the gold-standard for synthesising survival data from clinical trials. An IPD meta-analysis can be achieved by either a two-stage or a one-stage approach, depending on whether the trials are analysed separately or simultaneously. A range of one-stage hierarchical Cox models have been previously proposed, but these are known to be computationally intensive and are not currently available in all standard statistical software. We describe an alternative approach using Poisson based Generalised Linear Models (GLMs). Methods We illustrate, through application and simulation, the Poisson approach both classically and in a Bayesian framework, in two-stage and one-stage approaches. We outline the benefits of our one-stage approach through extension to modelling treatment-covariate interactions and non-proportional hazards. Ten trials of hypertension treatment, with all-cause death the outcome of interest, are used to apply and assess the approach. Results We show that the Poisson approach obtains almost identical estimates to the Cox model, is additionally computationally efficient and directly estimates the baseline hazard. Some downward bias is observed in classical estimates of the heterogeneity in the treatment effect, with improved performance from the Bayesian approach. Conclusion Our approach provides a highly flexible and computationally efficient framework, available in all standard statistical software, to the investigation of not only heterogeneity, but the presence of non-proportional hazards and treatment effect modifiers.",0
https://doi.org/10.1093/pan/mpl013,Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference,"Although published works rarely include causal estimates from more than a few model specifications, authors usually choose the presented estimates from numerous trial runs readers never see. Given the often large variation in estimates across choices of control variables, functional forms, and other modeling assumptions, how can researchers ensure that the few estimates presented are accurate or representative? How do readers know that publications are not merely demonstrations that it is possible to find a specification that fits the author's favorite hypothesis? And how do we evaluate or even define statistical properties like unbiasedness or mean squared error when no unique model or estimator even exists? Matching methods, which offer the promise of causal inference with fewer assumptions, constitute one possible way forward, but crucial results in this fast-growing methodological literature are often grossly misinterpreted. We explain how to avoid these misinterpretations and propose a unified approach that makes it possible for researchers to preprocess data with matching (such as with the easy-to-use software we offer) and then to apply the best parametric techniques they would have used anyway. This procedure makes parametric models produce more accurate and considerably less model-dependent causal inferences.",0
https://doi.org/10.1080/00273171.2010.519276,Multilevel Factor Analysis and Structural Equation Modeling of Daily Diary Coping Data: Modeling Trait and State Variation,"The current study used multilevel modeling of daily diary data to model within-person (state) and between-person (trait) components of coping variables. This application included the introduction of multilevel factor analysis (MFA) and a comparison of the predictive ability of these trait/state factors. Daily diary data was collected on a large (n = 366) multiethnic sample over the course of five days. Intraclass correlation coefficient for the derived factors suggested approximately equal amounts of variability in coping usage at the state and trait levels. MFAs showed that Problem-Focused Coping and Social Support emerged as stable factors at both the within-person and between-person levels. Other factors (Minimization, Emotional Rumination, Avoidance, Distraction) were specific to the within-person or between-person levels, but not both. Multilevel structural equation modeling (MSEM) showed that the prediction of daily positive and negative affect differed as a function of outcome and level of coping factor. The Discussion section focuses primarily on a conceptual and methodological understanding of modeling state and trait coping using daily diary data with MFA and MSEM to examine covariation among coping variables and predicting outcomes of interest.",0
https://doi.org/10.1214/06-ba117rej,Rejoinder,,0
https://doi.org/10.1037/a0029317,Investigating inter-individual differences in short-term intra-individual variability.,"Intra-individual variability over a short period of time may contain important information about how individuals differ from each other. In this article we begin by discussing diverse indicators for quantifying intra-individual variability and indicate their advantages and disadvantages. Then we propose an alternative method that models inter-individual differences in intra-individual variability by separately considering both the amplitude of fluctuations and temporal dependency in the data. In the proposed model, temporal dependency and amplitude of fluctuations are both included as random effects. Parameter estimation is done with a multiple-step approach using maximum likelihood, or with a recommended 1-step approach using a Bayesian method. The similarities and differences between the proposed method and some existing methods are discussed and investigated using diary study data from older adults. The results from empirical data analysis revealed that temporal dependency and amplitude of fluctuations have different predictability of health outcomes and thus should be modeled and considered separately.",0
https://doi.org/10.1006/jmps.1993.1033,Response Time Distributions in Memory Scanning,"Abstract This article reports the results of a memory scanning experiment (S. Sternberg. 1966, Science, 153 , 652-654) in which each of four subjects participated in about 1500 experimental trials per memory set size. These large samples made it possible to test a number of important nonparametric (i.e., model-free) properties of the response time (RT) distributions. These properties place severe constraints on the various memory scanning models and they provide a deeper description of the data than summary statistics or goodness-of-fit values. Five conclusions stood out. First, increasing the size of the memory set induced the strongest possible form of stochastic dominance on both target present and target absent trials. Second, the RT hazard functions were nonmonotonic, thereby falsifying a large class of serial searell models, Third, strong evidence was obtained against art exhaustive search. Fourth, some evidence was found that adding an item to the memory set inserts a stage with exponentially distributed duration into the processing claim, at least on largest absent trials. Fifth, the data supported the hypothesis that three of the subjects stored the representations of the memory set items in a visual short-term memory system and the fourth subject used all acoustic short-term system. To our knowledge, the only extant model of memory scanning that is consistent with all these results assumes that search is parallel, self-terminating, and of very limited capacity.",0
https://doi.org/10.1007/s00477-007-0139-9,Quantifying geographic variations in associations between alcohol distribution and violence: a comparison of geographically weighted regression and spatially varying coefficient models,"Past studies consistently indicate measurable local associations between alcohol distribution and the incidence of violence. These results, coupled with measurements of spatial correlation, reveal the importance of spatial analysis in the study of the interaction of alcohol and violence. While studies increasingly incorporate spatial correlation among model residuals to improve precision and reduce bias, to date, most analyses assume associations that are constant and independent of location, an assumption coming under increasing scrutiny in the quantitative geography literature. In this paper, we review and contrast two approaches for the estimation of and inference for spatially heterogeneous effects (i.e., associative factors whose impacts on the outcome of interest vary throughout geographic space). Specifically, we provide an in-depth comparison of 'geographically weighted regression' models (allowing covariate effects to vary in space but only allowing relatively ad hoc inference) with 'variable coefficient' models (allowing varying effects via spatial random fields and providing model-based estimation and inference, but requiring more advanced computational techniques). We compare the approaches with respect to underlying conceptual structures, computational implementation, and inferential output. We apply both approaches to violent crime, illegal drug arrest, and alcohol distribution data from Houston, Texas and compare results in light of the differing methodological structures of the two approaches. Ã‚Â© Springer-Verlag 2007.",0
https://doi.org/10.1037/1082-989x.10.3.259,People are variables too: Multilevel structural equations modeling.,"The article uses confirmatory factor analysis (CFA) as a template to explain didactically multilevel structural equation models (ML-SEM) and to demonstrate the equivalence of general mixed-effects models and ML-SEM. An intuitively appealing graphical representation of complex ML-SEMs is introduced that succinctly describes the underlying model and its assumptions. The use of definition variables (i.e., observed variables used to fix model parameters to individual specific data values) is extended to the case of ML-SEMs for clustered data with random slopes. Empirical examples of multilevel CFA and ML-SEM with random slopes are provided along with scripts for fitting such models in SAS Proc Mixed, Mplus, and Mx. Methodological issues regarding estimation of complex ML-SEMs and the evaluation of model fit are discussed. Further potential applications of ML-SEMs are explored.",0
https://doi.org/10.1111/rssa.12016,Handling missing values in cost effectiveness analyses that use data from cluster randomized trials,"Public policy makers use cost effectiveness analyses (CEAs) to decide which health and social care interventions to provide. Missing data are common in CEAs, but most studies use complete-case analysis. Appropriate methods have not been developed for handling missing data in complex settings, exemplified by CEAs that use data from cluster randomized trials. We present a multilevel multiple-imputation approach that recognizes the hierarchical structure of the data and is compatible with the bivariate multilevel models that are used to report cost effectiveness. We contrast this approach with single-level multiple imputation and complete-case analysis, in a CEA alongside a cluster randomized trial. The paper highlights the importance of adopting a principled approach to handling missing values in settings with complex data structures.",0
https://doi.org/10.1111/j.2044-8317.1993.tb01016.x,Confirmatory factor analysis of ordered categorical variables with large models,"This simulation study examined the utility of a categorical variable methodology developed by Muthen (1984) for confirmatory factor analysis of ordinal variables. Multivariate normal data were generated according to four different factor models (4, 9, 15 and 22 parameters) for samples of 500 and 1000. Indicators were classified into Five categories so that manifest variables displayed negative, zero, positive or highly positive kurtosis. Each of the 32 design cells was replicated 100 times. Parameter estimates exhibited little or no bias under any condition. Standard errors were underestimated with respect to the standard deviation of the parameter estimates. This negative bias worsened as model size grew or as positive kurtosis increased; it was more severe for factor correlations than indicator loadings. Chi-square fit statistics rejected the true model more often than expected for nine-parameter and larger models. Although variables with high positive kurtosis led to the greatest misfit in large models, fit was poor even with variables of zero kurtosis. As expected, larger samples always yielded more accurate results.",0
https://doi.org/10.1080/10705519809540106,Analyzing measurement models of latent variables through multilevel confirmatory factor analysis and hierarchical linear modeling approaches,"Confirmatory factor analysis (CFA) is often used in the social sciences to estimate a measurement model in which multiple measurement items are hypothesized to assess a particular latent construct. This article presents the utility of multilevel CFA (MCFA; Muthen, 1991, 1994) and hierarchical linear modeling (HLM; Raudenbush, Rowan, & Kang, 1991) methods in testing measurement models in which the underlying attribute may vary as a function of various levels of observation. An illustrative example using a real dataset is provided in which an unconditional model specification and parameter estimates from the MCFA and HLM are shown. The article demonstrates the comparability of the two methods in estimating measurement parameters of interest (i.e., true variance at levels the measures are used and measurement errors).",0
https://doi.org/10.1037/a0038100,Psychometric analysis of the Ten-Item Perceived Stress Scale.,"Although the 10-item Perceived Stress Scale (PSS-10) is a popular measure, a review of the literature reveals 3 significant gaps: (a) There is some debate as to whether a 1- or a 2-factor model best describes the relationships among the PSS-10 items, (b) little information is available on the performance of the items on the scale, and (c) it is unclear whether PSS-10 scores are subject to gender bias. These gaps were addressed in this study using a sample of 1,236 adults from the National Survey of Midlife Development in the United States II. Based on self-identification, participants were 56.31% female, 77% White, 17.31% Black and/or African American, and the average age was 54.48 years (SD = 11.69). Findings from an ordinal confirmatory factor analysis suggested the relationships among the items are best described by an oblique 2-factor model. Item analysis using the graded response model provided no evidence of item misfit and indicated both subscales have a wide estimation range. Although t tests revealed a significant difference between the means of males and females on the Perceived Helplessness Subscale (t = 4.001, df = 1234, p < .001), measurement invariance tests suggest that PSS-10 scores may not be substantially affected by gender bias. Overall, the findings suggest that inferences made using PSS-10 scores are valid. However, this study calls into question inferences where the multidimensionality of the PSS-10 is ignored.",0
https://doi.org/10.1037/h0023577,Some implications of psychotherapy research for therapeutic practice.,"Implications for practice and research are drawn from a survey of psychotherapy research findings, condensed into 6 broad conclusions: (1) psychotherapy causes clients to become better or worse adjusted than controls; (2) control Ss improve with time as a result of informal therapeutic encounters; (3) therapeutic progress varies with therapist warmth, empathy, adjustment, and experience; (4) client-centered therapy is the only interview-oriented method that has been validated by research; (5) traditional therapies are seriously limited in effectiveness and are relevant for a small minority of disturbances; and (6) behavior therapies have considerable promise for enhancing therapeutic effectiveness and should be utilized or experimented with more widely. (2 p. ref.) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1966 American Psychological Association.",0
https://doi.org/10.1037/0003-066x.60.2.170,Inference by Eye: Confidence Intervals and How to Read Pictures of Data.,"Wider use in psychology of confidence intervals (CIs), especially as error bars in figures, is a desirable development. However, psychologists seldom use CIs and may not understand them well. The authors discuss the interpretation of figures with error bars and analyze the relationship between CIs and statistical significance testing. They propose 7 rules of eye to guide the inferential use of figures with error bars. These include general principles: Seek bars that relate directly to effects of interest, be sensitive to experimental design, and interpret the intervals. They also include guidelines for inferential interpretation of the overlap of CIs on independent group means. Wider use of interval estimation in psychology has the potential to improve research communication substantially.",0
https://doi.org/10.1214/ss/1177011926,That BLUP is a Good Thing: The Estimation of Random Effects,"In animal breeding, Best Linear Unbiased Prediction, or BLUP, is a technique for estimating genetic merits. In general, it is a method of estimating random effects. It can be used to derive the Kalman filter, the method of Kriging used for ore reserve estimation, credibility theory used to work out insurance premiums, and Hoadley's quality measurement plan used to estimate a quality index. It can be used for removing noise from images and for small-area estimation. This paper presents the theory of BLUP, some examples of its application and its relevance to the foundations of statistics. Understanding of procedures for estimating random effects should help people to understand some complicated and controversial issues about fixed and random effects models and also help to bridge the apparent gulf between the Bayesian and Classical schools of thought.",0
https://doi.org/10.1037/a0030676,Further insights on the French WISC–IV factor structure through Bayesian structural equation modeling.,"The interpretation of the Wechsler Intelligence Scale for Children--Fourth Edition (WISC-IV) is based on a 4-factor model, which is only partially compatible with the mainstream Cattell-Horn-Carroll (CHC) model of intelligence measurement. The structure of cognitive batteries is frequently analyzed via exploratory factor analysis and/or confirmatory factor analysis. With classical confirmatory factor analysis, almost all cross-loadings between latent variables and measures are fixed to zero in order to allow the model to be identified. However, inappropriate zero cross-loadings can contribute to poor model fit, distorted factors, and biased factor correlations; most important, they do not necessarily faithfully reflect theory. To deal with these methodological and theoretical limitations, we used a new statistical approach, Bayesian structural equation modeling (BSEM), among a sample of 249 French-speaking Swiss children (8-12 years). With BSEM, zero-fixed cross-loadings between latent variables and measures are replaced by approximate zeros, based on informative, small-variance priors. Results indicated that a direct hierarchical CHC-based model with 5 factors plus a general intelligence factor better represented the structure of the WISC-IV than did the 4-factor structure and the higher order models. Because a direct hierarchical CHC model was more adequate, it was concluded that the general factor should be considered as a breadth rather than a superordinate factor. Because it was possible for us to estimate the influence of each of the latent variables on the 15 subtest scores, BSEM allowed improvement of the understanding of the structure of intelligence tests and the clinical interpretation of the subtest scores.",0
https://doi.org/10.1037/0022-0663.96.3.518,"Why Multicollinearity Matters: A Reexamination of Relations Between Self-Efficacy, Self-Concept, and Achievement.","Multicollinearity is a well-known general problem, but it also seriously threatens valid interpretations in structural equation models. Illustrating this problem, J. Pietsch, R. Walker, and E. Chapman (2003) found paths leading to achievement were apparently much larger for self-efficacy (.55) than self-concept (-.05), suggesting - erroneously, as the authors' reanalysis shows - that self-efficacy was a better predictor of achievement. However, because standard errors for these two paths were so huge (.25) thanks to the extremely high correlation between self-concept and self-efficacy (r = .93), interpretations were problematic. In a model comparison approach to this multicollinearity problem, constraining these two paths to be equal provided a better, more parsimonious fit to the data and also substantially reduced the standard errors (from .25 to .03).",0
https://doi.org/10.1016/j.cose.2014.11.002,Comparing intention to avoid malware across contexts in a BYOD-enabled Australian university: A Protection Motivation Theory approach,"Malware have been regarded as a persistent threat to both individuals and organisations due to its wide spread via various means of infection. With the increasing use of personal mobile devices and the trending adoption of Bring Your Own Device (BYOD) practices, this threat has become even more versatile and dreadful as it could hide behind the users' typical and daily Internet activities. The importance of investigating whether the user's intention to perform malware avoidance behaviours would change across multiple contexts is emphasised. Consequently, this study determines the contributing factors and compares their impacts on such intention by extending Protection Motivation Theory in two different contexts. A total of 252 Australian higher education students were surveyed when using mobile devices such as smartphone, laptop and tablet at home and at a BYOD-enabled university. Paired t- test, Bayesian structural equation modelling, and revised z -test were employed for data analysis. The empirical findings reveal that intention to perform malware avoidance behaviours differed across the contexts. Furthermore, the researchers found perceptions of self-efficacy and vulnerability to have different impacts on such intention and other variables in the model. As a result, such findings suggested developing community of practice and repeated trainings to maintain the users' confidence in their own abilities to cope with malware threats. Message that focuses on the threats' consequences was suggested to improve home users' intention to avoid malware, along with a number of factors that could be critical to designing information security education programs. Moreover, these implications particularly address information security management at educational institutions that adopt BYOD policy. Finally, theoretical contributions include an extended model based on Protection Motivation Theory that reflects the users' intention to avoid malware threats in BYOD context, from which directions for future research were also provided.",0
https://doi.org/10.4172/2155-6180.1000189,General Multiple Mediation Analysis With an Application to Explore Racial Disparities in Breast Cancer Survival,"Mediation refers to the effect transmitted by mediators that intervenes in the relationship between an exposure and a response variable. Mediation analysis has been broadly studied in many fields. However, it remains a challenge for researchers to differentiate individual effect from multiple mediators. This paper proposes general definitions of mediation effects that are consistent for all different types (categorical or continuous) of response, exposure, or mediation variables. With these definitions, multiple mediators can be considered simultaneously, and the indirect effects carried by individual mediators can be separated from the total effect. Moreover, the derived mediation analysis can be performed with general predictive models. For linear predictive models with continuous mediators, we show that the proposed method is equivalent to the conventional coefficients product method. We also establish the relationship between the proposed definitions of direct or indirect effect and the natural direct or indirect effect for binary exposure variables. The proposed method is demonstrated by both simulations and a real example examining racial disparities in three-year survival rates for female breast cancer patients in Louisiana.",0
https://doi.org/10.1111/j.1541-0420.2005.00499.x,Bayesian Covariance Selection in Generalized Linear Mixed Models,"The generalized linear mixed model (GLMM), which extends the generalized linear model (GLM) to incorporate random effects characterizing heterogeneity among subjects, is widely used in analyzing correlated and longitudinal data. Although there is often interest in identifying the subset of predictors that have random effects, random effects selection can be challenging, particularly when outcome distributions are nonnormal. This article proposes a fully Bayesian approach to the problem of simultaneous selection of fixed and random effects in GLMMs. Integrating out the random effects induces a covariance structure on the multivariate outcome data, and an important problem that we also consider is that of covariance selection. Our approach relies on variable selection-type mixture priors for the components in a special Cholesky decomposition of the random effects covariance. A stochastic search MCMC algorithm is developed, which relies on Gibbs sampling, with Taylor series expansions used to approximate intractable integrals. Simulated data examples are presented for different exponential family distributions, and the approach is applied to discrete survival data from a time-to-pregnancy study.",0
https://doi.org/10.1016/j.prevetmed.2009.10.004,A simulation study to assess statistical methods for binary repeated measures data,"Binary repeated measures data are commonly encountered in both experimental and observational veterinary studies. Among the wide range of statistical methods and software applicable to such data one major distinction is between marginal and random effects procedures. The objective of the study was to review and assess the performance of marginal and random effects estimation procedures for the analysis of binary repeated measures data. Two simulation studies were carried out, using relatively small, balanced, two-level (time within subjects) datasets. The first study was based on data generated from a marginal model with first order autocorrelation, the second on a random effects model with autocorrelated random effects within subjects. Three versions of the models were considered in which a dichotomous treatment was modelled additively, either between or within subjects, or modelled by a time interaction. Among the studied statistical procedures were: generalized estimating equations (GEE), Marginal Quasi Likelihood, likelihood based on numerical integration, penalized quasi-likelihood, restricted pseudo likelihood and Bayesian Markov Chain Monte Carlo. Results for data generated by the marginal model showed autoregressive GEE to be highly efficient when treatment was within subjects, even with strongly correlated responses. For treatment between subjects, random effects procedures also performed well in some situations; however, a relatively small number of subjects with a short time series proved a challenge for both marginal and random effects procedures. Results for data generated by the random effects model showed bias in estimates from random effects procedures when autocorrelation was present in the data, while the marginal procedures generally gave estimates close to the marginal parameters.",0
https://doi.org/10.3102/1076998609337138,Modeling Heterogeneity in Relationships Between Initial Status and Rates of Change: Treating Latent Variable Regression Coefficients as Random Coefficients in a Three-Level Hierarchical Model,"In studies of change in education and numerous other fields, interest often centers on how differences in the status of individuals at the start of a period of substantive interest relate to differences in subsequent change. In this article, the authors present a fully Bayesian approach to estimating three-level Hierarchical Models in which latent variable regression (LVR) coefficients capturing the relationship between initial status and rates of change within each of J schools (Bw j , j = 1, …, J) are treated as varying across schools. Specifically, the authors treat within-group LVR coefficients as random coefficients in three-level models. Through analyses of data from the Longitudinal Study of American Youth, the authors show how modeling differences in Bw j as a function of school characteristics can broaden the kinds of questions they can address in school effects research. They also illustrate the possibility of conducting sensitivity analyses using t distributional assumptions at each level of such models (termed latent variable regression in a three-level hierarchical model [LVR-HM3s]), and present results from a small-scale simulation study that help provide some guidance concerning the specification of priors for variance components in LVR-HM3s. They outline extensions of LVR-HM3s to settings in which growth is nonlinear, and discuss the use of LVR-HM3s in other types of research including multisite evaluation studies in which time-series data are collected during a preintervention period, and cross-sectional studies in which within-cluster LVR slopes are treated as varying across clusters.",0
https://doi.org/10.1111/j.1467-842x.2011.00623.x,SMALL AREA ESTIMATION USING SURVEY WEIGHTS WITH FUNCTIONAL MEASUREMENT ERROR IN THE COVARIATE,"Summary Nested error linear regression models using survey weights have been studied in small area estimation to obtain efficient model-based and design-consistent estimators of small area means. The covariates in these nested error linear regression models are not subject to measurement errors. In practical applications, however, there are many situations in which the covariates are subject to measurement errors. In this paper, we develop a nested error linear regression model with an area-level covariate subject to functional measurement error. In particular, we propose a pseudo-empirical Bayes (PEB) predictor to estimate small area means. This predictor borrows strength across areas through the model and makes use of the survey weights to preserve the design consistency as the area sample size increases. We also employ a jackknife method to estimate the mean squared prediction error (MSPE) of the PEB predictor. Finally, we report the results of a simulation study on the performance of our PEB predictor and associated jackknife MSPE estimator.",0
https://doi.org/10.1080/00273171.2011.589261,Bayesian Inference for Growth Mixture Models with Latent Class Dependent Missing Data,"Growth mixture models (GMMs) with nonignorable missing data have drawn increasing attention in research communities but have not been fully studied. The goal of this article is to propose and to evaluate a Bayesian method to estimate the GMMs with latent class dependent missing data. An extended GMM is first presented in which class probabilities depend on some observed explanatory variables and data missingness depends on both the explanatory variables and a latent class variable. A full Bayesian method is then proposed to estimate the model. Through the data augmentation method, conditional posterior distributions for all model parameters and missing data are obtained. A Gibbs sampling procedure is then used to generate Markov chains of model parameters for statistical inference. The application of the model and the method is first demonstrated through the analysis of mathematical ability growth data from the National Longitudinal Survey of Youth 1997 (Bureau of Labor Statistics, U.S. Department of Labor, 1997). A simulation study considering 3 main factors (the sample size, the class probability, and the missing data mechanism) is then conducted and the results show that the proposed Bayesian estimation approach performs very well under the studied conditions. Finally, some implications of this study, including the misspecified missingness mechanism, the sample size, the sensitivity of the model, the number of latent classes, the model comparison, and the future directions of the approach, are discussed.",0
,On Bayesian Analysis of Mixtures with an Unknown Number of Components,"SUMMARY New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.",0
https://doi.org/10.1186/2046-4053-3-11,HHV-8 seroprevalence: a global view,"Human herpes virus 8 (HHV-8) is the underlying infectious cause of Kaposi sarcoma (KS) and other proliferative diseases; that is, primary effusion lymphoma and multicentric Castleman disease. In regions with high HHV-8 seroprevalence in the general population, KS accounts for a major burden of disease. Outside these endemic regions, HHV-8 prevalence is high in men who have sex with men (MSM) and in migrants from endemic regions. We aim to conduct a systematic literature review and meta-analysis in order 1) to define the global distribution of HHV-8 seroprevalence (primary objective) and 2) to identify risk factors for HHV-8 infection, with a focus on HIV status (secondary objective).We will include observational studies reporting data on seroprevalence of HHV-8 in children and/or adults from any region in the world. Case reports and case series as well as any studies with fewer than 50 participants will be excluded. We will search MEDLINE, EMBASE, and relevant conference proceedings without language restriction. Two reviewers will independently screen the identified studies and extract data on study characteristics and quality, study population, risk factors, and reported outcomes, using a standardized form. For the primary objective we will pool the data using a fully bayesian approach for meta-analysis, with random effects at the study level. For the secondary objective (association of HIV and HHV-8) we aim to pool odds ratios for the association of HIV and HHV-8 using a fully bayesian approach for meta-analysis, with random effects at the study level. Sub-group analyses and meta-regression analyses will be used to explore sources of heterogeneity, including factors such as geographical region, calendar years of recruitment, age, gender, ethnicity, socioeconomic status, different risk groups for sexually and parenterally transmitted infections (MSM, sex workers, hemophiliacs, intravenous drug users), comorbidities such as organ transplantation and malaria, test(s) used to measure HHV-8 infection, study design, and study quality.Using the proposed systematic review and meta-analysis, we aim to better define the global seroprevalence of HHV-8 and its associated risk factors. This will improve the current understanding of HHV-8 epidemiology, and could suggest measures to prevent HHV-8 infection and to reduce its associated cancer burden.",0
https://doi.org/10.1177/0146621614533987,Computerized Adaptive Testing for the Random Weights Linear Logistic Test Model,"This article discusses four-item selection rules to design efficient individualized tests for the random weights linear logistic test model (RWLLTM): minimum posterior-weighted [Formula: see text]-error [Formula: see text]minimum expected posterior-weighted [Formula: see text]-error [Formula: see text]maximum expected Kullback–Leibler divergence between subsequent posteriors ( KLP), and maximum mutual information ( MUI). The RWLLTM decomposes test items into a set of subtasks or cognitive features and assumes individual-specific effects of the features on the difficulty of the items. The model extends and improves the well-known linear logistic test model in which feature effects are only estimated at the aggregate level. Simulations show that the efficiencies of the designs obtained with the different criteria appear to be equivalent. However, KLP and MUI are given preference over [Formula: see text] and [Formula: see text] due to their lesser complexity, which significantly reduces the computational burden.",0
https://doi.org/10.1016/j.foodqual.2012.02.006,Determining odour detection thresholds: Incorporating a method-independent definition into the implementation of ASTM E679,"Abstract ASTM international standard practice E679 prescribes the use of a 3-alternative forced-choice (3AFC) method with an ascending concentration series for measuring odour detection thresholds. It recommends obtaining an individual’s threshold by geometrically averaging the concentrations at which the judge’s detection response reverses from incorrect to correct. The legitimate reversal point is defined by a stopping rule. Previous researchers have identified some methodological flaws with this calculation approach, relating to the use of stopping rules and the method’s unconventional definition of thresholds. The current study aims to empirically investigate these issues in more depth. Thresholds for three odorants were obtained for 100 judges. 1,8-Cineole, isobutyraldehyde and β-damascenone were tested in accordance with the data collection procedure of ASTM E679. Initially, thresholds were estimated by the ASTM-based method with four different stopping rules. These estimates were subsequently compared against thresholds estimated by an alternative method; that of fitting psychometric functions. The theoretical grounds on which the latter method is based, and the clear connection between performance and stimulus concentration that it demonstrates, provide a thorough comparison of both conventionally-defined thresholds and method-independent thresholds against the ASTM-based method. Comparisons between the thresholds resulting from the various estimation methods demonstrated (1) the choice of stopping rule had a significant effect on threshold estimates (P",0
https://doi.org/10.1093/ije/31.1.96,Being sceptical about meta-analyses: a Bayesian perspective on magnesium trials in myocardial infarction,"There has been extensive discussion of the apparent conflict between meta-analyses and a mega-trial investigating the benefits of intravenous magnesium following myocardial infarction, in which the early trial results have been said to be 'too good to be true'.We apply Bayesian methods of meta-analysis to the trials available before and after the publication of the ISIS-4 results. We show how scepticism can be formally incorporated into an analysis as a Bayesian prior distribution, and how Bayesian meta-analysis models allow appropriate exploration of hypotheses that the treatment effect depends on the size of the trial or the risk in the control group.Adoption of a sceptical prior would have led early enthusiasm for magnesium to be suitably tempered, but only if combined with a random effects meta-analysis, rather than the fixed effect analysis that was actually conducted.We argue that neither a fixed effect nor a random effects analysis is appropriate when the mega-trial is included. The Bayesian framework provides many possibilities for flexible exploration of clinical hypotheses, but there can be considerable sensitivity to apparently innocuous assumptions.",0
https://doi.org/10.1016/s0047-259x(02)00076-3,Empirical Bayesian estimation of normal variances and covariances,"This paper derives and evaluates an algorithm for estimating normal covariances. A particular concern is the performance of the estimator when the dimension of the space exceeds the number of observations. The algorithm is simple, tolerably well founded, and seems to be more accurate for its purpose than the alternatives. Other topics discussed are the joint estimation of variances in one and many dimensions; the loss function appropriate to a variance estimator; and its connection with a certain Bayesian prescription.",0
https://doi.org/10.1201/9780203909935.ch6,Computer-modeling and Graphical Strategies for Meta-analysis,,0
https://doi.org/10.1177/01466210122032046,Nonparametric Item Response Function Estimation for Assessing Parametric Model Fit,"Methods are developed that investigate the fit of parametric item response models by comparing them to models fitted under nonparametric assumptions. The approach is primarily graphical, but is made inferential through resampling from an estimated parametric model. The identifiability and estimation consistency of item response theory models are discussed and shown to be vital to the interpretation of differences between two fitted item response theory models. Simulation studies and real-data examples illustrate these techniques.",0
https://doi.org/10.1111/j.1745-3984.1991.tb00350.x,A Comparison of Two Procedures for Computing IRT Equating Coefficients,"In order to equate tests under Item Response Theory (IRT), one must obtain the slope and intercept coefficients of the appropriate linear transformation. This article compares two methods for computing such equating coefficients–Loyd and Hoover (1980) and Stocking and Lord (1983). The former is based upon summary statistics of the test calibrations; the latter is based upon matching test characteristic curves by minimizing a quadratic loss function. Three types of equating situations: horizontal, vertical, and that inherent in IRT parameter recovery studies–were investigated. The results showed that the two computing procedures generally yielded similar equating coefficients in all three situations. In addition, two sets of SAT data were equated via the two procedures, and little difference in the obtained results was observed. Overall, the results suggest that the Loyd and Hoover procedure usually yields acceptable equating coefficients. The Stocking and Lord procedure improves upon the Loyd and Hoover values and appears to be less sensitive to atypical test characteristics. When the user has reason to suspect that the test calibrations may be associated with data sets that are typically troublesome to calibrate, the Stocking and Lord procedure is to be preferred.",0
https://doi.org/10.1007/s00359-006-0189-3,Psychometric function for nectar volume perception of a flower-visiting bat,"For foraging pollinators one aspect of floral quality is the volume of nectar available. Thus, nectar-feeding animals should be able to estimate volumes of received nectar. In this study, we determined the psychometric function for nectar volume discrimination of a Neotropical flower-visiting bat Glossophaga soricina. For this, we examined the ability of bats to discriminate between two nectar volumes in a two-alternative forced-choice paradigm. We used a Bayesian inference approach to determine psychometric functions. From the derived psychometric function we assessed the discrimination threshold at a value of 0.69. G. soricina could clearly distinguish between two volumes, when the difference between the two nectar volumes divided by their average exceeded this value. This indicates that bats possess a sense for the perception and discrimination of volumes of nectar that is better developed than in honeybees. Â© 2006 Springer-Verlag.",0
https://doi.org/10.1016/j.jglr.2014.07.006,Comparing Bayesian and frequentist methods of fisheries models: Hierarchical catch curves,"Bayesian inference is an emerging statistical paradigm and is becoming an increasingly used alternative to frequentist inference. Unfortunately, little is known about the efficacy of Bayesian inference and how it relates to the historical methodology of evaluating fisheries related models. Mortality information is routinely used in fisheries management to describe fish population abundance over time and has been historically estimated using catch curves and frequentist inference (i.e., maximum likelihood estimation). The objective of this study was to compare frequentist and Bayesian inference approaches to estimate instantaneous mortality (Z) from a hierarchical catch curve model. The data used in the comparison were from a long term monitoring program of yellow perch Perca flavescens from southern Lake Michigan in addition to a simulated dataset where parameter estimates were compared to known values. Point estimates of Z were similar among both methods. Similarly, Bayesian inference 95% credible intervals were concordant with frequentist 95% confidence intervals. However, the root mean squared error of frequentist inference increased at a higher rate than Bayesian inference with increasing variability in the simulated dataset. Our study builds on the literature that seeks to compare results between these two paradigms to assist managers to make the best decision possible when deciding what statistical paradigm to employ.",0
https://doi.org/10.3758/bf03193596,Lure similarity affects visual episodic recognition: Detailed tests of a noisy exemplar model,"Summed-similarity models of visual episodic recognition memory successfully predict the variation in false alarm rates across different test items. With data averaged across subjects, Kahana and Sekuler demonstrated that subjects' performance appears to change along with the mean similarity among study items; with high interstimulus similarity, subjects were less likely to commit false alarms to similar lures. We examined this effect in detail by systematically varying the coordinates of study and test items along a critical stimulus dimension and measuring memory performance at each point. To reduce uncontrolled variance associated with individual differences in vision, the coordinates of study and test items were scaled according to each subject's discrimination threshold. Fitting each of four summed-similarity models to the individual subjects' data demonstrated a clear superiority for models that take account of interitem similarity on a trialwise basis.",0
https://doi.org/10.3758/app.71.6.1414,Model-free estimation of the psychometric function,"A subject's response to the strength of a stimulus is described by the psychometric function, from which summary measures, such as a threshold or a slope, may be derived. Traditionally, this function is estimated by fitting a parametric model to the experimental data, usually the proportion of successful trials at each stimulus level. Common models include the Gaussian and Weibull cumulative distribution functions. This approach works well if the model is correct, but it can mislead if not. In practice, the correct model is rarely known. Here, a nonparametric approach based on local linear fitting is advocated. No assumption is made about the true model underlying the data, except that the function is smooth. The critical role of the bandwidth is identified, and its optimum value is estimated by a cross-validation procedure. As a demonstration, seven vision and hearing data sets were fitted by the local linear method and by several parametric models. The local linear method frequently performed better and never worse than the parametric ones. Supplemental materials for this article can be downloaded from app.psychonomic-journals.org/content/supplemental.",0
https://doi.org/10.1177/0146621615605307,Three-Element Item Selection Procedures for Multiple Forms Assembly,"In educational measurement, building multiple alternate test forms is usually highly desirable in practice. Test assembly problems are classified as NP-hard (Non-deterministic Polynomial-time hard) problems, and their computation time increases exponentially as the problem size grows. The purpose of this study was to construct parallel forms item-by-item based on a seed test, using two proposed item selection heuristic methods (random item sequence assigned to maximum distance form with content balance [R-MD-B] and random item sequence assigned to random form with content balancing [R-R-B]) incorporating a flexible content balancing method. The results were compared with two modified versions of the Armstrong, Jones, and Wu heuristics (random item sequence assigned to maximum distance form with Armstrong’s replacement [R-MD-R] and random item sequence assigned to random form with Armstrong’s replacement [R-R-R]) and two models incorporating mixed integer programming (MIP) methods. The results showed that the R-MD-B and R-R-B methods yielded results comparable with the two MIP methods with less computation time in terms of the item information functions, test characteristic curves, and content coverage. The results also showed that R-MD-B and R-R-B can be extended to handle test assembly constraints in practical settings.",0
https://doi.org/10.1007/s11336-013-9396-3,The Lognormal Race: A Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties,"We present a cognitive process model of response choice and response time performance data that has excellent psychometric properties and may be used in a wide variety of contexts. In the model there is an accumulator associated with each response option. These accumulators have bounds, and the first accumulator to reach its bound determines the response time and response choice. The times at which accumulator reaches its bound is assumed to be lognormally distributed, hence the model is race or minima process among lognormal variables. AÂ key property of the model is that it is relatively straightforward to place a wide variety of models on the logarithm of these finishing times including linear models, structural equation models, autoregressive models, growth-curve models, etc. Consequently, the model has excellent statistical and psychometric properties and can be used in a wide range of contexts, from laboratory experiments to high-stakes testing, to assess performance. We provide a Bayesian hierarchical analysis of the model, and illustrate its flexibility with an application in testing and one in lexical decision making, aÂ reading skill. Â© 2014, The Psychometric Society.",0
https://doi.org/10.3389/fpsyg.2015.01599,Comparing interval estimates for small sample ordinal CFA models,"Robust maximum likelihood (RML) and asymptotically generalized least squares (AGLS) methods have been recommended for fitting ordinal structural equation models. Studies show that some of these methods underestimate standard errors. However, these studies have not investigated the coverage and bias of interval estimates. An estimate with a reasonable standard error could still be severely biased. This can only be known by systematically investigating the interval estimates. The present study compares Bayesian, RML, and AGLS interval estimates of factor correlations in ordinal confirmatory factor analysis models (CFA) for small sample data. Six sample sizes, 3 factor correlations, and 2 factor score distributions (multivariate normal and multivariate mildly skewed) were studied. Two Bayesian prior specifications, informative and relatively less informative were studied. Undercoverage of confidence intervals and underestimation of standard errors was common in non-Bayesian methods. Underestimated standard errors may lead to inflated Type-I error rates. Non-Bayesian intervals were more positive biased than negatively biased, that is, most intervals that did not contain the true value were greater than the true value. Some non-Bayesian methods had non-converging and inadmissible solutions for small samples and non-normal data. Bayesian empirical standard error estimates for informative and relatively less informative priors were closer to the average standard errors of the estimates. The coverage of Bayesian credibility intervals was closer to what was expected with overcoverage in a few cases. Although some Bayesian credibility intervals were wider, they reflected the nature of statistical uncertainty that comes with the data (e.g., small sample). Bayesian point estimates were also more accurate than non-Bayesian estimates. The results illustrate the importance of analyzing coverage and bias of interval estimates, and how ignoring interval estimates can be misleading. Therefore, editors and policymakers should continue to emphasize the inclusion of interval estimates in research.",1
https://doi.org/10.1002/sim.1189,Bayesian random effects meta-analysis of trials with binary outcomes: methods for the absolute risk difference and relative risk scales,"When conducting a meta-analysis of clinical trials with binary outcomes, a normal approximation for the summary treatment effect measure in each trial is inappropriate in the common situation where some of the trials in the meta-analysis are small, or the observed risks are close to 0 or 1. This problem can be avoided by making direct use of the binomial distribution within trials. A fully Bayesian method has already been developed for random effects meta-analysis on the log-odds scale using the BUGS implementation of Gibbs sampling. In this paper we demonstrate how this method can be extended to perform analyses on both the absolute and relative risk scales. Within each approach we exemplify how trial-level covariates, including underlying risk, can be considered. Data from 46 trials of the effect of single-dose ibuprofen on post-operative pain are analysed and the results contrasted with those derived from classical and Bayesian summary statistic methods. The clinical interpretation of the odds ratio scale is not straightforward. The advantages and flexibility of a fully Bayesian approach to meta-analysis of binary outcome data, considered on an absolute risk or relative risk scale, are now available.",0
https://doi.org/10.3389/fpsyg.2015.00036,"Math achievement is important, but task values are critical, too: examining the intellectual and motivational factors leading to gender disparities in STEM careers","Although young women now obtain higher course grades in math than boys and are just as likely to be enrolled in advanced math courses in high school, females continue to be underrepresented in some Science, Technology, Engineering, and Mathematics (STEM) occupations. This study drew on expectancy-value theory to assess (1) which intellectual and motivational factors in high school predict gender differences in career choices and (2) whether students' motivational beliefs mediated the pathway of gender on STEM career via math achievement by using a national longitudinal sample in the United States. We found that math achievement in 12th grade mediated the association between gender and attainment of a STEM career by the early to mid-thirties. However, math achievement was not the only factor distinguishing gender differences in STEM occupations. Even though math achievement explained career differences between men and women, math task value partially explained the gender differences in STEM career attainment that were attributed to math achievement. The identification of potential factors of women's underrepresentation in STEM will enhance our ability to design intervention programs that are optimally tailored to female needs to impact STEM achievement and occupational choices.",0
https://doi.org/10.1037/a0020858,Leader–member exchange and affective organizational commitment: The contribution of supervisor's organizational embodiment.,"In order to account for wide variation in the relationship between leader-member exchange and employees' affective organizational commitment, we propose a concept termed supervisor's organizational embodiment (SOE), which involves the extent to which employees identify their supervisor with the organization. With samples of 251 social service employees in the United States (Study 1) and 346 employees in multiple Portuguese organizations (Study 2), we found that as SOE increased, the association between leader-member exchange and affective organizational commitment became greater. This interaction carried through to in-role and extra-role performance. With regard to antecedents, we found in Study 1 that supervisor's self-reported identification with the organization increased supervisor's expression of positive statements about the organization, which in turn increased subordinates' SOE.",0
https://doi.org/10.1111/j.0006-341x.2000.00768.x,Nonconjugate Bayesian Analysis of Variance Component Models,"Summary. We consider the usual normal linear mixed model for variance components from a Bayesian viewpoint. With conjugate priors and balanced data, Gibbs sampling is easy to implement; however, simulating from full conditionals can become difficult for the analysis of unbalanced data with possibly nonconjugate priors, thus leading one to consider alternative Markov chain Monte Carlo schemes. We propose and investigate a method for posterior simulation based on an independence chain. The method is customized to exploit the structure of the variance component model, and it works with arbitrary prior distributions. As a default reference prior, we use a version of Jeffreys' prior based on the integrated (restricted) likelihood. We demonstrate the ease of application and flexibility of this approach in familiar settings involving both balanced and unbalanced data.",0
https://doi.org/10.1007/s13253-010-0043-5,Statistical Modelling of Neighbor Treatment Effects in Aquaculture Clinical Trials,"In the design of clinical trials involving fish observed over time in tanks, there may be advantages in housing several treatment groups within the same tank. In particular, such ""within-tank"" designs will be more efficient than designs with treatment groups in separate tanks when substantial between-tank variability is expected. One potential problem with within-tank designs is that it may not be possible to include all treatments in one tank; in statistical terms this means that the blocks (tanks) are incomplete. In incomplete block designs, there may be a concern that the treatments present in the same tank (denoted here as ""neighbors"") affect each other in their performance; thus the need for an assessment of neighbor effects. In this paper, we propose two statistical approaches to assess and account for neighbor effects. The first approach is based on a non-linear mixed model and the second involves cross-classified and multiple membership models. Both approaches are illustrated on simulated data as well as data from a clinical ISAV (Infectious Salmon Anaemia Virus) trial; corresponding computer code is available online. The simulation studies demonstrated that both models show promise in capturing neighbor treatment effects of the type assumed for the models, whenever such neighbor effects are of at least moderate magnitude. In the absence of or with low magnitudes of neighbor effects, the non-linear mixed model faced numerical challenges and produced noisy results. One version of the cross-classified and multiple membership model was shown to depend strongly on prior information about variance-covariance parameters for datasets similar to the ISAV data. Analyses of the ISAV trial data by both models did not provide any evidence of substantial neighbor effects. Ã‚Â© 2010 International Biometric Society.",0
https://doi.org/10.1111/j.0963-7214.2005.00354.x,Mood and Emotion in Major Depression,"Nothing is more familiar to people than their moods and emotions. Oddly, however, it is not clear how these two kinds of affective processes are related. Intuitively, it makes sense that emotional reactions are stronger when they are congruent with a preexisting mood, an idea reinforced by contemporary emotion theory. Yet empirically, it is uncertain whether moods actually facilitate emotional reactivity to mood-congruent stimuli. One approach to the question of how moods affect emotions is to study mood-disturbed individuals. This review describes recent experimental studies of emotional reactivity conducted with individuals suffering from major depression. Counter to intuitions, major depression is associated with reduced emotional reactivity to sad contexts. A novel account of emotions in depression is advanced to assimilate these findings. Implications for the study of depression and normal mood variation are considered.",0
https://doi.org/10.1177/000312240607100606,Scar Effects of Unemployment: An Assessment of Institutional Complementarities,"This article uses panel data from the Survey of Income and Program Participation (SIPP) and the European Community Household Panel (ECHP) for a comparative analysis of workers' post-unemployment earnings trajectories in the United States and 12 Western European countries. Across the study sample of industrialized countries, results of difference-in-difference propensity score matching show post-unemployment earnings losses to be largely permanent and particularly significant for high-wage and older workers as well as for women. The analyses also show that negative effects of unemployment on workers' subsequent earnings are mitigated through either generous unemployment benefit systems or strict labor market regulation. These effects stem partly from favorable behavioral responses that prevent downward occupational and industrial mobility and partly from changes in the overall structure of labor markets favoring the transferability of worker skills between jobs. These positive effects materialize despite the fact that labor market policies tend to successfully protect the core work force from experiencing a job loss in the first place.",0
,Prior distributions for variance parameters in hierarchical models,"Various prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation pa- rameters, and then consider and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of noninformative prior distributions. We suggest instead to use a uni- form prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.",0
https://doi.org/10.1007/s11336-008-9060-5,A Robust Bayesian Approach for Structural Equation Models with Missing Data,"In this paper, normal/independent distributions, including but not limited to the multivariate t distribution, the multivariate contaminated distribution, and the multivariate slash distribution, are used to develop a robust Bayesian approach for analyzing structural equation models with complete or missing data. In the context of a nonlinear structural equation model with fixed covariates, robust Bayesian methods are developed for estimation and model comparison. Results from simulation studies are reported to reveal the characteristics of estimation. The methods are illustrated by using a real data set obtained from diabetes patients. © 2008 The Psychometric Society.",0
https://doi.org/10.1177/1471082x0700700301,Using cross-classified multivariate mixed response models with application to life history traits in great tits (Parus major),"Longitudinal observations on known individuals are an important source of data with which to test evolutionary theory within natural populations, in particular, the evolution and maintenance of life-history traits. In this paper, we concentrate on the reproductive behaviour and survival of a small passerine bird, the great tit ( Parus major). The dataset we consider is taken from the long-term study of great tits in Wytham Woods in Oxfordshire. The models we consider are designed to relate variation in several phenotypic response variables that are linked to evolutionary fitness, alongside the correlations between them, to both general environmental and individual-specific factors. We fit multivariate cross-classified random effects models using a Markov chain Monte Carlo (MCMC) estimation algorithm described in the paper. Our results show for which traits variability is influenced by environmental factors and for which traits individual bird factors are more important. The partitioning of correlations is particularly illuminating, producing some pairs of ‘antagonistic’ correlations which are biologically meaningful.",0
https://doi.org/10.1111/j.1541-0420.2012.01807.x,Modeling Seroadaptation and Sexual Behavior Among HIV<sup>+</sup>Study Participants with a Simultaneously Multilevel and Multivariate Longitudinal Count Model,"Longitudinal behavioral intervention trials to reduce HIV transmission risk collect complex multilevel and multivariate data longitudinally for each subject with important correlation structures across time, level, and variables. Accurately assessing the effects of these trials are critical for determining which interventions are effective. Both numbers of partners and numbers of sex acts with each partner are reported at each time point. Sex acts with each partner are further differentiated into protected and unprotected acts with correspondingly differing risks of HIV/STD transmission. These trials generally also have eligibility criteria limiting enrollment to participants with some minimal level of risky sexual behavior tied directly to the outcome of interest. The combination of these factors makes it difficult to quantify sexual behaviors and the effects of intervention. We propose a multivariate multilevel count model that simultaneously models the number of partners, acts within partners, and accounts for recruitment eligibility. Our methods are useful in the evaluation of intervention trials and provide a more accurate and complete model for sexual behavior. We illustrate the contributions of our model by examining seroadaptive behavior defined as risk reducing behavior that depends on the serostatus of the partner. Several forms of seroadaptive risk reducing behavior are quantified and distinguished from nonseroadaptive risk reducing behavior.",0
https://doi.org/10.1348/000711004849204,Bayesian model comparison of nonlinear structural equation models with missing continuous and ordinal categorical data,"Missing data are very common in behavioural and psychological research. In this paper, we develop a Bayesian approach in the context of a general nonlinear structural equation model with missing continuous and ordinal categorical data. In the development, the missing data are treated as latent quantities, and provision for the incompleteness of the data is made by a hybrid algorithm that combines the Gibbs sampler and the Metropolis-Hastings algorithm. We show by means of a simulation study that the Bayesian estimates are accurate. A Bayesian model comparison procedure based on the Bayes factor and path sampling is proposed. The required observations from the posterior distribution for computing the Bayes factor are simulated by the hybrid algorithm in Bayesian estimation. Our simulation results indicate that the correct model is selected more frequently when the incomplete records are used in the analysis than when they are ignored. The methodology is further illustrated with a real data set from a study concerned with an AIDS preventative intervention for Filipina sex workers.",0
https://doi.org/10.1198/10618600152418584,The Art of Data Augmentation,"The term data augmentation refers to methods for constructing iterative optimization or sampling algorithms via the introduction of unobserved data or latent variables. For deterministic algorithms, the method was popularized in the general statistical community by the seminal article by Dempster, Laird, and Rubin on the EM algorithm for maximizing a likelihood function or, more generally, a posterior density. For stochastic algorithms, the method was popularized in the statistical literature by Tanner and Wong's Data Augmentation algorithm for posterior sampling and in the physics literature by Swendsen and Wang's algorithm for sampling from the Ising and Potts models and their generalizations; in the physics literature, the method of data augmentation is referred to as the method of auxiliary variables. Data augmentation schemes were used by Tanner and Wong to make simulation feasible and simple, while auxiliary variables were adopted by Swendsen and Wang to improve the speed of iterative simulation. In...",0
https://doi.org/10.1007/978-3-642-19656-0_34,A Psychological Model for Aggregating Judgments of Magnitude,"In this paper, we develop and illustrate a psychologically-motivated model for aggregating judgments of magnitude across experts. The model assumes that experts’ judgments are perturbed from the truth by both systematic biases and random error, and it provides aggregated estimates that are implicitly based on the application of nonlinear weights to individual judgments. The model is also easily extended to situations where experts report multiple quantile judgments. We apply the model to expert judgments concerning flange leaks in a chemical plant, illustrating its use and comparing it to baseline measures.",0
https://doi.org/10.1026/0049-8637/a000051,Multivariate Veränderungsmodelle für Schulnoten und Schülerleistungen in Deutsch und Mathematik,"Zusammenfassung. Schülerleistungen in standardisierten Leistungstests weisen einen mittleren Zusammenhang zu Schulnoten im gleichen Fach auf. Über die Fächergrenzen hinweg sind die Zusammenhänge geringer aber positiv. Über längsschnittliche Zusammenhänge zwischen Schulnoten und Schülerleistungen ist wenig bekannt. In dieser Studie untersuchen wir längsschnittliche Zusammenhänge zwischen Schülerleistungen und Schulnoten in Deutsch und Mathematik. Die vorliegenden Analysen wurden an Prä- und Postmessungen einer Gymnasialstichprobe (N=168) vorgenommen, die zu Studien- und Kontrollgruppen einer intensiv-längsschnittlichen Studie gehörte. Multigruppenanalysen weisen auf vollständige Messinvarianz zwischen beiden Gruppen hin, Testungseffekte zeigen sich nicht. Die längsschnittlichen Zusammenhänge wurden mit dem Change-Score-Modell analysiert. Die Veränderungen in Schülerleistungen in beiden Schulfächern korrelieren nicht signifikant, die Veränderungen in Schulnoten beider Schulfächer korrelieren positiv. In beiden Schulfächern zeigen Veränderungen der Schülerleistungen und der Schulnoten einen positiven Zusammenhang. Die Korrelation ist nur moderat, da Schulnoten aus Gründen wie Referenzrahmeneffekte auch querschnittlich nur moderat mit Schülerleistungen korrelieren. Der Leistungszuwachs lässt sich am effektivsten mit normreferenzierten Skalen quantifizieren.",0
https://doi.org/10.1002/sim.7374,A Dirichlet process mixture model for clustering longitudinal gene expression data,"Subgroup identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to define subgroups. Longitudinal gene expression profiles might provide additional information on disease progression than what is captured by baseline profiles alone. Therefore, subgroup identification could be more accurate and effective with the aid of longitudinal gene expression data. However, existing statistical methods are unable to fully utilize these data for patient clustering. In this article, we introduce a novel clustering method in the Bayesian setting based on longitudinal gene expression profiles. This method, called BClustLonG, adopts a linear mixed-effects framework to model the trajectory of genes over time, while clustering is jointly conducted based on the regression coefficients obtained from all genes. In order to account for the correlations among genes and alleviate the high dimensionality challenges, we adopt a factor analysis model for the regression coefficients. The Dirichlet process prior distribution is utilized for the means of the regression coefficients to induce clustering. Through extensive simulation studies, we show that BClustLonG has improved performance over other clustering methods. When applied to a dataset of severely injured (burn or trauma) patients, our model is able to identify interesting subgroups. Copyright © 2017 John Wiley & Sons, Ltd.",0
https://doi.org/10.1002/sim.3915,A Bayesian analysis of mixture structural equation models with non-ignorable missing responses and covariates,"In behavioral, biomedical, and social-psychological sciences, it is common to encounter latent variables and heterogeneous data. Mixture structural equation models (SEMs) are very useful methods to analyze these kinds of data. Moreover, the presence of missing data, including both missing responses and missing covariates, is an important issue in practical research. However, limited work has been done on the analysis of mixture SEMs with non-ignorable missing responses and covariates. The main objective of this paper is to develop a Bayesian approach for analyzing mixture SEMs with an unknown number of components, in which a multinomial logit model is introduced to assess the influence of some covariates on the component probability. Results of our simulation study show that the Bayesian estimates obtained by the proposed method are accurate, and the model selection procedure via a modified DIC is useful in identifying the correct number of components and in selecting an appropriate missing mechanism in the proposed mixture SEMs. A real data set related to a longitudinal study of polydrug use is employed to illustrate the methodology.",0
https://doi.org/10.2307/2331838,Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population,,0
https://doi.org/10.5705/ss.2009.180,Center-adjusted inference for a nonparametric Bayesian random effect distribution,"Dirichlet process (DP) priors are a popular choice for semiparametric Bayesian random effect models. The fact that the DP prior implies a non-zero mean for the random effect distribution creates an identifiability problem that complicates the interpretation of, and inference for, the fixed effects that are paired with the random effects. Similarly, the interpretation of, and inference for, the variance components of the random effects also becomes a challenge. We propose an adjustment of conventional inference using a post-processing technique based on an analytic evaluation of the moments of the random moments of the DP. The adjustment for the moments of the DP can be conveniently incorporated into Markov chain Monte Carlo simulations at essentially no additional computational cost. We conduct simulation studies to evaluate the performance of the proposed inference procedure in both a linear mixed model and a logistic linear mixed effect model. We illustrate the method by applying it to a prostate specific antigen dataset. We provide an R function that allows one to implement the proposed adjustment in a post-processing step of posterior simulation output, without any change to the posterior simulation itself.",0
,Making BUGS open,,0
https://doi.org/10.1007/s11336-012-9301-5,Methods for Mediation Analysis with Missing Data,"Despite wide applications of both mediation models and missing data techniques, formal discussion of mediation analysis with missing data is still rare. We introduce and compare four approaches to dealing with missing data in mediation analysis including listwise deletion, pairwise deletion, multiple imputation (MI), and a two-stage maximum likelihood (TS-ML) method. An R package bmem is developed to implement the four methods for mediation analysis with missing data in the structural equation modeling framework, and two real examples are used to illustrate the application of the four methods. The four methods are evaluated and compared under MCAR, MAR, and MNAR missing data mechanisms through simulation studies. Both MI and TS-ML perform well for MCAR and MAR data regardless of the inclusion of auxiliary variables and for AV-MNAR data with auxiliary variables. Although listwise deletion and pairwise deletion have low power and large parameter estimation bias in many studied conditions, they may provide useful information for exploring missing mechanisms. Â© 2012 The Psychometric Society.",0
https://doi.org/10.1007/s11336-004-1215-4,"The applicability of deadline models: Comment on Glickman, Gray, and Morales (2005)","Glickman, Gray, and Morales (this issue) propose a statistical model for measuring the unobserved latency of stimulus-controlled processes. The model accounts for both speed and accuracy and does so by assuming that participants set an internal deadline. If a stimulus-controlled response is not produced by the deadline, the participant then guesses. The applicability of the model is discussed in this Comment. The deadline model yields specific predictions for the case in which stimulus difficulty is manipulated in a within-block manner. In this case, it is reasonable to assume that stimulus difficulty does not affect the deadline. It is shown that in common perceptual and cognitive domains, extant data do not fully meet these predictions. Hence, practitioners need be aware of the possibility and consequences of model misspecification. Â© 2005 The Psychometric Society.",0
https://doi.org/10.1016/0022-2496(80)90001-2,Decomposing the reaction time distribution: Pure insertion and selective influence revisited,"Abstract This paper investigates the consequences of extending the assumptions of pure insertion and selective influence (popular in RT theorizing) to the level of the distribution. In the case of pure insertion and under the additional assumption that the additive random variable is exponentially distributed, a solution is obtained which not only allows estimation of the exponential-rate parameter but also provides a test of the assumptions. The result is shown to be applicable not only when processing is serial but also for certain parallel models. In addition, discrimination between self-terminating and exhaustive search strategies is provided, and in the case of either, both parameter estimation and tests of the model are possible. Extensions to nonexponential models are investigated and a general method of moments solution is outlined. In the case of selective influence a general nonparametric alternative to Sternberg's additive factor method is developed. The problem of empirical estimation and application is then considered. Simulations which place bounds on the type I and II error are reported. Finally the first theorem is provided an illustrative application with data from a memory scanning experiment. The results provide some support for the double assumption of pure insertion and that the additive random variable is distributed exponentially.",0
https://doi.org/10.1177/0049124199027003004,Evaluating and Using Statistical Methods in the Social Sciences,,0
,AN ANALYSIS OF SURVEY DATA ON SMOKING USING PROPENSITY SCORES,"SUMMARY. Responses to questions on perceived health risks from smoking, amount of exercise, overall health, and desire to quit smoking vary across smoking groups. The smoking groups also have very different distributions of covariates. Categorizing respondents based on estimated propensities to smoke, as in blocking, creates groups with less measurable imbalance in distributions of covariates. Adjustment of results using propensity score classes lessens some effect estimates while increasing others and changes results of some significance tests. However, even after adjusting for covariates through the use of propensity scores, smokers report a higher risk of disease than do nonsmokers.",0
https://doi.org/10.1080/00031305.1995.10476177,Understanding the Metropolis-Hastings Algorithm,"Abstract We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.",0
https://doi.org/10.1037/a0027558,"Changing the pond, not the fish: Following high-ability students across different educational environments.","Big-fish-little-pond effect (BFLPE) research (e.g., Marsh & Parker, 1984) has found that perceptions of academic ability are generally positively related to individual ability and negatively related to classroom and school average ability. However, BFLPE research typically relies on environmental differences as a between-subjects factor. Unlike most previous BFLPE research, the current study used group average ability as a within-subject variable by measuring student self-concept before and after high-ability students left their regular school environment to participate in a supplemental academic summer program. Results revealed that academic self-concept (ASC) and educational aspirations did not undergo significant declines when students were in the relatively higher ability environment. Even with ceiling effects limiting potential increases in ASC, participants were more than 2 times as likely to increase or maintain their ASC as they were to report declines in ASC. Further, several boosts were found in nonacademic self-concepts. Such findings indicate that BFLPEs are not necessarily associated with supplemental educational environments.",0
https://doi.org/10.1007/bf02295939,Generalized multilevel structural equation modeling,"A unifying framework for generalized multilevel structural equation modeling is introduced. The models in the framework, called generalized linear latent and mixed models (GLLAMM), combine features of generalized linear mixed models (GLMM) and structural equation models (SEM) and consist of a response model and a structural model for the latent variables. The response model generalizes GLMMs to incorporate factor structures in addition to random intercepts and coefficients. As in GLMMs, the data can have an arbitrary number of levels and can be highly unbalanced with different numbers of lower-level units in the higher-level units and missing data. A wide range of response processes can be modeled including ordered and unordered categorical responses, counts, and responses of mixed types. The structural model is similar to the structural part of a SEM except that it may include latent and observed variables varying at different levels. For example, unit-level latent variables (factors or random coefficients) can be regressed on cluster-level latent variables. Special cases of this framework are explored and data from the British Social Attitudes Survey are used for illustration. Maximum likelihood estimation and empirical Bayes latent score prediction within the GLLAMM framework can be performed using adaptive quadrature in gllamm, a freely available program running in Stata.",0
https://doi.org/10.1007/978-0-387-73186-5_2,Bayesian Multilevel Analysis and MCMC,"Multilevel models have gained wide acceptance over the past 20 years in many fields, including education and medicine [e.g., 26, 43, 45], as an important methodology for dealing appropriately with nested or clustered data. The idea of conducting an experiment in such a way that the levels of one factor are nested inside those of another goes back all the way to the initial development, in the 1920s, of the analysis of variance (ANOVA; [34]), so there?s nothing new in working with nested data; the novelty in recent decades is in the methods for fitting multilevel models, the ability to work with data possessing many levels of nesting and multiple predictor variables at any or all levels, and an increased flexibility in distributional assumptions. The earliest designs featured one-way ANOVA models such as1 yij = Î¼ + Î±Tj + aSij , j = 1, . . . , J, i = 1, . . . , nj , âˆ‘j=1J nj = N, âˆ‘Jj=1 Î±Tj = 0, aSijâˆ¼iid N(0, Ïƒ2S), in which the subject factor S (indexed by i), treated as random, is nested within the treatment factor T (indexed by j), treated as fixed. Under the normality assumption in (2.1) such models required little for the (frequentist) estimation of the parameters Î¼, Ïƒ2S , and the Î±Tj beyond minor extensions of the least squares methods known since the time of Legendre [51] and Gauss [36]. Regarding the treatment factor as random, however, by changing the Î±Tj to aTj âˆ¼iid N(0, Î±2T ) (with the a Tj and aS ij mutually independent), created substantial new difficulties in model fitting-indeed, as late as the 1950s, one of the leading estimation methods [e.g., 65] was based on unbiased estimates of the variance components Î±2T and Î±2 s , the former of which can easily, and embarrassingly, go negative when Î±2T is small. Fisher [33] had much earlier pioneered the use of maximum likelihood estimation, but before the widespread use of fast computers this approach was impractical in random-effects and mixed models such as yij = Î²0 + Î²1(xij ? xÌ„) + aTj + aSij , j = 1, . . . , J, i = 1, . . . , nj , XJj=1 nj = N, aTjâˆ¼iid N(0, Ïƒ;a2 T), aSijâˆ¼iid N(0, Ïƒ2S) (where the xij are fixed known values of a predictor variable and ?x is the sample mean of this variable), because the likelihood equations in such models can only be solved iteratively. Multilevel modeling entered a new phase in the 1980s, with the development of computer programs such as ML3, VARCL, and HLM using likelihood-based estimation approaches based on iterative generalized least squares [42], Fisher scoring [52], and the EM algorithm [e.g., 15], respectively. In particular, the latest versions of MLwiN (the successor to ML3; [60]) and HLM [66] have worldwide user bases in the social and biomedical sciences numbering in the thousands, and likelihood-based fitting of at least some multilevel models is also now obtainable in more general-purpose statistical packages such as SAS [64] and Stata [71]. However, the use of the likelihood function alone in multilevel modeling can lead to the following technical problems: Maximum-likelihood estimates (MLEs) and their (estimated asymptotic) standard errors (SEs) can readily be found by iterative means for the parameters in Gaussian multilevel models such as (2.2), but interval stimates of those parameters can be problematic when J, the number of level-2 units, is small. For example, simple ?95%? intervals of the form Î±Ì‚2T Â± 1.96 b se(Î±Ì‚2T ) (based on the large-sample Gaussian repeated-sampling distribution of Î±Ì‚2T ) can go negative and can have actual coverage levels substantially below 95%, and other methods based only on Î±Ì‚2T and b se(Î±Ì‚2T ) (which are the default outputs of packages such as MLwiN and HLM) are not guaranteed to do much better, in part because (with small sample sizes) the MLE of Î±2T can be 0 even when the true value of Î±2T is well away from 0 [e.g., 12]. The situation becomes even more difficult when the outcome variable y in the multilevel model is dichotomous rather than Gaussian, as in randomeffects logistic regression (RELR) models such as (yij | p ij) âˆ¼indep Bernoulli(pij), where logit(pij) = Î²0 + Î²1(xij ? xÌ„) + uj , uj âˆ¼iid N(0, Î±2u). Here the likelihood methods that work with Gaussian outcomes fail; the likelihood function itself cannot even be evaluated without integrating out the random effects uj from (2.3). Available software such as MLwiN fits RELR models via quasi-likelihood methods [7]; this approach to fitting nonlinear models such as (2.3) proceeds by linearizing the second line of the model via Taylor series expansion, yielding marginal and penalized quasi-likelihood (MQL and PQL) estimates according to the form of the expansion used. These are not full likelihood methods and would be better termed likelihood-based techniques. Browne and Draper [12] have shown that the actual coverage of nominal 95% interval estimates with this approach in RELR models can be far less than 95% when the intervals are based only on MQL and PQL point estimates and their (estimated asymptotic) SEs; see Section 2.3.3 below. Calibration results of this kind for other methods which attempt to more accurately approximate the actual likelihood function [e.g., 1, 50, 53, 57, 61] are sparse and do not yet fully cover the spectrum of models in routine use, and user-friendly software for many of these methods is still hard to come by. This chapter concerns the Bayesian approach to fitting multilevel models, which (a) attempts to remedy the above problems (though not without introducing some new challenges of its own) and (b) additionally provides a mechanism for the formal incorporation of any prior information which may be available about the parameters of the multilevel model of interest external to the current data set. A computing revolution based on Markov chain Monte Carlo (MCMC) methods, and the availability of much faster (personal) computers, have together made the Bayesian fitting of multilevel models increasingly easier since the early 1990s. In this chapter I (1) describe the basic outline of a Bayesian analysis (multilevel or not), in the context of a case study, (2) motivate the need for simulation-based computing methods, (3) describe MCMC methods in general and their particular application to multilevel modeling, (4) discuss MCMC diagnostic methods (to ensure accuracy of the computations), and (5) present an MCMC solution to the multilevel modeling case study. Â© 2008 Springer Science+Business Media, LLC.",0
https://doi.org/10.1027/1614-2241.2.3.124,Does Money Matter? A Theory-Driven Growth Mixture Model to Explain Travel-Mode Choice with Experimental Data,"In the present article we apply a growth mixture model using Mplus via STREAMS to delineate the mechanism underlying travel-mode choice. Three waves of an experimental field study conducted in Frankfurt Main, Germany, are applied for the statistical analysis. Five major questions are addressed: (1) whether the choice of public transport rather than the car changes over time; (2) whether a soft policy intervention to change travel mode choice has any effect on the travel-mode chosen; (3) whether one can identify different groups of people regarding the importance allocated to monetary and time considerations for the decision of which travel mode to use; (4) whether the different subgroups of people have different initial states and rates of change in their travel-model choices; (5) whether sociodemographic variables have an additional effect on the latent class variables and on the changes in travel-mode choice over time. We also found that choice of public transportation in our study is stable over time. Moreover, the intervention has an effect only on one of the classes. We identify four classes of individuals. One class allocates a low importance to both monetary and time considerations, the second allocates high importance to money and low importance to time, the third allocates high importance to both, and the fourth allocates a low importance to money and a high importance to time. We found no difference in the patterns of travel-mode changes over time in the four classes. We also found some additional effects of sociodemographic characteristics on the latent class variables and on behavior in the different classes. The model specification and the empirical findings are discussed in light of the theory of the allocation of time of Gary Becker.",0
https://doi.org/10.1080/00031305.1989.10475663,Errors-in-Variables Regression Using Stein Estimates,"Abstract A method is proposed for estimating regression parameters from data containing covariate measurement errors by using Stein estimates of the unobserved true covariates. The method produces consistent estimates for the slope parameter in the classical linear errors-in-variables model and applies to a broad range of nonlinear regression problems, provided the measurement error is Gaussian with known variance. Simulations are used to examine the performance of the estimates in a nonlinear regression problem and to compare them with the usual naive ones obtained by ignoring error and with other estimates proposed recently in the literature.",0
https://doi.org/10.1214/aoms/1177704568,Classification into two Multivariate Normal Distributions with Different Covariance Matrices,"Linear procedures for classifying an observation as coming from one of two multivariate normal distributions are studied in the case that the two distributions differ both in mean vectors and covariance matrices. We find the class of admissible linear procedures, which is the minimal complete class of linear procedures. It is shown how to construct the linear procedure which minimizes one probability of misclassification given the other and how to obtain the minimax linear procedure; Bayes linear procedures are also discussed.",0
https://doi.org/10.1007/bf02294709,Heterogeneous factor analysis models: A bayesian approach,"Multilevel factor analysis models are widely used in the social sciences to account for heterogeneity in mean structures. In this paper we extend previous work on multilevel models to account for general forms of heterogeneity in confirmatory factor analysis models. We specify various models of mean and covariance heterogeneity in confirmatory factor analysis and develop Markov Chain Monte Carlo (MCMC) procedures to perform Bayesian inference, model checking, and model comparison. We test our methodology using synthetic data and data from a consumption emotion study. The results from synthetic data show that our Bayesian model perform well in recovering the true parameters and selecting the appropriate model. More importantly, the results clearly illustrate the consequences of ignoring heterogeneity. Specifically, we find that ignoring heterogeneity can lead to sign reversals of the factor covariances, inflation of factor variances and underappreciation of uncertainty in parameter estimates, The results from the emotion study show that subjects vary both in means and covariances, Thus traditional psychometric methods cannot fully capture the heterogeneity in our data.",0
https://doi.org/10.1016/j.pain.2005.02.009,Minocycline attenuates mechanical allodynia and proinflammatory cytokine expression in rat models of pain facilitation,"Activated glial cells (microglia and astroglia) in the spinal cord play a major role in mediating enhanced pain states by releasing proinflammatory cytokines and other substances thought to facilitate pain transmission. In the present study, we report that intrathecal administration of minocycline, a selective inhibitor of microglial cell activation, inhibits low threshold mechanical allodynia, as measured by the von Frey test, in two models of pain facilitation. In a rat model of neuropathic pain induced by sciatic nerve inflammation (sciatic inflammatory neuropathy, SIN), minocycline delayed the induction of allodynia in both acute and persistent paradigms. Moreover, minocycline was able to attenuate established SIN-induced allodynia 1 day, but not 1 week later, suggesting a limited role of microglial activation in more perseverative pain states. Our data are consistent with a crucial role for microglial cells in initiating, rather than maintaining, enhanced pain responses. In a model of spinal immune activation by intrathecal HIV-1 gp120, we show that the anti-allodynic effects of minocycline are associated with decreased microglial activation, attenuated mRNA expression of interleukin-1beta (IL-1beta), tumor necrosis factor-alpha (TNF-alpha), IL-1beta-converting enzyme, TNF-alpha-converting enzyme, IL-1 receptor antagonist and IL-10 in lumbar dorsal spinal cord, and reduced IL-1beta and TNF-alpha levels in the CSF. In contrast, no significant effects of minocycline were observed on gp120-induced IL-6 and cyclooxygenase-2 expression in spinal cord or CSF IL-6 levels. Taken together these data highlight the importance of microglial activation in the development of exaggerated pain states.",0
https://doi.org/10.1007/s10869-014-9351-z,"RWA Web: A Free, Comprehensive, Web-Based, and User-Friendly Tool for Relative Weight Analyses","Over the last 15 years, a number of methodological developments have enabled researchers to draw more accurate inferences concerning the relative contribution (i.e., relative importance) among multiple (often correlated) predictor variables in a regression analysis. One such development has been relative weight analysis (RWA). Researchers can use a RWA to decompose the total variance predicted in a regression model (R2) into weights that accurately reflect the proportional contribution of the various predictor variables. Prior to RWA, researchers were forced to rely on traditional statistics (e.g., correlations; standardized regression weights), which are known to yield faulty or misleading information concerning variable importance (especially when predictor variables are correlated with one another, which is often the case in organizational research). Although there has been a surge of interest in RWA over the last 10 years, integration of this statistical tool into organizational research has been hampered by the lack of a user-friendly statistical package for implementing RWA. Indeed, most popular statistical packages (e.g., SPSS, SAS) have yet to include RWA protocols into their regression modules. The purpose of this paper is to present a new, free, comprehensive, web-based, user-friendly resource, RWA-Web, which may be used by anyone having simple access to the internet. Our paper is structured as a tutorial on using RWA-Web to examine relative importance in the classic multiple regression model, the multivariate multiple regression model, and the logistic regression model. We also illustrate how RWA-Web may be used to conduct null hypothesis significance tests using advanced bootstrapping procedures.",0
https://doi.org/10.2522/ptj.20130302,"Association Between Physical Activity and Sleep in Adults With Chronic Pain: A Momentary, Within-Person Perspective","Background Individuals with chronic pain consider improved sleep to be one of the most important outcomes of treatment. Physical activity has been shown to have beneficial effects on sleep in the general population. Despite these findings, the physical activity–sleep relationship has not been directly examined in a sample of people with chronic pain. Objective This study aimed to examine the association between objective daytime physical activity and subsequent objective sleep for individuals with chronic pain while controlling for pain and psychosocial variables. Design An observational, prospective, within-person study design was used. Methods A clinical sample of 50 adults with chronic pain was recruited. Participation involved completing a demographic questionnaire followed by 5 days of data collection. Over this period, participants wore a triaxial accelerometer to monitor their daytime activity and sleep. Participants also carried a handheld computer that administered a questionnaire measuring pain, mood, catastrophizing, and stress 6 times throughout the day. Results The results demonstrated that higher fluctuations in daytime activity significantly predicted shorter sleep duration. Furthermore, higher mean daytime activity levels and a greater number of pain sites contributed significantly to the prediction of longer periods of wakefulness at night. Limitations The small sample size used in this study limits the generalizability of the findings. Missing data may have led to overestimations or underestimations of effect sizes, and additional factors that may be associated with sleep (eg, medication usage, environmental factors) were not measured. Conclusions The results of this study suggest that engagement in high-intensity activity and high fluctuations in activity are associated with poorer sleep at night; hence, activity modulation may be a key treatment strategy to address sleep complaints in individuals with chronic pain.",0
https://doi.org/10.1027/1614-2241.4.2.51,Multicollinearity and Missing Constraints,"Multicollinearity complicates the simultaneous estimation of interaction and quadratic effects in structural equation modeling (SEM). So far, approaches developed within the Kenny-Judd (1984 ) tradition have failed to specify additional and necessary constraints on the measurement error covariances of the nonlinear indicators. Given that the constraints comprise, in part, latent linear predictor correlations, multicollinearity poses a problem for such approaches. Klein and Moosbrugger’s (2000 ) latent moderated structural equations approach (LMS) approach does not utilize nonlinear indicators and should therefore not be affected by this problem. In the context of a simulation study, we varied predictor correlation and the number of nonlinear effects in order to compare the performance of three approaches developed for the estimation of simultaneous nonlinear effects: Ping’s (1996 ) two-step approach, a correctly extended Jöreskog-Yang (1996 ) approach, and LMS. Results show that in contrast to the Jöreskog-Yang approach and LMS, the two-step approach produces biased parameter estimates and false inferences under heightened multicollinearity. Ping’s approach resulted in overestimated interaction effects and underestimated quadratic effects.",0
https://doi.org/10.1080/00273171003680336,The Multigroup Multilevel Categorical Latent Growth Curve Models,"Longitudinal data describe developmental patterns and enable predictions of individual changes beyond sampled time points. Major methodological issues in longitudinal data include modeling random effects, subject effects, growth curve parameters, and autoregressive residuals. This study embedded the longitudinal model within a multigroup multilevel framework and allowed for autoregressive residuals. The parameter in the new model can be estimated using the computer program WinBUGS, which adopts Markov Chain Monte Carlo algorithms. Two simulation studies were conducted. An empirical example was raised and established based on models generated by the results of empirical data, which have been fitted and compared.",0
https://doi.org/10.1186/1471-2288-9-10,Overstating the evidence – double counting in meta-analysis and related problems,"The problem of missing studies in meta-analysis has received much attention. Less attention has been paid to the more serious problem of double counting of evidence.Various problems in overstating the precision of results from meta-analyses are described and illustrated with examples, including papers from leading medical journals. These problems include, but are not limited to, simple double counting of the same studies, double counting of some aspects of the studies, inappropriate imputation of results, and assigning spurious precision to individual studies.Some suggestions are made as to how the quality and reliability of meta-analysis can be improved. It is proposed that the key to quality in meta-analysis lies in the results being transparent and checkable.Existing quality check lists for meta-analysis do little to encourage an appropriate attitude to combining evidence and to statistical analysis. Journals and other relevant organisations should encourage authors to make data available and make methods explicit. They should also act promptly to withdraw meta-analyses when mistakes are found.",0
https://doi.org/10.3102/0034654317727727,Challenging Conventional Wisdom for Multivariate Statistical Models With Small Samples,"In education research, small samples are common because of financial limitations, logistical challenges, or exploratory studies. With small samples, statistical principles on which researchers rely...",0
https://doi.org/10.1177/0149206314549252,Rendezvous Overdue,"Bayesian estimation and inference have been core features of scientific knowledge generation since the work of Sir Thomas Bayes was built upon by Pierre-Simone Laplace from the late 1700s through the early 1800s. Although present-day statistical analysis in organizational research is “frequentist” in nature (due to the influence of scholars such as Sir Ronald Fisher and Jerzey Neyman), the past 20 years has seen a veritable explosion of Bayesian applications across the social and physical sciences. This special issue highlights these applications and the many opportunities they carry, including precise and flexible methods for testing hypotheses and very intuitive ways of describing results. For this special issue, three editorial commentaries were solicited from world-renowned experts in statistics, probability, and their historical and current applications. These papers offer a view from outside management, giving fresh insight into topics that are rarely covered in management research, including critical perspectives on existing paradigms in our field and recommendations for improvements in statistical methods and research design. The topics covered relate to (a) the apparent desire for universal or default methods of inquiry and inference—whether Bayesian or frequentist—which narrows researchers’ focus and reduces their ability to develop and deploy a larger “toolbox” of methods and approaches; (b) the many limitations of existing frequentist tools, which tend to be underestimated or ignored because of their institutionalized and habitual nature; and (c) the existence and importance of",0
https://doi.org/10.1111/j.1467-8624.2007.01091.x,"Living Arrangements and Children’s Development in Low-Income White, Black, and Latino Families","This article uses longitudinal data from approximately 2,000 low-income families participating in the national evaluation of the Comprehensive Child Development Program to examine the associations between preschool children's living arrangements and their cognitive achievement and emotional adjustment. The analysis distinguishes families in which children live only with their mothers from children who live in biological father, blended, and multigenerational households. Linkages are examined separately for White, Black, and Latino children. Fixed effects regression techniques reveal few significant associations between living arrangements and child development. These findings suggest that substantial diversity exists in the developmental contexts among children living in the same family structure. Policies seeking to change the living arrangements of low-income children may do little to improve child well-being.",0
https://doi.org/10.1002/sim.5866,An assessment of estimation methods for generalized linear mixed models with binary outcomes,"Two main classes of methodology have been developed for addressing the analytical intractability of generalized linear mixed models: likelihood-based methods and Bayesian methods. Likelihood-based methods such as the penalized quasi-likelihood approach have been shown to produce biased estimates especially for binary clustered data with small clusters sizes. More recent methods using adaptive Gaussian quadrature perform well but can be overwhelmed by problems with large numbers of random effects, and efficient algorithms to better handle these situations have not yet been integrated in standard statistical packages. Bayesian methods, although they have good frequentist properties when the model is correct, are known to be computationally intensive and also require specialized code, limiting their use in practice. In this article, we introduce a modification of the hybrid approach of Capanu and Begg, 2011, Biometrics 67, 371-380, as a bridge between the likelihood-based and Bayesian approaches by employing Bayesian estimation for the variance components followed by Laplacian estimation for the regression coefficients. We investigate its performance as well as that of several likelihood-based methods in the setting of generalized linear mixed models with binary outcomes. We apply the methods to three datasets and conduct simulations to illustrate their properties. Simulation results indicate that for moderate to large numbers of observations per random effect, adaptive Gaussian quadrature and the Laplacian approximation are very accurate, with adaptive Gaussian quadrature preferable as the number of observations per random effect increases. The hybrid approach is overall similar to the Laplace method, and it can be superior for data with very sparse random effects.",0
https://doi.org/10.1023/b:ejep.0000036572.00663.f2,Longitudinal Data Analysis. A Comparison Between Generalized Estimating Equations and Random Coefficient Analysis,"The analysis of data from longitudinal studies requires special techniques, which take into account the fact that the repeated measurements within one individual are correlated. In this paper, the two most commonly used techniques to analyze longitudinal data are compared: generalized estimating equations (GEE) and random coefficient analysis. Both techniques were used to analyze a longitudinal dataset with six measurements on 147 subjects. The purpose of the example was to analyze the relationship between serum cholesterol and four predictor variables, i.e., physical fitness at baseline, body fatness (measured by sum of the thickness of four skinfolds), smoking and gender. The results showed that for a continuous outcome variable, GEE and random coefficient analysis gave comparable results, i.e., GEE-analysis with an exchangeable correlation structure and random coefficient analysis with only a random intercept were the same. There was also no difference between both techniques in the analysis of a dataset with missing data, even when the missing data was highly selective on earlier observed data. For a dichotomous outcome variable, the magnitude of the regression coefficients and standard errors was higher when calculated with random coefficient analysis then when calculated with GEE-analysis. Analysis of a dataset with missing data with a dichotomous outcome variable showed unpredictable results for both GEE and random coefficient analysis. It can be concluded that for a continuous outcome variable, GEE and random coefficient analysis are comparable. Longitudinal data-analysis with dichotomous outcome variables should, however, be interpreted with caution, especially when there are missing data. Â© 2004 Kluwer Academic Publishers.",0
https://doi.org/10.1037/h0031322,Belief in the law of small numbers.,"Reports that people have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, I.e., similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of 84 professional psychologists to a questionnaire concerning research decisions. (PsycINFO Database Record (c) 2012 APA, all rights reserved) Language: en",0
https://doi.org/10.1037/0033-295x.111.1.94,Reconceptualizing Individual Differences in Self-Enhancement Bias: An Interpersonal Approach.,"Self-enhancement bias has been studied from 2 perspectives: L. Festinger's (1954) social comparison theory (self-enhancers perceive themselves more positively than they perceive others) and G. W. Allport's (1937) self-insight theory (self-enhancers perceive themselves more positively than they are perceived by others). These 2 perspectives are theoretically and empirically distinct, and the failure to recognize their differences has led to a protracted debate. A new interpersonal approach to self-enhancement decomposes self-perception into 3 components: perceiver effect, target effect, and unique self-perception. Both theoretical derivations and an illustrative study suggest that this resulting measure of self-enhancement is less confounded by unwanted components of interpersonal perception than previous social comparison and self-insight measures. Findings help reconcile conflicting views about whether self-enhancement is adaptive or maladaptive.",0
https://doi.org/10.1097/01.ede.0000256320.30737.c0,Bayesian Methods for Highly Correlated Exposure Data,"Studies that include individuals with multiple highly correlated exposures are common in epidemiology. Because standard maximum likelihood techniques often fail to converge in such instances, hierarchical regression methods have seen increasing use. Bayesian hierarchical regression places prior distributions on exposure-specific regression coefficients to stabilize estimation and incorporate prior knowledge, if available. A common parametric approach in epidemiology is to treat the prior mean and variance as fixed constants. An alternative parametric approach is to place distributions on the prior mean and variance to allow the data to help inform their values. As a more flexible semiparametric option, one can place an unknown distribution on the coefficients that simultaneously clusters exposures into groups using a Dirichlet process prior. We also present a semiparametric model with a variable-selection prior to allow clustering of coefficients at 0. We compare these 4 hierarchical regression methods and demonstrate their application in an example estimating the association of herbicides with retinal degeneration among wives of pesticide applicators.",0
https://doi.org/10.1214/ss/1030037958,Publication bias in meta-analysis: a Bayesian data-augmentation approach to account for issues exemplified in the passive smoking debate,"Publication bias is a relatively new statistical phenomenon that only arises when one attempts through a meta-analysis to review all studies, significant or insignificant, in order to provide a total perspective on a particular issue. This has recently received some notoriety as an issue in the evaluation of the relative risk of lung cancer associated with passive smoking, following legal challenges to a 1992 Environmental Protection Agency analysis which concluded that such exposure is associated with significant excess risk of lung cancer. We introduce a Bayesian approach which estimates and adjusts for publication bias. Estimation is based on a data-augmentation principle within a hierarchical model, and the number and outcomes of unobserved studies are simulated using Gibbs sampling methods. This technique yields a quantitative adjustment for the passive smoking meta-analysis. We estimate that there may be both negative and positive but insignificant studies omitted, and that failing to allow for these would mean that the estimated excess risk may be overstated by around 30%, both in U.S. studies and in the global collection of studies.",0
https://doi.org/10.7551/mitpress/7503.003.0106,A Bayesian Approach to Diffusion Models of Decision-Making and Response Time,"We present a computational Bayesian approach for Wiener diffusion models, which are prominent accounts of response time distributions in decision-making. We first develop a general closed-form analytic approximation to the response time distributions for one-dimensional diffusion processes, and derive the required Wiener diffusion as a special case. We use this result to undertake Bayesian modeling of benchmark data, using posterior sampling to draw inferences about the interesting psychological parameters. With the aid of the benchmark data, we show the Bayesian account has several advantages, including dealing naturally with the parameter variation needed to account for some key features of the data, and providing quantitative measures to guide decisions about model construction.",0
https://doi.org/10.1080/00273170701341316,"Addressing Moderated Mediation Hypotheses: Theory, Methods, and Prescriptions","This article provides researchers with a guide to properly construe and conduct analyses of conditional indirect effects, commonly known as moderated mediation effects. We disentangle conflicting definitions of moderated mediation and describe approaches for estimating and testing a variety of hypotheses involving conditional indirect effects. We introduce standard errors for hypothesis testing and construction of confidence intervals in large samples but advocate that researchers use bootstrapping whenever possible. We also describe methods for probing significant conditional indirect effects by employing direct extensions of the simple slopes method and Johnson-Neyman technique for probing significant interactions. Finally, we provide an SPSS macro to facilitate the implementation of the recommended asymptotic and bootstrapping methods. We illustrate the application of these methods with an example drawn from the Michigan Study of Adolescent Life Transitions, showing that the indirect effect of intrinsic student interest on mathematics performance through teacher perceptions of talent is moderated by student math self-concept.",0
https://doi.org/10.1016/j.electstud.2015.03.002,Reassessing the trade-off hypothesis: How misery drives the corruption effect on presidential approval,"Do economic conditions drive voters to punish politicians that tolerate corruption? Previous scholarly work contends that citizens in young democracies support corrupt governments that are capable of promoting good economic outcomes, the so-called trade-off hypothesis. We test this hypothesis based on mass surveys in eighteen Latin American countries throughout 2004–2012. We find that citizens that report bribe attempts from bureaucrats are always more likely to report presidential disapproval than citizens that report no such attempts, that is, Latin American victims of corruption are not duped by good economic performance. However, we find some evidence for a weaker form of the trade-off hypothesis: presidential disapproval among corruption victims might be more pronounced in contexts of high inflation and high unemployment.",0
https://doi.org/10.1136/bmj.h5392,Treatment strategies for coronary in-stent restenosis: systematic review and hierarchical Bayesian network meta-analysis of 24 randomised trials and 4880 patients,"What is the most safe and effective interventional treatment for coronary in-stent restenosis?In a hierarchical Bayesian network meta-analysis, PubMed, Embase, Scopus, Cochrane Library, Web of Science, ScienceDirect, and major scientific websites were screened up to 10 August 2015. Randomised controlled trials of patients with any type of coronary in-stent restenosis (either of bare metal stents or drug eluting stents; and either first or recurrent instances) were included. Trials including multiple treatments at the same time in the same group or comparing variants of the same intervention were excluded. Primary endpoints were target lesion revascularisation and late lumen loss, both at six to 12 months. The main analysis was complemented by network subanalyses, standard pairwise comparisons, and subgroup and sensitivity analyses.Twenty four trials (4880 patients), including seven interventional treatments, were identified. Compared with plain balloons, bare metal stents, brachytherapy, rotational atherectomy, and cutting balloons, drug coated balloons and drug eluting stents were associated with a reduced risk of target lesion revascularisation and major adverse cardiac events, and with reduced late lumen loss. Treatment ranking indicated that drug eluting stents had the highest probability (61.4%) of being the most effective for target lesion vascularisation; drug coated balloons were similarly indicated as the most effective treatment for late lumen loss (probability 70.3%). The comparative efficacy of drug coated balloons and drug eluting stents was similar for target lesion revascularisation (summary odds ratio 1.10, 95% credible interval 0.59 to 2.01) and late lumen loss reduction (mean difference in minimum lumen diameter 0.04 mm, 95% credible interval -0.20 to 0.10). Risks of death, myocardial infarction, and stent thrombosis were comparable across all treatments, but these analyses were limited by a low number of events. Trials had heterogeneity regarding investigation periods, baseline characteristics, and endpoint reporting, with a lack of information at long term follow-up. Direct and indirect evidence was also inconsistent for the comparison between drug eluting stents and drug coated balloons.Compared with other currently available interventional treatments for coronary in-stent restenosis, drug coated balloons and drug eluting stents are associated with superior clinical and angiographic outcomes, with a similar comparative efficacy.This study received no external funding. The authors declare no competing interests. No additional data available.",0
https://doi.org/10.1002/ir.155,Weighting and adjusting for design effects in secondary data analyses,Institutional researchers frequently use national datasets such as those provided by the National Center for Education Statistics (NCES). The authors of this chapter explore the adjustments required when analyzing NCES data collected using complex sample designs.,0
https://doi.org/10.3758/s13428-011-0150-4,TripleR: An R package for social relations analyses based on round-robin designs,"In this article, we present TripleR, an R package for the calculation of social relations analyses (Kenny, 1994) based on round-robin designs. The scope of existing software solutions is ported to R and enhanced with previously unimplemented methods of significance testing in single groups (Lashley & Bond, 1997) and handling of missing values. The package requires only minimal knowledge of R, and results can be exported for subsequent analyses to other software packages. We demonstrate the use of TripleR with several didactic examples.",0
https://doi.org/10.1177/1094428105285144,A Tale of Two Methods,"The structural equation modeling approach to testing for mediation is compared to the Baron and Kenny approach. The approaches are essentially the same when the hypothesis being tested predicts partial mediation. The approaches differ, however, in how each tests for complete mediation. Disparities in both theory and statistical estimators are identified and discussed. A strategy for future tests of mediation is recommended.",0
https://doi.org/10.1080/00031305.1994.10476079,The Shape of Correlation Matrices,"Abstract A correlation matrix between three variables has to satisfy certain conditions. Such a matrix essentially contains three numbers and thus can be represented by a point in three dimensions. The set of all possible correlation matrices yields a convex solid body with an uncommon shape. All its cross sections perpendicular to the axes are ellipses. At the same time, its surface contains the vertices and edges of a regular tetrahedron. Another unusual shape is obtained for banded correlation matrices between four variables.",0
https://doi.org/10.1007/bf01639850,Ueber die Maassbestimmungen des Ortssinnes der Haut mittels der Methode der richtigen und falschen Fälle,,0
https://doi.org/10.1007/978-3-658-07327-5,Trust and Rationality,"Combining economic, social-psychological and sociological approaches to trust, this book provides a general theoretical framework to causally explain conditional and unconditional trust; it also presents an experimental test of the corresponding integrative model and its predictions. Broadly, it aims at advancing a cognitive turn in trust research by highlighting the importance of (1) an actorâ€™s context-dependent definition of the situation and (2) the flexible and dynamic degree of rationality involved. In essence, trust is as ""multi-faceted"" as there are cognitive routes that take us to the choice of a trusting act. Therefore, variable rationality has to be incorporated as an orthogonal dimension to the typological space of trust. The theory presents an analytically tractable model; the empirical test combines trust games, high- and low-incentive conditions, framing manipulations, and psychometric measurements, and is complemented by decision-time analyses. Â© Springer Fachmedien Wiesbaden 2015.",0
https://doi.org/10.1177/1536867x0400400307,Implementing Matching Estimators for Average Treatment Effects in Stata,This paper presents an implementation of matching estimators for average treatment effects in Stata. The nnmatch command allows you to estimate the average effect for all units or only for the treated or control units; to choose the number of matches; to specify the distance metric; to select a bias adjustment; and to use heteroskedastic-robust variance estimators.,0
https://doi.org/10.1007/bf02301417,Isotonic ordinal probabilistic models (ISOP),"The concept of an ordinal instrumental probabilistic comparison is introduced. It relies on an ordinal scale given a priori and on the concept of stochastic dominance. It is used to define a weakly independently ordered system, or isotonic ordinal probabilistic (ISOP) model, which allows the construction of separate ""sample-free"" ordinal scales on a set of ""subjects"" and a set of ""items"". The ISOP-model is a common nonparametric theoretical structure for unidimensional models for quantitative, ordinal and dichotomous variables. Fundamental theorems on dichotomous and polytomous weakly independently ordered systems are derived. It is shown that the raw score system has the same formal properties as the latent system, and therefore the latter can be tested at the observed empirical level. Â© 1995 The Psychometric Society.",0
https://doi.org/10.1080/01621459.1973.10481353,Fitting Segmented Polynomial Regression Models Whose Join Points Have to Be Estimated,Abstract The study considers the problem of finding the least squares estimates for the unknown parameters of a regression model which consists of grafted polynomial submodels. The abscissae of the join points are a subset of the unknown parameters. Examples are given to illustrate how continuity and differentiability conditions on the model can be used to reparameterize the model so as to allow Modified Gauss-Newton fitting. A slightly generalized version of Hartley's theorem is stated to extend the Modified Gauss-Newton method to this problem.,0
https://doi.org/10.1016/j.neuroscience.2010.06.014,Spinal upregulation of glutamate transporter GLT-1 by ceftriaxone: therapeutic efficacy in a range of experimental nervous system disorders,"Glutamate neurotransmission is highly regulated, largely by glutamate transporters. In the spinal cord, the glutamate transporter GLT-1 is primarily responsible for glutamate clearance. Downregulation of GLT-1 can occur in activated astrocytes, and is associated with increased extracellular glutamate and neuroexcitation. Among other conditions, astrocyte activation occurs following repeated opioids and in models of chronic pain. If GLT-1 downregulation occurs in these states, GLT-1 could be a pharmacological target for improving opioid efficacy and controlling chronic pain. The present studies explored whether daily intrathecal treatment of rats with ceftriaxone, a beta-lactam antibiotic that upregulates GLT-1 expression, could prevent development of hyperalgesia and allodynia following repeated morphine, reverse pain arising from central or peripheral neuropathy, and reduce glial activation in these models. Ceftriaxone pre-treatment attenuated the development of hyperalgesia and allodynia in response to repeated morphine, and prevented associated astrocyte activation. In a model of multiple sclerosis (experimental autoimmune encephalomyelitis; EAE), ceftriaxone reversed tactile allodynia and halted the progression of motor weakness and paralysis. Similarly, ceftriaxone reversed tactile allodynia induced by chronic constriction nerve injury (CCI). EAE and CCI each significantly reduced the expression of membrane-bound, dimerized GLT-1 protein in lumbar spinal cord, an effect normalized by ceftriaxone. Lastly, ceftriaxone normalized CCI- and EAE-induced astrocyte activation in lumbar spinal cord. Together, these data indicate that increasing spinal GLT-1 expression attenuates opioid-induced paradoxical pain, alleviates neuropathic pain, and suppresses associated glial activation. GLT-1 therefore may be a therapeutic target that could improve available treatment options for patients with chronic pain.",0
https://doi.org/10.1590/s0104-40362013005000001,Bayesian computerized adaptive testing,"Computerized adaptive testing (CAT) comes with many advantages. Unfortunately, it still is quite expensive to develop and maintain an operational CAT. In this paper, various steps involved in developing an operational CAT are described and literature on these topics is reviewed. Bayesian CAT is introduced as an alternative, and the use of empirical priors is proposed for estimating item and person parameters to reduce the costs of CAT. Methods to elicit empirical priors are presented and a two small examples are presented that illustrate the advantages of Bayesian CAT. Implications of the use of empirical priors are discussed, limitations are mentioned and some suggestions for further research are formulated.",0
https://doi.org/10.1002/1097-0258(20010215)20:3<453::aid-sim803>3.0.co;2-l,Bayesian methods of analysis for cluster randomized trials with binary outcome data,"We explore the potential of Bayesian hierarchical modelling for the analysis of cluster randomized trials with binary outcome data, and apply the methods to a trial randomized by general practice. An approximate relationship is derived between the intracluster correlation coefficient (ICC) and the between-cluster variance used in a hierarchical logistic regression model. By constructing an informative prior for the ICC on the basis of available information, we are thus able implicitly to specify an informative prior for the between-cluster variance. The approach also provides us with a credible interval for the ICC for binary outcome data. Several approaches to constructing informative priors from empirical ICC values are described. We investigate the sensitivity of results to the prior specified and find that the estimate of intervention effect changes very little in this data set, while its interval estimate is more sensitive. The Bayesian approach allows us to assume distributions other than normality for the random effects used to model the clustering. This enables us to gain insight into the robustness of our parameter estimates to the classical normality assumption. In a model with a more complex variance structure, Bayesian methods can provide credible intervals for a difference between two variance components, in order for example to investigate whether the effect of intervention varies across clusters. We compare our results with those obtained from classical estimation, discuss the relative merits of the Bayesian framework, and conclude that the flexibility of the Bayesian approach offers some substantial advantages, although selection of prior distributions is not straightforward.",0
https://doi.org/10.1198/016214507000000428,Application of Multidimensional Selective Item Response Regression Model for Studying Multiple Gene Methylation in SV40 Oncogenic Pathways,"Alteration of gene methylation patterns has been reported to be involved in the early onsets of many human malignancies. Many exogenous risk factors, such as cigarette smoke, dietary additives, chemical exposures, radiation, and biologic agents including viral infection, are involved in the methylation pathways of cancers. We propose a multidimensional selective item response regression model to describe and test how a risk factor may alter molecular pathways involving aberrant methylation of multiple genes in oncogenesis. Our modeling framework is built on an item response model for multivariate dichotomous responses of high dimension, such as aberrant methylation of multiple tumor-suppressor genes, but we allow risk factors such as SV40 viral infection to alter the distribution of the latent factors that subsequently affect the outcome of cancer. We postulate empirical identification conditions under our model formulation. Moreover, we do not prespecify the links between the multiple dichotomous methylation responses and the latent factors, but rather conduct specification searches with a genetic algorithm to discover the links. Parameter estimation through maximum likelihood and specification searches in models with multidimensional latent factors for multivariate binary responses have become practical only recently, due to modern statistical computing development. We illustrate our proposal with the biological finding that simultaneous methylation of multiple tumor-suppressor genes is associated with the presence of SV40 viral sequences and with the cancer status of lymphoma/leukemia.We are able to test whether the data are consistent with the causal hypothesis that SV40 induces aberrant methylation of multiple genes in its oncogenic pathways. At the same time, we are able to evaluate the role of SV40 in the methylation pathway and to determine whether the methylation pathway is responsible for the development of leukemia/lymphoma.",0
https://doi.org/10.1348/096317905x52869,The team-level model of climate for innovation: A two-level confirmatory factor analysis,"The level structure of West's (1990) four-factor model of team climate for innovation was assessed by means of multi-level confirmatory factor analysis (MCFA). The sample consisted of 1,487 individuals (195 teams) from a wide range of professions. Results showed that a considerable portion of the variance in the data was explained on the team level with intra-class correlations ranging from .30 to .39. Furthermore, the results demonstrated that the overall measurement model fitted the data well at both the team and individual levels, while the factor loadings were slightly different across the levels with item loadings showing partial invariance. Results from confirmatory factor analyses conducted on separate levels, however, showed that the four-factor model displayed the best fit to the data for both individual and team levels. A second-order one-factor model also fitted the data well on both levels. The results indicate that the team climate for innovation model can be used as a team-level consensus model of team climate for innovation.",0
https://doi.org/10.1155/2013/493019,"New aQTL SNPs for the CYP2D6 Identified by a Novel Mediation Analysis of Genome-Wide SNP Arrays, Gene Expression Arrays, and CYP2D6 Activity","Background . The genome-wide association studies (GWAS) have been successful during the last few years. A key challenge is that the interpretation of the results is not straightforward, especially for transacting SNPs. Integration of transcriptome data into GWAS may provide clues elucidating the mechanisms by which a genetic variant leads to a disease. Methods . Here, we developed a novel mediation analysis approach to identify new expression quantitative trait loci (eQTL) driving CYP2D6 activity by combining genotype, gene expression, and enzyme activity data. Results . 389,573 and 1,214,416 SNP-transcript-CYP2D6 activity trios are found strongly associated (<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" id=""M1""><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>,<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" id=""M2""><mml:mtext>FDR</mml:mtext><mml:mo>=</mml:mo><mml:mn>16.6</mml:mn></mml:math>% and 11.7%) for two different genotype platforms, namely, Affymetrix and Illumina, respectively. The majority of eQTLs are trans-SNPs. A single polymorphism leads to widespread downstream changes in the expression of distant genes by affecting major regulators or transcription factors (TFs), which would be visible as an eQTL hotspot and can lead to large and consistent biological effects. Overlapped eQTL hotspots with the mediators lead to the discovery of 64 TFs. Conclusions . Our mediation analysis is a powerful approach in identifying the trans-QTL-phenotype associations. It improves our understanding of the functional genetic variations for the liver metabolism mechanisms.",0
https://doi.org/10.1080/10705511.2014.859510,Relative Performance of Categorical Diagonally Weighted Least Squares and Robust Maximum Likelihood Estimation,"Robust maximum likelihood (ML) and categorical diagonally weighted least squares (cat-DWLS) estimation have both been proposed for use with categorized and nonnormally distributed data. This study compares results from the 2 methods in terms of parameter estimate and standard error bias, power, and Type I error control, with unadjusted ML and WLS estimation methods included for purposes of comparison. Conditions manipulated include model misspecification, level of asymmetry, level and categorization, sample size, and type and size of the model. Results indicate that cat-DWLS estimation method results in the least parameter estimate and standard error bias under the majority of conditions studied. Cat-DWLS parameter estimates and standard errors were generally the least affected by model misspecification of the estimation methods studied. Robust ML also performed well, yielding relatively unbiased parameter estimates and standard errors. However, both cat-DWLS and robust ML resulted in low power under cond...",0
https://doi.org/10.1198/016214501753382309,A Note on the Efficiency of Sandwich Covariance Matrix Estimation,"The sandwich estimator, also known as robust covariance matrix estimator, heteroscedasticity-consistent covariance matrix estimate, or empirical covariance matrix estimator, has achieved increasing use in the econometric literature as well as with the growing popularity of generalized estimating equations. Its virtue is that it provides consistent estimates of the covariance matrix for parameter estimates even when the fitted parametric model fails to hold or is not even specified. Surprisingly though, there has been little discussion of properties of the sandwich method other than consistency. We investigate the sandwich estimator in quasi-likelihood models asymptotically, and in the linear case analytically. We show that under certain circumstances when the quasi-likelihood model is correct, the sandwich estimate is often far more variable than the usual parametric variance estimate. The increased variance is a fixed feature of the method and the price that one pays to obtain consistency even when the p...",0
https://doi.org/10.1177/0958928714542732,The capacity of social policies to combat poverty among new social risk groups,"This article considers groups who are most likely to be vulnerable to new social risks and tests the effects of social policies on their poverty levels. Specifically, the article conducts multi-level regression analyses across 18 OECD countries around the year 2004, analysing the effects of social policies on the likelihood of being poor for low-skilled young women and men, and for those at risk of possessing obsolete skills, namely low-educated men aged 55–64 years. The central question asks which policies – active labour market policies (ALMP), passive labour market policies (PLMP), employment protection legislation (EPL), family policies, and government daycare spending – are effective at combating new social risks. In addition to analysing social policies, the article also considers union density and representation of women in national parliaments as two measures that depict agents who are most intent on combating old and new social risks, respectively. The findings show that ALMP are the most important predictor of a decrease in poverty levels among the low skilled. The negative effect of PLMP on poverty is only significant for the older male group. Family policies are related to a reduction in poverty for both low-skilled young women and men. Gross public social spending as a measure of overall welfare generosity is found to be associated with a reduction in poverty only of the older male group, but not that of the younger groups. The article’s analyses suggest that some social policies remain geared towards older segments of society, leaving the younger population at greater financial and therefore social risk.",0
https://doi.org/10.1016/j.brat.2012.08.003,Investigating the effect of intolerance of uncertainty on catastrophic worrying and mood,"Intolerance of uncertainty (IU) is a construct known to influence catastrophic worry and is often observed in Generalized Anxiety Disorder (GAD). Research into the psychological manifestations of GAD suggests IU is associated with worry, but has not confirmed a causal link. The current study investigated the relationship between catastrophic worry and IU in a non-clinical undergraduate and postgraduate population (n = 46), with a mean age of 26.8 (SD = 5.52 years), where 71.74% were women. Participants received either a high or low IU manipulation, mood was measured throughout the study on 100 point visual analogue scales (VAS), and worry was measured using the catastrophising interview (CI). The high IU group generated significantly more catastrophising steps than the low IU group. Increased levels of sadness and anxiety were observed in the high as compared to the low IU group post IU manipulation, and this difference was maintained throughout the CI interview. A mediation analysis revealed that sadness and anxiety did not significantly mediate the relationship between IU and number of CI steps. These findings have implications for GAD treatment, as they suggest that manipulating IU affects measures of worry and its associated emotional and behavioural symptoms.",0
https://doi.org/10.1103/physrevlett.115.211103,"Search for GeV<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" display=""inline""><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:math>-Ray Pair Halos Around Low Redshift Blazars","We report on the results of a search for $\gamma$-ray pair halos with a stacking analysis of low-redshift blazars using data from the Fermi Large Area Telescope. For this analysis we used a number of a-priori selection criteria, including the spatial and spectral properties of the Fermi sources. The angular distribution of $\sim$ 1GeV photons around 24 stacked isolated high-synchrotron-peaked BL Lacs with redshift $z<0.5$ shows an excess over that of point-like sources. A statistical analysis yields a Bayes factor of $\mathrm{log}_{10}B_{10}>2$, providing evidence in favor of extended emission against the point-source hypothesis, consistent with expectations for pair halos produced in the IGMF with strength $B_{\mathrm{IGMF}}\sim 10^{-17}-10^{-15}\mathrm{G}$.",0
https://doi.org/10.1017/s0140525x01003922,The magical number 4 in short-term memory: A reconsideration of mental storage capacity,"Miller (1956) summarized evidence that people can remember about seven chunks in short-term memory (STM) tasks. However, that number was meant more as a rough estimate and a rhetorical device than as a real capacity limit. Others have since suggested that there is a more precise capacity limit, but that it is only three to five chunks. The present target article brings together a wide variety of data on capacity limits suggesting that the smaller capacity limit is real. Capacity limits will be useful in analyses of information processing only if the boundary conditions for observing them can be carefully described. Four basic conditions in which chunks can be identified and capacity limits can accordingly be observed are: (1) when information overload limits chunks to individual stimulus items, (2) when other steps are taken specifically to block the recording of stimulus items into larger chunks, (3) in performance discontinuities caused by the capacity limit, and (4) in various indirect effects of the capacity limit. Under these conditions, rehearsal and long-term memory cannot be used to combine stimulus items into chunks of an unknown size; nor can storage mechanisms that are not capacity-limited, such as sensory memory, allow the capacity-limited storage mechanism to be refilled during recall. A single, central capacity limit averaging about four chunks is implicated along with other, noncapacity-limited sources. The pure STM capacity limit expressed in chunks is distinguished from compound STM limits obtained when the number of separately held chunks is unclear. Reasons why pure capacity estimates fall within a narrow range are discussed and a capacity limit for the focus of attention is proposed.",0
https://doi.org/10.1016/b978-0-444-53737-9.50010-4,Structural Equation Modeling,,0
https://doi.org/10.1515/1557-4679.1360,A Reproducing Kernel-Based Spatial Model in Poisson Regressions,"A semi-parametric spatial model for spatial dependence is proposed in Poisson regressions to study the effects of risk factors on incidence outcomes. The spatial model is constructed through an application of reproducing kernels. A Bayesian framework is proposed to infer the unknown parameters. Simulations are performed to compare the reproducing kernel-based method with several commonly used approaches in spatial modeling, including independent Gaussian and CAR models. Compared with these models, the reproducing kernel-based method is easy to implement and more flexible in terms of the ability to model various spatial dependence patterns. To further demonstrate the proposed method, two real data applications are discussed: Scottish lip cancer data and Florida smoke-related cancer data.",0
https://doi.org/10.1093/biomet/80.4.791,Laplace's approximation for nonlinear mixed models,"SUMMARY An approximation to Laplace's method for integrals is applied to marginal distributions of data arising from models in which both fixed and random effects enter nonlinearly. The approach provides alternative derivations of some recent algorithms for fitting such models, and it has direct ties with Gaussian restricted maximum likelihood and the accompanying mixed model equations.",0
https://doi.org/10.1027/1614-2241.3.1.14,Methods for Restricting Maximum Exposure Rate in Computerized Adaptative Testing,"Abstract. The Sympson-Hetter (1985) method provides a means of controlling maximum exposure rate of items in Computerized Adaptive Testing. Through a series of simulations, control parameters are set that mark the probability of administration of an item on being selected. This method presents two main problems: it requires a long computation time for calculating the parameters and the maximum exposure rate is slightly above the fixed limit. Van der Linden (2003) presented two alternatives which appear to solve both of the problems. The impact of these methods in the measurement accuracy has not been tested yet. We show how these methods over-restrict the exposure of some highly discriminating items and, thus, the accuracy is decreased. It also shown that, when the desired maximum exposure rate is near the minimum possible value, these methods offer an empirical maximum exposure rate clearly above the goal. A new method, based on the initial estimation of the probability of administration and the probability of selection of the items with the restricted method ( Revuelta &amp; Ponsoda, 1998 ), is presented in this paper. It can be used with the Sympson-Hetter method and with the two van der Linden's methods. This option, when used with Sympson-Hetter, speeds the convergence of the control parameters without decreasing the accuracy.",0
https://doi.org/10.1111/1467-9876.00237,On the Effect of the Number of Quadrature Points in a Logistic Random Effects Model: An Example,"SUMMARY Although generalized linear mixed models are recognized to be of major practical importance, it is also known that they can be computationally demanding. The problem is the evaluation of the integral in calculating the marginalized likelihood. The straightforward method is based on the Gauss–Hermite technique, based on Gaussian quadrature points. Another approach is provided by the class of penalized quasi-likelihood methods. It is commonly believed that the Gauss–Hermite method works relatively well in simple situations but fails in more complicated structures. However, we present here a strikingly simple example of a logistic random-intercepts model in the context of a longitudinal clinical trial where the method gives valid results only for a high number of quadrature points (Q). As a consequence, this result warns the practitioner to examine routinely the dependence of the results on Q. The adaptive Gaussian quadrature, as implemented in the new SAS procedure NLMIXED, offered the solution to our problem. However, even the adaptive version of Gaussian quadrature needs careful handling to ensure convergence.",0
https://doi.org/10.1016/j.jval.2014.10.006,Network Meta-Analysis: Development of a Three-Level Hierarchical Modeling Approach Incorporating Dose-Related Constraints,"Network meta-analysis (NMA) is commonly used in evidence synthesis; however, in situations in which there are a large number of treatment options, which may be subdivided into classes, and relatively few trials, NMAs produce considerable uncertainty in the estimated treatment effects, and consequently, identification of the most beneficial intervention remains inconclusive.To develop and demonstrate the use of evidence synthesis methods to evaluate extensive treatment networks with a limited number of trials, making use of classes.Using Bayesian Markov chain Monte Carlo methods, we build on the existing work of a random effects NMA to develop a three-level hierarchical NMA model that accounts for the exchangeability between treatments within the same class as well as for the residual between-study heterogeneity. We demonstrate the application of these methods to a continuous and binary outcome, using a motivating example of overactive bladder. We illustrate methods for incorporating ordering constraints in increasing doses, model selection, and assessing inconsistency between the direct and indirect evidence.The methods were applied to a data set obtained from a systematic literature review of trials for overactive bladder, evaluating the mean reduction in incontinence episodes from baseline and the number of patients reporting one or more adverse events. The data set involved 72 trials comparing 34 interventions that were categorized into nine classes of interventions, including placebo.Bayesian three-level hierarchical NMAs have the potential to increase the precision in the effect estimates while maintaining the interpretability of the individual interventions for decision making.",0
https://doi.org/10.1111/1467-9868.00265,Dealing with label switching in mixture models,"Summary. In a Bayesian analysis of finite mixture models, parameter estimation and clustering are sometimes less straightforward than might be expected. In particular, the common practice of estimating parameters by their posterior mean, and summarizing joint posterior distributions by marginal distributions, often leads to nonsensical answers. This is due to the so-called 'label switching' problem, which is caused by symmetry in the likelihood of the model parameters. A frequent response to this problem is to remove the symmetry by using artificial identifiability constraints. We demonstrate that this fails in general to solve the problem, and we describe an alternative class of approaches, relabelling algorithms, which arise from attempting to minimize the posterior expected loss under a class of loss functions. We describe in detail one particularly simple and general relabelling algorithm and illustrate its success in dealing with the label switching problem on two examples.",0
https://doi.org/10.2307/2347027,Some Comments on Multicollinearity in Regression,"In contrast to the views put forward in a recent paper, the following points are made concerning multicollinearity in regression models: firstly, multicollinearity is almost invariably a problem of degree rather than kind; and, secondly, the search for an appropriate functional form for a regression equation should not be confused with the treatment of multicollinearity. Finally, some comments are made on the role of prior detrending in both linear and nonlinear models.",0
https://doi.org/10.1016/j.jmva.2011.01.001,Constructing priors based on model size for nondecomposable Gaussian graphical models: A simulation based approach,"A method for constructing priors is proposed that allows the off-diagonal elements of the concentration matrix of Gaussian data to be zero. The priors have the property that the marginal prior distribution of the number of nonzero off-diagonal elements of the concentration matrix (referred to below as model size) can be specified flexibly. The priors have normalizing constants for each model size, rather than for each model, giving a tractable number of normalizing constants that need to be estimated. The article shows how to estimate the normalizing constants using Markov chain Monte Carlo simulation and supersedes the method of Wong et al. (2003) [24] because it is more accurate and more general. The method is applied to two examples. The first is a mixture of constrained Wisharts. The second is from Wong et al. (2003) [24] and decomposes the concentration matrix into a function of partial correlations and conditional variances using a mixture distribution on the matrix of partial correlations. The approach detects structural zeros in the concentration matrix and estimates the covariance matrix parsimoniously if the concentration matrix is sparse.",0
https://doi.org/10.1111/j.1467-9574.2009.00445.x,The effect of estimation method and sample size in multilevel structural equation modeling,"Multilevel structural equation modeling (multilevel SEM) has become an established method to analyze multilevel multivariate data. The first useful estimation method was the pseudobalanced method. This method is approximate because it assumes that all groups have the same size, and ignores unbalance when it exists. In addition, full information maximum likelihood (ML) estimation is now available, which is often combined with robust chi-squares and standard errors to accommodate unmodeled heterogeneity (MLR). In addition, diagonally weighted least squares (DWLS) methods have become available as estimation methods. This article compares the pseudobalanced estimation method, ML(R), and two DWLS methods by simulating a multilevel factor model with unbalanced data. The simulations included different sample sizes at the individual and group levels and different intraclass correlation (ICC). The within-group part of the model posed no problems. In the between part of the model, the different ICC sizes had no effect. There is a clear interaction effect between number of groups and estimation method. ML reaches unbiasedness fastest, then the two DWLS methods, then MLR, and then the pseudobalanced method (which needs more than 200 groups). We conclude that both ML(R) and DWLS are genuine improvements on the pseudobalanced approximation. With small sample sizes, the robust methods are not recommended.",0
https://doi.org/10.1207/s15516709cog0000_69,A Hierarchical Bayesian Model of Human Decision-Making on an Optimal Stopping Problem,"We consider human performance on an optimal stopping problem where people are presented with a list of numbers independently chosen from a uniform distribution. People are told how many numbers are in the list, and how they were chosen. People are then shown the numbers one at a time, and are instructed to choose the maximum, subject to the constraint that they must choose a number at the time it is presented, and any choice below the maximum is incorrect. We present empirical evidence that suggests people use threshold-based models to make decisions, choosing the first currently maximal number that exceeds a fixed threshold for that position in the list. We then develop a hierarchical generative account of this model family, and use Bayesian methods to learn about the parameters of the generative process, making inferences about the threshold decision models people use. We discuss the interesting aspects of human performance on the task, including the lack of learning, and the presence of large individual differences, and consider the possibility of extending the modeling framework to account for individual differences. We also use the modeling results to discuss the merits of hierarchical, generative and Bayesian models of cognitive processes more generally.",0
https://doi.org/10.1057/ap.2013.32,Individual socialization or polito-cultural context? The cultural roots of volunteering in Switzerland,"Volunteering rates in Switzerland vary substantially across language regions. In this article, we investigate the cultural roots of this variation by presenting and empirically testing two different conceptualizations of how linguistic culture is related to individual volunteering. Whereas the first perspective perceives the individual as belonging to a particular language community and its norms and values as crucial for individual volunteering, the other sees the linguistic culture mainly as an important context in which an individual lives and which therefore influences individual volunteering. Empirically, we base our analysis on new survey data from 60 Swiss communes and apply a Bayesian multi-level analysis in order to disentangle the linguistic group from contextual effects. Our analysis supports the view that cultural patterns of civic self-organization can indeed explain regional volunteering behaviour in Switzerland. Whereas the propensity to volunteer is generally highest in German-speaking Switzerland, our findings reveal that it is the group of French speakers that exhibits the highest propensity to volunteer when controlling for language region. Ã‚Â© 2015 Macmillan Publishers Limited.",0
https://doi.org/10.1177/0962280206074463,Multiple imputation of discrete and continuous data by fully conditional specification,"The goal of multiple imputation is to provide valid inferences for statistical estimates from incomplete data. To achieve that goal, imputed values should preserve the structure in the data, as well as the uncertainty about this structure, and include any knowledge about the process that generated the missing data. Two approaches for imputing multivariate data exist: joint modeling (JM) and fully conditional specification (FCS). JM is based on parametric statistical theory, and leads to imputation procedures whose statistical properties are known. JM is theoretically sound, but the joint model may lack flexibility needed to represent typical data features, potentially leading to bias. FCS is a semi-parametric and flexible alternative that specifies the multivariate model by a series of conditional models, one for each incomplete variable. FCS provides tremendous flexibility and is easy to apply, but its statistical properties are difficult to establish. Simulation work shows that FCS behaves very well in the cases studied. The present paper reviews and compares the approaches. JM and FCS were applied to pubertal development data of 3801 Dutch girls that had missing data on menarche (two categories), breast development (five categories) and pubic hair development (six stages). Imputations for these data were created under two models: a multivariate normal model with rounding and a conditionally specified discrete model. The JM approach introduced biases in the reference curves, whereas FCS did not. The paper concludes that FCS is a useful and easily applied flexible alternative to JM when no convenient and realistic joint distribution can be specified.",0
https://doi.org/10.3758/bf03193505,Short-term recognition memory for serial order and timing,"Recent evidence suggests that a common temporal representation underlies memory for serial order of items in a sequence, and the timing of items in a sequence. This stands in contrast to other data suggesting a reliance on only ordinal information in short-term memory tasks. An experiment is reported here in which participants were post cued to perform a comparison between a probe and study list of items irregularly spaced in time, on the basis of order or temporal information.Participants' performance on theserial recognition task was notaffected by the temporal proximity of items, although participants were able to use temporal information to perform a temporal recognition task. Application of a temporal matching model of serial and temporal recognition suggests that although participants were able to remember the timing of items, this memory for timing was unlikely to determine serial recognition performance. The results suggest a dissociation between ordinal and temporal information in short-term memory.",0
https://doi.org/10.3758/brm.40.3.879,Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models,"Hypotheses involving mediation are common in the behavioral sciences. Mediation exists when a predictor affects a dependent variable indirectly through at least one intervening variable, or mediator. Methods to assess mediation involving multiple simultaneous mediators have received little attention in the methodological literature despite a clear need. We provide an overview of simple and multiple mediation and explore three approaches that can be used to investigate indirect processes, as well as methods for contrasting two or more mediators within a single model. We present an illustrative example, assessing and contrasting potential mediators of the relationship between the helpfulness of socialization agents and job satisfaction. We also provide SAS and SPSS macros, as well as Mplus and LISREL syntax, to facilitate the use of these methods in applications.",0
https://doi.org/10.1111/j.1467-985x.2005.00389.x,A Bayesian hierarchical model for categorical longitudinal data from a social survey of immigrants,"The paper investigates a Bayesian hierarchical model for the analysis of categorical longitudinal data from a large social survey of immigrants to Australia. Data for each subject are observed on three separate occasions, or waves, of the survey. One of the features of the data set is that observations for some variables are missing for at least one wave. A model for the employment status of immigrants is developed by introducing, at the first stage of a hierarchical model, a multinomial model for the response and then subsequent terms are introduced to explain wave and subject effects. To estimate the model, we use the Gibbs sampler, which allows missing data for both the response and the explanatory variables to be imputed at each iteration of the algorithm, given some appropriate prior distributions. After accounting for significant covariate effects in the model, results show that the relative probability of remaining unemployed diminished with time following arrival in Australia.",0
https://doi.org/10.1002/env.877,A comparison of the hierarchical likelihood and Bayesian approaches to spatial epidemiological modelling,"Recently Bayesian methods have been widely used in disease mapping. Hierarchical (h-) likelihood methods allow reliable likelihood inference in random-effect models and it is therefore interesting to compare h-likelihood and Bayesian methods. For comparison we consider three examples: low birth weight and cancer mortality data in South Carolina and lip cancer data in Scotland. Mean estimates from both h-likelihood and Bayesian approaches are almost identical, while variance-component estimates can be somewhat different, depending upon choice of priors. Copyright © 2007 John Wiley & Sons, Ltd.",0
https://doi.org/10.1080/02664763.2013.807331,A multiple group item response theory model with centered skew-normal latent trait distributions under a Bayesian framework,"Very often, in psychometric research, as in educational assessment, it is necessary to analyze item response from clustered respondents. The multiple group item response theory (IRT) model proposed by Bock and Zimowski [12] provides a useful framework for analyzing such type of data. In this model, the selected groups of respondents are of specific interest such that group-specific population distributions need to be defined. The usual assumption for parameter estimation in this model, which is that the latent traits are random variables following different symmetric normal distributions, has been questioned in many works found in the IRT literature. Furthermore, when this assumption does not hold, misleading inference can result. In this paper, we consider that the latent traits for each group follow different skew-normal distributions, under the centered parameterization. We named it skew multiple group IRT model. This modeling extends the works of Azevedo et al. [4], Bazan et al. [11] and Bock and Zimo...",0
https://doi.org/10.1136/bmjopen-2015-010160,Safety and effectiveness of long-acting versus intermediate-acting insulin for patients with type 1 diabetes: protocol for a systematic review and individual patient data network meta-analysis,"The choice of a basal insulin regimen to manage type 1 diabetes mellitus (T1DM) may have different risks of adverse events and effectiveness, due to the difference in the effectiveness of these agents across patient characteristics (eg, baseline glycosylated haemoglobin; A1C). Currently, there is a lack of high quality evidence to support the tailoring of insulin regimens according to an individual's needs. The aim of this study is to update our previous systematic review and perform an individual patient data network meta-analysis (IPD-NMA) to evaluate the comparative safety and effectiveness of long-acting versus intermediate-acting insulin in different subgroups of patients with T1DM.We will update our previous literature search from January 2013 onwards searching relevant electronic databases (eg, MEDLINE), as well as perform grey literature search through relevant society/association websites, and conference abstracts, and scan reference lists of the eligible studies. We will include randomised clinical trials of any duration examining long-acting versus intermediate-acting insulin preparations for adult patients with T1DM. We will focus on A1C and severe hypoglycaemia outcomes. For each pairwise treatment comparison, we will combine all IPD from all studies in a single multilevel model, where each study is a different cluster. For a connected network of trials, we will perform an IPD-NMA to identify potential effect modifiers, and estimate the most effective and safe treatments for patients with different characteristics. If we are not successful in obtaining IPD for at least one study, we will include aggregated data (AD) abstracted from the included RCTs in our analysis, combining IPD and AD into a single model. We will report our results using the PRISMA-IPD statement.The results of this systematic review and IPD-NMA will be of interest to stakeholders and will help in improving existing guideline recommendations.CRD42015023511.",0
https://doi.org/10.1016/0270-0255(86)90088-6,A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect,"In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.",0
https://doi.org/10.1037/0096-3445.108.3.356,Automatic and effortful processes in memory.,"Proposes a framework for the conceptualization of a broad range of memory phenomena that integrates research on memory performance in young children, the elderly, and individuals under stress with research on memory performance in normal college students. One basic assumption is that encoding operations vary in their attentional requirements. Operations that drain minimal energy from limited-capacity attentional mechanisms are called automatic. Automatic operations function at a constant level under all circumstances, occur without intention, and do not benefit from practice. Effortful operations, such as rehearsal and elaborative mnemonic activities, require considerable capacity, interfere with other cognitive activities also requiring capacity, are initiated intentionally, and show benefits from practice. A 2nd assumption is that attentional capacity varies both within and among individuals. Depression, high arousal levels, and old age are variables thought to reduce attentional capacity. The conjunction of the 2 assumptions of the framework yields the prediction that the aged and individuals under stress will show a decrease in performance only on tasks requiring effortful processing. Evidence from the literature on development, aging, depression, arousal, and normal memory is presented in support of the framework, and 4 experiments with 301 5-40 yr old Ss are described. (51/2 p ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1979 American Psychological Association.",0
https://doi.org/10.1198/016214506000000104,A Hierarchical Multivariate Two-Part Model for Profiling Providers' Effects on Health Care Charges,"Procedures for analyzing and comparing health care providers' effects on health services delivery and outcomes have been referred to as provider profiling. In a typical profiling procedure, patient-level responses are measured for clusters of patients treated by providers that in turn can be considered statistically exchangeable. Thus a hierarchical model naturally represents the structure of the data. When provider effects on multiple responses are profiled, a multivariate model rather than a series of univariate models can capture associations among responses at both the provider and patient levels. When responses are in the form of charges for health care services and sampled patients include nonusers of services, charge variables are a mix of 0's and highly skewed positive values that present a modeling challenge. For analysis of covariate effects on charges for a single service, a frequently used approach is a two-part model that combines logistic or probit regression on any use of the service and li...",0
https://doi.org/10.2307/3088424,Modeling Multilevel Data Structures,"data are becoming quite common in political science and provide numerous opportunities for theory testing and development. Unfortunately this type of data typically generates a number of statistical problems, of which clustering is particularly impor? tant. To exploit the opportunities of? fered by multilevel data, and to solve the statistical problems inherent in them, special statistical techniques are required. In this article, we focus on a technique that has become popular in educational statistics and sociology?multilevel analysis. In multilevel analysis, researchers build models that capture the layered structure of multilevel data, and determine how layers interact and impact a dependent variable of interest. Our objective in this article is to introduce the logic and statistical theory behind multilevel models, to illustrate how such models can be applied fruitfully in political science, and to call atten? tion to some of the pitfalls in multilevel analysis.",0
,Correlation and causality,,0
https://doi.org/10.1016/j.prevetmed.2015.12.010,Investigating preventive-medicine consultations in first-opinion small-animal practice in the United Kingdom using direct observation,"Preventive-medicine consultations account for a large proportion of the veterinary caseload and previous research has suggested these consultations are fundamentally different from those in which the animal is presented for a specific health problem. There has been recent controversy around some aspects of preventive medicine for cats and dogs, and the full health benefits of the preventive-medicine consultation remain unclear. The aim of this study was to compare characteristics of the consultation and the problems discussed during the consultation between preventive-medicine consultations and other types of consultations. Data were gathered during direct observation of small-animal consultations in seven first-opinion practices in the United Kingdom. Data collected included type of clinical examination performed, patient signalment, and details of all problems discussed (including whether the problem was presenting or non-presenting, new or pre-existing, who had raised the problem, body system affected and whether an action was taken). A two-level multivariable logistic-regression model was developed, with canine and feline patients at Level 1 nested within consulting veterinary surgeons at Level 2, and a binary outcome variable of preventive-medicine consultation versus specific health-problem consultation. A total of 1807 patients were presented, of which 690 (38.2%) presented for a preventive-medicine consultation. Dogs were the most frequently presented species (n=1168; 64.6%) followed by cats (n=510; 28.2%), rabbits (n=86; 4.8%) and patients of other species (n=43; 2.4%). The five variables remaining in the multi-level model were whether multiple patients were presented, patient age, clinical examination type, weighing and number of problems discussed. Species, breed, sex, neutering status and practice did not remain in the final model. Many non-presenting problems, including both preventive-medicine problems and specific-health problems, were discussed and acted upon during all types of consultations. Dental and behavioural non-presenting problems were discussed more frequently during preventive-medicine consultations compared with specific health-problem consultations. Preventive-medicine consultations represent an opportunity for veterinary surgeons to discuss other aspects of preventive medicine, and to detect and manage new and ongoing health problems. A greater evidence base is needed to understand whether detecting and managing underlying disease during the preventive-medicine consultation has a positive impact on lifelong patient health and welfare.",0
https://doi.org/10.1037/1082-989x.10.1.40,Meta-analytic structural equation modeling: A two-stage approach.,"To synthesize studies that use structural equation modeling (SEM), researchers usually use Pearson correlations (univariate r), Fisher z scores (univariate z), or generalized least squares (GLS) to combine the correlation matrices. The pooled correlation matrix is then analyzed by the use of SEM. Questionable inferences may occur for these ad hoc procedures. A 2-stage structural equation modeling (TSSEM) method is proposed to incorporate meta-analytic techniques and SEM into a unified framework. Simulation results reveal that the univariate-r, univariate-z, and TSSEM methods perform well in testing the homogeneity of correlation matrices and estimating the pooled correlation matrix. When fitting SEM, only TSSEM works well. The GLS method performed poorly in small to medium samples.",0
https://doi.org/10.1080/01621459.1975.10479871,A Bayesian Sequential Procedure for Quantal Response in the Context of Adaptive Mental Testing,Abstract This article is concerned with a generalization of the problem of estimation of median effective dose in bioassay with a normal quantal response curve. A new kind of Bayesian motivated procedure is introduced which leads to a strongly consistent estimator. The convergence is robust in that it holds for a bundle of sequences of design vectors—an important feature in a mental testing context where a specified design vector cannot be produced on demand.,0
https://doi.org/10.1027/1614-2241.3.4.149,Are There Test Administrator Effects in Large-Scale Educational Assessments?,"Abstract. In large-scale educational assessments such as the Third International Mathematics and Sciences Study (TIMSS) or the Program for International Student Assessment (PISA), sizeable numbers of test administrators (TAs) are needed to conduct the assessment sessions in the participating schools. TA training sessions are run and administration manuals are compiled with the aim of ensuring standardized, comparable, assessment situations in all student groups. To date, however, there has been no empirical investigation of the effectiveness of these standardizing efforts. In the present article, we probe for systematic TA effects on mathematics achievement and sample attrition in a student achievement study. Multilevel analyses for cross-classified data using Markov Chain Monte Carlo (MCMC) procedures were performed to separate the variance that can be attributed to differences between schools from the variance associated with TAs. After controlling for school effects, only a very small, nonsignificant proportion of the variance in mathematics scores and response behavior was attributable to the TAs (&lt; 1%). We discuss practical implications of these findings for the deployment of TAs in educational assessments.",0
https://doi.org/10.1111/j.1540-5907.2009.00369.x,"Contextual Factors and the Extreme Right Vote in Western Europe, 1980-2002","Research on the voters of the extreme right in Western Europe has become a minor industry, but relatively little attention has been paid to the twin question of why support for these parties is often unstable, and why the extreme right is so weak in many countries. Moreover, the findings from different studies often contradict each other. This article aims at providing a more comprehensive and satisfactory answer to this research problem by employing a broader database and a more adequate modeling strategy. The main finding is that while immigration and unemployment rates are important, their interaction with other political factors is much more complex than suggested by previous research. Moreover, persistent country effects prevail even if a whole host of individual and contextual variables is controlled for.",0
https://doi.org/10.1111/j.1468-0262.2006.00655.x,Large Sample Properties of Matching Estimators for Average Treatment Effects,"Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N 1/2 -consistent in general and describe conditions under which matching estimators do attain N 1/2 -consistency. Second, we show that even in settings where matching estimators are N 1/2 -consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.",0
https://doi.org/10.1146/annurev-psych-010814-015258,Advances in Mediation Analysis: A Survey and Synthesis of New Developments,"Mediation processes are fundamental to many classic and emerging theoretical paradigms within psychology. Innovative methods continue to be developed to address the diverse needs of researchers studying such indirect effects. This review provides a survey and synthesis of four areas of active methodological research: (a) mediation analysis for longitudinal data, (b) causal inference for indirect effects, (c) mediation analysis for discrete and nonnormal variables, and (d) mediation assessment in multilevel designs. The aim of this review is to aid in the dissemination of developments in these four areas and suggest directions for future research.",0
https://doi.org/10.1198/073500103288619331,Bayesian Analysis of the Heterogeneity Model,"We consider Bayesian estimation of a finite mixture of models with random effects, which is also known as the heterogeneity model. First, we discuss the properties of various Markov chain Monte Carlo samplers that are obtained from full conditional Gibbs sampling by grouping and collapsing. Whereas full conditional Gibbs sampling turns out to be sensitive to the parameterization chosen for the mean structure of the model, the alternative sampler is robust in this respect. However, the logical extension of the approach to the sampling of the group variances does not further increase the efficiency of the sampler. Second, we deal with the identifiability problem due to the arbitrary labeling within the model. Finally, a case study involving metric conjoint analysis serves as a practical illustration.",0
https://doi.org/10.1007/s13524-015-0430-1,"Ethnic Residential Segregation: A Multilevel, Multigroup, Multiscale Approach Exemplified by London in 2011","Abstract We develop and apply a multilevel modeling approach that is simultaneously capable of assessing multigroup and multiscale segregation in the presence of substantial stochastic variation that accompanies ethnicity rates based on small absolute counts. Bayesian MCMC estimation of a log-normal Poisson model allows the calculation of the variance estimates of the degree of segregation in a single overall model, and credible intervals are obtained to provide a measure of uncertainty around those estimates. The procedure partitions the variance at different levels and implicitly models the dependency (or autocorrelation) at each spatial scale below the topmost one. Substantively, we apply the model to 2011 census data for London, one of the world’s most ethnically diverse cities. We find that the degree of segregation depends both on scale and group.",0
https://doi.org/10.1111/spsr.12184,How Exclusive is Assembly Democracy? Citizens' Assembly and Ballot Participation Compared,"This paper analyses the difference between two specific forms of citizens’ involvements, namely whether a vote is cast by ballot or in a citizens’ assembly in which people gather in town halls to decide legislative questions in a deliberative manner. We show both theoretically and empirically how citizens’ assemblies and decisions at the ballot box substantially differ not only in terms of their underlying model of democracy, but also in their structural conditions and, thus, with respect to the social inequality of participation. We test our hypotheses in a Bayesian multilevel framework using real participation data collected from 15 political decisions made in a Swiss commune. Our results show that citizens’ assemblies are not only characterised by lower participation rates, but also by a particular composition of the electorate. While citizens’ assemblies are more equal regarding income groups, ballots favour a more equitable participation in terms of gender and age.",0
https://doi.org/10.1080/00220973.2014.952397,Alternatives to Multilevel Modeling for the Analysis of Clustered Data,"Multilevel modeling has grown in use over the years as a way to deal with the nonindependent nature of observations found in clustered data. However, other alternatives to multilevel modeling are available that can account for observations nested within clusters, including the use of Taylor series linearization for variance estimation, the design effect adjusted standard errors approach, and fixed effects modeling. Using 1,000 replications of 12 conditions with varied Level 1 and Level 2 sample sizes, the author compared parameter estimates, standard errors, and statistical significance using various alternative procedures. Results indicate that several acceptable procedures can be used in lieu of or together with multilevel modeling, depending on the type of research question asked and the number of clusters under investigation. Guidelines for applied researchers are discussed.",0
https://doi.org/10.1007/978-1-4684-6994-3_16,Statistical Problems of Fitting Individual Growth Curves,"A thorough-going longitudinal study of a child’s growth can produce upward of forty observations spaced over the years from birth to maturity. Such a data record is too long and inevitably too noisy (because of measurement error and short-run growth variation) to be interpreted without some sort of condensation and smoothing. The length of the record forces attention to certain critical regions or features of the curve, but the noisiness of the data makes it risky to characterize these regions or features by a few isolated measurements. The only safe approach to interpretation of individual growth data is via a statistical method capable of revealing the essential trend and concisely describing its main features.",0
,YAAP: yet another adaptive procedure.,"YAAP is an implementation of an adaptive psychophysical method, based on Bayesian statistics, for estimating the threshold of a psychometric function. On the basis of Bayesian probability intervals, a dynamic termination criterion allows for threshold estimation within pre-specified confidence limits.",0
https://doi.org/10.1037//0021-9010.87.2.377,Comparison of two random-effects methods of meta-analysis.,"Two studies compared the Schmidt-Hunter method of meta-analysis (J. E. Hunter &amp; F. L. Schmidt, 1990) with the method described by L. V. Hedges and J. L. Vevea (1998). Study 1 evaluated estimates of Ã ï¿½ÃŒâ€ž, Ã Æ’Ã ï¿½, and resulting credibility intervals for both models through Monte Carlo methods. Results showed slight differences between the 2 methods. In Study 2, a reanalysis of published meta-analyses using both methods with several artifact distributions showed that although both choice of technique and type of correction could matter, the technique of meta-analysis used is less influential on the study outcome than is the choice of artifact correction.",0
https://doi.org/10.2307/2346572,Multicollinearity caused by Specification Errors,The advantages of using linear least squares regressions induce us to adopt functions which are linear in parameters. Often this imposes unrealistically rigid constraints which may create multicollinearity. Using more realistic non‐linear forms and non‐linear least squares regressions is likely to overcome this problem as shown in a study of a production function.,0
https://doi.org/10.3758/bf03201438,Efficient estimation of sensory thresholds,"Laboratory computers permit detection and discrimination thresholds to be measured rapidly, efficiently, and accurately. In this paper, the general natures of psychometric functions and of thresholds are reviewed, and various methods for estimating sensory thresholds are summarized. The most efficient method, in principle, using maximum-likelihood threshold estimations, is examined in detail. Four techniques are discussed that minimize the reported problems found with the maximum-likelihood method. A package of FORTRAN subroutines, ML-TEST, which implements the maximum-likelihood method, is described. These subroutines are available on request from the author.",0
https://doi.org/10.1007/s00221-014-3877-1,Fast transfer of crossmodal time interval training,"Sub-second time perception is essential for many important sensory and perceptual tasks including speech perception, motion perception, motor coordination, and crossmodal interaction. This study investigates to what extent the ability to discriminate sub-second time intervals acquired in one sensory modality can be transferred to another modality. To this end, we used perceptual classification of visual Ternus display (Ternus in Psychol Forsch 7:81-136, 1926) to implicitly measure participants' interval perception in pre- and posttests and implemented an intra- or crossmodal sub-second interval discrimination training protocol in between the tests. The Ternus display elicited either an ""element motion"" or a ""group motion"" percept, depending on the inter-stimulus interval between the two visual frames. The training protocol required participants to explicitly compare the interval length between a pair of visual, auditory, or tactile stimuli with a standard interval or to implicitly perceive the length of visual, auditory, or tactile intervals by completing a non-temporal task (discrimination of auditory pitch or tactile intensity). Results showed that after fast explicit training of interval discrimination (about 15 min), participants improved their ability to categorize the visual apparent motion in Ternus displays, although the training benefits were mild for visual timing training. However, the benefits were absent for implicit interval training protocols. This finding suggests that the timing ability in one modality can be rapidly acquired and used to improve timing-related performance in another modality and that there may exist a central clock for sub-second temporal processing, although modality-specific perceptual properties may constrain the functioning of this clock. Â© 2014 Springer-Verlag.",0
https://doi.org/10.1111/j.1475-6811.1999.tb00204.x,The social relations model for family data: A multilevel approach,"Multilevel models are proposed to study relational or dyadic data from multiple persons in families or other groups. The variable under study is assumed to refer to a dyadic relation between individuals in the groups. The proposed models are elaborations of the Social Relations Model. The different roles of father, mother, and child are emphasized in these models. Multilevel models provide researchers with a method to estimate the variances and correlations of the Social Relations Model and to incorporate the effects of covariates and test specialized models, even with missing observations.",0
https://doi.org/10.1037/0893-3200.19.2.314,The Interpersonal Process Model of Intimacy in Marriage: A Daily-Diary and Multilevel Modeling Approach.,"This study used daily reports of interactions in marriage to examine predictions from the conceptualization of intimacy as the outcome of an interpersonal process. Both partners of 96 married couples completed daily diaries assessing self-disclosure, partner disclosure, perceived partner responsiveness, and intimacy on each of 42 consecutive days. Multivariate multilevel modeling revealed that self-disclosure and partner disclosure both significantly and uniquely contributed to the contemporaneous prediction of intimacy. Perceived partner responsiveness partially mediated the effects of self-disclosure and partner disclosure on intimacy. Global marital satisfaction, relationship intimacy, and demand-withdraw communication were related to daily levels of intimacy. Implications for the importance of perceived partner responsiveness in the intimacy process for married partners are discussed.",0
https://doi.org/10.3758/bf03211951,Fitting the psychometric function,"A constrained generalized maximum likelihood routine for fitting psychometric functions is proposed, which determines optimum values for the complete parameter set—that is, threshold and slopeas well as for guessing and lapsing probability. The constraints are realized by Bayesian prior distributions for each of these parameters. The fit itself results from maximizing the posterior distribution of the parameter values by a multidimensional simplex method. We present results from extensive Monte Carlo simulations by which we can approximate bias and variability of the estimated parameters of simulated psychometric functions. Furthermore, we have tested the routine with data gathered in real sessions of psychophysical experimenting.",0
https://doi.org/10.1086/588741,What Can We Learn about Neighborhood Effects from the Moving to Opportunity Experiment?,Experimental estimates from Moving to Opportunity (MTO) show no significant impacts of moves to lower‐poverty neighborhoods on adult economic self‐sufficiency four to seven years after random assignment. The authors disagree with Clampet‐Lundquist and Massey's claim that MTO was a weak intervention and therefore uninformative about neighborhood effects. MTO produced large changes in neighborhood environments that improved adult mental health and many outcomes for young females. Clampet‐Lundquist and Massey's claim that MTO experimental estimates are plagued by selection bias is erroneous. Their new nonexperimental estimates are uninformative because they add back the selection problems that MTO's experimental design was intended to overcome.,0
https://doi.org/10.1177/1354068813520271,Allure or alternative? Direct democracy and party identification,"This article presents the first investigation of whether and how party identification is influenced by direct democratic institutions. The concept of party identification is of central interest to political science. Despite declining partisan attachment and increasing dealignment among voters, little systematic evidence exists as to which factors influence individual party identification. Our article contributes to improving on this lacuna by considering the educative effects of direct democratic institutions. Theoretically, two competing hypotheses are plausible. On the one hand, direct democracy might strengthen political parties and promote the need for cues so that voters succumb to the allure of partisan attachment. On the other hand, direct democracy might provide an alternative to the representational function of political parties thus rendering party identification less essential. Drawing on recent data from the Swiss cantons, we estimate multi-level models. Our analyses, though giving support to the alternative hypothesis, yield some surprising findings.",0
https://doi.org/10.1111/j.2044-8317.1984.tb00789.x,Asymptotically distribution-free methods for the analysis of covariance structures,Methods for obtaining tests of fit of structural models for covariance matrices and estimator standard errors which are asymptotically distribution free are derived. Modifications to standard normal theory tests and standard errors which make them applicable to the wider class of elliptical distributions are provided. A random sampling experiment to investigate some of the proposed methods is described.,0
https://doi.org/10.1093/swr/18.4.247,The M word: Multicollinearity in multiple regression,,0
https://doi.org/10.1214/088342304000000116,The Interplay of Bayesian and Frequentist Analysis,"Statistics has struggled for nearly a century over the issue of whether the Bayesian or frequentist paradigm is superior. This debate is far from over and, indeed, should continue, since there are fundamental philosophical and pedagogical issues at stake. At the methodological level, however, the debate has become considerably muted, with the recognition that each approach has a great deal to contribute to statistical practice and each is actually essential for full development of the other approach. In this article, we embark upon a rather idiosyncratic walk through some of these issues.",0
https://doi.org/10.1006/jmps.1996.0030,Premature Sampling in Random Walks,"Abstract Premature sampling is a plausible modification of the random walk that is regarded as sufficient in allowing the sequential probability ratio test (SPRT) model of choice response time to account for the frequent observation that mean error response latency is smaller than that for correct responses. This paper establishes a mathematical framework for random walks with premature sampling. Four important points are made. First, Wald's Identity and the small steps assumption fail to make the problem tractable in its fullest generality. Second, Laming's (1968) derivation of the important result that premature sampling leads to smaller mean latencies for error responses than correct responses in the SPRT model may be flawed by a tenuous approximation. Third, expressions for response probabilities and mean latencies are derived for the general model on the assumption that premature sampling is not sufficient, by itself, to trigger a response, although it does influence the process. Fourth, the SPRT model with premature sampling does not necessarily imply that when conditioned on a response, mean response time for error responses is smaller than that for correct responses—and counterexamples to this claim are provided.",0
https://doi.org/10.1037/0096-3445.136.3.414,Individual differences in components of reaction time distributions and their relations to working memory and intelligence.,"The authors bring together approaches from cognitive and individual differences psychology to model characteristics of reaction time distributions beyond measures of central tendency. Ex-Gaussian distributions and a diffusion model approach are used to describe individuals' reaction time data. The authors identified common latent factors for each of the 3 ex-Gaussian parameters and for 3 parameters central to the diffusion model using structural equation modeling for a battery of choice reaction tasks. These factors had differential relations to criterion constructs. Parameters reflecting the tail of the distribution (i.e., tau in the ex-Gaussian and drift rate in the diffusion model) were the strongest unique predictors of working memory, reasoning, and psychometric speed. Theories of controlled attention and binding are discussed as potential theoretical explanations.",0
https://doi.org/10.1348/026151008x329517,An investigation of the impact of young children's self-knowledge of trustworthiness on school adjustment: A test of the realistic self-knowledge and positive illusion models,"The study aimed to examine the relationship between self-knowledge of trustworthiness and young children's school adjustment. One hundred and seventy-three (84 male and 89 female) children from school years 1 and 2 in the United Kingdom (mean age 6 years 2 months) were tested twice over 1-year. Children's trustworthiness was assessed using: (a) self-report at Time 1 and Time 2; (b) peers' reports at Time 1 and Time 2; and (c) teacher-reports at Time 2. School adjustment was assessed by child-rated school-liking and the Short-Form Teacher Rating Scale of School Adjustment (Short-Form TRSSA). Longitudinal quadratic relationships were found between school adjustment and children's self-knowledge, using peer-reported trustworthiness as a reference: more accurate self-knowledge of trustworthiness predicted increases in school adjustment. Comparable concurrent quadratic relationships were found between teacher-rated school adjustment and children's self-knowledge, using teacher-reported trustworthiness as a reference, at Time 2. The findings support the conclusion that young children's psychosocial adjustment is best accounted for by the realistic self-knowledge model (Colvin & Block, 1994).",0
https://doi.org/10.1080/10503300802326038,Beyond the individual: Group effects in mindfulness-based stress reduction,"The authors explored the group as a source of change in mindfulness-based stress reduction (MBSR). Participants consisted of 606 adults in 59 groups who completed an 8-week MBSR program. The authors examined change in the General Symptom Index (GSI) of the Symptom Checklist-90-Revised and the Medical Symptom Checklist (MSC) from pre- to postintervention. Multilevel models were used to examine the extent to which groups differed in the amount of change reported by the participants. After controlling for pretreatment severity, group accounted for 7% of the variability in the GSI and 0% in the MSC. The authors discuss the implications of these findings for the practice of MBSR as well as for research investigating the effects of MBSR and other programs or psychotherapies.",0
https://doi.org/10.1037/h0045764,Intelligence at adulthood: A twenty-five year follow-up.,"This paper reports findings from 111 Ss tested with the Stanford-Binet (S-B) at preschool and adolescence who were administered the S-B and WAIS at adulthood. Correlations of preschool IQs with adult S-B and full WAIS IQs are .59 and .64; of adolescent IQs with adult IQs, .85 and .80. Mean S-B IQ increase from adolescence to adulthood is 11 points, indicating that mental growth continues beyond 16 years. Males show more IQ gain after adolescence than do females (p<.01). Girls with high IQs increase least. Analysis of increases in percent passing S-B items shows more growth after adolescence in abstract reasoning and vocabulary than in rote memory and practical reasoning. Preschool verbal and memory items are better predictors than nonverbal items of both verbal and performance adult IQs. Pattern of individual differences in relative amounts of these abilities shows some stability over 25 years. (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1962 American Psychological Association.",0
https://doi.org/10.1207/s15327906mbr4104_4,Distinguishing Between Latent Classes and Continuous Factors: Resolution by Maximum Likelihood?,"Latent variable models exist with continuous, categorical, or both types of latent variables. The role of latent variables is to account for systematic patterns in the observed responses. This article has two goals: (a) to establish whether, based on observed responses, it can be decided that an underlying latent variable is continuous or categorical, and (b) to quantify the effect of sample size and class proportions on making this distinction. Latent variable models with categorical, continuous, or both types of latent variables are fitted to simulated data generated under different types of latent variable models. If an analysis is restricted to fitting continuous latent variable models assuming a homogeneous population and data stem from a heterogeneous population, overextraction of factors may occur. Similarly, if an analysis is restricted to fitting latent class models, overextraction of classes may occur if covariation between observed variables is due to continuous factors. For the data-generating models used in this study, comparing the fit of different exploratory factor mixture models usually allows one to distinguish correctly between categorical and/or continuous latent variables. Correct model choice depends on class separation and within-class sample size.",0
https://doi.org/10.1177/1094428108318065,Scale Coarseness as a Methodological Artifact,"Scale coarseness is a pervasive yet ignored methodological artifact that attenuates observed correlation coefficients in relation to population coefficients. The authors describe how to disattenuate correlations that are biased by scale coarseness in primary-level as well as meta-analytic studies and derive the sampling error variance for the corrected correlation. Results of two Monte Carlo simulations reveal that the correction procedure is accurate and show the extent to which coarseness biases the correlation coefficient under various conditions (i.e., value of the population correlation, number of item scale points, and number of scale items). The authors also offer a Web-based computer program that disattenuates correlations at the primary-study level and computes the sampling error variance as well as confidence intervals for the corrected correlation. Using this program, which implements the correction in primary-level studies, and incorporating the suggested correction in meta-analytic reviews will lead to more accurate estimates of construct-level correlation coefficients.",0
https://doi.org/10.1016/0042-6989(94)90039-6,"Efficient and unbiased modifications of the QUEST threshold method: Theory, simulations, experimental evaluation and practical implementation","QUEST [Watson and Pelli, Perception and Psychophysics, 13, 113-120 (1983)] is an efficient method of measuring thresholds which is based on three steps: (1) Specification of prior knowledge and assumptions, including an initial probability density function (p.d.f.) of threshold (i.e. relative probability of different thresholds in the population). (2) A method for choosing the stimulus intensity of any trial. (3) A method for choosing the final threshold estimate. QUEST introduced a Bayesian framework for combining prior knowledge with the results of previous trials to calculate a current p.d.f.; this is then used to implement Steps 2 and 3. While maintaining this Bayesian approach, this paper evaluates whether modifications of the QUEST method (particularly Step 2, but also Steps 1 and 3) can lead to greater precision and reduced bias. Four variations of the QUEST method (differing in Step 2) were evaluated by computer simulations. In addition to the standard method of setting the stimulus intensity to the mode of the current p.d.f. of threshold, the alternatives of using the mean and the median were evaluated. In the fourth variation--the Minimum Variance Method--the next stimulus intensity is chosen to minimize the expected variance at the end of the next trial. An exact enumeration technique with up to 20 trials was used for both yes-no and two-alternative forced-choice (2AFC) experiments. In all cases, using the mean (here called ZEST) provided better precision than using the median which in turn was better than using the mode. The Minimum Variance Method provided slightly better precision than ZEST. The usual threshold criterion--based on the ""ideal sweat factor""--may not provide optimum precision; efficiency can generally be improved by optimizing the threshold criterion. We therefore recommend either using ZEST with the optimum threshold criterion or the more complex Minimum Variance Method. A distinction is made between ""measurement bias"", which is derived from the mean of repeated threshold estimates for a single real threshold, and ""interpretation bias"", which is derived from the mean of real thresholds yielding a single threshold estimate. If their assumptions are correct, the current methods have no interpretation bias, but they do have measurement bias. Interpretation bias caused by errors in the assumptions used by ZEST is evaluated. The precisions and merits of yes-no and 2AFC techniques are compared.(ABSTRACT TRUNCATED AT 400 WORDS)",0
https://doi.org/10.2200/s00371ed1v01y201107aim013,Human Computation,"Human computation is a new and evolving research area that centers around harnessing human intelligence to solve computational problems that are beyond the scope of existing Artificial Intelligence (AI) algorithms. With the growth of the Web, human computation systems can now leverage the abilities of an unprecedented number of people via the Web to perform complex computation. There are various genres of human computation applications that exist today. Games with a purpose (e.g., the ESP Game) specifically target online gamers who generate useful data (e.g., image tags) while playing an enjoyable game. Crowdsourcing marketplaces (e.g., Amazon Mechanical Turk) are human computation systems that coordinate workers to perform tasks in exchange for monetary rewards. In identity verification tasks, users perform computation in order to gain access to some online content; an example is reCAPTCHA, which leverages millions of users who solve CAPTCHAs every day to correct words in books that optical character recognition (OCR) programs fail to recognize with certainty. This book is aimed at achieving four goals: (1) defining human computation as a research area; (2) providing a comprehensive review of existing work; (3) drawing connections to a wide variety of disciplines, including AI, Machine Learning, HCI, Mechanism/Market Design and Psychology, and capturing their unique perspectives on the core research questions in human computation; and (4) suggesting promising research directions for the future. Â© 2011 by Morgan & Claypool.",0
https://doi.org/10.1111/j.1744-6570.2000.tb02426.x,"SO MANY JOBS, SO LITTLE ""N"": APPLYING EXPANDED VALIDATION MODELS TO SUPPORT GENERALIZATION OF COGNITIVE TEST VALIDITY","This paper describes a case study in which practitioners were faced with the challenge of validating cognitive ability tests in a setting where additional criterion-related validation research was not technically feasible. Research conducted within this organization had reached the point of diminishing returns because most of the “large incumbent” jobs had already been the subject of validation research, and the remaining jobs had relatively few incumbents. Landy (1986), and more recently, Binning and Barrett (1989), characterized validation as the process of accumulating a variety of forms of judgmental and empirical evidence to support inferences regarding psychological constructs and operational measures of those constructs. The converging lines of evidence brought together in this study by the synthesisof data from externally conducted VG research, internal validation studies, test transportability, job component validity, and analysis of attributes requirements support inferences regarding the validity of cognitive ability tests for predicting training and job performance for company nonmanagement jobs. This study demonstrates the soundness and practicality of the advice that Landy and Binning and Barrett provided regarding validity models. Although this study does not fit neatly into any one of the three “boxes” (Landy, 1986) the Guidelines allow in supporting validation efforts, it is likely more defensible than if we had followed Guidefines prescriptions by rote. The interlinking systems of job families and test batteries described here and in Hoffman (1999) are also responsive to company needs regarding cost containment and quick implementation of staffing systems.",0
https://doi.org/10.1080/01621459.2013.779832,Mediation and Spillover Effects in Group-Randomized Trials: A Case Study of the 4Rs Educational Intervention,"Peer influence and social interactions can give rise to spillover effects in which the exposure of one individual may affect outcomes of other individuals. Even if the intervention under study occurs at the group or cluster level as in group-randomized trials, spillover effects can occur when the mediator of interest is measured at a lower level than the treatment. Evaluators who choose groups rather than individuals as experimental units in a randomized trial often anticipate that the desirable changes in targeted social behaviors will be reinforced through interference among individuals in a group exposed to the same treatment. In an empirical evaluation of the effect of a school-wide intervention on reducing individual students' depressive symptoms, schools in matched pairs were randomly assigned to the 4Rs intervention or the control condition. Class quality was hypothesized as an important mediator assessed at the classroom level. We reason that the quality of one classroom may affect outcomes of children in another classroom because children interact not simply with their classmates but also with those from other classes in the hallways or on the playground. In investigating the role of class quality as a mediator, failure to account for such spillover effects of one classroom on the outcomes of children in other classrooms can potentially result in bias and problems with interpretation. Using a counterfactual conceptualization of direct, indirect and spillover effects, we provide a framework that can accommodate issues of mediation and spillover effects in group randomized trials. We show that the total effect can be decomposed into a natural direct effect, a within-classroom mediated effect and a spillover mediated effect. We give identification conditions for each of the causal effects of interest and provide results on the consequences of ignoring ""interference"" or ""spillover effects"" when they are in fact present. Our modeling approach disentangles these effects. The analysis examines whether the 4Rs intervention has an effect on children's depressive symptoms through changing the quality of other classes as well as through changing the quality of a child's own class.",0
https://doi.org/10.1191/1740774505cn082oa,The use of random effects models to allow for clustering in individually randomized trials,"Background We describe different forms of clustering that may occur in individually randomized trials, where the observed outcomes for different individuals cannot be regarded as independent. We propose random effects models to allow for such clustering, across a range of contexts and trial designs, and investigate their effect on estimation and interpretation of the treatment effect. Methods We apply our proposed models to two individually randomized trials with potential for clustering, a trial of teleconsultation in hospital referral (the main outcome being offer of a further hospital appointment) and a trial of exercise therapy delivered by physiotherapists for low back pain (the outcome being a back pain score). Extensions to the methods include the possibility of explaining heterogeneity between clusters using cluster level characteristics and the potential dilution of cluster effects due to noncompliance. Results In the teleconsultation trial, the odds ratio was significant (1.52, 95% CI 1.27 to 1.82) when clustering was ignored, but smaller and nonsignificant (1.36, 95% CI 0.85 to 2.13) when clustering by hospital consultant was taken into account. The 95% range of estimated treatment effects across consultants was from 0.21 to 8.76. This variability was only partially explained by the specialty of the consultant. In the back pain trial, although there was an overall benefit of exercise (change of 20.51 points on the back pain score) and little evidence of clustering, the estimated treatment effects for different physiotherapists ranged from 21.26 to +0.26 points. Conclusions Clustering is an important issue in many individually randomized trials. Ignoring it can lead to underestimates of the uncertainty and too extreme P-values. Even when there is little apparent heterogeneity across clusters, it can still have a large impact on the estimation and interpretation of the treatment effect.",0
https://doi.org/10.1098/rspl.1897.0091,Mathematical contributions to the theory of evolution. IV. On the probable errors of frequency constants and on the influence of random selection on variation and correlation,"This memoir starts with a general theorem, by which the probable errors made in calculating the constants of any frequency distribution may be determined. It is shown that these probable errors form a correlated system approximately following the normal law of frequency, whatever be the nature of the original frequency distribution, i. e .,whether it be skew or normal. The importance of this result for the theory of evolution is then drawn attention to.",0
https://doi.org/10.1080/01402382.2014.929341,Social Inequality in Political Participation: The Dark Sides of Individualisation,"Has the participatory gap between social groups widened over the past decades? And if so, how can it be explained? Based on a re-analysis of 94 electoral surveys in eight Western European countries between 1956 and 2009, this article shows that the difference in national election turnout between the half of the population with the lowest level of education and the half with the highest has increased. It shows that individualisation – the decline of social integration and social control – is a major cause of this trend. In their electoral choices, citizens with fewer resources – in terms of education – rely more heavily on cues and social control of the social groups to which they belong. Once the ties to these groups loosen, these cues and mobilising norms are no longer as strong as they once were, resulting in an increasing abstention of the lower classes on Election Day. In contrast, citizens with abundant resources rely much less on cues and social control, and the process of individualisation impacts ...",0
https://doi.org/10.1002/sim.7612,A Bayesian confirmatory factor model for multivariate observations in the form of two-way tables of data,"Researchers collected multiple measurements on patients with schizophrenia and their relatives, as well as control subjects and their relatives, to study vulnerability factors for schizophrenics and their near relatives. Observations across individuals from the same family are correlated, and also the multiple outcome measures on the same individuals are correlated. Traditional data analyses model outcomes separately and thus do not provide information about the interrelationships among outcomes. We propose a novel Bayesian family factor model (BFFM), which extends the classical confirmatory factor analysis model to explain the correlations among observed variables using a combination of family-member and outcome factors. Traditional methods for fitting confirmatory factor analysis models, such as full-information maximum likelihood (FIML) estimation using quasi-Newton optimization (QNO), can have convergence problems and Heywood cases (lack of convergence) caused by empirical underidentification. In contrast, modern Bayesian Markov chain Monte Carlo handles these inference problems easily. Simulations compare the BFFM to FIML-QNO in settings where the true covariance matrix is identified, close to not identified, and not identified. For these settings, FIML-QNO fails to fit the data in 13%, 57%, and 85% of the cases, respectively, while MCMC provides stable estimates. When both methods successfully fit the data, estimates from the BFFM have smaller variances and comparable mean-squared errors. We illustrate the BFFM by analyzing data on data from schizophrenics and their family members.",0
https://doi.org/10.1007/bf02294778,Joint consistency of nonparametric item characteristic curve and ability estimation,The simultaneous and nonparametric estimation of latent abilities and item characteristic curves is considered. The asymptotic properties of ordinal ability estimation and kernel smoothed nonparametric item characteristic curve estimation are investigated under very general assumptions on the underlying item response theory model as both the test length and the sample size increase. A large deviation probability inequality is stated for ordinal ability estimation. The mean squared error of kernel smoothed item characteristic curve estimates is studied and a strong consistency result is obtained showing that the worst case error in the item characteristic curve estimates over all items and ability levels converges to zero with probability equal to one.,0
https://doi.org/10.1371/journal.pone.0096431,A Cognitive Model for Aggregating People's Rankings,"We develop a cognitive modeling approach, motivated by classic theories of knowledge representation and judgment from psychology, for combining people's rankings of items. The model makes simple assumptions about how individual differences in knowledge lead to observed ranking data in behavioral tasks. We implement the cognitive model as a Bayesian graphical model, and use computational sampling to infer an aggregate ranking and measures of the individual expertise. Applications of the model to 23 data sets, dealing with general knowledge and prediction tasks, show that the model performs well in producing an aggregate ranking that is often close to the ground truth and, as in the ""wisdom of the crowd"" effect, usually performs better than most of individuals. We also present some evidence that the model outperforms the traditional statistical Borda count method, and that the model is able to infer people's relative expertise surprisingly well without knowing the ground truth. We discuss the advantages of the cognitive modeling approach to combining ranking data, and in wisdom of the crowd research generally, as well as highlighting a number of potential directions for future model development.",0
https://doi.org/10.1002/j.2162-6057.2008.tb01298.x,A Longitudinal Analysis of Student Creativity Scripts,"In the present study we used bayesian latent growth modeling to asses the impact of a one-semester creativity course on the development of engineering students' creativity scripts. We compared a treatment (N = 52) and a control (N = 42) group with respect to individual differences in initial status and in rate of change. Results revealed that the development of creativity scripts followed a linear change over time, with the rate of change being higher for the treatment compared to the control group. Furthermore, substantial individual variability in the rate of creativity scripts gain across students was detected, where students with richer creativity scripts had a weaker rate of increase. The present exploratory effort lays the groundwork for further theoretical and empirical research on the effects of intervention programs for fostering creativity.",0
https://doi.org/10.1214/11-ba631,Mean Field Variational Bayes for Elaborate Distributions,"We develop strategies for mean eld variational Bayes approximate inference for Bayesian hierarchical models containing elaborate distributions. We loosely dene elaborate distributions to be those having more complicated forms compared with common distributions such as those in the Normal and Gamma families. Examples are Asymmetric Laplace, Skew Normal and Generalized Ex- treme Value distributions. Such models suer from the diculty that the param- eter updates do not admit closed form solutions. We circumvent this problem through a combination of (a) specially tailored auxiliary variables, (b) univariate quadrature schemes and (c) nite mixture approximations of troublesome den-",0
https://doi.org/10.1016/j.ress.2007.03.038,Spatial generalized linear mixed models of electric power outages due to hurricanes and ice storms,"Abstract This paper presents new statistical models that predict the number of hurricane- and ice storm-related electric power outages likely to occur in each 3 km×3 km grid cell in a region. The models are based on a large database of recent outages experienced by three major East Coast power companies in six hurricanes and eight ice storms. A spatial generalized linear mixed modeling (GLMM) approach was used in which spatial correlation is incorporated through random effects. Models were fitted using a composite likelihood approach and the covariance matrix was estimated empirically. A simulation study was conducted to test the model estimation procedure, and model training, validation, and testing were done to select the best models and assess their predictive power. The final hurricane model includes number of protective devices, maximum gust wind speed, hurricane indicator, and company indicator covariates. The final ice storm model includes number of protective devices, ice thickness, and ice storm indicator covariates. The models should be useful for power companies as they plan for future storms. The statistical modeling approach offers a new way to assess the reliability of electric power and other infrastructure systems in extreme events.",0
https://doi.org/10.5964/ejop.v7i4.163,Doing Bayesian Data Analysis: A Tutorial with R and BUGS,"Bayesian reasoning is a blessed relief to those who have always struggled with the idea that the probability of heads coming up in a supposedly fair coin flip is always 50%, even after a long series of coin flips has come up tails each time. According to Bayes, if a coin keeps coming up tails we should adjust our prior belief that the probability is 50% in the light of the posterior belief that the coin appears to be biased towards tails. John Kruschke’s book is a 600 page development of this Bayesian theme. The 23 chapters cover the basics of parameters, probability, Baye’s rule, the R and BUGS statistical programmes, the fundamentals applied to inferring a binomial proportion, and how all of this is applied to the generalized linear model. Kruschke has the rare ability amongst statistical textbook authors of writing very engagingly about knotty topics. For those who want to do what the title of the book suggests – learning to do Bayesian data analysis by learning programs languages R and BUGS – this book must be ideal.",0
https://doi.org/10.1016/0304-4076(94)90064-7,An exact likelihood analysis of the multinomial probit model,"Abstract We develop new methods for conducting a finite sample, likelihood-based analysis of the multinomial probit model. Using a variant of the Gibbs sampler, an algorithm is developed to draw from the exact posterior of the multinomial probit model with correlated errors. This approach avoids direct evaluation of the likelihood and, thus, avoids the problems associated with calculating choice probabilities which affect both the standard likelihood and method of simulated moments approaches. Both simulated and actual consumer panel data are used to fit six-dimensional choice models. We also develop methods for analyzing random coefficient and multiperiod probit models.",0
https://doi.org/10.1509/jmkr.39.1.87.18936,"Hierarchical Bayes versus Finite Mixture Conjoint Analysis Models: A Comparison of Fit, Prediction, and Partworth Recovery","A study conducted by Vriens, Wedel, and Wilms (1996) and published in Journal of Marketing Research found that finite mixture (FM) conjoint models had the best overall performance of nine conjoint segmentation methods in terms of fit, prediction, and parameter recovery. Since that study, hierarchical Bayes (HB) conjoint analysis methods have been proposed to estimate individual-level partworths and have received much attention in the marketing research literature. However, no study has compared the relative effectiveness of FM and HB conjoint analysis models in terms of fit, prediction, and parameter recovery. To conduct such a comparison, the authors employ the simulation methodology proposed by Vriens, Wedel, and Wilms with some modification. The authors estimate traditional individual-level conjoint models as well. The authors show that FM and HB models are equally effective in recovering individual-level parameters and predicting ratings of holdout profiles. Two surprising findings are that (1) HB performs well even when partworths come from a mixture of distributions and (2) FM produces good parameter estimates, even at the individual level. The authors show that both models are quite robust to violations of underlying assumptions and that traditional individual-level models overfit the data.",0
https://doi.org/10.1111/j.1745-3984.2001.tb01117.x,Item Analysis by the Hierarchical Generalized Linear Model,"The hierarchical generalized linear model (HGLM) is presented as an explicit, two-level formulation of a multilevel item response model. In this paper, it is shown that the HGLM is equivalent to the Rasch model and that, characteristic of the HGLM, person ability can be expressed in the form of random effects rather than parameters. The two-level item analysis model is presented as a latent regression model with person-characteristic variables. Furthermore, it is shown that the two-level HGLM model can be extended to a three-level latent regression model that permits investigation of the variation of students' performance across groups, such as is found in classrooms and schools, and of the interactive effect of person-and group-characteristic variables.",0
https://doi.org/10.1016/0169-2607(96)01723-3,MIXREG: a computer program for mixed-effects regression analysis with autocorrelated errors,"MIXREG is a program that provides estimates for a mixed-effects regression model (MRM) for normally-distributed response data including autocorrelated errors. This model can be used for analysis of unbalanced longitudinal data, where individuals may be measured at a different number of timepoints, or even at different timepoints. Autocorrelated errors of a general form or following an AR(1), MA(1), or ARMA(1,1) form are allowable. This model can also be used for analysis of clustered data, where the mixed-effects model assumes data within clusters are dependent. The degree of dependency is estimated jointly with estimates of the usual model parameters, thus adjusting for clustering. MIXREG uses maximum marginal likelihood estimation, utilizing both the EM algorithm and a Fisher-scoring solution. For the scoring solution, the covariance matrix of the random effects is expressed in its Gaussian decomposition, and the diagonal matrix reparameterized using the exponential transformation. Estimation of the individual random effects is accomplished using an empirical Bayes approach. Examples illustrating usage and features of MIXREG are provided.",0
https://doi.org/10.1002/1099-0992(200007/08)30:4<533::aid-ejsp6>3.0.co;2-f,Self-schemas and the theory of planned behaviour,"This paper argues that empirical, conceptual, and statistical difficulties characterise previous demonstrations that self-schemas moderate the relationship between intentions and behaviour. A longitudinal study (n=163) was designed to overcome limitations of previous research. Theory of planned behaviour variables, past behaviour, and self-schemas were assessed in relation to exercise. Behaviour was followed up two weeks later. Findings showed that self-schemas moderated the intention–behaviour relation such that schematics were more likely to enact their intentions to exercise compared to unschematics. Evidence suggested that the importance dimension of self-schema measures was responsible for the moderator effect. Self-schemas were also associated with improved prediction of behavioural intentions after controlling for the other predictors. Copyright © 2000 John Wiley & Sons, Ltd.",0
https://doi.org/10.2307/2971733,Matching As An Econometric Evaluation Estimator: Evidence from Evaluating a Job Training Programme,"This paper considers whether it is possible to devise a nonexperimental procedure for evaluating a prototypical job training programme. Using rich nonexperimental data, we examine the performance of a two-stage evaluation methodology that (a) estimates the probability that a person participates in a programme and (b) uses the estimated probability in extensions of the classical method of matching. We decompose the conventional measure of programme evaluation bias into several components and find that bias due to selection on unobservables, commonly called selection bias in econometrics, is empirically less important than other components, although it is still a sizeable fraction of the estimated programme impact. Matching methods applied to comparison groups located in the same labour markets as participants and administered the same questionnaire eliminate much of the bias as conventionally measured, but the remaining bias is a considerable fraction of experimentally-determined programme impact estimates. We test and reject the identifying assumptions that justify the classical method of matching. We present a nonparametric conditional difference-in-differences extension of the method of matching that is consistent with the classical index-sufficient sample selection model and is not rejected by our tests of identifying assumptions. This estimator is effective in eliminating bias, especially when it is due to temporally-invariant omitted variables.",0
https://doi.org/10.1002/jrsm.1103,Bayesian network meta-analysis for unordered categorical outcomes with incomplete data,"We develop a Bayesian multinomial network meta-analysis model for unordered (nominal) categorical outcomes that allows for partially observed data in which exact event counts may not be known for each category. This model properly accounts for correlations of counts in mutually exclusive categories and enables proper comparison and ranking of treatment effects across multiple treatments and multiple outcome categories. We apply the model to analyze 17 trials, each of which compares two of three treatments (high and low dose statins and standard care/control) for three outcomes for which data are complete: cardiovascular death, non-cardiovascular death and no death. We also analyze the cardiovascular death category divided into the three subcategories (coronary heart disease, stroke and other cardiovascular diseases) that are not completely observed. The multinomial and network representations show that high dose statins are effective in reducing the risk of cardiovascular disease.",0
https://doi.org/10.1177/0146621610370152,A Method for the Comparison of Item Selection Rules in Computerized Adaptive Testing,"In a typical study comparing the relative efficiency of two item selection rules in computerized adaptive testing, the common result is that they simultaneously differ in accuracy and security, making it difficult to reach a conclusion on which is the more appropriate rule. This study proposes a strategy to conduct a global comparison of two or more selection rules. A plot showing the performance of each selection rule for several maximum exposure rates is obtained and the whole plot is compared with other rule plots. The strategy was applied in a simulation study with fixed-length CATs for the comparison of six item selection rules: the point Fisher information, Fisher information weighted by likelihood, Kullback-Leibler weighted by likelihood, maximum information stratification with blocking, progressive and proportional methods. Our results show that there is no optimal rule for any overlap value or root mean square error (RMSE). The fact that a rule, for a given level of overlap, has lower RMSE than another does not imply that this pattern holds for another overlap rate. A fair comparison of the rules requires extensive manipulation of the maximum exposure rates. The best methods were the Kullback-Leibler weighted by likelihood, the proportional method, and the maximum information stratification method with blocking.",0
https://doi.org/10.1186/s12874-015-0007-0,Network meta-analysis combining individual patient and aggregate data from a mixture of study designs with an application to pulmonary arterial hypertension,"BackgroundNetwork meta-analysis (NMA) is a methodology for indirectly comparing, and strengthening direct comparisons of two or more treatments for the management of disease by combining evidence from multiple studies. It is sometimes not possible to perform treatment comparisons as evidence networks restricted to randomized controlled trials (RCTs) may be disconnected. We propose a Bayesian NMA model that allows to include single-arm, before-and-after, observational studies to complete these disconnected networks. We illustrate the method with an indirect comparison of treatments for pulmonary arterial hypertension (PAH).MethodsOur method uses a random effects model for placebo improvements to include single-arm observational studies into a general NMA. Building on recent research for binary outcomes, we develop a covariate-adjusted continuous-outcome NMA model that combines individual patient data (IPD) and aggregate data from two-arm RCTs with the single-arm observational studies. We apply this model to a complex comparison of therapies for PAH combining IPD from a phase-III RCT of imatinib as add-on therapy for PAH and aggregate data from RCTs and single-arm observational studies, both identified by a systematic review.ResultsThrough the inclusion of observational studies, our method allowed the comparison of imatinib as add-on therapy for PAH with other treatments. This comparison had not been previously possible due to the limited RCT evidence available. However, the credible intervals of our posterior estimates were wide so the overall results were inconclusive. The comparison should be treated as exploratory and should not be used to guide clinical practice.ConclusionsOur method for the inclusion of single-arm observational studies allows the performance of indirect comparisons that had previously not been possible due to incomplete networks composed solely of available RCTs. We also built on many recent innovations to enable researchers to use both aggregate data and IPD. This method could be used in similar situations where treatment comparisons have not been possible due to restrictions to RCT evidence and where a mixture of aggregate data and IPD are available.",0
https://doi.org/10.3758/bf03194552,"Measuring, estimating, and understanding the psychometric function: A commentary","The psychometric function, relating the subject's response to the physical stimulus, is fundamental to psychophysics. This paper examines various psychometric function topics, many inspired by this special symposium issue of Perception & Psychophysics: What are the relative merits of objective yes/no versus forced choice tasks (including threshold variance)? What are the relative merits of adaptive versus constant stimuli methods? What are the relative merits of likelihood versus up-down staircase adaptive methods? Is 2AFC free of substantial bias? Is there no efficient adaptive method for objective yes/no tasks? Should adaptive methods aim for 90% correct? Can adding more responses to forced choice and objective yes/no tasks reduce the threshold variance? What is the best way to deal with lapses? How is the Weibull function intimately related to the d' function? What causes bias in the likelihood goodness-of-fit? What causes bias in slope estimates from adaptive methods? How good are nonparametric methods for estimating psychometric function parameters? Of what value is the psychometric function slope? How are various psychometric functions related to each other? The resolution of many of these issues is surprising.",0
,An analysis of the costs of treating schizophrenia in Spain: a hierarchical Bayesian approach.,"Health care decisions should incorporate cost of illness and treatment data, particularly for disorders such as schizophrenia with a high morbidity rate and a disproportionately low allocation of resources. Previous cost of illness analyses may have disregarded geographical aspects relevant for resource consumption and unit cost calculation.To compare the utilisation of resources and the care costs of schizophrenic patients in four mental-health districts in Spain (in Madrid, Catalonia, Navarra and Andalusia), and to analyse factors that determine the costs and the differences between areas.A treated prevalence bottom-up three year follow-up design was used for obtaining data concerning socio-demography, clinical evolution and the utilisation of services. 1997 reference prices were updated for years 1998-2000 in euros. We propose two different scenarios, varying in the prices applied. In the first (Scenario 0) the reference prices are those obtained for a single geographic area, and so the cost variations are only due to differences in the use of resources. In the second situation (Scenario 1), we analyse the variations in resource utilisation at different levels, using the prices applicable to each healthcare area. Bayesian hierarchical models are used to discuss the factors that determine such costs and the differences between geographic areas.In scenario 0, the estimated mean cost was 4918.948 euros for the first year. In scenario 1 the highest cost was in Gava (Catalonia) and the lowest in Loja (Andalusia). Mean costs were respectively 4547.24 and 2473.98 euros. With respect to the evolution of costs over time, we observed an increase during the second year and a reduction during the third year. Geographical differences appeared in follow-up costs. The variables related to lower treatment costs were: residence in the family household, higher patient age and being in work. On the contrary, the number of relapses is directly related to higher treatment costs. No differences were observed between health areas concerning resource utilisation.Calculating the costs of a given disease involves two principal factors: the resource utilisation and the prices. In most studies, emphasis is placed on the analysis of resource utilisation. Other evaluations, however, have recognized the implications of incorporating different prices into the final results. In this study we show both scenarios. The factors that determine the cost of schizophrenia for the Spanish case are similar to the factors encountered in studies carried out in other countries.Treatment costs may be reduced by the prevention of psychotic symptoms and relapse.The use of the same price data in multicentre studies may not be realistic. More effort should be made to obtain price data from all the centres or countries participating in a study. In the present study, only direct healthcare and social costs have been included. Future research should consider informal and indirect costs.",0
https://doi.org/10.1093/aje/kwm167,Joint Effects of the N-Acetyltransferase 1 and 2 (NAT1 and NAT2) Genes and Smoking on Bladder Carcinogenesis: A Literature-based Systematic HuGE Review and Evidence Synthesis,"Bladder cancer is an increasingly important international public health problem, with over 330,000 new cases being diagnosed each year worldwide. In a systematic review and evidence synthesis, the authors investigated the joint effects of the N-acetyltransferase genes NAT1 and NAT2 and cigarette smoking on bladder carcinogenesis. Studies were identified through an exhaustive search of multiple electronic databases and reference lists and through direct contact with study authors and experts. Random-effects meta-analysis was used within a Bayesian framework to investigate individual effects of NAT1 and NAT2 acetylation status on bladder cancer risk, while a novel approach was used to investigate joint effects of these two genes with cigarette smoking. An increased risk of bladder cancer was found in NAT2 slow acetylators (odds ratio = 1.46, 95% credible interval (CI): 1.26, 1.68) but not in NAT1 fast acetylators (odds ratio = 1.01, 95% CI: 0.86, 1.22). The joint effects in the highest risk category (NAT2 slow acetylator, NAT1 fast acetylator, and current or ever cigarette smoking) as compared with the reference category (NAT2 fast acetylator, NAT1 slow acetylator, and never smoking) were associated with an odds ratio of 2.73 (95% CI: 1.70, 4.31). The importance of considering joint effects between genetic and environmental factors in the etiology of common complex diseases is underlined.",0
https://doi.org/10.4324/9780203864746,Statistical Methods for Modeling Human Dynamics,,0
https://doi.org/10.1080/09540129850124460,"The effects of establishment practices, knowledge and attitudes on condom use among Filipina sex workers","The findings for a baseline assessment for a community-based HIV/STD prevention intervention for commercial sex workers (CSWs) and managers of the establishments that employ them in the Philippines is presented in this study. CSW knowledge, attitudes, behaviours and establishment policies concerning HIV prevention were assessed. Baseline assessments are part of an iterative process that will be used to modify the planned intervention. The preliminary findings point to the importance of an intervention that stresses changes in establishment policies and expectations as a means of reducing risk behaviours associated with HIV/STD transmission.As of May 1996, 1025 HIV-infected individuals had been identified in the Philippines, 260 of whom had AIDS. However, in the Philippines' total population of 65 million, approximately 18,000 adults are estimated to carry HIV. Unprotected sex and multiple partners place prostitutes at risk of contracting and transmitting HIV and other STDs. There are 65,000 registered prostitutes and 200,000 or more freelance sex workers in the Philippines. 1394 registered prostitutes, of whom 98.6% were female, were recruited from commercial sex work establishments in 4 sites about 400 miles from Manila for participation in a study assessing prostitutes' knowledge, attitudes, behaviors, and establishment policies concerning HIV prevention. The participants were aged 15-54 of mean age 23.5 years. Establishment policies and practices appear to be more important than prostitutes' knowledge of HIV transmission or their attitudes toward condoms. Any intervention to prevent HIV/STD among prostitutes in the Philippines should therefore consider and possibly target sex work establishments' policies.",0
https://doi.org/10.1111/j.1541-0420.2007.00806.x,Penalized Item Response Theory Models: Application to Epigenetic Alterations in Bladder Cancer,"Increasingly used in health-related applications, latent variable models provide an appealing framework for handling high-dimensional exposure and response data. Item response theory (IRT) models, which have gained widespread popularity, were originally developed for use in the context of educational testing, where extremely large sample sizes permitted the estimation of a moderate-to-large number of parameters. In the context of public health applications, smaller sample sizes preclude large parameter spaces. Therefore, we propose a penalized likelihood approach to reduce mean square error and improve numerical stability. We present a continuous family of models, indexed by a tuning parameter, that range between the Rasch model and the IRT model. The tuning parameter is selected by cross validation or approximations such as Akaike Information Criterion. While our approach can be placed easily in a Bayesian context, we find that our frequentist approach is more computationally efficient. We demonstrate our methodology on a study of methylation silencing of gene expression in bladder tumors. We obtain similar results using both frequentist and Bayesian approaches, although the frequentist approach is less computationally demanding. In particular, we find high correlation of methylation silencing among 16 loci in bladder tumors, that methylation is associated with smoking and also with patient survival.",0
https://doi.org/10.1080/19345747.2011.618213,Why We (Usually) Don't Have to Worry About Multiple Comparisons,"Abstract Applied researchers often find themselves making statistical inferences in settings that would seem to require multiple comparisons adjustments. We challenge the Type I error paradigm that underlies these corrections. Moreover we posit that the problem of multiple comparisons can disappear entirely when viewed from a hierarchical Bayesian perspective. We propose building multilevel models in the settings where multiple comparisons arise. Multilevel models perform partial pooling (shifting estimates toward each other), whereas classical procedures typically keep the centers of intervals stationary, adjusting for multiple comparisons by making the intervals wider (or, equivalently, adjusting the p values corresponding to intervals of fixed width). Thus, multilevel models address the multiple comparisons problem and also yield more efficient estimates, especially in settings with low group-level variation, which is where multiple comparisons are a particular concern.",0
https://doi.org/10.1145/2422105.2422109,Sound sample detection and numerosity estimation using auditory display,"This article investigates the effect of various design parameters of auditory information display on user performance in two basic information retrieval tasks. We conducted a user test with 22 participants in which sets of sound samples were presented. In the first task, the test participants were asked to detect a given sample among a set of samples. In the second task, the test participants were asked to estimate the relative number of instances of a given sample in two sets of samples. We found that the stimulus onset asynchrony (SOA) of the sound samples had a significant effect on user performance in both tasks. For the sample detection task, the average error rate was about 10% with an SOA of 100 ms. For the numerosity estimation task, an SOA of at least 200 ms was necessary to yield average error rates lower than 30%. Other parameters, including the samples' sound type (synthesized speech or earcons) and spatial quality (multichannel loudspeaker or diotic headphone playback), had no substantial effect on user performance. These results suggest that diotic, or indeed monophonic, playback with appropriately chosen SOA may be sufficient in practical applications for users to perform the given information retrieval tasks, if information about the sample location is not relevant. If location information was provided through spatial playback of the samples, test subjects were able to simultaneously detect and localize a sample with reasonable accuracy.",0
https://doi.org/10.1177/0146621606290248,A Comparison of Methods for Nonparametric Estimation of Item Characteristic Curves for Binary Items,"This study compares the performance of three nonparametric item characteristic curve (ICC) estimation procedures: isotonic regression, smoothed isotonic regression, and kernel smoothing. Smoothed isotonic regression, employed along with an appropriate kernel function, provides better estimates and also satisfies the assumption of strict monotonicity. As the number of items and the sample size increase, the kernel smoothing and smoothed isotonic regression ICC estimation procedures yield similar results across all conditions.",0
https://doi.org/10.1093/biomet/91.1.1,Bayesian correlation estimation,"We propose prior probability models for variance-covariance matrices in order to address two important issues. First, the models allow a researcher to represent substantive prior information about the strength of correlations among a set of variables. Secondly, even in the absence of such information, the increased flexibility of the models mitigates dependence on strict parametric assumptions in standard prior models. For example, the model allows a posteriori different levels of uncertainty about correlations among different subsets of variables. We achieve this by including a clustering mechanism in the prior probability model. Clustering is with respect to variables and pairs of variables. Our approach leads to shrinkage towards a mixture structure implied by the clustering. We discuss appropriate posterior simulation schemes to implement posterior inference in the proposed models, including the evaluation of normalising constants that are functions of parameters of interest. The normalising constants result from the restriction that the correlation matrix be positive definite. We discuss examples based on simulated data, a stock return dataset and a population genetics dataset.",0
https://doi.org/10.1109/health.2011.6026776,Semantic models for ranking medical images using Dirichlet non-parametric mixture models,"With recent advances in diagnostic medical imaging, huge quantities of medical images are produced and stored in digital image repositories. While these repositories are difficult to be analyzed manually by medical experts, they can be evaluated using computer-based methods to enrich the process of decision making. For example, query by image methods can be used by medical experts for differential diagnosis by displaying previously evaluated cases that contain similar visual patterns. Also, less experienced practitioners can benefit from query-by-semantic methods in training processes especially for difficult-to-interpret cases with multiple pathologies. In this article we develop a methodology for ranking medical images based on Dirichlet process nonparametric distributions. Our approach uses natural groupings of images in a generated feature space to evaluate associative semantic mappings. Relevant semantic mappings are then used to generate additive computer models of semantic understanding of visual patterns found in images. We evaluate the performance of our method using mean average precision and precision-recall charts.",0
https://doi.org/10.1167/11.12.1,Perceptual compression of visual space during eye-head gaze shifts,"In primates, inspection of a visual scene is typically interrupted by frequent gaze shifts, occurring at an average rate of three to five times per second. Perceptually, these gaze shifts are accompanied by a compression of visual space toward the saccade target, which may be attributed to an oculomotor signal that transiently influences visual processing. While previous studies of compression have focused exclusively on saccadic eye movements made with the head artificially immobilized, many brain structures involved in saccade generation also encode combined eye-head gaze shifts. Thus, in order to understand the interaction between gaze motor and visual signals, we studied perception during eye-head gaze shifts and found a powerful compression of visual space that was spatially directed toward the intended gaze (and not the eye movement) target location. This perceptual compression was nearly constant in duration across gaze shift amplitudes, suggesting that the signal that triggers compression is largely independent of the size and kinematics of the gaze shift. The spatial pattern of results could be captured by a model that involves interactions, on a logarithmic map of visual space, between two loci of neural activity that encode the gaze shift vector and visual stimulus position relative to the fovea.",0
https://doi.org/10.1093/pan/mpi027,Introduction to the Special Issue,"The use of multilevel models—models in which lower-level (“micro”) units are nested within higher-level (“macro”) units—has blossomed recently in political science. Possible relationships in such models include macro variables influencing macro variables; micro variables influencing micro variables; macro variables influencing micro variables, and vice versa; and often most interestingly, micro-micro relationships varying interactively with macro variables. Most work in political science has drawn on the useful introductions of Raudenbush and Bryk (2002), Western (1998), and Steenbergen and Jones (2002). We refer readers to good general introductions/reviews of multi-level modeling in the articles in this issue by Bowers and Drake and by Franzese.",0
https://doi.org/10.1177/0013164417709314,Evaluating Model Fit in Bayesian Confirmatory Factor Analysis With Large Samples: Simulation Study Introducing the BRMSEA,"Bayesian confirmatory factor analysis (CFA) offers an alternative to frequentist CFA based on, for example, maximum likelihood estimation for the assessment of reliability and validity of educational and psychological measures. For increasing sample sizes, however, the applicability of current fit statistics evaluating model fit within Bayesian CFA is limited. We propose, therefore, a Bayesian variant of the root mean square error of approximation (RMSEA), the BRMSEA. A simulation study was performed with variations in model misspecification, factor loading magnitude, number of indicators, number of factors, and sample size. This showed that the 90% posterior probability interval of the BRMSEA is valid for evaluating model fit in large samples ( N≥ 1,000), using cutoff values for the lower (&lt;.05) and upper limit (&lt;.08) as guideline. An empirical illustration further shows the advantage of the BRMSEA in large sample Bayesian CFA models. In conclusion, it can be stated that the BRMSEA is well suited to evaluate model fit in large sample Bayesian CFA models by taking sample size and model complexity into account.",0
https://doi.org/10.1080/13636820.2012.727849,Value added as an indicator of educational effectiveness in Dutch senior secondary vocational education,"This study investigates the possibilities of estimating value added as a performance indicator in senior secondary vocational education. Value added is interesting in this context because it is considered as a reliable tool for comparing the effectiveness of educational institutions. Although value added indicators have been developed since the 1980s for both primary and secondary educations, the research on school effectiveness has largely neglected vocational education because of its complexity. For estimating value added in this study, data concerning almost 90,000 students in Dutch senior secondary vocational education are used. Factors such as ethnicity, living in problematic neighbourhoods and students’ prior educational attainment appear to be significant predictors of student outcomes. The results indicate considerable differences in the effectiveness among clusters of training programmes, whereas there are hardly any differences between the educational institutions. Of the total variance among th...",0
https://doi.org/10.1016/j.jad.2009.11.010,Unveiling patterns of affective responses in daily life may improve outcome prediction in depression: A momentary assessment study,"Daily life affective responses are closely linked to vulnerability and resilience in depression. Prediction of future clinical course may be improved if information on daily life emotional response patterns is taken into account.Female subjects with a history of major depression (n=83), recruited from a population twin register, participated in a longitudinal study using momentary assessment technology with 4 follow-up measurements. The effect of baseline daily life emotional response patterns (affect variability, stress-sensitivity and reward experience) on follow-up depressive symptomatology was examined.Both reward experience (B=-0.30, p=0.001) and negative affect variability (B=0.46, p=0.001) predicted future negative affective symptoms independent of all other dynamic emotional patterns and conventional predictors.Daily life information on dynamic emotional patterns adds to the prediction of future clinical course, independent of severity of symptoms and neuroticism score. Better prediction of course may improve decision-making regarding quantitative and qualitative aspects of treatment.",0
https://doi.org/10.3726/978-3-653-04521-5,Sociologies of Formality and Informality,status: publishe,0
https://doi.org/10.1177/1471082x0800900301,Multilevel models with multivariate mixed response types,"We build upon the existing literature to formulate a class of models for multivariate mixtures of Gaussian, ordered or unordered categorical responses and continuous distributions that are not Gaussian, each of which can be defined at any level of a multilevel data hierarchy. We describe a Markov chain Monte Carlo algorithm for fitting such models. We show how this unifies a number of disparate problems, including partially observed data and missing data in generalized linear modelling. The two-level model is considered in detail with worked examples of applications to a prediction problem and to multiple imputation for missing data. We conclude with a discussion outlining possible extensions and connections in the literature. Software for estimating the models is freely available.",0
https://doi.org/10.1007/s10928-012-9263-3,Combining patient-level and summary-level data for Alzheimer’s disease modeling and simulation: a beta regression meta-analysis,"Our objective was to develop a beta regression (BR) model to describe the longitudinal progression of the 11 item Alzheimer's disease (AD) assessment scale cognitive subscale (ADAS-cog) in AD patients in both natural history and randomized clinical trial settings, utilizing both individual patient and summary level literature data. Patient data from the coalition against major diseases database (3,223 patients), the Alzheimer's disease neruroimaging initiative study database (186 patients), and summary data from 73 literature references (representing 17,235 patients) were fit to a BR drug-disease-trial model. Treatment effects for currently available acetyl cholinesterase inhibitors, longitudinal changes in disease severity, dropout rate, placebo effect, and factors influencing these parameters were estimated in the model. Based on predictive checks and external validation, an adequate BR meta-analysis model for ADAS-cog using both summary-level and patient-level data was developed. Baseline ADAS-cog was estimated from baseline MMSE score. Disease progression was dependent on time, ApoE4 status, age, and gender. Study drop out was a function of time, baseline age, and baseline MMSE. The use of the BR constrained simulations to the 0-70 range of the ADAS-cog, even when residuals were incorporated. The model allows for simultaneous fitting of summary and patient level data, allowing for integration of all information available. A further advantage of the BR model is that i t constrains values to the range of the original instrument for simulation purposes, in contrast to methodologies that provide appropriate constraints only for conditional expectations. Ã‚Â© Springer Science+Business Media, LLC 2012.",0
https://doi.org/10.2307/2531248,Longitudinal Data Analysis for Discrete and Continuous Outcomes,"Longitudinal data sets are comprised of repeated observations of an outcome and a set of covariates for each of many subjects. One objective of statistical analysis is to describe the marginal expectation of the outcome variable as a function of the covariates while accounting for the correlation among the repeated observations for a given subject. This paper proposes a unifying approach to such analysis for a variety of discrete and continuous outcomes. A class of generalized estimating equations (GEEs) for the regression parameters is proposed. The equations are extensions of those used in quasi-likelihood (Wedderburn, 1974, Biometrika 61, 439-447) methods. The GEEs have solutions which are consistent and asymptotically Gaussian even when the time dependence is misspecified as we often expect. A consistent variance estimate is presented. We illustrate the use of the GEE approach with longitudinal data from a study of the effect of mothers' stress on children's morbidity.",0
https://doi.org/10.1007/bf02294400,A general model for the analysis of multilevel data,"A general model is developed for the analysis of multivariate multilevel data structures. Special cases of the model include repeated measures designs, multiple matrix samples, multilevel latent variable models, multiple time series, and variance and covariance component models.",0
https://doi.org/10.1093/carcin/bgq063,"Wheel running, skeletal muscle aerobic capacity and 1-methyl-1-nitrosourea induced mammary carcinogenesis in the rat","Emerging evidence indicates that intrinsic differences and induced changes in aerobic capacity are probably to play a critical role in the development of chronic diseases like cancer. This study was initiated: (i) to determine how citrate synthase activity, which is routinely used as a marker of aerobic capacity and mitochondrial density in skeletal muscle, was affected by voluntary running on either a motorized activity wheel or a non-motorized free wheel and (ii) to investigate the association between aerobic capacity and the carcinogenic response induced in the mammary gland by intraperitoneal injection of 1-methyl-1-nitrosurea. Overall, wheel running reduced cancer incidence (96 versus 72%, P = 0.0006) and the number of cancers per animal (2.84 versus 1.78, P < 0.0001) and induced citrate synthase activity (276 versus 353 U/mg, P < 0.0001, sedentary control versus wheel running,respectively). Both motorized and free wheel running increased citrate synthase activity (373 +/- 24, 329 +/- 11 and 276 +/- 9 U/mg protein, P < 0.0001) and reduced the average number of cancers per rat (2.84, 1.96 and 1.63, P < 0.01), sedentary control, free wheel and motorized wheel, respectively. However, regression analyses failed to provide evidence of a significant association between citrate synthase activity and either cancer incidence or cancer multiplicity. Citrate synthase activity is a single measure in a complex pathway that determines aerobic capacity. The multifaceted nature of intrinsic and inducible aerobic capacity limits the usefulness of citrate synthase activity alone in elucidating the relationship between aerobic capacity and the carcinogenic response.",0
https://doi.org/10.1037/0021-9010.75.3.227,Operationalization of goal difficulty as a moderator of the goal difficulty-performance relationship.,"Examined the research studies cumulated in recent quantitative reviews of the relationship between goal difficulty and performance to determine how goal difficulty has been operationalized. 4 categories (assigned goal level, self-set goal level, performance improvement, and difficulty perceptions) of operationalization were discovered, and the operationalization of goal difficulty was tested as a moderator of the relationship between goal difficulty and performance",0
https://doi.org/10.1186/1471-2288-12-64,Systematic review of methods used in meta-analyses where a primary outcome is an adverse or unintended event,"Abstract Background Adverse consequences of medical interventions are a source of concern, but clinical trials may lack power to detect elevated rates of such events, while observational studies have inherent limitations. Meta-analysis allows the combination of individual studies, which can increase power and provide stronger evidence relating to adverse events. However, meta-analysis of adverse events has associated methodological challenges. The aim of this study was to systematically identify and review the methodology used in meta-analyses where a primary outcome is an adverse or unintended event, following a therapeutic intervention. Methods Using a collection of reviews identified previously, 166 references including a meta-analysis were selected for review. At least one of the primary outcomes in each review was an adverse or unintended event. The nature of the intervention, source of funding, number of individual meta-analyses performed, number of primary studies included in the review, and use of meta-analytic methods were all recorded. Specific areas of interest relating to the methods used included the choice of outcome metric, methods of dealing with sparse events, heterogeneity, publication bias and use of individual patient data. Results The 166 included reviews were published between 1994 and 2006. Interventions included drugs and surgery among other interventions. Many of the references being reviewed included multiple meta-analyses with 44.6% (74/166) including more than ten. Randomised trials only were included in 42.2% of meta-analyses (70/166), observational studies only in 33.7% (56/166) and a mix of observational studies and trials in 15.7% (26/166). Sparse data, in the form of zero events in one or both arms where the outcome was a count of events, was found in 64 reviews of two-arm studies, of which 41 (64.1%) had zero events in both arms. Conclusions Meta-analyses of adverse events data are common and useful in terms of increasing the power to detect an association with an intervention, especially when the events are infrequent. However, with regard to existing meta-analyses, a wide variety of different methods have been employed, often with no evident rationale for using a particular approach. More specifically, the approach to dealing with zero events varies, and guidelines on this issue would be desirable.",0
https://doi.org/10.1177/0020852314563899,"Extrinsic motivation, PSM and labour market characteristics: a multilevel model of public sector employment preference in 26 countries","Research findings have been contradictory with respect to the determinants of why people choose a public sector job. In this article we use an internationally comparative design with data from 26 countries to explain public sector employment preference. The study shows that on the individual level, public service motivation and extrinsic motivation are both important drivers for this preference. Intrinsic motivation, in turn, is negatively related to people’s inclination to work for the public sector. Moreover, having a lower income and lower education is associated with a greater preference for public sector employment. This suggests that working for the public sector is seen as a good and safe career option. Our results furthermore show that variation in this preference can only partly be explained by country differences. Nevertheless, in countries with a career- rather than position-based system of public employment, people are more likely to prefer public employment. Points for practitioners Attracting the best and brightest to work for the public sector requires an insight into why people prefer public over private sector employment. This article looks at what makes people prefer public sector employment in 26 countries. Findings reveal that public service motivation (helping other people, being useful to society) and extrinsic motives (job security, a high income, opportunities for advancement) play an important role in this preference. Still, there are considerable differences between countries. In countries with a career-based system of public employment, working in the public sector is seen as more attractive.",0
https://doi.org/10.1002/sim.1091,Approaches to heterogeneity in meta-analysis,"This paper reviews publications from January 1999 to March 2001 on reproductive health topics that were self-identified as meta-analysis or were indexed as meta-analysis in MEDLINE. It sought to assess whether tests of statistical heterogeneity were done, whether the results were reported, and how a finding of significance for a test of statistical heterogeneity was handled and the results interpreted. The review identified some concerns. Tests of statistical heterogeneity were not done universally even though virtually all writers on the topic emphasize their importance. Even when done, results of these tests were not universally reported. Although the consensus appears to be that heterogeneity tests are conservative for meta-analysis of studies and a probability value of 0.10 is preferred, many meta-analyses used the conventional value of 0.05 without providing a reason. The rationale for the choice of a random or fixed effects model was not generally evident. The review also provided some positive models and some recommendations for assessing, reporting and exploring heterogeneity are made considering these models and the published recommendations of experts.",0
https://doi.org/10.1002/9781119942986,Evidence Synthesis for Decision Making in Healthcare,"In the evaluation of healthcare, rigorous methods of quantitative assessment are necessary to establish interventions that are both effective and cost-effective. Usually a single study will not fully address these issues and it is desirable to synthesize evidence from multiple sources. This book aims to provide a practical guide to evidence synthesis for the purpose of decision making, starting with a simple single parameter model, where all studies estimate the same quantity (pairwise meta-analysis) and progressing to more complex multi-parameter structures (including meta-regression, mixed treatment comparisons, Markov models of disease progression, and epidemiology models). A comprehensive, coherent framework is adopted and estimated using Bayesian methods. Key features: A coherent approach to evidence synthesis from multiple sources. Focus is given to Bayesian methods for evidence synthesis that can be integrated within cost-effectiveness analyses in a probabilistic framework using Markov Chain Monte Carlo simulation. Provides methods to statistically combine evidence from a range of evidence structures. Emphasizes the importance of model critique and checking for evidence consistency. Presents numerous worked examples, exercises and solutions drawn from a variety of medical disciplines throughout the book. WinBUGS code is provided for all examples. Evidence Synthesis for Decision Making in Healthcare is intended for health economists, decision modelers, statisticians and others involved in evidence synthesis, health technology assessment, and economic evaluation of health technologies. Ã‚Â© 2012 John Wiley & Sons, Ltd. All rights reserved.",0
https://doi.org/10.1177/0735633115620432,Preservice Teachers and Self-Assessing Digital Competence,"This study compares matched surveys of subjective self-assessment and objective assessment on seven domains of digital competence for preservice teachers at a large Southwest public university. The results, consistent with earlier studies, confirm that the participating preservice teachers inaccurately self-assessed their digital competence. The study concluded that subjective self-assessment lacks appropriate validity and is not an accurate stand-alone predictor of digital competence among preservice teachers. However, if considered in conjunction with other means, self-assessment may prove to be useful for preservice teachers to aid in their reflection of their competence, skills, and knowledge and to aid them in adjusting their perceptions and attitudes regarding technology throughout their professional practice. In addition, self-assessment in conjunction with other means may assist teacher educators in providing opportunities to improve the competence in teacher training programs.",0
https://doi.org/10.2307/2531694,Estimation and comparison of Changes in the Presence of Informative Right Censoring: Conditional Linear Model,"A general linear regression model for the usual least squares estimated rate of change (slope) on censoring time is described as an approximation to account for informative right censoring in estimating and comparing changes of a continuous variable in two groups. Two noniterative estimators for the group slope means, the linear minimum variance unbiased (LMVUB) estimator and the linear minimum mean squared error (LMMSE) estimator, are proposed under this conditional model. In realistic situations, we illustrate that the LMVUB and LMMSE estimators, derived under a simple linear regression model, are quite competitive compared to the pseudo maximum likelihood estimator (PMLE) derived by modeling the censoring probabilities. Generalizations to polynomial response curves and general linear models are also described.",0
https://doi.org/10.1177/0011392115589599,"Recent development of propensity score methods in observational studies: Multi-categorical treatment, causal mediation, and heterogeneity","This article reviews and comments on three major expansions of propensity score methods in recent decades. First, how to use generalized propensity scores to tackle multi-categorical or continuous treatment variables is shown in procedures of propensity score regression adjustment and propensity score weighting. Second, the counterfactual framework of causal inference in the analysis of mediation mechanisms is reviewed and the decomposition of the causal relationship between variables into causal direct effects and causal indirect effects is illustrated. Third, the heterogeneous treatment effect across the distribution of propensity score values is discussed in the framework of the stratification-multilevel model. For each methodological breakthrough, this article comments on potential issues which deserve serious attention in the practical application of these methods.",0
https://doi.org/10.1086/209277,State versus Action Orientation and the Theory of Reasoned Action: An Application to Coupon Usage,"This article investigates how the individual difference variable of state versus action orientation moderates the pattern of relationships among constructs in the theory of reasoned action. State orientation refers to a low capacity for the enactment of action-related mental structures, whereas action orientation refers to a high capacity for this type of enactment. A field study was conducted in the context of consumers' self-reported usage of coupons for grocery shopping. The results showed that state versus action orientation moderates the relative importance of determinants of intentions; specifically, subjective norms become more important as people become state oriented, whereas the relative importance of attitudes increases as people become action oriented. In addition, the study showed that past behavior is a determinant of intentions to use coupons.",0
https://doi.org/10.1007/s11109-011-9164-y,Does Satisfaction with Democracy Really Increase Happiness? Direct Democracy and Individual Satisfaction in Switzerland,"This paper takes the influential “direct democracy makes people happy”-research as a starting point and asks whether direct democracy impacts individual satisfaction. Unlike former studies we distinguish two aspects of individual satisfaction, namely satisfaction with life (“happiness”) and with how democracy works. Based on multilevel analysis of the 26 Swiss cantons we show that the theoretical assumption on which the happiness hypothesis is based has to be questioned, as there is very little evidence for a robust relationship between satisfaction with democracy and life satisfaction. Furthermore, we do not find a substantive positive effect of direct democracy on happiness. However, with respect to satisfaction with democracy, our analysis shows some evidence for a procedural effect of direct democracy, i.e. positive effects related to using direct democratic rights, rather than these rights per se.",0
https://doi.org/10.1177/0170840603024003910,Corporate Social and Financial Performance: A Meta-Analysis,"Most theorizing on the relationship between corporate social/environmental performance (CSP) and corporate financial performance (CFP) assumes that the current evidence is too fractured or too variable to draw any generalizable conclusions. With this integrative, quantitative study, we intend to show that the mainstream claim that we have little generalizable knowledge about CSP and CFP is built on shaky grounds. Providing a methodologically more rigorous review than previous efforts, we conduct a meta-analysis of 52 studies (which represent the population of prior quantitative inquiry) yielding a total sample size of 33,878 observations. The meta-analytic findings suggest that corporate virtue in the form of social responsibility and, to a lesser extent, environmental responsibility is likely to pay off, although the operationalizations of CSP and CFP also moderate the positive association. For example, CSP appears to be more highly correlated with accounting-based measures of CFP than with market-based indicators, and CSP reputation indices are more highly correlated with CFP than are other indicators of CSP. This meta-analysis establishes a greater degree of certainty with respect to the CSP-CFP relationship than is currently assumed to exist by many business scholars.",0
https://doi.org/10.1016/j.pain.2014.04.017,Relationship satisfaction moderates the associations between male partner responses and depression in women with vulvodynia: A dyadic daily experience study,"Abstract Vulvodynia is a prevalent vulvovaginal pain condition that interferes with women’s psychological health. Given the central role of sexuality and relationships in vulvodynia, relationship satisfaction may be an important moderator of daily partner responses to this pain and associated negative sequelae, such as depression. Sixty-nine women (M age = 28.12 years, SD = 6.68) with vulvodynia and their cohabiting partners (M age = 29.67 years, SD = 8.10) reported their daily relationship satisfaction, and male partner responses on sexual intercourse days (M = 3.74, SD = 2.47) over 8 weeks. Women also reported their depressive symptoms. Relationship satisfaction on the preceding day moderated the associations between partner responses and women’s depressive symptoms in several significant ways: (1) On days after women reported higher relationship satisfaction than usual, their perception of greater facilitative male partner responses was associated with their decreased depression; (2) on days after women reported lower relationship satisfaction than usual, their perception of greater negative male partner responses was associated with their increased depression; (3) on days after men reported higher relationship satisfaction than usual, their self-reported higher negative responses were associated with decreased women’s depression, and higher solicitous responses were associated with increased women’s depression, whereas (4) on days after men reported lower relationship satisfaction than usual, their self-reported higher negative responses were related to increased women’s depression, and higher solicitous responses were associated with decreased women’s depression. Targeting partner responses and relationship satisfaction may enhance the quality of interventions aimed at reducing depression in women with vulvodynia.",0
https://doi.org/10.1111/1475-6765.12096,"Authoritarianism, socioethnic diversity and political participation across countries","It is argued in this article that threatening stimuli affect political participation levels among non-authoritarians more than among authoritarians. Focusing on socioethnic diversity, which is known to be particularly threatening to authoritarians and to relate negatively to political participation in the general public, analyses of individual- and macro-level data from 53 countries is presented which supports this thesis. Participation levels among authoritarians are largely static, regardless of a country's level of socioethnic heterogeneity, while non-authoritarians participate considerably less in countries with relatively high levels of socioethnic heterogeneity. This suggests that authoritarians participate to a proportionately greater degree in the most diverse countries.",0
https://doi.org/10.1111/j.1745-3984.1990.tb00738.x,Parameter Recovery in the Graded Response Model Using MULTILOG,"The graded response model can be used to describe test-taking behavior when item responses are classified into ordered categories. In this study, parameter recovery in the graded response model was investigated using the MULTILOG computer program under default conditions. Based on items having five response categories, 36 simulated data sets were generated that varied on true θ distribution, true item discrimination distribution, and calibration sample size. The findings suggest, first, the correlations between the true and estimated parameters were consistently greater than 0.85 with sample sizes of at least 500. Second, the root mean square error differences between true and estimated parameters were comparable with results from binary data parameter recovery studies. Of special note was the finding that the calibration sample size had little influence on the recovery of the true ability parameter but did influence item-parameter recovery. Therefore, it appeared that item-parameter estimation error, due to small calibration samples, did not result in poor person-parameter estimation. It was concluded that at least 500 examinees are needed to achieve an adequate calibration under the graded model.",0
https://doi.org/10.1093/biomet/88.4.987,"Hierarchical generalised linear models: A synthesis of generalised linear models, random-effect models and structured dispersions","SUMMARY Hierarchical generalised linear models are developed as a synthesis of generalised linear models, mixed linear models and structured dispersions. We generalise the restricted maximum likelihood method for the estimation of dispersion to the wider class and show how the joint fitting of models for mean and dispersion can be expressed by two interconnected generalised linear models. The method allows models with (i) any combination of a generalised linear model distribution for the response with any conjugate distribution for the random effects, (ii) structured dispersion components, (iii) different link and variance functions for the fixed and random effects, and (iv) the use of quasilikelihoods in place of likelihoods for either or both of the mean and dispersion models. Inferences can be made by applying standard procedures, in particular those for model checking, to components of either generalised linear model. We also show by numerical studies that the new method gives an efficient estimation procedure for substantial class of models of practical importance. Likelihood-type inference is extended to this wide class of models in a unified way.",0
https://doi.org/10.1186/1471-2288-14-92,Network meta-analysis of multiple outcome measures accounting for borrowing of information across outcomes,"BackgroundNetwork meta-analysis (NMA) enables simultaneous comparison of multiple treatments while preserving randomisation. When summarising evidence to inform an economic evaluation, it is important that the analysis accurately reflects the dependency structure within the data, as correlations between outcomes may have implication for estimating the net benefit associated with treatment. A multivariate NMA offers a framework for evaluating multiple treatments across multiple outcome measures while accounting for the correlation structure between outcomes.MethodsThe standard NMA model is extended to multiple outcome settings in two stages. In the first stage, information is borrowed across outcomes as well across studies through modelling the within-study and between-study correlation structure. In the second stage, we make use of the additional assumption that intervention effects are exchangeable between outcomes to predict effect estimates for all outcomes, including effect estimates on outcomes where evidence is either sparse or the treatment had not been considered by any one of the studies included in the analysis. We apply the methods to binary outcome data from a systematic review evaluating the effectiveness of nine home safety interventions on uptake of three poisoning prevention practices (safe storage of medicines, safe storage of other household products, and possession of poison centre control telephone number) in households with children. Analyses are conducted in WinBUGS using Markov Chain Monte Carlo (MCMC) simulations.ResultsUnivariate and the first stage multivariate models produced broadly similar point estimates of intervention effects but the uncertainty around the multivariate estimates varied depending on the prior distribution specified for the between-study covariance structure. The second stage multivariate analyses produced more precise effect estimates while enabling intervention effects to be predicted for all outcomes, including intervention effects on outcomes not directly considered by the studies included in the analysis.ConclusionsAccounting for the dependency between outcomes in a multivariate meta-analysis may or may not improve the precision of effect estimates from a network meta-analysis compared to analysing each outcome separately.",0
https://doi.org/10.1080/00273171.2014.973990,Iteration of Partially Specified Target Matrices: Applications in Exploratory and Bayesian Confirmatory Factor Analysis,"We describe and evaluate a factor rotation algorithm, iterated target rotation (ITR). Whereas target rotation (Browne, 2001) requires a user to specify a target matrix a priori based on theory or prior research, ITR begins with a standard analytic factor rotation (i.e., an empirically informed target) followed by an iterative search procedure to update the target matrix. In Study 1, Monte Carlo simulations were conducted to evaluate the performance of ITR relative to analytic rotations from the Crawford-Ferguson family with population factor structures varying in complexity. Simulation results: (a) suggested that ITR analyses will be particularly useful when evaluating data with complex structures (i.e., multiple cross-loadings) and (b) showed that the rotation method used to define an initial target matrix did not materially affect the accuracy of the various ITRs. In Study 2, we: (a) demonstrated the application of ITR as a way to determine empirically informed priors in a Bayesian confirmatory factor analysis (BCFA; Muthén & Asparouhov, 2012) of a rater-report alexithymia measure (Haviland, Warren, & Riggs, 2000) and (b) highlighted some of the challenges when specifying empirically based priors and assessing item and overall model fit.",0
https://doi.org/10.1016/s0167-9473(03)00149-x,Piecewise growth curve modeling approach for longitudinal prevention study,"Abstract Conventional growth curve modeling typically involves a single growth profile to represent changes in an outcome variable across time. This study applies an alternative growth curve model with multiple profiles to incorporate multiple developmental stages from which longitudinal data have been obtained. Identifying and modeling the multiple transitional stages involved in repeated measures are both theoretically and empirically important in longitudinal study. A longitudinal data set obtained from a substance use prevention study was used as an example to illustrate this application. The data contained a total of 50 junior high schools (23 control and 27 program schools) observed for seven waves crossing junior high school and high school stages, with the prevalence rate of monthly cigarette use as the outcome measure. Comparisons showed that the piecewise growth curve models incorporating multiple stages demonstrated significant improvement on model fitting compared to the single-piece growth curve model. Results showed marginal prevention effects in the junior high school stage but not in the high school stage. Piecewise growth curve models offer more both substantively and analytically appropriate model specification and flexibility in incorporating transitional periods in studying changes across time.",0
https://doi.org/10.1177/0146167215610520,The Double-Edged Sword of Genetic Accounts of Criminality,"Much debate exists surrounding the applicability of genetic information in the courtroom, making the psychological processes underlying how people consider this information important to explore. This article addresses how people think about different kinds of causal explanations in legal decision-making contexts. Three studies involving a total of 600 Mechanical Turk and university participants found that genetic, versus environmental, explanations of criminal behavior lead people to view the applicability of various defense claims differently, perceive the perpetrator's mental state differently, and draw different causal attributions. Moreover, mediation and path analyses highlight the double-edged nature of genetic attributions-they simultaneously reduce people's perception of the perpetrator's sense of control while increasing people's tendencies to attribute the cause to internal factors and to expect the perpetrator to reoffend. These countervailing relations, in turn, predict sentencing in opposite directions, although no overall differences in sentencing or ultimate verdicts were found.",0
https://doi.org/10.1080/00223891.2015.1044604,Conceptualizing and Measuring Self-Criticism as Both a Personality Trait and a Personality State,"Blatt's ( 2004 , 2008 ) conceptualization of self-criticism is consistent with a state-trait model that postulates meaningful variation in self-criticism both between persons (traits) and within person (states). We tested the state-trait model in a 7-day diary study with 99 college student participants. Each evening they completed a 6-item measure of self-criticism, as well as measures of perceived social support, positive and negative affect, compassionate and self-image goals during interactions with others, and interpersonal behavior, including overt self-criticism and given social support. As predicted, self-criticism displayed both trait-like variance between persons and daily fluctuations around individuals' mean scores for the week; slightly more than half of the total variance was between persons (ICC = .56). Numerous associations at both the between-persons and within-person levels were found between self-criticism and the other variables, indicating that individuals' mean levels of self-criticism over the week, and level of self-criticism on a given day relative to their personal mean, were related to their cognitions, affect, interpersonal goals, and behavior. The results supported the construct validity of the daily self-criticism measure. Moreover, the findings were consistent with the state-trait model and with Blatt's theoretical analysis of self-critical personality.",0
https://doi.org/10.1111/j.1467-8624.2011.01630.x,Nonlinear Growth Curves in Developmental Research,"Developmentalists are often interested in understanding change processes, and growth models are the most common analytic tool for examining such processes. Nonlinear growth curves are especially valuable to developmentalists because the defining characteristics of the growth process such as initial levels, rates of change during growth spurts, and asymptotic levels can be estimated. A variety of growth models are described beginning with the linear growth model and moving to nonlinear models of varying complexity. A detailed discussion of nonlinear models is provided, highlighting the added insights into complex developmental processes associated with their use. A collection of growth models are fit to repeated measures of height from participants of the Berkeley Growth and Guidance Studies from early childhood through adulthood.",0
https://doi.org/10.1002/sim.3196,The use of hierarchical models for estimating relative risks of individual genetic variants: An application to a study of melanoma,"For major genes known to influence the risk of cancer, an important task is to determine the risks conferred by individual variants, so that one can appropriately counsel carriers of these mutations. This is a challenging task, since new mutations are continually being identified, and there is typically relatively little empirical evidence available about each individual mutation. Hierarchical modeling offers a natural strategy to leverage the collective evidence from these rare variants with sparse data. This can be accomplished when there are available higher-level covariates that characterize the variants in terms of attributes that could distinguish their association with disease. In this article, we explore the use of hierarchical modeling for this purpose using data from a large population-based study of the risks of melanoma conferred by variants in the CDKN2A gene. We employ both a pseudo-likelihood approach and a Bayesian approach using Gibbs sampling. The results indicate that relative risk estimates tend to be primarily influenced by the individual case-control frequencies when several cases and/or controls are observed with the variant under study, but that relative risk estimates for variants with very sparse data are more influenced by the higher-level covariate values, as one would expect. The analysis offers encouragement that we can draw strength from the aggregating power of hierarchical models to provide guidance to medical geneticists when they offer counseling to patients with rare or even hitherto unobserved variants. However, further research is needed to validate the application of asymptotic methods to such sparse data.",0
https://doi.org/10.1027/1015-5759/a000062,Consequences of Test Anxiety on Adaptive Versus Fixed Item Testing,"We investigated the effects of test anxiety on test performance using computerized adaptive testing (CAT) versus conventional fixed item testing (FIT). We hypothesized that tests containing mainly items with medium probabilities of being solved would have negative effects on test performance for testtakers high in test anxiety. A total of 110 students (aged 16 to 20) from a German secondary modern school filled out a short form of the Test Anxiety Inventory (TAI-G; Wacker, Jaunzeme, &amp; Jaksztat, 2008 ) and then were presented with items from the Adaptive Matrices Test (AMT; Hornke, Etzel, &amp; Rettig, 1999 ) on the computer, either in CAT form or in a fixed item test form with a selection of items arranged in order of increasing item difficulty. Additionally, half of the students were given a short summary of information about the mode of item selection in adaptive testing before working on the CAT. In a moderated regression approach, a significant interaction of test anxiety and test mode was revealed. The effect of test mode on the AMT score was stronger for students with higher scores on test anxiety than for students with lower test anxiety. Furthermore, getting information about CAT led to significantly better results than receiving standard test instructions. Results are discussed with reference to test fairness.",0
https://doi.org/10.1177/0146621603027003004,"MetaAnalysisCorr: An SAS/IML Program to Synthesize Correlation Matrices with Hunter and Schmidt, Hedges and Olkin, and Generalized Least Squares Approaches","Meta-analysis, the analysis of analyses, is a standard statistical procedure for summarizing research findings in behavioral sciences. The Hunter-Schmidt approach (Hunter &amp; Schmidt, 1990) and the Hedges-Olkin (Hedges &amp; Olkin, 1985) approach are the two most frequently used procedures. When dealing with multivariate cases, for instance, correlation matrices, these procedures are applied to individual correlation coefficients of the correlation matrix separately. A better approach, however, is to use the generalized least squares approach, which was proposed for multivariate effect sizes (e.g., Becker, 1992, 1995; Hedges &amp; Olkin, 1985).",0
https://doi.org/10.1525/auk.2008.07134,SOURCES OF VARIATION IN DETECTION OF WADING BIRDS FROM AERIAL SURVEYS IN THE FLORIDA EVERGLADES,"Abstract We conducted dual-observer trials to estimate detection probabilities (probability that a group that is present and available is detected) for fixed-wing aerial surveys of wading birds in the Everglades system, Florida. Detection probability ranged from <0.2 to ~0.75 and varied according to species, group size, observer, and the observer's position in the aircraft (front or rear seat). Aerial-survey simulations indicated that incomplete detection can have a substantial effect on assessment of population trends, particularly over relatively short intervals (≤3 years) and small annual changes in population size (≤3%). We conclude that detection bias is an important consideration for interpreting observations from aerial surveys of wading birds, potentially limiting the use of these data for comparative purposes and trend analyses. We recommend that workers conducting aerial surveys for wading birds endeavor to reduce observer and other controllable sources of detection bias and account for uncontro...",0
https://doi.org/10.1037/10099-004,Models for learning data.,,0
https://doi.org/10.1016/0304-4076(94)90003-5,Journal of econometrics,,0
https://doi.org/10.1086/209170,The Theory of Reasoned Action: A Meta-Analysis of Past Research with Recommendations for Modifications and Future Research,"Two meta-analyses were conducted to Investigate the effectiveness of the Fishbein and Ajzen model in research to date. Strong overall evidence for the predictive utility of the model was found. Although numerous instances were identified in which researchers overstepped the boundary conditions initially proposed for the model, the predictive utility remained strong across conditions. However, three variables were proposed and found to moderate the effectiveness of the model. Suggested extensions to the model are discussed and general directions for future research are given.",0
https://doi.org/10.1037/0033-295x.112.4.744,The Cultural Mind: Environmental Decision Making and Cultural Modeling Within and Across Populations.,"This article describes cross-cultural research on the relation between how people conceptualize nature and how they act in it. Mental models of nature differ dramatically among populations living in the same area and engaged in similar activities. This has novel implications for environmental decision making and management, including common problems. The research offers a distinct perspective on cultural modeling and a unified approach to studies of culture and cognition. The authors argue that cultural transmission and formation consist primarily not in shared rules or norms but in complex distributions of causally connected representations across minds interacting with the environment. The cultural stability and diversity of these representations often derive from rich, biologically prepared mental mechanisms that limit variation to readily transmissible psychological forms. This framework addresses several methodological issues, such as limitations on conceiving culture to be a well-defined system, bounded entity, independent variable, or an internalized component of minds.",0
https://doi.org/10.1111/jopy.12136,"Explaining Differences in Subjective Well-Being Across 33 Nations Using Multilevel Models: Universal Personality, Cultural Relativity, and National Income","This multinational study simultaneously tested three prominent hypotheses--universal disposition, cultural relativity, and livability--that explained differences in subjective well-being across nations. We performed multilevel structural equation modeling to examine the hypothesized relationships at both individual and cultural levels in 33 nations. Participants were 6,753 university students (2,215 men; 4,403 women; 135 did not specify), and the average age of the entire sample was 20.97 years (SD = 2.39). Both individual- and cultural-level analyses supported the universal disposition and cultural relativity hypotheses by revealing significant associations of subjective well-being with Extraversion, Neuroticism, and independent self-construal. In addition, interdependent self-construal was positively related to life satisfaction at the individual level only, whereas aggregated negative affect was positively linked with aggregate levels of Extraversion and interdependent self-construal at the cultural level only. Consistent with the livability hypothesis, gross national income (GNI) was related to aggregate levels of negative affect and life satisfaction. There was also a quadratic relationship between GNI and aggregated positive affect. Our findings reveal that universal disposition, cultural self-construal, and national income can elucidate differences in subjective well-being, but the multilevel analyses advance the literature by yielding new findings that cannot be identified in studies using individual-level analyses alone.",0
https://doi.org/10.1037/a0022658,Effect size measures for mediation models: Quantitative strategies for communicating indirect effects.,"The statistical analysis of mediation effects has become an indispensable tool for helping scientists investigate processes thought to be causal. Yet, in spite of many recent advances in the estimation and testing of mediation effects, little attention has been given to methods for communicating effect size and the practical importance of those effect sizes. Our goals in this article are to (a) outline some general desiderata for effect size measures, (b) describe current methods of expressing effect size and practical importance for mediation, (c) use the desiderata to evaluate these methods, and (d) develop new methods to communicate effect size in the context of mediation analysis. The first new effect size index we describe is a residual-based index that quantifies the amount of variance explained in both the mediator and the outcome. The second new effect size index quantifies the indirect effect as the proportion of the maximum possible indirect effect that could have been obtained, given the scales of the variables involved. We supplement our discussion by offering easy-to-use R tools for the numerical and visual communication of effect size for mediation effects.",0
https://doi.org/10.1177/0146621606292933,Two-Phase Item Selection Procedure for Flexible Content Balancing in CAT,"Content balancing is an important issue in the design and implementation of computerized adaptive testing (CAT). Content-balancing techniques that have been applied in fixed content balancing, where the number of items from each content area is fixed, include constrained CAT (CCAT), the modified multinomial model (MMM), modified constrained CAT (MCCAT), and others. In this article, four methods are proposed to address the flexible content-balancing issue with the a-stratification design, named STR_C. The four methods are MMM+, an extension of MMM; MCCAT+, an extension of MCCAT; the TPM method, a two-phase content-balancing method using MMM in both phases; and the TPF method, a two-phase content-balancing method using MMM in the first phase and MCCAT in the second. Simulation results show that all of the methods work well in content balancing, and TPF performs the best in item exposure control and item pool utilization while maintaining measurement precision.",0
https://doi.org/10.1093/biomet/76.1.1,On the use of nonparametric regression for model checking,"SUMMARY The use of nonparametric regression is explored to check the fit of a parametric regression model. The principal aim is to check the validity of the regression curve rather than necessarily to detect outliers. A pseudo likelihood ratio test is developed to provide a global assessment of fit and simulation bands are used to indicate the nature of departures from the model. The types of data considered include discrete response variables, where standard diagnostic techniques are often not appropriate, and first-order autoregressive series. Several numerical examples are given. Nonparametric regression can be used in an informal graphical way to assess the relationship between a response and an explanatory variable. In this paper we aim to develop more formal methods of assessing the assumptions of a parametric model, in particular when regression diagnostics of the type developed for normal linear models are not readily available. The principal aim is to check the validity of the systematic part of the model by comparing a nonparametric estimate of the regression curve with a parametric one. Such a comparison may also identify outliers, although the distinction between outliers and model inadequacy is not always easy. Two techniques are used to assess the fit of a parametric model. In ? 2, confidence bands are constructed around the fitted regression curve by simulation. A comparison of these with the nonparametric curve gives an indication of the nature of any departures from the model. In ? 3, a pseudo likelihood ratio test is developed. This provides a quantitative global assessment of fit. In applying these ideas, special emphasis is given to discrete data, and notably logistic regression, because of the difficulty in applying standard residual-based model checking techniques to this type of response variable. A Poisson regression example is discussed in ? 4. However, the underlying ideas have wider applications. Autoregressive time series of order 1 are discussed in ? 6. Sections 5 and 7 discuss general issues. We first discuss the context of binary regression with a single covariate and the difficulties caused by the discreteness of the response variable. The observed data are assumed to be of the form (xi, yi, ni), where xi is a covariate value, and yi has a binomial",0
https://doi.org/10.1111/jomf.12112,Grasping the Diversity of Cohabitation: Fertility Intentions Among Cohabiters Across Europe,"The authors examined the association between different meanings of cohabitation and fertility intentions. Using data from the Generations and Gender Surveys on 5,565 cohabiters from 9 European countries (Austria, Bulgaria, France, Germany, Hungary, Lithuania, Norway, Romania, and Russia), they proposed a cohabitation typology based on attitudes toward marriage, intentions to marry, and perceived economic deprivation. Despite substantial variation in the prevalence and types of cohabiting relationships across Europe, cohabitation has become a living arrangement within which childbearing intentions are commonly formed and at times carried out. The authors found that the meaning that cohabiters attached to their union influenced significantly their short-term fertility intentions, net of other covariates. Cohabiters who viewed their unions as a prelude to marriage were the most likely to plan to have a child in the near future, both in Western and Eastern European societies. The association between fertility intentions and marriage intentions was particularly strong among cohabiters who do not as yet have children in common, but it was also present in a more muted form among cohabitating parents. The findings suggest that, although marriage and childbearing are becoming less closely linked life events, they are not disconnected decisions for a large majority of cohabiters across Europe. Keywords: cohabitation; cohabitation typology; Europe; fertility intentions; Generations and Gender Surveys",0
https://doi.org/10.1017/cbo9780511601118.010,The robustness of maximum likelihood estimation in structural equation models,"Introduction General methods for the analysis of covariance structures were introduced by Joreskog (1970, 1973). Within the general theoretical framework it is possible to estimate parameters and their corresponding standard errors and to test the goodness-of-fit of a linear structural equation system by means of maximum likelihood methods . Although other methods of estimating such models (least squares procedures, instrumental variable methods) are available, we do not discuss them here. For an introduction to the general model the reader is referred to Chapter 2 and for more detailed statistical discussions to Joreskog (1978, 1982a, b). The LISREL model considers a data matrix Z ( N × k ) of N observations on k random vaŕiables. It is assumed that the rows of Z are independently distributed, each having a multivariate normal distribution with the same mean vector μ and the same covariance matrix ∑; that is, each case in the data is independently sampled from the same population. In a specified model there are s independent model parameters co, to be estimated. For large samples the sampling distribution of the estimated parameters ŵ i and the sampling distribution of the likelihood ratio estimate for goodness-of-fit are approximately known, provided that the preceding assumptions hold. Under the same conditions the standard errors of the estimated parameters se ŵ i are also known asymptotically. In the following, standardized parameter estimates are defined by ŵ* i = (ŵ i − ω i )/ se ŵ i . For large samples the sampling distribution of ŵ* i is approximately standard normal, and the goodness-of-fit estimate has an approximate chi-square distribution with k ( k + 1)/2 – s degrees of freedom.",0
https://doi.org/10.1080/0141192970230405,Differential Secondary School Effectiveness: comparing the performance of different pupil groups,"Abstract This article reports the results of an Economic and Social Research Council (ESRC) funded study which focuses on the differential academic achievement of different groups of pupils. The paper describes the findings on the size and extent of school effects across 3 years (1990, 1991, 1992) for different groups of pupils (classified by gender, eligibility for free school means [FSM], ethnic group and by prior attainment). Pupils’ overall General Certificate of Secondary Education performance and their performance in selected subjects (English, English literature, French, history, mathematics and science) have been analysed using multilevel modelling, employing a total sample of 94 inner London secondary schools. A ‘value added’ approach is adopted, controlling for selected student background measures of prior attainment (at secondary transfer), gender, age, ethnicity and low income to provide statistical controls for differences between schools in the characteristics of their intakes. Differential ...",0
https://doi.org/10.1016/j.pain.2004.04.008,"Peri-sciatic proinflammatory cytokines, reactive oxygen species, and complement induce mirror-image neuropathic pain in rats","In inflammatory neuropathy, immune activation near intact peripheral nerves induces mechanical allodynia. The identity of the peripheral immune product(s) that lead to these changes in pain behavior is unknown. The present series of studies utilized the sciatic inflammatory neuropathy (SIN) model to examine this question. Here, inflammatory neuropathy is created by injecting an immune activator (zymosan) around one sciatic nerve via an indwelling catheter. Our prior studies demonstrated that peri-sciatic zymosan activated macrophages and neutrophils to release proinflammatory cytokines and reactive oxygen species (ROS). In addition, zymosan is a classical activator of the complement cascade. Thus the present series of experiments examined whether any of these inflammatory mediators are involved in the initial induction of SIN-induced ipsilateral or bilateral allodynias. Peri-sciatic injection of selective inhibitors/antagonists revealed that a number of immune products are early mediators of the resultant allodynias, including proinflammatory cytokines (tumor necrosis factor, interleukin-1, and interleukin-6), ROS, and complement. Thus these immune-derived substances can markedly alter sensory nerve function at mid-axon.",0
https://doi.org/10.1037/0096-1523.10.3.340,Are lexical decisions a good measure of lexical access? The role of word frequency in the neglected decision stage.,"Three experiments with 80 undergraduates investigated the impact of 5 lexical variables (instance dominance, category dominance, word frequency, word length in letters, and word length in syllables) on performance in 3 tasks involving word recognition: category verification, lexical decision, and pronunciation. Although the same set of words was used in each task, the relationship of the lexical variables to RT varied significantly with the task within which the words were embedded. The effect of word frequency was minimal in the category verification task, whereas it was significantly larger in the pronunciation task and significantly larger yet in the lexical decision task. It is argued that decision processes having little to do with lexical access accentuate the word-frequency effect in the lexical decision task and that results from this task have questionable value in testing the assumption that word frequency orders the lexicon, thereby affecting time to access the mental lexicon. A simple 2-stage model is outlined to account for the role of word frequency and other variables in lexical decision. The model is applied to the results of the reported experiments and some of the findings in other studies of lexical decision and pronunciation. (49 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1984 American Psychological Association.",0
https://doi.org/10.1093/biomet/89.3.553,Bayesian analysis of covariance matrices and dynamic models for longitudinal data,"Parsimonious modelling of the within‐subject covariance structure while heeding its positive‐definiteness is of great importance in the analysis of longitudinal data. Using the Cholesky decomposition and the ensuing unconstrained and statistically meaningful reparameterisation, we provide a convenient and intuitive framework for developing conditionally conjugate prior distributions for covariance matrices and show their connections with generalised inverse Wishart priors. Our priors offer many advantages with regard to elicitation, positive definiteness, computations using Gibbs sampling, shrinking covariances toward a particular structure with considerable flexibility, and modelling covariances using covariates. Bayesian estimation methods are developed and the results are compared using two simulation studies. These simulations suggest simpler and more suitable priors for the covariance structure of longitudinal data.",0
https://doi.org/10.1007/0-306-47531-6_14,MML and EAP Estimation in Testlet-based Adaptive Testing,"In the previous chapter, Wainer, Bradlow and Du (this volume) presented a generalization of the three-parameter item response model in which the dependencies generated by a testlet structure are explicitly taken into account. That chapter is an extension of their prior work that developed the generalization for the two-parameter model (Bradlow, Wainer, & Wang, 1999). Their approach is to use a fully Bayesian formulation of the problem, coupled with a Markov Chain Monte Carlo (MCMC) procedure to estimate the posterior distribution of the model parameters. They then use the estimated posterior distribution to compute interval and point estimates. In this chapter, we derive estimates for the parameters of the Wainer, Bradlow, and Du testlet model using more traditional estimation methodology; maximum marginal likelihood (MML) and expected a posteriori (EAP) estimates. We also show how the model might be used within a CAT environment. After deriving the estimation equations, we compare the results of MCMC and MML estimation procedures and we examine the extent to which ignoring the testlet structure affects the precision of item calibration and the estimation of ability within a CAT procedure.",0
https://doi.org/10.1016/j.jmp.2014.06.004,A cognitive latent variable model for the simultaneous analysis of behavioral and personality data,"I describe a cognitive latent variable model , a combination of a cognitive model and a latent variable model that can be used to aggregate information regarding cognitive parameters across participants and tasks. The model is ideally suited for uncovering relationships between latent task abilities as they are expressed in experimental paradigms, but can also be used as data fusion tools to connect latent abilities with external covariates from entirely different data sources. An example application deals with the structure of cognitive abilities underlying an executive functioning task and its relation to personality traits. • Cognitive models and latent variable models are combined into a single framework. • The framework can be applied in a Bayesian inferential context. • An application uses data from an RT experiment and questionnaires in a single model.",0
https://doi.org/10.1177/014662169301700305,Equating Tests Under The Nominal Response Model,"Under item response theory, test equating involves finding the coefficients of a linear trans formation of the metric of one test to that of another. A procedure for finding these equating coefficients when the items in the two tests are nominally scored was developed. A quadratic loss function based on the differences between response category probabilities in the two tests is employed. The gradients of this loss function needed by the iterative multivariate search procedure used to obtain the equating coefficients were derived for the nominal response case. Examples of both hori zontal and vertical equating are provided. The empirical results indicated that tests scored under a nominal response model can be placed on a com mon metric in both horizontal and vertical equatings.",0
https://doi.org/10.1002/jrsm.1164,Methods to estimate the between‐study variance and its uncertainty in meta‐analysis,"Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a 'generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios.",0
https://doi.org/10.1017/cbo9781107323667.008,Putting Broca’s region into context: fMRI evidence for a role in predictive language processing,"Brocaâ€™s region is known to play a key role in speech production as well as in the processing of language input. Still, the exact function (or functions) of Brocaâ€™s region remains highly disputed. Within the generativist framework it has been argued that part of Brocaâ€™s region is dedicated to syntactical analysis. Others, however, have related Brocaâ€™s region activity to more domain-general processes, e.g. working memory load and argument hierarchy demands. We here present results that show how contextual cues completely alter the effects of syntax in behaviour and in Brocaâ€™s region, and suggest that activation in this area reflects general linguistic processing costs or prediction error. We review the fMRI literature in the light of this theory. Introduction: the controversy over Broca's region In 1861 Paul Broca presented the brain of one of his patients to the anthropological society in Paris. Before his death, this patient had displayed a severe speech deficit, being unable to say more than a single word, Tan', while apparently maintaining many of his other mental faculties (Broca, 1861). Broca found that the patient had a large lesion in the brain's left inferior frontal gyrus (LIFG). Since then, this area, now often referred to as Broca's region, has been considered a key speech/language brain region. With the advent of cell-staining techniques, Korbinian Brodmann (Brodmann, 1909) found that the LIFG, based on the cytoarchitecture, could be subdivided into distinct regions: Brodmann areas 44, 45 and 47 (BA 44/45/47). The subregions are depicted in Plate 8.1 (see colour plate section). Agrammatical speech was already early considered to be a specific symptom in aphasiology (e.g. Kussmaul, 1877), and it has subsequently been argued that Broca's region plays a significant role in the processing of syntax, both in comprehension and production of language (Friedmann, 2006; Grodzinsky & Santi, 2008). Â© Cambridge University Press 2015.",0
https://doi.org/10.1080/10705510903203466,Level-Specific Evaluation of Model Fit in Multilevel Structural Equation Modeling,"In multilevel structural equation modeling, the “standard” approach to evaluating the goodness of model fit has a potential limitation in detecting the lack of fit at the higher level. Level-specific model fit evaluation can address this limitation and is more informative in locating the source of lack of model fit. We proposed level-specific test statistics for the test of overall model fit, comparative fit index, and root mean squared error of approximation using partially saturated models, and we also considered another level-specific approach proposed by Yuan and Bentler (2007) Yuan, K.-H. and Bentler, P. M. 2007. Multilevel covariance structure analysis by fitting multiple single-level models.. Sociological Methodology, 37: 53–82. [Crossref], [Web of Science ®] , [Google Scholar]. A simulation study showed that the standard approach failed to detect the lack of fit at the group level. The fit indexes produced by the level-specific approaches both successfully detected the lack of model fit at each level. There were only minor differences in the performance of the 2 level-specific approaches.",0
https://doi.org/10.1201/b10905-7,Inference from Simulations and Monitoring Convergence,"Markov chainMonteCarlo (MCMC)methodshavebeenaround for almost as longasMonte Carlo techniques, even though their impact on statistics was not truly felt until the very early 1990s, except in the specialized ﬁelds of spatial statistics and image analysis, where those methods appeared earlier. The emergence of Markov based techniques in physics is a story that remains untold within this survey (see Landau and Binder, 2005). Also, we will not enter into a description of MCMC techniques, unless they have some historical link, as the remainder of this volume covers the technical aspects.Acomprehensive treatment with further references can also be found in Robert and Casella (2004). We will distinguish between the introduction of Metropolis-Hastings based algorithmsand those related to Gibbs sampling, since they each stem from radically different origins, even though their mathematical justiﬁcation via Markov chain theory is the same. Tracing the development of Monte Carlo methods, we will also brieﬂy mention what we might call the “second-generation MCMC revolution.” Starting in the mid to late 1990s, this includes the development of particle ﬁlters, reversible jump and perfect sampling, and concludes with more current work on population or sequential Monte Carlo and regeneration and the computing of “honest” standard errors. As mentioned above, the realization that Markov chains could be used in a wide varietyof situations only came (tomainstream statisticians)withGelfand and Smith (1990), despite earlier publications in the statistical literature such as Hastings (1970), Geman and Geman (1984), and Tanner and Wong (1987). Several reasons can be advanced: lack of computing machinery (think of the computers of 1970!), or background onMarkov chains, or hesitation to trust in the practicality of themethod. It thus required visionary researchers like Gelfand and Smith to convince the community, supported by papers that demonstrated, through a series of applications, that the method was easy to understand, easy to implement and practical (Gelfand et al., 1990, 1992; Smith and Gelfand, 1992; Wakeﬁeld et al., 1994). The rapid emergence of the dedicated BUGS (Bayesian inference using Gibbs sampling) software as early as 1991, when a paper on BUGS was presented at the Valencia meeting, was another compelling argument for adopting, at large, MCMC algorithms.∗Monte Carlo methods were born in Los Alamos, New Mexico, during World War II, eventually resulting in theMetropolis algorithm in the early 1950s. WhileMonte Carlo methods were in use by that time, MCMC was brought closer to statistical practicality by the work of Hastings in the 1970s. What can be reasonably seen as the ﬁrst MCMC algorithm is what we now call theMetropolis algorithm, published by Metropolis et al. (1953). It emanates from the same group of scientists who produced the Monte Carlo method, namely the research scientists of Los Alamos, mostly physicists working on mathematical physics and the atomic bomb. MCMC algorithms therefore date back to the same time as the development of regular(MC only) Monte Carlo methods, which are usually traced to Ulam and von Neumann in the late 1940s. StanislawUlam associates the original ideawith an intractable combinatorial computation he attempted in 1946 (calculating the probability of winning at the solitaire card game). This ideawas enthusiastically adopted by John vonNeumann for implementationwith direct applications to neutron diffusion, the name “Monte Carlo” being suggested by Nicholas Metropolis. Eckhardt (1987) describes these early Monte Carlo developments, and Hitchcock (2003) gives a brief history of the Metropolis algorithm. These occurrences very closely coincide with the appearance of the very ﬁrst general-purpose digital computer, the ENIAC,which came to life in February 1946, after three years of construction. The Monte Carlo method was set up by von Neumann, who was using it on thermonuclear and ﬁssion problems as early as 1947. That same year, Ulam and vonNeumann invented inversion andaccept-reject techniques (also recounted inEckhardt, 1987) to simulate from nonuniform distributions. Without computers, a rudimentary version invented by Fermi in the 1930s went unrecognized (Metropolis, 1987). Note also that, as early as 1949, a symposiumonMonteCarlowas supported byRand, theNational Bureau of Standards, and theOakRidge laboratory and thatMetropolis andUlam (1949) published the very ﬁrst paper about the Monte Carlo method.",0
https://doi.org/10.1016/0022-2496(92)90038-9,The structure of simple reaction time to step-function signals,"Simple reaction time to a step-function signal of amplitude A can be additively decomposed into a signal-dependent component, T ( A ) (converging to zero as A increases), and a signal-independent component, R . The two components, however, are not mutually independent, they both are increasing deterministic functions of a single random variable; T ( A ) = T ( A , C ) and R = R ( C ). C is interpreted as a “criterion” or “inhibition factor” controlling simultaneously “readiness to detect” and “readiness to respond”. The model is not based on a priori distributional assumptions. RT percentiles of a given rank P computed across RT distributions for different values of A , have a deterministic structure, RT P ( A ) = T ( A , C P ) + R ( C P ), where C P is the P th percentile of C . Subtracting RT P (A ∗ ) for a very large A ∗ from RT P ( A ), one gets an estimate of T ( A , C P ), the P th percentile of T ( A ). Applying this to different values of P , one can a posteriori reconstruct the joint distribution of the two RT components. The asymptotic consequences of this model (for sufficiently large A ) are corroborated by data on the RT to instantaneous displacements of a visual target.",0
https://doi.org/10.1006/jmps.2000.1319,Testing Evidence Accrual Models by Manipulating Stimulus Onset,"Many models of response time assume that subjects accrue stimulus ""evidence"" samples in time (e.g., random walk models, counter models). In this paper, the concept of one stimulus dominating another is used to construct a test of the whole class of evidence accrual models. For an example of dominance, consider stimuli that are presented either virtually instantaneously (stepped) or in a gradually increasing manner (ramped). Ramped stimuli are presented such that the ramped portion precedes the stepped onset of stepped stimuli. In this case ramped stimuli dominate stepped stimuli. In this paper the class of evidence accrual models is formalized. It is shown that under appropriate assumptions evidence accrual models do predict more accurate responses to dominating stimuli. However, this result does not hold for response latencies. There are anomalous cases where an evidence accrual model (the accumulator model of Vickers (1970, Ergonomics 13, 37-58)) predicts slower mean correct response latencies to dominating stimuli. It is shown through extensive computer simulation that these anomalous cases occur only when response criteria are so asymmetric that there are exceedingly extreme response biases. For experiments where response biases are not exceedingly extreme, random walk and accumulator models predict more accurate and quicker correct responses to dominating stimuli. In sum, manipulating the time course of stimuli in accordance with the concept of dominance can provide empirical tests of the class of evidence accrual models. Copyright 2001 Academic Press.",0
https://doi.org/10.3389/fpsyg.2013.00770,"Facing off with Scylla and Charybdis: a comparison of scalar, partial, and the novel possibility of approximate measurement invariance","Measurement invariance (MI) is a pre-requisite for comparing latent variable scores across groups. The current paper introduces the concept of approximate MI building on the work of Muthén and Asparouhov and their application of Bayesian Structural Equation Modeling (BSEM) in the software Mplus. They showed that with BSEM exact zeros constraints can be replaced with approximate zeros to allow for minimal steps away from strict MI, still yielding a well-fitting model. This new opportunity enables researchers to make explicit trade-offs between the degree of MI on the one hand, and the degree of model fit on the other. Throughout the paper we discuss the topic of approximate MI, followed by an empirical illustration where the test for MI fails, but where allowing for approximate MI results in a well-fitting model. Using simulated data, we investigate in which situations approximate MI can be applied and when it leads to unbiased results. Both our empirical illustration and the simulation study show approximate MI outperforms full or partial MI In detecting/recovering the true latent mean difference when there are (many) small differences in the intercepts and factor loadings across groups. In the discussion we provide a step-by-step guide in which situation what type of MI is preferred. Our paper provides a first step in the new research area of (partial) approximate MI and shows that it can be a good alternative when strict MI leads to a badly fitting model and when partial MI cannot be applied.",0
https://doi.org/10.1037/0278-6133.27.2(suppl.).s101,How and why criteria defining moderators and mediators differ between the Baron &amp; Kenny and MacArthur approaches.,"In recognition of the increasingly important role of moderators and mediators in clinical research, clear definitions are sought of the two terms to avoid inconsistent, ambiguous, and possibly misleading results across clinical research studies.The criteria used to define moderators and mediators proposed by the Baron & Kenny approach, which have been long used in social/behavioral research, are directly compared to the criteria proposed by the recent MacArthur approach, which modified the Baron & Kenny criteria.After clarifying the differences in criteria between approaches, the rationale for the modifications is clarified and the implications for the design and interpretation of future studies considered.Researchers may find modifications introduced in the MacArthur approach more appropriate to their research objectives, particularly if their research might have a direct influence on decision making.",0
https://doi.org/10.1214/aos/1176348244,The Variational Form of Certain Bayes Estimators,"A general representation is obtained for formal Bayes of a parameter matrix. We assume that prior distribution is symmetric in some sense, but it is not specified otherwise. The formal Bayes risk is minimized subject to order constraints by a variational technique; hence our representation is called the variational form of Bayes estimator (VFBE). The VFBE is used to obtain estimators that have good frequency properties relative to usual estimators. Such estimators are obtained for mean vector and covariance matrix of a multivariate normal distribution. Also, for possibly nonnormal data, we give VFBE of several Pearson means. A certain emphasis is placed on problem of estimating covariance matrix. For that problem, our constrained optimization provides an with very good properties: Its eigenvalues are in proper order, and they are not as distorted as those in sample covariance matrix. The VFBE for covariance matrix is related to an of Stein. Of two, VFBE deals with order relations in a more natural way; that is, it is more criterion dependent. In addition, it is easier to compute than Stein's estimator, and a brief Monte Carlo simulation indicates that it has better risk properties as well.",0
https://doi.org/10.1007/978-3-642-14064-8_30,The Influence of Different Haptic Environments on Time Delay Discrimination in Force Feedback,"Time delay in haptic telepresence arising from compression or communication alters the phase characteristics of the environment impedance. This paper describes how well a human operator can discriminate these changes in haptic environments. Three different environments are rendered on a haptic interface and manually excited by a human operator using sinusoidal movements. We find that time delay in haptic feedback can be discriminated starting from 15 ms in a pure damper environment, 36 ms in a spring system, and 72 ms when moving a damped inertia. We conclude that the discrimination thresholds increase with the absolute phase between velocity and force signals resulting from the remote environment characteristics. These results may benefit the human-centered design of high-fidelity haptic communication protocols and haptic filters.",0
https://doi.org/10.1037/a0024338,"Effect size estimates: Current use, calculations, and interpretation.","The Publication Manual of the American Psychological Association (American Psychological Association, 2001, American Psychological Association, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial η2 was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen's d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.",0
https://doi.org/10.1214/ss/1028905934,Simulating normalizing constants: from importance sampling to bridge sampling to path sampling,"Computing (ratios of) normalizing constants of probability models is a fundamental computational problem for many statistical and scientific studies. Monte Carlo simulation is an effective technique, especially with complex and high-dimensional models. This paper aims to bring to the attention of general statistical audiences of some effective methods originating from theoretical physics and at the same time to explore these methods from a more statistical perspective, through establishing theoretical connections and illustrating their uses with statistical problems. We show that the acceptance ratio method and thermodynamic integration are natural generalizations of importance sampling, which is most familiar to statistical audiences. The former generalizes importance sampling through the use of a single ""bridge"" density and is thus a case of bridge sampling in the sense of Meng and Wong. Thermodynamic integration, which is also known in the numerical analysis literature as Ogata's method for high-dimensional integration, corresponds to the use of infinitely many and continuously connected bridges (and thus a ""path""). Our path sampling formulation offers more flexibility and thus potential efficiency to thermodynamic integration, and the search of optimal paths turns out to have close connections with the Jeffreys prior density and the Rao and Hellinger distances between two densities. We provide an informative theoretical example as well as two empirical examples (involving 17- to 70-dimensional integrations) to illustrate the potential and implementation of path sampling. We also discuss some open problems.",0
https://doi.org/10.1198/016214502753479419,Bootstrap Tests for Distributional Treatment Effects in Instrumental Variable Models,"This article considers the problem of assessing the distributional consequences of a treatment on some outcome variable of interest when treatment intake is (possibly) nonrandomized, but there is a binaryinstrument available for the researcher. Such a scenario is common in observational studies and in randomized experiments with imperfect compliance. One possible approach to this problem is to compare the counterfactual cumulative distribution functions of the outcome with and without the treatment. This article shows how to estimate these distributions using instrumental variable methods and a simple bootstrap procedure is proposed to test distributional hypotheses, such as equality of distributions, first-order and second-order stochastic dominance. These tests and estimators are applied to the study of the effects of veteran status on the distribution of civilian earnings. The results show a negative effect of military service during the Vietnam era that appears to be concentrated on the lower tail of ...",0
https://doi.org/10.1093/biomet/90.4.809,Efficient estimation of covariance selection models,"SUMMARY A Bayesian method is proposed for estimating an inverse covariance matrix from Gaussian data. The method is based on a prior that allows the off-diagonal elements of the inverse covariance matrix to be zero, and in many applications results in a parsimonious parameterisation of the covariance matrix. No assumption is made about the structure of the corresponding graphical model, so the method applies to both nondecomposable and decomposable graphs. All the parameters are estimated by model averaging using an efficient Metropolis-Hastings sampling scheme. A simulation study demonstrates that the method produces statistically efficient estimators of the covariance matrix, when the inverse covariance matrix is sparse. The methodology is illustrated by applying it to three examples that are high-dimensional relative to the sample size.",0
https://doi.org/10.18637/jss.v061.i07,WebBUGS: Conducting Bayesian Statistical Analysis Online,"A web interface, named WebBUGS, is developed to conduct Bayesian analysis online over the Internet through OpenBUGS and R. WebBUGS can be used with the minimum requirement of a web browser both remotely and locally. WebBUGS has many collaborative features such as email notification and sharing. WebBUGS also eases the use of OpenBUGS by providing built-in model templates, data management module, and other useful modules. In this paper, the use of WebBUGS is illustrated and discussed.",0
,Graphical models for visual object recognition and tracking,"We develop statistical methods which allow effective visual detection, categorization, and tracking of objects in complex scenes. Such computer vision systems must be robust to wide variations in object appearance; the often small size of training databases, and ambiguities induced by articulated or partially occluded objects. Graphical models provide a powerful framework for encoding the statistical structure of visual scenes, and developing corresponding learning and inference algorithms. In this thesis, we describe several models which integrate graphical representations with nonparametric statistical methods. This approach leads to inference algorithms which tractably recover high-dimensional, continuous object pose variations, and learning procedures which transfer knowledge among related recognition tasks.  Motivated by visual tracking problems, we first develop a nonparametric extension of the belief propagation (BP) algorithm. Using Monte Carlo methods, we provide general procedures for recursively updating particle-based approximations of continuous sufficient statistics. Efficient multiscale sampling methods then allow this nonparametric BP algorithm to be flexibly adapted to many different applications. As a particular example, we consider a graphical model describing the hand's three-dimensional (3D) structure, kinematics, and dynamics. This graph encodes global hand pose via the 3D position and orientation of several rigid components, and thus exposes local structure in a high-dimensional articulated model. Applying nonparametric BP, we recover a hand tracking algorithm which is robust to outliers and local visual ambiguities. Via a set of latent occupancy masks, we also extend our approach to consistently infer occlusion events in a distributed fashion.  In the second half of this thesis, we develop methods for learning hierarchical models of objects, the parts composing them, and the scenes surrounding them. Our approach couples topic models originally developed for text analysis with spatial transformations, and thus consistently accounts for geometric constraints. By building integrated scene models, we may discover contextual relationships, and better exploit partially labeled training images. We first consider images of isolated objects, and show that sharing parts among object categories improves accuracy when learning from few examples. Turning to multiple object scenes, we propose nonparametric models which use Dirichlet processes to automatically learn the number of parts underlying each object category, and objects composing each scene. Adapting these transformed Dirichlet processes to images taken with a binocular stereo camera, we learn integrated, 3D models of object geometry and appearance. This leads to a Monte Carlo algorithm which automatically infers 3D scene structure from the predictable geometry of known object categories. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)",0
https://doi.org/10.1016/j.socscimed.2015.09.040,Educational expansion and the education gradient in health: A hierarchical age-period-cohort analysis,"Researchers have recently been investigating the temporal variation in the educational gradient in health. While there is abundant literature concerning age trajectories, theoretical knowledge about cohort differences is relatively limited. Therefore, in analogy with the life course perspective, we introduce two contrasting cohort-specific hypotheses. The diminishing health returns hypothesis predicts a decrease in educational disparities in health across cohorts. By contrast, the cohort accretion hypothesis suggests that the education-health gap will be more pronounced among younger cohorts. To shed light on this, we perform a hierarchical age-period-cohort analysis (HAPC), using data from a subsample of individuals between 25 and 85 years of age (N = 232,573) from 32 countries in the European Social Survey (six waves: 2002-2012). The analysis leads to three important conclusions. First, we observe a widening health gap between different educational levels over the life course. Second, we find that these educational differences in the age trajectories of health seem to strengthen with each successive birth cohort. However, the two age-related effects disappear when we control for employment status, household income, and family characteristics. Last, when adjusting for these mediators, we reveal evidence to support the diminishing health returns hypothesis, implying that it is primarily the direct association between education and health that decreases across cohorts. This finding raises concerns about potential barriers to education being a vehicle for empowerment and the promotion of health.",0
https://doi.org/10.1093/chemse/bjj001,Evaluation of the Validity of a Maximum Likelihood Adaptive Staircase Procedure for Measurement of Olfactory Detection Threshold in Mice,"Threshold is defined as the stimulus intensity necessary for a subject to reach a specified percent correct on a detection test. MLPEST (maximum likelihood parameter estimation by sequential testing) is a method that is able to determine threshold accurately and more rapidly than many other methods. Originally developed for human auditory and visual tasks, it has been adapted for human olfactory and gustatory tests. In order to utilize this technique for olfactory testing in mice, we have adapted MLPEST methodology for use with computerized olfactometry as a tool to estimate odor detection thresholds. Here we present Monte Carlo simulations and operant conditioning data that demonstrate the potential utility of this technique in mice, we explore the ramifications of altering MLPEST test parameters on performance, and we discuss the advantages and disadvantages of using MLPEST compared to other methods for the estimation of thresholds in rodents. Using MLPEST, we find that olfactory detection thresholds in mice deficient for the cyclic nucleotide-gated channel subunit A2 are similar to those of wild-type animals for odorants the knockout animals are able to detect.",0
https://doi.org/10.1080/01621459.1991.10475006,Generalized Linear Models with Random Effects; a Gibbs Sampling Approach,"Abstract Generalized linear models have unified the approach to regression for a wide variety of discrete, continuous, and censored response variables that can be assumed to be independent across experimental units. In applications such as longitudinal studies, genetic studies of families, and survey sampling, observations may be obtained in clusters. Responses from the same cluster cannot be assumed to be independent. With linear models, correlation has been effectively modeled by assuming there are cluster-specific random effects that derive from an underlying mixing distribution. Extensions of generalized linear models to include random effects has, thus far, been hampered by the need for numerical integration to evaluate likelihoods. In this article, we cast the generalized linear random effects model in a Bayesian framework and use a Monte Carlo method, the Gibbs sampler, to overcome the current computational limitations. The resulting algorithm is flexible to easily accommodate changes in the number...",0
https://doi.org/10.3758/bf03193007,Distribution of the product confidence limits for the indirect effect: Program PRODCLIN,"This article describes a program, PRODCLIN (distribution of the PRODuct Confidence Limits for INdirect effects), written for SAS, SPSS, and R, that computes confidence limits for the product of two normal random variables. The program is important because it can be used to obtain more accurate confidence limits for the indirect effect, as demonstrated in several recent articles (MacKinnon, Lockwood, & Williams, 2004; Pituch, Whittaker, & Stapleton, 2005). Tests of the significance of and confidence limits for indirect effects based on the distribution of the product method have more accurate Type I error rates and more power than other, more commonly used tests. Values for the two paths involved in the indirect effect and their standard errors are entered in the PRODCLIN program, and distribution of the product confidence limits are computed. Several examples are used to illustrate the PRODCLIN program. The PRODCLIN programs in rich text format may be downloaded from www.psychonomic.org/archive.",0
https://doi.org/10.1037/0033-2909.93.3.549,Analysis of multiplicative combination rules when the causal variables are measured with error.,"Evaluates the validity of the observational method used to test multiplicative combination rules with respect to 2 measurement issues: measurement level (i.e., the effects produced by allowing monotonic transformations of the measures) and measurement error (i.e., the effects produced by using unreliable measures of the causal variables). The evaluation is based on a theoretical distinction between the structural model (the set of equations relating theoretical constructs to each other) and the measurement model (the set of equations relating the theoretical constructs to the observed measures). It is concluded that hierarchical regression analysis is inadequate for determining whether the structural model is additive or multiplicative for 2 reasons: First, an additive structural model may produce multiplicative effects through a nonlinear measurement model. Second, a multiplicative structural model may produce nondetectable multiplicative effects because of multiplicative measurement error. Some alternatives to hierarchical regression analysis are described. (35 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1983 American Psychological Association.",0
https://doi.org/10.1198/016214501750332758,Gibbs Sampling Methods for Stick-Breaking Priors,"A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson–Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a Polya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies t...",0
https://doi.org/10.1371/journal.pbio.0050056,Auditory Short-Term Memory Behaves Like Visual Short-Term Memory,"Are the information processing steps that support short-term sensory memory common to all the senses? Systematic, psychophysical comparison requires identical experimental paradigms and comparable stimuli, which can be challenging to obtain across modalities. Participants performed a recognition memory task with auditory and visual stimuli that were comparable in complexity and in their neural representations at early stages of cortical processing. The visual stimuli were static and moving Gaussian-windowed, oriented, sinusoidal gratings (Gabor patches); the auditory stimuli were broadband sounds whose frequency content varied sinusoidally over time (moving ripples). Parallel effects on recognition memory were seen for number of items to be remembered, retention interval, and serial position. Further, regardless of modality, predicting an item's recognizability requires taking account of (1) the probe's similarity to the remembered list items (summed similarity), and (2) the similarity between the items in memory (inter-item homogeneity). A model incorporating both these factors gives a good fit to recognition memory data for auditory as well as visual stimuli. In addition, we present the first demonstration of the orthogonality of summed similarity and inter-item homogeneity effects. These data imply that auditory and visual representations undergo very similar transformations while they are encoded and retrieved from memory.",0
https://doi.org/10.1016/j.csda.2010.03.016,An encompassing prior generalization of the Savage–Dickey density ratio,"An encompassing prior (EP) approach to facilitate Bayesian model selection for nested models with inequality constraints has been previously proposed. In this approach, samples are drawn from the prior and posterior distributions of an encompassing model that contains an inequality restricted version as a special case. The Bayes factor in favor of the inequality restriction then simplifies to the ratio of the proportions of posterior and prior samples consistent with the inequality restriction. This formalism has been applied almost exclusively to models with inequality or ''about equality'' constraints. It is shown that the EP approach naturally extends to exact equality constraints by considering the ratio of the heights for the posterior and prior distributions at the point that is subject to test (i.e., the Savage-Dickey density ratio). The EP approach generalizes the Savage-Dickey ratio method, and can accommodate both inequality and exact equality constraints. The general EP approach is found to be a computationally efficient procedure to calculate Bayes factors for nested models. However, the EP approach to exact equality constraints is vulnerable to the Borel-Kolmogorov paradox, the consequences of which warrant careful consideration.",0
https://doi.org/10.1016/j.jmp.2010.06.005,Distribution-free tests of stochastic dominance for small samples,"One variable is said to “stochastically dominate” another if the probability of observations smaller than x is greater for one variable than the other, for all x. Inferring stochastic dominance from data samples is important for many applications of econometrics and experimental psychology, but little is known about the performance of existing inferential methods. Through simulation, we show that three of the most widely used inferential methods are inadequate for use in small samples of the size commonly encountered in many applications (up to 400 observations from each distribution). We develop two new inferential methods that perform very well in a limited, but practically important, case where the two variables are guaranteed not to be equal in distribution. We also show that extensions of these new methods, and an improved version of an existing method, perform quite well in the original, unlimited case.",0
https://doi.org/10.4324/9780203882924,Cross-Cultural Analysis,"Intended to bridge the gap between the latest methodological developments and cross-cultural research, this interdisciplinary resource presents the latest strategies for analyzing cross-cultural data. Techniques are demonstrated through the use of applications that employ cross national data sets such as the latest European Social Survey. With an emphasis on the generalized latent variable approach, internationally-prominent researchers from a variety of fields explain how the methods work, how to apply them, and how they relate to other methods presented in the book. Syntax and graphical and verbal explanations of the techniques are included. A website features some of the data sets and syntax commands used in the book.  Applications from the behavioral and social sciences that use real data-sets demonstrate:   * The use of samples from 17 countries to validate the resistance to change scale across these nations  * How to test the cross-national invariance properties of social trust  * The interplay between social structure, religiosity, values, and social attitudes  * A comparison of anti-immigrant attitudes and patterns of religious orientations across European countries.  The book is divided into techniques for analyzing cross-cultural data within the generalized-latent-variable approach: multiple-group confirmatory factor analysis and multiple-group structural equation modeling; multi-level analysis; latent class analysis; and item-response theory. Since researchers from various disciplines often use different methodological approaches, a consistent framework for describing and applying each method is used so as to cross 'methodological borders' between disciplines. Some chapters describe the basic strategy and how it relates to other techniques presented in the book, others apply the techniques and address specific research questions, and a few combine the two. A table in the preface highlights for each chapter: a brief description of the contents, the statistical methods used, the goal(s) of the analysis, and the data set employed. This book is intended for researchers, practitioners, and advanced students interested in cross-cultural research. Because the applications span a variety of disciplines, the book will appeal to cross-cultural researchers and students in: psychology, political science, sociology, education, marketing and economics, geography, criminology, psychometrics, epidemiology, and public health, as well as those interested in social-science and behavioral methodology. It is also appropriate for an advanced methods course in cross-cultural analysis.",0
https://doi.org/10.1016/b978-0-12-742780-5.50009-3,Estimation of Parameters in the Three-Parameter Latent Trait Model,"This chapter discusses a study to investigate the efficiency of the Urry procedure and the maximum likelihood procedure to estimate parameters in the three-parameter model, to study the properties of the estimators, and to provide some guidelines regarding the conditions under which they should be employed. In particular, the issues investigated were (1) the accuracy of the two estimation procedures; (2) the relations among the number of items, examinees, and the accuracy of estimation; (3) the effect of the distribution of ability on the estimates of item and ability parameters; and (4) the statistical properties, such as bias and consistency, of the estimators. To investigate the issues mentioned above, artificial data were generated according to the three-parameter logistic model using the DATGEN program of Hambleton and Rovinelli. Data were generated to simulate various testing situations by varying the test length, the number of examinees, and the ability distribution of the examinees. In the Urry estimation procedure, the relationships that exist for item discrimination and item difficulty between the latent trait theory parameters and the classical item parameters are exploited. These relationships are derived under the assumption that ability is normally distributed and that the item characteristic curve is the normal ogive. To study how the departures from the assumption of normally distributed abilities affect the Urry procedure, three ability distributions were considered: normal, uniform, and negatively skewed.",0
https://doi.org/10.2307/2530095,The Product of Two Normally Distributed Random Variables,,0
https://doi.org/10.1037//0021-9010.86.3.468,Implications of empirical Bayes meta-analysis for test validation.,"Empirical Bayes meta-analysis provides a useful framework for examining test validation. The fixedeffects case in which Ï� has a single value corresponds to the inference that the situational specificity hypothesis can be rejected in a validity generalization study. A Bayesian analysis of such a case provides a simple and powerful test of Ï� = 0; such a test has practical implications for significance testing in test validation. The random-effects case in which Ïƒ2Ï� > 0 provides an explicit method with which to assess the relative importance of local validity studies and previous meta-analyses. Simulated data are used to illustrate both cases. Results of published meta-analyses are used to show that local validation becomes increasingly important as Ïƒ2Ï� increases. The meaning of the term validity generalization is explored, and the problem of what can be inferred about test transportability in the random-effects case is described.",0
https://doi.org/10.1523/jneurosci.0362-10.2010,A Study of Clustered Data and Approaches to Its Analysis,"Statistical analysis is critical in the interpretation of experimental data across the life sciences, including neuroscience. The nature of the data collected has a critical role in determining the best statistical approach to take. One particularly prevalent type of data is referred to as ""clustered data."" Clustered data are characterized as data that can be classified into a number of distinct groups or ""clusters"" within a particular study. Clustered data arise most commonly in neuroscience when data are compiled across multiple experiments, for example in electrophysiological or optical recordings taken from synaptic terminals, with each experiment providing a distinct cluster of data. However, there are many other types of experimental design that can yield clustered data. Here, we provide a statistical model for intracluster correlation and systematically investigate a range of methods for analyzing clustered data. Our analysis reveals that it is critical to take data clustering into account and suggests appropriate statistical approaches that can be used to account for data clustering.",0
,DEALING WITH SPATIAL NORMALIZATION ERRORS IN fMRI GROUP INFERENCE USING HIERARCHICAL MODELING,"An important challenge in neuroimaging multi-subject studies is to take into account that different brains cannot be aligned perfectly. To this end, we extend the classical mass univariate model for group analysis to incorporate uncer- tainty on localization by introducing, for each subject, a spatial jitter variable to be marginalized out. We derive a Bayes factor to test for the mean popula- tion effect's sign in each voxel of a search volume, and discuss a Gibbs sampler to compute it. This Bayes factor, which generalizes the classical t-statistic, may be combined with a permutation test in order to control the frequentist false positive rate. Results on both simulated and experimental data suggest that this test may outperform conventional mass univariate tests in terms of detection power, while limiting the problem of overestimating the size of activity clusters.",0
https://doi.org/10.1080/00949659608811729,The posterior distribution of the fixed and random effects in a mixed-effects linear model,"The subject of this paper is Bayesian inference about the fixed and random effects of a mixed-effects linear statistical model with two variance components. It is assumed that a priori the fixed effects have a noninformative distribution and that the reciprocals of the variance components are distributed independently (of each other and of the fixed effects) as gamma random variables. It is shown that techniques similar to those employed in a ridge analysis of a response surface can be used to construct a one-dimensional curve that contains all of the stationary points of the posterior density of the random effects. The “ridge analysis” (of the posterior density) can be useful (from a computational standpoint) in finding the number and the locations of the stationary points and can be very informative about various features of the posterior density. Depending on what is revealed by the ridge analysis, a multivariate normal or multivariate-t distribution that is centered at a posterior mode may provide a s...",0
https://doi.org/10.1198/016214508000000724,Domain-Level Covariance Analysis for Multilevel Survey Data With Structured Nonresponse,"Health care quality surveys in the United States are administered to individual respondents (i.e., hospital patients, health plan members) to evaluate the performance of health care organizations (i.e., hospitals, health plans), which thus constitute estimation domains. For better understanding and more parsimonious reporting of dimensions of quality, we analyze relationships among quality measures at the domain level. Rather than specifying a full parametric model for the observed responses and the nonresponse patterns at the lower (patient) level, we first fit generalized variance–covariance functions that take into account nonresponse patterns in the survey responses, then specify a likelihood function for the domain mean responses using these generalized variance–covariance functions. This allows us to model directly the relationships among domain means for different items. Because the response scales are bounded, we assume that these means follow a truncated multivariate normal distribution. We calcu...",0
https://doi.org/10.2307/2533455,Variance Components Testing in the Longitudinal Mixed Effects Model,"This article discusses the asymptotic behavior of likelihood ratio tests for nonzero variance components in the longitudinal mixed effects linear model described by Laird and Ware (1982, Biometrics 38, 963-974). Our discussion of the large-sample behavior of likelihood ratio tests for nonzero variance components is based on the results for nonstandard testing situations by Self and Liang (1987, Journal of the American Statistical Association 82, 605-610).",0
https://doi.org/10.1016/s0169-7161(07)27013-0,13 Structural Equation Modeling,"Structural equation modeling (SEM) is a multivariate statistical technique for testing hypotheses about the influences of sets of variables on other variables. Hypotheses can involve correlational and regression-like relations among observed variables as well as latent variables. The adequacy of such hypotheses is evaluated by modeling the mean and covariance structures of the observed variables. After an introduction, we present the statistical model. Then we discuss estimation methods and hypothesis tests with an emphasis on the maximum likelihood method based on the assumption of multivariate normal data, including the issues of model (parameter) identification and regularity conditions. We also discuss estimation and testing with non-normal data and with misspecified models, as well as power analysis. To supplement model testing, fit indices have been developed to measure the degree of fit for a SEM model. We describe the major ones. When an initial model does not fit well, Lagrange Multiplier (score) and Wald tests can be used to identify how an initial model might be modified. In addition to these standard topics, we discuss extensions of the model to multiple groups, to repeated observations (growth curve SEM), to data with a hierarchical structure (multi-level SEM), and to nonlinear relationships between latent variables. We also discuss more practical topics such as treatment of missing data, categorical dependent variables, and software information.",0
https://doi.org/10.1037/fam0000098,"Dyadic conflict, drinking to cope, and alcohol-related problems: A psychometric study and longitudinal actor–partner interdependence model.","The motivational model of alcohol use posits that individuals may consume alcohol to cope with negative affect. Conflict with others is a strong predictor of coping motives, which in turn predict alcohol-related problems. Two studies examined links between conflict, coping motives, and alcohol-related problems in emerging adult romantic dyads. It was hypothesized that the association between conflict and alcohol-related problems would be mediated by coping-depression and coping-anxiety motives. It was also hypothesized that this would be true for actor (i.e., how individual factors influence individual behaviors) and partner effects (i.e., how partner factors influence individual behaviors) and at the between- (i.e., does not vary over the study period) and within-subjects (i.e., varies over the study period) levels. Both studies examined participants currently in a romantic relationship who consumed ≥12 alcoholic drinks in the past year. Study 1 was cross-sectional using university students (N = 130 students; 86.9% female; M = 21.02 years old, SD = 3.43). Study 2 used a 4-wave, 4-week longitudinal design with romantic dyads (N = 100 dyads; 89% heterosexual; M = 22.13 years old, SD = 5.67). In Study 2, coping-depression motives emerged as the strongest mediator of the conflict-alcohol-related problems association, and findings held for actor effects but not partner effects. Supplemental analyses revealed that this mediational pathway only held among women. Within any given week, alcohol-related problems changed systematically in the same direction between romantic partners. Interventions may wish to target coping-depression drinking motives within couples in response to conflict to reduce alcohol-related problems.",0
https://doi.org/10.1027/1614-2241.5.1.7,Item Selection Rules in Computerized Adaptive Testing,"The item selection rule (ISR) most commonly used in computerized adaptive testing (CAT) is to select the item with maximum Fisher information for the current trait estimation (PFI). Several alternative ISRs have been proposed. Among them, Fisher information considered in an interval (FI*I), Fisher information weighted with the likelihood function (FI*L), Kullback-Leibler information considered in an interval (KL*I) and Kullback-Leibler weighted with the likelihood function (KL*L) have shown a greater precision of trait estimation at the early stages of CAT. A new ISR is proposed, Fisher information by interval with geometric mean (FI*IG), which tries to rectify some detected problems in FI*I. We evaluate accuracy and item bank security for these six ISRs. FI*IG is the only ISR which simultaneously outperforms PFI in both variables. For the other ISRs, there seems to be a trade-off between accuracy and security, PFI being the one with worse accuracy and greater security, and the ISRs using the likelihood function the reverse.",0
https://doi.org/10.2307/3235360,"Economics and Public Support for the European Union: An Analysis at the National, Regional, and Individual Levels","The intention of this paper is to explore the sources of public attitudes toward the European Union over the last two decades, using cumulative data available from the Eurobarometer series of publi...",0
https://doi.org/10.5705/ss.2010.106,Default Bayesian analysis for multivariate generalized CAR models,"In recent years, multivariate spatial models have been proven to be an effective tool for analyzing spatially related multidimensional data arising from a common underlying spatial process. Currently, the Bayesian analysis is perhaps the only solution available in this framework where prior selection plays an important role in the inference. The present article contributes towards the development of Bayesian inferential methodology for multivariate generalized linear mixed models, in particular, multivariate conditional autoregressive (CAR) models. The two main contributions of this article are the development of a shrinkage-type default prior for the covariance matrices and innovative computational techniques for the Gibbs sampling implementation. The default prior elicitation is non-informative but results in a proper posterior on the related parameter spaces. This elicitation not only provides robust inference (with respect to prior choice), but also provides improved estimation. In the computational step, we have developed a transformation of the parameters that avoids sampling from restricted domains, thus providing more stability and efficiency in the Gibbs implementation. The methodology has been extended to the case of missing responses in the multi-dimensional setup. Both simulations and real examples are provided to validate and illustrate the proposed methodology.",0
https://doi.org/10.1207/s15328007sem0802_7,Monte Carlo Experiments: Design and Implementation,"The use of Monte Carlo simulations for the empirical assessment of statistical estimators is becoming more common in structural equation modeling research. Yet, there is little guidance for the researcher interested in using the technique. In this article we illustrate both the design and implementation of Monte Carlo simulations. We present 9 steps in planning and performing a Monte Carlo analysis: (1) developing a theoretically derived research question of interest, (2) creating a valid model, (3) designing specific experimental conditions, (4) choosing values of population parameters, (5) choosing an appropriate software package, (6) executing the simulations, (7) file storage, (8) troubleshooting and verification, and (9) summarizing results. Throughout the article, we use as a running example a Monte Carlo simulation that we performed to illustrate many of the relevant points with concrete information and detail.",0
https://doi.org/10.1177/0146621605282774,A Feedback Control Strategy for Enhancing Item Selection Efficiency in Computerized Adaptive Testing,"A computerized adaptive test (CAT) may be modeled as a closed-loop system, where item selection is influenced by trait level (θ) estimation and vice versa. When discrepancies exist between an examinee's estimated and true θ levels, nonoptimal item selection is a likely result. Nevertheless, examinee response behavior consistent with optimal item selection can be predicted using item response theory (IRT), without knowledge of an examinee's true θ level, yielding a specific reference point for applying an internal correcting or feedback control mechanism. Incorporating such a mechanism in a CAT is shown to be an effective strategy for increasing item selection efficiency. Results from simulation studies using maximum likelihood (ML) and modal a posteriori (MAP) trait-level estimation and Fisher information (FI) and Fisher interval information (FII) item selection are provided.",0
https://doi.org/10.1080/1359432x.2015.1063486,"The effect of state core self-evaluations on task performance, organizational citizenship behaviour, and counterproductive work behaviour","Although the personality–performance relationship has been studied extensively, most studies focused on the relationship between between-person differences in the Big Five personality dimensions and between-person differences in job performance. The current paper extends this research in two ways. First, we build on core self-evaluations (CSEs): an alternative, broad personality dimension that has proven to be a good predictor of job performance. Second, we tested concurrent and lagged within-person relationships between CSEs and task performance, organizational citizenship behaviour (OCB), and counterproductive work behaviour (CWB). To this end, we conducted two experience sampling studies; the first one assessing the relationship between state CSEs and levels of momentary task performance and OCB, and a second study in which employees reported on their level of state CSEs and momentary CWB. Results showed that there is substantial within-person variability in CSEs and that these within-person fluctuatio...",0
https://doi.org/10.1007/s11031-015-9494-x,Attuned to the positive? Awareness and responsiveness to others’ positive emotion experience and display,"Positive emotions are implicated in affiliation and cooperation processes that are central to human social life. For this reason, we hypothesized that people should be highly aware of and responsive to the positive emotions of others. Study 1 examined awareness by testing the accuracy with which perceivers tracked othersâ€™ positive emotions. Study 2 examined responsiveness by testing whether positive emotions were predictive of perceivers responding to new relationship opportunity. In Study 1, multilevel analyses of dating couplesâ€™ estimates of their partnerâ€™s emotions across four semi-structured interactions revealed that both women and men tracked partner positive emotions with considerable accuracy. Additional analyses indicated that tracking accuracy was most pronounced for positive emotions whose display is known to include the Duchenne smile. In Study 2, multilevel analyses of dyads who watched a set of positive and negative emotion-eliciting film clips with a stranger indicated that only positive emotion display predicted subsequent closeness. Together, these findings show that people are highly attuned to the positive emotions of others and can be more attuned to othersâ€™ positive emotions than negative emotions. Â© 2015, Springer Science+Business Media New York.",0
https://doi.org/10.1111/j.1541-0420.2009.01227.x,Bayesian Inference in Semiparametric Mixed Models for Longitudinal Data,"We consider Bayesian inference in semiparametric mixed models (SPMMs) for longitudinal data. SPMMs are a class of models that use a nonparametric function to model a time effect, a parametric function to model other covariate effects, and parametric or nonparametric random effects to account for the within-subject correlation. We model the nonparametric function using a Bayesian formulation of a cubic smoothing spline, and the random effect distribution using a normal distribution and alternatively a nonparametric Dirichlet process (DP) prior. When the random effect distribution is assumed to be normal, we propose a uniform shrinkage prior (USP) for the variance components and the smoothing parameter. When the random effect distribution is modeled nonparametrically, we use a DP prior with a normal base measure and propose a USP for the hyperparameters of the DP base measure. We argue that the commonly assumed DP prior implies a nonzero mean of the random effect distribution, even when a base measure with mean zero is specified. This implies weak identifiability for the fixed effects, and can therefore lead to biased estimators and poor inference for the regression coefficients and the spline estimator of the nonparametric function. We propose an adjustment using a postprocessing technique. We show that under mild conditions the posterior is proper under the proposed USP, a flat prior for the fixed effect parameters, and an improper prior for the residual variance. We illustrate the proposed approach using a longitudinal hormone dataset, and carry out extensive simulation studies to compare its finite sample performance with existing methods.",0
https://doi.org/10.1080/00031305.1992.10475842,A Note on the Delta Method,Abstract The delta method is an intuitive technique for approximating the moments of functions of random variables. This note reviews the delta method and conditions under which delta-method approximate moments are accurate.,0
https://doi.org/10.1080/07399330802523824,Violence and Women's Psychological Distress After Birth: An Exploratory Study in Italy,"Our aim in conducting this study was to analyze the relationships between violence and maternal psychological distress 8 months after a birth, taking into account other important psychosocial factors, known to be associated both with violence and with new mothers’ mental health. A total of 352 women responded to a questionnaire after the birth at a maternity hospital in northern Italy, and 292 also participated in a telephone interview 8 months later. We evaluated psychological distress with the General Health Questionnaire (GHQ), and partner and family violence with a 28-item scale. Eight months postpartum, 5% of women showed high psychological distress; 10% were currently experiencing violence from the partner or another family member. After adjustment for covariates, the odds ratio for depressive symptoms was 13.74 for women experiencing violence. We believe that these results provide support for the important role of violence in postpartum maternal psychological distress.",0
https://doi.org/10.1002/9780470181218.ch7,Spatial Epidemiology,,0
https://doi.org/10.1080/10705511.2013.797827,Multivariate Meta-Analysis as Structural Equation Models,"Multivariate meta-analysis has become increasingly popular in the educational, social, and medical sciences. It is because the outcome measures in a meta-analysis can involve more than one effect size. This article proposes 2 mathematically equivalent models to implement multivariate meta-analysis in structural equation modeling (SEM). Specifically, this article shows how multivariate fixed-, random- and mixed-effects meta-analyses can be formulated as structural equation models. metaSEM (a free R package based on OpenMx) and Mplus are used to implement the proposed procedures. A real data set is used to illustrate the procedures. Formulating multivariate meta-analysis as structural equation models provides many new research opportunities for methodological development in both meta-analysis and SEM. Issues related to and extensions on the SEM-based meta-analysis are discussed.",0
https://doi.org/10.1073/pnas.0711295105,An assessment of fixed-capacity models of visual working memory,"Visual working memory is often modeled as having a fixed number of slots. We test this model by assessing the receiver operating characteristics (ROC) of participants in a visual-working-memory change-detection task. ROC plots yielded straight lines with a slope of 1.0, a tell-tale characteristic of all-or-none mnemonic representations. Formal model assessment yielded evidence highly consistent with a discrete fixed-capacity model of working memory for this task.",0
https://doi.org/10.1080/0305498930190401,A Multilevel Analysis of School Examination Results,"Data on examination results from inner London schools are analysed in relation to intake achievement, pupil gender and school type. The examination achieve- ment, averaged over subjects, is studied as is achievement in the separate subjects of mathematics and English. Multilevel models are fitted, so that the variation between schools can be studied. It is shown that confidence intervals for school 'residuals' or 'effects' are wide, so that few schools can be separated reliably. In particular, no fine rank ordering of schools legitimately can be produced. A bivariate model for mathematics and English examination achievement scores is fitted. The student level variance for both subjects is shown to increase from the lowest to the highest intake achievement group, with moderately high correlation between the subjects. The paper discusses the implications of these findings for the publication of 'league tables' of school examination and test scores.",0
https://doi.org/10.1111/1467-9868.00082,The EM Algorithm-an Old Folk-song Sung to a Fast New Tune,"Celebrating the 20th anniversary of the presentation of the paper by Dempster, Laird and Rubin which popularized the EM algorithm, we investigate, after a brief historical account, strategies that aim to make the EM algorithm converge faster while maintaining its simplicity and stability (e.g. automatic monotone convergence in likelihood). First we introduce the idea of a ‘working parameter’ to facilitate the search for efficient data augmentation schemes and thus fast EM implementations. Second, summarizing various recent extensions of the EM algorithm, we formulate a general alternating expectation–conditional maximization algorithm AECM that couples flexible data augmentation schemes with model reduction schemes to achieve efficient computations. We illustrate these methods using multivariate t-models with known or unknown degrees of freedom and Poisson models for image reconstruction. We show, through both empirical and theoretical evidence, the potential for a dramatic reduction in computational time with little increase in human effort. We also discuss the intrinsic connection between EM-type algorithms and the Gibbs sampler, and the possibility of using the techniques presented here to speed up the latter. The main conclusion of the paper is that, with the help of statistical considerations, it is possible to construct algorithms that are simple, stable and fast.",0
https://doi.org/10.1016/j.jmva.2014.11.007,"A Bayesian method for analyzing combinations of continuous, ordinal, and nominal categorical data with missing values","From a Bayesian perspective, we propose a general method for analyzing a combination of continuous, ordinal (including binary), and categorical/nominal multivariate measures with missing values. We assume multivariate normal linear regression models for multivariate continuous measures, multivariate probit models for correlated ordinal measures, and multivariate multinomial probit models for multivariate categorical/nominal measures. Then we assume a multivariate normal linear model on the continuous vector comprised of continuous variables and those underlying normal variables for ordinal variables from multivariate probit models and for categorical variables from multinomial probit models. We develop a Markov chain Monte Carlo (MCMC) algorithm to estimate unknown parameters including regression parameters, cut-points for ordinal data from the multivariate probit models, and the covariance matrix encompassing both continuous variables and the underlying normal latent variables. Combining the continuous variables and the normal latent variables allows us to model combinations of continuous, ordinal, and categorical multivariate data simultaneously. The framework incorporates flexible priors for the covariance matrix, provides a foundation for inference about the underlying covariance structure, and imputes missing data where needed. The method is illustrated through simulated examples and two real data applications.",0
https://doi.org/10.1016/j.jpain.2010.01.271,Pain Intensity and Duration Can Be Enhanced by Prior Challenge: Initial Evidence Suggestive of a Role of Microglial Priming,"<h2>Abstract</h2> Activation of spinal microglia and consequent release of proinflammatory mediators facilitate pain. Under certain conditions, responses of activated microglia can become enhanced. Enhanced microglial production of proinflammatory products may result from priming (sensitization), similar to macrophage priming. We hypothesized that if spinal microglia were primed by an initial inflammatory challenge, subsequent challenges may create enhanced pain. Here, we used a ""two-hit"" paradigm using 2 successive challenges, which affect overlapping populations of spinal microglia, presented 2 weeks apart. Mechanical allodynia and/or activation of spinal glia were assessed. Initially, laparotomy preceded systemic lipopolysaccharide (LPS). Prior laparotomy caused prolonged microglial (not astrocyte) activation plus enhanced LPS-induced allodynia. In this ""two-hit"" paradigm, minocycline, a microglial activation inhibitor, significantly reduced later exaggerated pain induced by prior surgery when minocycline was administered intrathecally for 5 days starting either at the time of surgery or 5 days before LPS administration. To test generality of the priming effect, subcutaneous formalin preceded intrathecal HIV-1 gp120, which activates spinal microglia and causes robust allodynia. Prior formalin enhanced intrathecal gp120-induced allodynia, suggesting that microglial priming is not limited to laparotomy and again supporting a spinal site of action. Therefore, spinal microglial priming may increase vulnerability to pain enhancement. <h3>Perspective</h3> Spinal microglia may become ""primed"" (sensitized) following their activation by disparate forms of peripheral trauma/inflammation. As a result, such primed microglia may overrespond to subsequent challenges, thereby enhancing pain intensity and duration.",0
https://doi.org/10.1007/978-3-319-11424-8_6,Flexibly-Bounded Rationality in Interstate Conflict,"This chapter applies the theory of flexibly bounded rationality to interstate conflict. Flexibly bounded rationality is a theory that states that the bounds prescribed by Herbert Simon in his theory of bounded rationality are flexible. On contextualizing the theory of flexibly bounded rationality, inference, the theory of rational expectation, the theory of rational choice and the theory of rational conterfactuals are described. The theory of flexibly bounded rationality is applied for decision making process. This is done by using a multi-layer perceptron network and particle swarm optimization.",0
https://doi.org/10.1111/j.1541-0420.2008.01107.x,Testing Random Effects in the Linear Mixed Model Using Approximate Bayes Factors,"Deciding which predictor effects may vary across subjects is a difficult issue. Standard model selection criteria and test procedures are often inappropriate for comparing models with different numbers of random effects due to constraints on the parameter space of the variance components. Testing on the boundary of the parameter space changes the asymptotic distribution of some classical test statistics and causes problems in approximating Bayes factors. We propose a simple approach for testing random effects in the linear mixed model using Bayes factors. We scale each random effect to the residual variance and introduce a parameter that controls the relative contribution of each random effect free of the scale of the data. We integrate out the random effects and the variance components using closed-form solutions. The resulting integrals needed to calculate the Bayes factor are low-dimensional integrals lacking variance components and can be efficiently approximated with Laplace's method. We propose a default prior distribution on the parameter controlling the contribution of each random effect and conduct simulations to show that our method has good properties for model selection problems. Finally, we illustrate our methods on data from a clinical trial of patients with bipolar disorder and on data from an environmental study of water disinfection by-products and male reproductive outcomes.",0
https://doi.org/10.1348/000711007x270843,Nearly unbiased estimators for the three-parameter Weibull distribution with greater efficiency than the iterative likelihood method,"The maximum likelihood estimation (MLE) method is the most commonly used method to estimate the parameters of the three-parameter Weibull distribution. However, it returns biased estimates. In this paper, we show how to calculate weights which cancel the biases contained in the MLE equations. The exact weights can be computed when the population parameters are known and the expected weights when they are not. Two of the three weights' expected values are dependent only on the sample size, whereas the third also depends on the population shape parameters. Monte Carlo simulations demonstrate the practicability of the weighted MLE method. When compared with the iterative MLE technique, the bias is reduced by a factor of 7 (irrespective of the sample size) and the variability of the parameter estimates is also reduced by a factor of 7 for very small sample sizes, but this gain disappears for large sample sizes.",0
https://doi.org/10.1093/esr/jcv059,Multilevel Modelling of Country Effects: A Cautionary Tale,"Country effects on outcomes for individuals are often analysed using multilevel (hierarchical) models applied to harmonized multi-country data sets such as ESS, EU-SILC, EVS, ISSP, and SHARE. We point out problems with the assessment of country effects that appear not to be widely appreciated, and develop our arguments using Monte Carlo simulation analysis of multilevel linear and logit models. With large sample sizes of individuals within each country but only a small number of countries, analysts can reliably estimate individual-level effects but estimates of parameters summarizing country effects are likely to be unreliable. Multilevel modelling methods are no panacea.",0
https://doi.org/10.2307/2998560,On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects,"In this paper, the role of the propensity score in the efficient estimation of average treatment effects is examined. Under the assumption that the treatment is ignorable given some observed characteristics, it is shown that the propensity score is ancillary for estimation of the average treatment effects. The propensity score is not ancillary for estimation of average treatment effects on the treated. It is suggested that the marginal value of the propensity score lies entirely in the dimension reduction. Efficient semiparametric estimators of average treatment effects and average treatment effects on the treated are shown to take the form of relevant sample averages of the data completed by the nonparametric imputation method. It is shown that the projection on the propensity score is not necessary for efficient semiparametric estimation of average treatment effects on the treated even if the propensity score is known. An application to the experimental data reveals that conditioning on the propensity score may even result in a loss of efficiency.",0
https://doi.org/10.3758/pbr.15.6.1209,Bayesian and maximum likelihood estimation of hierarchical response time models,"Hierarchical (or multilevel) statistical models have become increasingly popular in psychology in the last few years. In this article, we consider the application of multilevel modeling to the ex-Gaussian, a popular model of response times. We compare single-level and hierarchical methods for estimation of the parameters of ex-Gaussian distributions. In addition, for each approach, we compare maximum likelihood estimation with Bayesian estimation. A set of simulations and analyses of parameter recovery show that although all methods perform adequately well, hierarchical methods are better able to recover the parameters of the ex-Gaussian, by reducing variability in the recovered parameters. At each level, little overall difference was observed between the maximum likelihood and Bayesian methods.",1
https://doi.org/10.1214/12-ba722,Commensurate Priors for Incorporating Historical Information in Clinical Trials Using General and Generalized Linear Models,"Assessing between-study variability in the context of conventional random-effects meta-analysis is notoriously difficult when incorporating data from only a small number of historical studies. In order to borrow strength, historical and current data are often assumed to be fully homogeneous, but this can have drastic consequences for power and Type I error if the historical information is biased. In this paper, we propose empirical and fully Bayesian modifications of the commensurate prior model (Hobbs et al. 2011) extending Pocock (1976), and evaluate their frequentist and Bayesian properties for incorporating patient-level historical data using general and generalized linear mixed regression models. Our proposed commensurate prior models lead to preposterior admissible estimators that facilitate alternative bias-variance trade-offs than those offered by pre-existing methodologies for incorporating historical data from a small number of historical studies. We also provide a sample analysis of a colon cancer trial comparing time-to-disease progression using a Weibull regression model.",0
,The College Board computerized placement tests: an application of computerized adaptive testing,,0
https://doi.org/10.1002/sim.1847,Statistical assessment of mediational effects for logistic mediational models,"The concept of mediation has broad applications in medical health studies. Although the statistical assessment of a mediational effect under the normal assumption has been well established in linear structural equation models (SEM), it has not been extended to the general case where normality is not a usual assumption. In this paper, we propose to extend the definition of mediational effects through causal inference. The new definition is consistent with that in linear SEM and does not rely on the assumption of normality. Here, we focus our attention on the logistic mediation model, where all variables involved are binary. Three approaches to the estimation of mediational effects-Delta method, bootstrap, and Bayesian modelling via Monte Carlo simulation are investigated. Simulation studies are used to examine the behaviour of the three approaches. Measured by 95 per cent confidence interval (CI) coverage rate and root mean square error (RMSE) criteria, it was found that the Bayesian method using a non-informative prior outperformed both bootstrap and the Delta methods, particularly for small sample sizes. Case studies are presented to demonstrate the application of the proposed method to public health research using a nationally representative database. Extending the proposed method to other types of mediational model and to multiple mediators are also discussed.",0
https://doi.org/10.1111/j.1745-3984.2000.tb01076.x,Monte Carlo Based Null Distribution for an Alternative Goodness-of-Fit Test Statistic in IRT Models,"Assessing the correspondence between model predictions and observed data is a recommended procedure for justifying the application of an IRT model. However, with shorter tests, current goodness-of-fit procedures that assume precise point estimates of ability, are inappropriate. The present paper describes a goodness-of-fit statistic that considers the imprecision with which ability is estimated and involves constructing item fit tables based on each examinee's posterior distribution of ability, given the likelihood of their response pattern and an assumed marginal ability distribution. However, the posterior expectations that are computed are dependent and the distribution of the goodness-of-fit statistic is unknown. The present paper also describes a Monte Carlo resampling procedure that can be used to assess the significance of the fit statistic and compares this method with a previously used method. The results indicate that the method described herein is an effective and reasonably simple procedure for assessing the validity of applying IRT models when ability estimates are imprecise.",0
https://doi.org/10.1007/bf02294210,"A general structural equation model with dichotomous, ordered categorical, and continuous latent variable indicators","A structural equation model is proposed with a generalized measurement part, allowing for dichotomous and ordered categorical variables (indicators) in addition to continuous ones. A computationally feasible three-stage estimator is proposed for any combination of observed variable types. This approach provides large-sample chi-square tests of fit and standard errors of estimates for situations not previously covered. Two multiple-indicator modeling examples are given. One is a simultaneous analysis of two groups with a structural equation model underlying skewed Likert variables. The second is a longitudinal model with a structural model for multivariate probit regressions.",0
https://doi.org/10.1111/j.0006-341x.1999.00463.x,Finite Mixture Modeling with Mixture Outcomes Using the EM Algorithm,"This paper discusses the analysis of an extended finite mixture model where the latent classes corresponding to the mixture components for one set of observed variables influence a second set of observed variables. The research is motivated by a repeated measurement study using a random coefficient model to assess the influence of latent growth trajectory class membership on the probability of a binary disease outcome. More generally, this model can be seen as a combination of latent class modeling and conventional mixture modeling. The EM algorithm is used for estimation. As an illustration, a random-coefficient growth model for the prediction of alcohol dependence from three latent classes of heavy alcohol use trajectories among young adults is analyzed.",0
https://doi.org/10.1016/j.jspi.2010.03.046,Pseudo-empirical Bayes estimation of small area means under a nested error linear regression model with functional measurement errors,"Abstract Small area estimation is studied under a nested error linear regression model with area level covariate subject to measurement error. Ghosh and Sinha (2007) obtained a pseudo-Bayes (PB) predictor of a small area mean and a corresponding pseudo-empirical Bayes (PEB) predictor, using the sample means of the observed covariate values to estimate the true covariate values. In this paper, we first derive an efficient PB predictor by using all the available data to estimate true covariate values. We then obtain a corresponding PEB predictor and show that it is asymptotically “optimal”. In addition, we employ a jackknife method to estimate the mean squared prediction error (MSPE) of the PEB predictor. Finally, we report the results of a simulation study on the performance of our PEB predictor and associated jackknife MSPE estimator. Our results show that the proposed PEB predictor can lead to significant gain in efficiency over the previously proposed PEB predictor. Area level models are also studied.",0
https://doi.org/10.1207/s15327906mbr3604_06,Sample Size in Factor Analysis: The Role of Model Error,"This article examines effects of sample size and other design features on correspondence between factors obtained from analysis of sample data and those present in the population from which the samples were drawn. We extend earlier work on this question by examining these phenomena in the situation in which the common factor model does not hold exactly in the population. We present a theoretical framework for representing such lack of fit and examine its implications in the population and sample. Based on this approach we hypothesize that lack of fit of the model in the population will not, on the average, influence recovery of population factors in analysis of sample data, regardless of degree of model error and regardless of sample size. Rather, such recovery will be affected only by phenomena related to sampling error which have been studied previously. These hypotheses are investigated and verified in two sampling studies, one using artificial data and one using empirical data.",0
https://doi.org/10.1348/000711005x81403,Bayesian analysis of structural equation models with mixed exponential family and ordered categorical data,"Structural equation models are very popular for studying relationships among observed and latent variables. However, the existing theory and computer packages are developed mainly under the assumption of normality, and hence cannot be satisfactorily applied to non-normal and ordered categorical data that are common in behavioural, social and psychological research. In this paper, we develop a Bayesian approach to the analysis of structural equation models in which the manifest variables are ordered categorical and/or from an exponential family. In this framework, models with a mixture of binomial, ordered categorical and normal variables can be analysed. Bayesian estimates of the unknown parameters are obtained by a computational procedure that combines the Gibbs sampler and the Metropolis-Hastings algorithm. Some goodness-of-fit statistics are proposed to evaluate the fit of the posited model. The methodology is illustrated by results obtained from a simulation study and analysis of a real data set about non-adherence of hypertension patients in a medical treatment scheme.",0
https://doi.org/10.1126/science.153.3736.652,High-Speed Scanning in Human Memory,"When subjects judge whether a test symbol is contained in a short memorized sequence of symbols, their mean reaction-time increases linearly with the length of the sequence. The linearity and slope of the function imply the existence of an internal serial-comparison process whose average rate is between 25 and 30 symbols per second.",0
https://doi.org/10.3758/s13428-014-0526-3,Estimation of the latent mediated effect with ordinal data using the limited-information and Bayesian full-information approaches,"It is common to encounter latent variables with ordinal data in social or behavioral research. Although a mediated effect of latent variables (latent mediated effect, or LME) with ordinal data may appear to be a straightforward combination of LME with continuous data and latent variables with ordinal data, the methodological challenges to combine the two are not trivial. This research covers model structures as complex as LME and formulates both point and interval estimates of LME for ordinal data using the Bayesian full-information approach. We also combine weighted least squares (WLS) estimation with the bias-corrected bootstrapping (BCB; Efron Journal of the American Statistical Association, 82, 171–185, 1987) method or the traditional delta method as the limited-information approach. We evaluated the viability of these different approaches across various conditions through simulation studies, and provide an empirical example to illustrate the approaches. We found that the Bayesian approach with reasonably informative priors is preferred when both point and interval estimates are of interest and the sample size is 200 or above. © 2014, Psychonomic Society, Inc.",1
https://doi.org/10.1037/a0027131,Hopes and cautions in implementing Bayesian structural equation modeling.,"Muthén and Asparouhov (2012) have proposed and demonstrated an approach to model specification and estimation in structural equation modeling (SEM) using Bayesian methods. Their contribution builds on previous work in this area by (a) focusing on the translation of conventional SEM models into a Bayesian framework wherein parameters fixed at zero in a conventional model can be respecified using small-variance priors and (b) implementing their approach in software that is widely accessible. We recognize potential benefits for applied researchers as discussed by Muthén and Asparouhov, and we also see a tradeoff in that effective use of the proposed approach introduces increased demands in terms of expertise of users to navigate new complexities in model specification, parameter estimation, and evaluation of results. We also raise cautions regarding the issues of model modification and model fit. Although we see significant potential value in the use of Bayesian SEM, we also believe that effective use will require an awareness of these complexities.",0
https://doi.org/10.2307/3002019,An Approximate Distribution of Estimates of Variance Components,,0
https://doi.org/10.1080/00273171.2013.784861,A Multidimensional and Multilevel Extension of a Random-Effect Approach to Subjective Judgment in Rating Scales,"In responding to rating scale items, respondents may hold different perspectives on the given categories. The random-effect rating scale model (RERSM), developed to account for variations in the category thresholds across respondents, is unidimensional and unilevel. It becomes statistically inefficient when multiple unidimensional tests have to be analyzed and inapplicable when data have a multilevel structure (e.g., respondents nested within organizations, students nested within schools). To resolve these problems, this study develops a multidimensional and multilevel version of the RERSM. The parameters can be estimated with existing computer software. Thus, there is no need to develop estimation procedures or corresponding computer programs. Simulation studies were conducted to evaluate the parameter recovery of the multidimensional RERSM, the multilevel RERSM, and the multidimensional and multilevel RERSM using WinBUGS. The results showed that the parameter recovery was generally satisfactory. An empirical example of the application of the multidimensional and multilevel RERSM to 2006 Program for International Student Assessment inventories about attitudes toward learning sciences is provided.",0
https://doi.org/10.2307/3316114,Multiple roots of estimating functions,en,0
https://doi.org/10.3109/09286586.2013.867508,Structural Equation Modeling: A Framework for Ocular and Other Medical Sciences Research,"Structural equation modeling (SEM) is a modeling framework that encompasses many types of statistical models and can accommodate a variety of estimation and testing methods. SEM has been used primarily in social sciences but is increasingly used in epidemiology, public health, and the medical sciences. SEM provides many advantages for the analysis of survey and clinical data, including the ability to model latent constructs that may not be directly observable. Another major feature is simultaneous estimation of parameters in systems of equations that may include mediated relationships, correlated dependent variables, and in some instances feedback relationships. SEM allows for the specification of theoretically holistic models because multiple and varied relationships may be estimated together in the same model. SEM has recently expanded by adding generalized linear modeling capabilities that include the simultaneous estimation of parameters of different functional form for outcomes with different distributions in the same model. Therefore, mortality modeling and other relevant health outcomes may be evaluated. Random effects estimation using latent variables has been advanced in the SEM literature and software. In addition, SEM software has increased estimation options. Therefore, modern SEM is quite general and includes model types frequently used by health researchers, including generalized linear modeling, mixed effects linear modeling, and population average modeling. This article does not present any new information. It is meant as an introduction to SEM and its uses in ocular and other health research.",0
https://doi.org/10.1111/hepr.12467,Relative efficacy and safety of simeprevir and telaprevir in treatment-naïve hepatitis C-infected patients in a Japanese population: A Bayesian network meta-analysis,"Simeprevir (SMV) is an oral, once-daily protease inhibitor for the treatment of chronic hepatitis C virus (HCV) genotype 1 infection. In phase II/III randomized controlled trials (RCT) conducted in Japan, SMV, in combination with peginterferon-α and ribavirin (PEG IFN/RBV), demonstrated potent efficacy in HCV genotype 1-infected patients relative to PEG IFN/RBV and was generally well tolerated. Telaprevir (TVR) in combination with PEG IFN/RBV is licensed for the treatment of HCV in Japan. In the absence of head-to-head comparisons of TVR and SMV in a Japanese population, we undertook a network meta-analysis (NMA) to examine the relative efficacy and safety of SMV and TVR in combination with PEG IFN/RBV.A systematic review identified SMV and TVR RCT in Japanese treatment-naïve patients. Bayesian NMA was performed assuming fixed study effects.Three studies met our inclusion criteria: two SMV and one TVR. SMV showed a higher mean odds ratio (OR) of achieving SVR versus TVR (OR, 1.68 (95% credible interval 0.66-4.26)). SMV showed a lower mean OR of discontinuation: overall, 0.35 (0.12-1.00); and due to AE, 0.87 (0.23-3.34) versus TVR. SMV showed a lower mean OR of experiencing anemia 0.20 (0.07-0.56) and rash 0.41 (0.17-0.99) but a higher mean OR of experiencing pruritus 1.26 (0.46-3.47) versus TVR.In this indirect treatment comparison, SMV, in combination with PEG IFN/RBV, showed a favorable risk-benefit profile compared with TVR with PEG IFN/RBV in Japanese treatment-naïve HCV patients.",0
https://doi.org/10.1111/j.0081-1750.2006.00163.x,Editor's Introduction,,0
https://doi.org/10.2307/3172669,A Monte Carlo Comparison of Estimators for the Multinomial Logit Model,,0
https://doi.org/10.1177/0013164410387336,Item Selection Criteria With Practical Constraints for Computerized Classification Testing,"This study compares four item selection criteria for a two-category computerized classification testing: (1) Fisher information (FI), (2) Kullback—Leibler information (KLI), (3) weighted log-odds ratio (WLOR), and (4) mutual information (MI), with respect to the efficiency and accuracy of classification decision using the sequential probability ratio test as well as the extent of item usage. The comparability of the four item selection criteria are examined primarily under three types of item selection conditions: (1) using only the four item selection algorithms, (2) using the four item selection algorithms and content balancing control, and (3) using the four item selection algorithms, content balancing control, and item exposure control. The comparability of the four item selection criteria is also evaluated in two types of proficiency distribution and three levels of indifference region width. The results show that the differences of the four item selection criteria are washed out as more realistic constraints are imposed. Moreover, within two-category classification testing, the use of MI does not necessarily generate greater efficiency than FI, WLOR, and KLI, although MI might seem attractive for its general form of formula in item selection.",0
https://doi.org/10.1080/10705510701301511,"Bayesian Methods for Analyzing Structural Equation Models With Covariates, Interaction, and Quadratic Latent Variables","The analysis of interaction among latent variables has received much attention. This article introduces a Bayesian approach to analyze a general structural equation model that accommodates the general nonlinear terms of latent variables and covariates. This approach produces a Bayesian estimate that has the same statistical optimal properties as a maximum likelihood estimate. Other advantages over the traditional approaches are discussed. More important, we demonstrate through examples how to use the freely available software WinBUGS to obtain Bayesian results for estimation and model comparison. Simulation studies are conducted to assess the empirical performances of the approach for situations with various sample sizes and prior inputs.",0
https://doi.org/10.1111/1467-9884.00242,On Estimating Standard Errors in Multilevel Analysis,"Standard errors of maximum likelihood estimators are commonly estimated from the inverse of the information matrix. When the information matrix is a function of parameters, some of which are estimated with little precision, the standard error may be estimated very poorly. This problem is discussed in the context of two-level (random-coefficient) models, and some remedies are proposed.",0
https://doi.org/10.1214/08-sts257,Markov Chain Monte Carlo: Can We Trust the Third Significant Figure?,"Current reporting of results based on Markov chain Monte Carlo computations could be improved. In particular, a measure of the accuracy of the resulting estimates is rarely reported. Thus we have little ability to objectively assess the quality of the reported estimates. We address this issue in that we discuss why Monte Carlo standard errors are important, how they can be easily calculated in Markov chain Monte Carlo and how they can be used to decide when to stop the simulation. We compare their use to a popular alternative in the context of two examples.",0
https://doi.org/10.1016/s0277-9536(02)00370-2,"The health impact of health care on families: a matched cohort study of hospice use by decedents and mortality outcomes in surviving, widowed spouses","Alternative ways of caring for seriously ill patients might have implications not only for patients' own outcomes, but also, indirectly, for the health outcomes of their family members. Clinical observation suggests that patients who die ""good deaths"" may impose less stress on their spouses. Consequently, we sought to assess whether hospice use by a decedent is associated with decreased risk of death in surviving, bereaved spouses. We conducted a matched retrospective cohort study involving a population-based sample of 195,553 elderly couples in the USA. A total of 30,838 couples where the decedent used hospice care were matched using the propensity score method to 30,838 couples where the decedent did not use hospice care. Our principal outcome of interest was the duration of survival of bereaved widow/ers. After adjustment for other measured variables, 5.4% of bereaved wives died by 18 months after the death of their husband when their deceased husband did not use hospice and 4.9% died when their deceased husband did use hospice, yielding an odds ratio (OR) of 0.92 (95% CI: 0.84-0.99) in favor of hospice use. Similarly, whereas 13.7% of bereaved husbands died by 18 months when their deceased wife did not use hospice, 13.2% died when their deceased wife did use hospice, yielding an OR of 0.95 (95% CI: 0.84-1.06) in favor of hospice use. Our findings suggest a possible beneficial impact of hospice--as a particularly supportive type of end-of-life care--on the spouses of patients who succumb to their disease. Hospice care might attenuate the ordinarily increased mortality associated with becoming widowed. This effect is present in both men and women, but it is statistically significant and possibly larger in bereaved wives. The size of this effect is comparable to the reductions in the risk of death seen in a variety of other modifiable risk factors in women. Health care may have positive, group-level health ""externalities"": it may affect the health not only of patients but also of patients' family members.",0
https://doi.org/10.3102/0002831207308230,Classroom Effects on Children’s Achievement Trajectories in Elementary School,"This nonexperimental, longitudinal field study examines the extent to which variation in observed classroom supports (quality of emotional and instructional interactions and amount of exposure to literacy and math activities) predicts trajectories of achievement in reading and math from 54 months to fifth grade. Growth mixture modeling detected two latent classes of readers: fast readers whose skills developed rapidly and leveled off, and a typical group for which reading growth was somewhat less rapid. Only one latent class was identified for math achievement. For reading, there were small positive associations between observed emotional quality of teacher-child interactions and growth. Growth in math achievement showed small positive relations with observed emotional interactions and exposure to math activities. There was a significant interaction between quality and quantity of instruction for reading such that at higher levels of emotional quality there was less of a negative association between amount of literacy exposure and reading growth.",0
https://doi.org/10.1080/00949655.2012.692368,An item response model for Likert-type data that incorporates response time in personality measurements,"This study defines a measurement index for the differences in response probabilities to an item. Based on the difference index, a probability–difficulty (PD) hypothesis is proposed. A general framework for modelling responses and response times (RTs) on Likert-type personality items is presented, in which the submodel describing the item responses can be a graded item response theory model, and the submodel describing RTs is developed based on the PD hypothesis. This framework is exemplified by employing the generalized partial credit model for responses and a log-normal model for RTs. Furthermore, Bayesian methods for estimating model parameters and for assessing the model–data fit are described. A simulation study shows that the new approach improves the accuracy of estimating the individual trait levels with the ancillary information contained in RTs. Finally, the applicability of our approach is illustrated by an empirical example in personality measurements.",0
,"The Rasch model, additive conjoint measurement, and new models of probabilistic measurement theory.","This research describes some of the similarities and differences between additive conjoint measurement (a type of fundamental measurement) and the Rasch model. It seems that there are many similarities between the two frameworks, however, their differences are nontrivial. For instance, while conjoint measurement specifies measurement scales using a data-free, non-numerical axiomatic frame of reference, the Rasch model specifies measurement scales using a numerical frame of reference that is, by definition, data dependent. In order to circumvent difficulties that can be realistically imposed by this data dependence, this research formalizes new non-parametric item response models. These models are probabilistic measurement theory models in the sense that they explicitly integrate the axiomatic ideas of measurement theory with the statistical ideas of order-restricted inference and Markov Chain Monte Carlo. The specifications of these models are rather flexible, as they can represent any one of several models used in psychometrics, such as Mokken's (1971) monotone homogeneity model, Scheiblechner's (1995) isotonic ordinal probabilistic model, or the Rasch (1960) model. The proposed non-parametric item response models are applied to analyze both real and simulated data sets.",0
https://doi.org/10.1037/met0000029,Monotonicity of effect sizes: Questioning kappa-squared as mediation effect size measure.,"Mediation analysis is important for research in psychology and other social and behavioral sciences. Great progress has been made in testing mediation effects and in constructing their confidence intervals. Mediation effect sizes have also been considered. Preacher and Kelley (2011) proposed and recommended κ² as an effect size measure for a mediation effect. In this article, we argue that κ² is not an appropriate effect size measure for mediation models, because of its lack of the property of rank preservation (e.g., the magnitude of κ² may decrease when the mediation effect that κ² represents increases). Furthermore, κ² can lead to paradoxical results in multiple mediation models. We show that the problem of κ² is due to (a) the improper calculation of the maximum possible value of the indirect effect, and (b) mathematically, the maximum possible indirect effect is infinity, implying that the definition of κ² is mathematically incorrect. At this time, it appears that the traditional mediation effect size measure PM (the ratio of the indirect effect to the total effect), together with some other statistical information, should be preferred for basic mediation models. But for inconsistent mediation models where the indirect effect and the direct effect have opposite signs, the situation is less clear. Other considerations and suggestions for future research are also discussed.",0
https://doi.org/10.11575/prism/28116,The Effects of School Work Pressure on Depression and Substance Use: A Cross-National Study of School-Aged Children in Canada and Finland,,0
https://doi.org/10.1016/j.ssresearch.2010.09.012,The typification of Hispanics as criminals and support for punitive crime control policies,"The Hispanic population is now the largest and fastest growing minority in the United States, so it is not surprising that ethnic threat linked to Hispanics has been associated with harsher crime control. While minority threat research has found that individuals who associate blacks with crime are more likely to support harsh criminal policies, the possibility that this relationship exists for those who typify Hispanics as criminal has yet to be examined. Using a national random sample, this study is the first to use HLM to find that perceptions of Hispanics as criminals do increase support for punitive crime control measures, controlling for various individual and state influences. Moderated and contextual analyses indicate this relationship is most applicable for individuals who are less apt to typify criminals as black, less prejudiced, less fearful of victimization, politically liberal or moderate, not parents, and living in states with relatively fewer Latin American immigrants.",0
https://doi.org/10.1016/j.csda.2007.03.024,A Bayesian propensity score adjustment for latent variable modeling and MCMC algorithm,"The estimation of the differences among groups in observational studies is frequently inaccurate owing to a bias caused by differences in the distributions of covariates. In order to estimate the average treatment effects when the treatment variable is binary, Rosenbaum and Rubin [1983. The central role of the propensity score in observational studies for causal effects. Biometrika 70, 41-55] proposed an adjustment method for pre-treatment variables using propensity scores. Imbens [2000. The role of the propensity score in estimating dose-response functions. Biometrika 87, 706-710] extended the propensity score methodology for estimation of average treatment effects with multivalued treatments. However, these studies focused only on estimating the marginal mean structure. In many substantive sciences such as the biological and social sciences, a general estimation method is required to deal with more complex analyses other than regression, such as testing group differences on latent variables. For latent variable models, the EM algorithm or the traditional Monte Carlo methods are necessary. However, in propensity score adjustment, these methods cannot be used because the full distribution is not specified. In this paper, we propose a quasi-Bayesian estimation method for general parametric models that integrate out the distributions of covariates using propensity scores. Although the proposed Bayes estimates are shown to be consistent, they can be calculated by existing Markov chain Monte Carlo methods such as Gibbs sampler. The proposed method is useful to estimate parameters in latent variable models, while the previous methods were unable to provide valid estimates for complex models such as latent variable models. We also illustrated the procedure using the data obtained from the US National Longitudinal Survey of Children and Youth (NLSY1979-2002) for estimating the effect of maternal smoking during pregnancy on the development of the child's cognitive functioning.",0
https://doi.org/10.1177/026540758800500207,Interpersonal Perception: A Social Relations Analysis,"Seven basic research questions in interpersonal perception are posed concerning issues of consensus, assimilation, reciprocity, accuracy, congruence, assumed similarity and self—other agreement. All questions can be addressed at the individual level, and three at the dyadic level. It is shown how the Social Relations Model can be used to answer the questions.",0
https://doi.org/10.1080/00273170903333665,Doubly-Latent Models of School Contextual Effects: Integrating Multilevel and Structural Equation Approaches to Control Measurement and Sampling Error,"This article is a methodological-substantive synergy. Methodologically, we demonstrate latent-variable contextual models that integrate structural equation models (with multiple indicators) and multilevel models. These models simultaneously control for and unconfound measurement error due to sampling of items at the individual (L1) and group (L2) levels and sampling error due the sampling of persons in the aggregation of L1 characteristics to form L2 constructs. We consider a set of models that are latent or manifest in relation to sampling items (measurement error) and sampling of persons (sampling error) and discuss when different models might be most useful. We demonstrate the flexibility of these 4 core models by extending them to include random slopes, latent (single-level or cross-level) interactions, and latent quadratic effects. Substantively we use these models to test the big-fish-little-pond effect (BFLPE), showing that individual student levels of academic self-concept (L1-ASC) are positively associated with individual level achievement (L1-ACH) and negatively associated with school-average achievement (L2-ACH)-a finding with important policy implications for the way schools are structured. Extending tests of the BFLPE in new directions, we show that the nonlinear effects of the L1-ACH (a latent quadratic effect) and the interaction between gender and L1-ACH (an L1 × L1 latent interaction) are not significant. Although random-slope models show no significant school-to-school variation in relations between L1-ACH and L1-ASC, the negative effects of L2-ACH (the BFLPE) do vary somewhat with individual L1-ACH. We conclude with implications for diverse applications of the set of latent contextual models, including recommendations about their implementation, effect size estimates (and confidence intervals) appropriate to multilevel models, and directions for further research in contextual effect analysis.",0
https://doi.org/10.1016/j.ophtha.2013.04.010,A Systematic Review of Endophthalmitis after Microincisional versus 20-Gauge Vitrectomy,"Endophthalmitis is a rare but severe complication of vitrectomy.Post-surgical endophthalmitis is suspected to be more frequent after microincisional (23- and 25-gauge) compared with standard (20-gauge) vitrectomy.We conducted a systematic review of studies that compared microincisional and standard vitrectomy by searching MEDLINE and EMBASE up to November 2012. We used the Bayesian meta-analysis method to compute the odds ratio (OR) of endophthalmitis. We conducted subgroup analyses to compare the effect of different incision types and use of perioperative antibiotics.We identified 3 small randomized and 18 nonrandomized studies that reported 68 cases of endophthalmitis in 148 643 participants. The overall OR of endophthalmitis for microincisional versus standard vitrectomy was 2.3 (95% credible interval [CrI], 0.8-5.8). We found an increased risk of endophthalmitis using a microincisional straight approach compared with standard vitrectomy (OR, 15.1; 95% CrI, 2.01-179), but not for a beveled approach (OR, 0.82; 95% CrI, 0.23-2.28). The OR of studies that reported on mixed microincision was between these 2 values (OR, 4.4; 95% CrI, 1.32-14.3). We estimated that the overall rate of endophthalmitis with 20-gauge vitrectomy was 3 cases in 10 000 procedures, and the probability that a beveled microincision increases the rate of endophthalmitis to more than 6 or 9 events was small (no more than 5% or 1%, respectively).We did not find an increased risk of endophthalmitis for microincisional vitrectomy compared with standard vitrectomy. The beveled approach seems to be safer than a straight approach, supporting the current recommendation of its adoption in microincisional vitrectomy. However, these findings must be interpreted cautiously because of the small number of endophthalmitis events reported from included studies.",0
https://doi.org/10.1371/journal.pcbi.1004003,A Probabilistic Palimpsest Model of Visual Short-term Memory,"Working memory plays a key role in cognition, and yet its mechanisms remain much debated. Human performance on memory tasks is severely limited; however, the two major classes of theory explaining the limits leave open questions about key issues such as how multiple simultaneously-represented items can be distinguished. We propose a palimpsest model, with the occurrent activity of a single population of neurons coding for several multi-featured items. Using a probabilistic approach to storage and recall, we show how this model can account for many qualitative aspects of existing experimental data. In our account, the underlying nature of a memory item depends entirely on the characteristics of the population representation, and we provide analytical and numerical insights into critical issues such as multiplicity and binding. We consider representations in which information about individual feature values is partially separate from the information about binding that creates single items out of multiple features. An appropriate balance between these two types of information is required to capture fully the different types of error seen in human experimental data. Our model provides the first principled account of misbinding errors. We also suggest a specific set of stimuli designed to elucidate the representations that subjects actually employ.",0
https://doi.org/10.2307/2986138,Adaptive Rejection Metropolis Sampling within Gibbs Sampling,"Gibbs sampling is a powerful technique for statistical inference. It involves little more than sampling from full conditional distributions, which can be both complex and computationally expensive to evaluate. Gilks and Wild have shown that in practice full conditionals are often log‐concave, and they proposed a method of adaptive rejection sampling for efficiently sampling from univariate log‐concave distributions. In this paper, to deal with non‐log‐concave full conditional distributions, we generalize adaptive rejection sampling to include a Hastings‐Metropolis algorithm step. One important field of application in which statistical models may lead to non‐log‐concave full conditionals is population pharmacokinetics. Here, the relationship between drug dose and blood or plasma concentration in a group of patients typically is modelled by using nonlinear mixed effects models. Often, the data used for analysis are routinely collected hospital measurements, which tend to be noisy and irregular. Consequently, a robust (t‐distributed) error structure is appropriate to account for outlying observations and/or patients. We propose a robust nonlinear full probability model for population pharmacokinetic data. We demonstrate that our method enables Bayesian inference for this model, through an analysis of antibiotic administration in new‐born babies.",0
https://doi.org/10.1207/s15328007sem1203_1,Sensitivity of Fit Indexes to Misspecified Structural or Measurement Model Components: Rationale of Two-Index Strategy Revisited,"In previous research (Hu & Bentler, 1998, 1999), 2 conclusions were drawn: standardized root mean squared residual (SRMR) was the most sensitive to misspecified factor covariances, and a group of other fit indexes were most sensitive to misspecified factor loadings. Based on these findings, a 2-index strategy-that is, SRMR coupled with another index-was proposed in model fit assessment to detect potential misspecification in both the structural and measurement model parameters. Based on our reasoning and empirical work presented in this article, we conclude that SRMR is not necessarily most sensitive to misspecified factor covariances (structural model misspecification), the group of indexes (TLI, BL89, RNI, CFI, Gamma hat, Mc, or RMSEA) are not necessarily more sensitive to misspecified factor loadings (measurement model misspecification), and the rationale for the 2-index presentation strategy appears to have questionable validity.",0
https://doi.org/10.1108/ijphm-05-2014-0028,Healthcare service quality: what really matters to the female patient?,Purpose – This paper aims to develop a model that encompasses the constructs and sub-constructs consumers use in evaluating healthcare service quality (HSQ) in Egypt. Design/methodology/approach – Factor analysis was performed on 40 variables to identify the constructs. Ordinal logistic regression was also used to identify the sub-constructs and examine the effect of each sub-construct on patients’ overall perception of service quality. Findings – Factor analysis confirmed an eight-construct framework: hospital premises and employees; doctor medical service; nursing medical service; diagnostic medical service; admission; discharge; rooms and housekeeping; and meals. Ordinal logistic regression established 17 sub-constructs – physician reliability; physician assurance; physician interaction; physician’s competence; nursing tangibles; nursing reliability; nursing assurance; nursing interaction; nursing responsiveness; diagnostic service competence; diagnostic service reliability; hospital premises and employees tangibles; admission responsiveness; admission knowledge and courtesy; meals tangibles; rooms tangibles and housekeeping courtesy; and discharge knowledge and courtesy – that have significant effect on HSQ. Some sub-constructs had a significantly greater impact on overall perception of service quality than others. Practical implications – Healthcare providers will be able to pinpoint areas of service quality shortfall and better satisfy their patients. This will ultimately lead to repeat patronage and positive recommendation behavior. Originality/value – The model is the first comprehensive model in the Middle East that takes into account all constructs and sub-constructs patients use for evaluation of HSQ.,0
https://doi.org/10.1007/s00180-010-0215-3,Prediction interval for disease mapping using hierarchical likelihood,"In disease mapping, the Bayesian approach is widely used for forming the prediction interval of relative risks. In this paper we propose a hierarchical-likelihood interval for disease mapping, which accounts for the inflation of standard error estimates caused by uncertainty in the estimation of the fixed parameters. Comparison is made with the Bayesian prediction intervals derived from penalized quasi-likelihood and fully Bayesian methods. Through simulation studies, we show that prediction intervals for random effects using hierarchical likelihood maintains the required level. Â© 2010 Springer-Verlag.",0
https://doi.org/10.1007/bf02295614,A hierarchical bayesian statistical framework for response time distributions,"This paper provides a statistical framework for estimating higher-order characteristics of the response time distribution, such as the scale (variability) and shape. Consideration of these higher order characteristics often provides for more rigorous theory development in cognitive and perceptual psychology (e.g., Luce, 1986). RT distribution for a single participant depends on certain participant characteristics, which in turn can be thought of as arising from a distribution of latent variables. The present work focuses on the three-parameter Weibull distribution, with parameters for shape, scale, and shift (initial value). Bayesian estimation in a hierarchical framework is conceptually straightforward. Parameter estimates, both for participant quantities and population parameters, are obtained through Markov Chain Monte Carlo methods. The methods are illustrated with an application to response time data in an absolute identification task. The behavior of the Bayes estimates are compared to maximum likelihood (ML) estimates through Monte Carlo simulations. For small sample size, there is an occasional tendency for the ML estimates to be unreasonably extreme. In contrast, by borrowing strength across participants, Bayes estimation “shrinks” extreme estimates. The results are that the Bayes estimators are more accurate than the corresponding ML estimators.",0
https://doi.org/10.1007/s00127-015-1168-1,The Great Smoky Mountains Study: developmental epidemiology in the southeastern United States,"To describe the Great Smoky Mountains Study (GSMS).GSMS is a longitudinal study of child psychiatric disorders that began in 1992 to look at need for mental health services in a rural area of the USA. Over 20 years it has expanded its range to include developmental epidemiology more generally, not only the development of psychiatric and substance abuse problems but also their correlates and predictors: family and environmental risk, physical development including puberty, stress and stress-related hormones, trauma, the impact of poverty, genetic markers, and epigenetics. Now that participants are in their 30s the focus has shifted to adult outcomes of childhood psychopathology and risk, and early physical, cognitive, and psychological markers of aging.This paper describes the results from over 11,000 interviews, examples of the study's contributions to science and policy, and plans for the future.Longitudinal studies can provide insights that aid in policy planning.",0
https://doi.org/10.1177/0010414008325286,Ethnic Diversity and Generalized Trust in Europe,"While most current research documents a negative relation between ethnic diversity and generalized trust, it has to be acknowledged that these results often originate from one-country analyses in North America. In this article, attitudinal measurements from the European Social Survey are combined with Organization for Economic Co-Operation and Development data on migration patterns, thus examining the relationship between diversity and trust in a comparative manner across 20 European countries. More fine-grained measurements of diversity (including type and rise of diversity over time and legal status of immigrants) are included in a multilevel model. At the individual level, most of the familiar relations were confirmed. At the country level, hardly any indicators for migration or diversity proved to be strongly and consistently related to generalized trust. Results suggest that the pessimistic conclusions about the negative effects of ethnic diversity on generalized trust cannot be confirmed at the aggregate level across European countries.",0
https://doi.org/10.1093/ije/dyn088,Morbidity and mortality in Brazilian municipalities: a multilevel study of the association between socioeconomic and healthcare indicators,"Socioeconomic and healthcare indicators are major determinants of health outcomes. The impact of social and healthcare inequalities on Brazilian morbidity and mortality indicators is of concern but it is not well studied.A multilevel ecological study was performed in order to investigate the association between a set of socioeconomic and healthcare indicators and five morbidity and mortality outcomes. Datasets were presented at three hierarchical levels: local (lower level), regional (intermediate level) and state (higher level). A Poisson regression model was estimated for each outcome with random intercept and fixed regression coefficients for independent variables at the three levels. The magnitude of outcome variability at intermediate and higher levels was assessed for all models.All outcomes were associated with both socioeconomic and healthcare variables, with predominance of associations at the local level. General and high-complexity healthcare infrastructures were directly associated with indicators related to later stages of the demographic and epidemiological transition process. A mild effect on morbidity and mortality related to political voting patterns was found at the local level.Healthcare conditions and socioeconomic indicators are associated with health outcomes in a complex way at the local level in Brazil, but part of the variability of health outcomes is related to factors operating at higher levels. Some possible interaction effects and cross-sectional design limitations of this study must be considered.",0
https://doi.org/10.1177/1534735408322849,Biological Mediators of Effect of Diet and Stress Reduction on Prostate Cancer,"Background. A 6-month pilot intervention trial was conducted to determine whether adoption of a plant-based diet, reinforced by stress reduction, could reduce the rate of prostate-specific antigen (PSA) increase, a marker of disease progression, in asymptomatic, hormonally untreated patients experiencing consistently increasing PSA levels after surgery or radiation. Methods. A pre—post design was used to examine (1) the effect of intervention on potential mediators of disease progression, including body composition and weight-related biomarkers (sex steroid hormones and cytokines), and (2) whether changes in these variables were associated with change in rate of PSA increase. The baseline rate of PSA increase (from the time of posttreatment recurrence to the start of intervention) was ascertained from medical records. Body composition and biomarkers were assessed at baseline (prior to intervention), during the intervention (3 months), and at the end of the intervention (6 months). Changes in body composition and biomarkers were determined and compared with rates of PSA increase over the corresponding time intervals. Results. There was a significant reduction in waist-to-hip ratio ( P = .03) and increase in circulating sex hormone binding globulin ( P = .04). The rate of PSA increase decreased from the preintervention period (PSA slope = 0.059) to the period from 0 to 3 months (PSA slope = 0.002, P &lt; .01) and increased slightly, although not significantly, from 0 to 3 months to the period from 3 to 6 months (0.029, P = .43). Conclusions. Adoption of a plant-based diet and stress reduction may reduce central adiposity and improve the hormonal milieu in patients with recurrent PC. Changes in the rate of increase in PSA were in the same direction as changes in waist-to-hip ratio and opposite those of sex hormone binding globulin, raising the possibility that the effect of the intervention may have been mediated, in part, by these variables.",0
https://doi.org/10.1002/sim.1187,How should meta-regression analyses be undertaken and interpreted?,"Appropriate methods for meta-regression applied to a set of clinical trials, and the limitations and pitfalls in interpretation, are insufficiently recognized. Here we summarize recent research focusing on these issues, and consider three published examples of meta-regression in the light of this work. One principal methodological issue is that meta-regression should be weighted to take account of both within-trial variances of treatment effects and the residual between-trial heterogeneity (that is, heterogeneity not explained by the covariates in the regression). This corresponds to random effects meta-regression. The associations derived from meta-regressions are observational, and have a weaker interpretation than the causal relationships derived from randomized comparisons. This applies particularly when averages of patient characteristics in each trial are used as covariates in the regression. Data dredging is the main pitfall in reaching reliable conclusions from meta-regression. It can only be avoided by prespecification of covariates that will be investigated as potential sources of heterogeneity. However, in practice this is not always easy to achieve. The examples considered in this paper show the tension between the scientific rationale for using meta-regression and the difficult interpretative problems to which such analyses are prone.",0
https://doi.org/10.1201/b14835-24,MCMC for nonlinear hierarchical models,,0
https://doi.org/10.1111/j.1744-6570.1991.tb00688.x,THE BIG FIVE PERSONALITY DIMENSIONS AND JOB PERFORMANCE: A META-ANALYSIS,"This study investigated the relation of the “Big Five” personality dimensions (Extraversion, Emotional Stability, Agreeableness, Conscientiousness, and Openness to Experience) to three job performance criteria (job proficiency, training proficiency, and personnel data) for five occupational groups (professionals, police, managers, sales, and skilled/semi-skilled). Results indicated that one dimension of personality, Conscientiousness, showed consistent relations with all job performance criteria for all occupational groups. For the remaining personality dimensions, the estimated true score correlations varied by occupational group and criterion type. Extraversion was a valid predictor for two occupations involving social interaction, managers and sales (across criterion types). Also, both Openness to Experience and Extraversion were valid predictors of the training proficiency criterion (across occupations). Other personality dimensions were also found to be valid predictors for some occupations and some criterion types, but the magnitude of the estimated true score correlations was small (ρ < .10). Overall, the results illustrate the benefits of using the 5-factor model of personality to accumulate and communicate empirical findings. The findings have numerous implications for research and practice in personnel psychology, especially in the subfields of personnel selection, training and development, and performance appraisal.",0
https://doi.org/10.1111/j.1460-9568.2004.03709.x,Evidence that exogenous and endogenous fractalkine can induce spinal nociceptive facilitation in rats,"Recent evidence suggests that spinal cord glia can contribute to enhanced nociceptive responses. However, the signals that cause glial activation are unknown. Fractalkine (CX3C ligand-1; CX3CL1) is a unique chemokine expressed on the extracellular surface of spinal neurons and spinal sensory afferents. In the dorsal spinal cord, fractalkine receptors are primarily expressed by microglia. As fractalkine can be released from neurons upon strong activation, it has previously been suggested to be a neuron-to-glial signal that induces glial activation. The present series of experiments provide an initial investigation of the spinal pain modulatory effects of fractalkine. Intrathecal fractalkine produced dose-dependent mechanical allodynia and thermal hyperalgesia. In addition, a single injection of fractalkine receptor antagonist (neutralizing antibody against rat CX3C receptor-1; CX3CR1) delayed the development of mechanical allodynia and/or thermal hyperalgesia in two neuropathic pain models: chronic constriction injury (CCI) and sciatic inflammatory neuropathy. Intriguingly, anti-CX3CR1 reduced nociceptive responses when administered 5–7 days after CCI, suggesting that prolonged release of fractalkine may contribute to the maintenance of neuropathic pain. Taken together, these initial investigations of spinal fractalkine effects suggest that exogenous and endogenous fractalkine are involved in spinal sensitization, including that induced by peripheral neuropathy.",0
https://doi.org/10.1177/0146621605285517,Posterior Predictive Assessment of Item Response Theory Models,"Model checking in item response theory (IRT) is an underdeveloped area. There is no universally accepted tool for checking IRT models. The posterior predictive model-checking method is a popular Bayesian model-checking tool because it has intuitive appeal, is simple to apply, has a strong theoretical basis, and can provide graphical or numerical evidence about model misfit. An important issue with the application of the posterior predictive model-checking method is the choice of a discrepancy measure (which plays a role like that of a test statistic in traditional hypothesis tests). This article examines the performance of a number of discrepancy measures for assessing different aspects of fit of the common IRT models and makes specific recommendations about what measures are most useful in assessing model fit. Graphical summaries of model-checking results are demonstrated to provide useful insights about model fit.",0
https://doi.org/10.1093/acprof:oso/9780195152968.001.0001,Applied Longitudinal Data Analysis,"Abstract Change is constant in everyday life. Infants crawl and then walk, children learn to read and write, teenagers mature in myriad ways, and the elderly become frail and forgetful. Beyond these natural processes and events, external forces and interventions instigate and disrupt change: test scores may rise after a coaching course, drug abusers may remain abstinent after residential treatment. By charting changes over time and investigating whether and when events occur, researchers reveal the temporal rhythms of our lives. This book is concerned with behavioral, social, and biomedical sciences. It offers a presentation of two of today's most popular statistical methods: multilevel models for individual change and hazard/survival models for event occurrence (in both discrete- and continuous-time). Using data sets from published studies, the book takes you step by step through complete analyses, from simple exploratory displays that reveal underlying patterns through sophisticated specifications of complex statistical models.",0
https://doi.org/10.1111/j.1468-0491.2011.01521.x,Dimensions of Family Policy and Female Labor Market Participation: Analyzing Group-Specific Policy Effects,"This article investigates whether and how family policy influences the probability and intensity of mothers' labor market participation. Unlike previous studies, this contribution focuses on group-specific policy effects, thereby accounting for the fact that, theoretically, women with different resources and preferences should respond differently to given policy measures. The analyses show that varying individual characteristics indeed influence the impact family policy measures have on women's individual behavior. First and foremost, family policies most strongly influence mothers with medium levels of education, for whom labor market participation tends to be “optional.” Moreover, high direct and indirect cash benefits to families, which primarily stem from traditional conservative family policy, reduce the probability of employment for women with low to medium levels of education.",0
https://doi.org/10.1167/5.3.8,Relational information in visual short-term memory: The structural gist,"Over the past 20 years, storage of visual items in visual short-term memory has been extensively studied by many research groups. In addition to questions concerning the format of object storage is a more global question that focuses on the organization of information in visual short-term memory. In a series of experiments we investigated how relations across visual items determined the accessibility of individual item information. This relational information seems to be very strong within the store devoted to each feature dimension. We also investigated the role of selective attention on the storage of relational information. The experiments suggest a broadening of the parallel store model of visual short-term memory proposed by M. E. Wheeler and A. M. Treisman (2002) to include the notion of what we call ""structural gist.""",0
,"Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage","The covariance matrix plays an important role in statistical inference, yet modeling a covariance matrix is often a difficult task in practice due to its dimensionality and the non-negative definite constraint. In order to model a co- variance matrix effectively, it is typically broken down into components based on modeling considerations or mathematical convenience. Decompositions that have received recent research attention include variance components, spectral decompo- sition, Cholesky decomposition, and matrix logarithm. In this paper we study a statistically motivated decomposition which appears to be relatively unexplored for the purpose of modeling. We model a covariance matrix in terms of its correspond- ing standard deviations and correlation matrix. We discuss two general modeling situations where this approach is useful: shrinkage estimation of regression co- efficients, and a general location-scale model for both categorical and continuous variables. We present some simple choices for priors in terms of standard deviations and the correlation matrix, and describe a straightforward computational strategy for obtaining the posterior of the covariance matrix. We apply our method to real and simulated data sets in the context of shrinkage estimation.",0
https://doi.org/10.1096/fj.12-222992,Rifampin inhibits Toll‐like receptor 4 signaling by targeting myeloid differentiation protein 2 and attenuates neuropathic pain,"Rifampin has been used for the treatment of bacterial infections for many years. Clinically, rifampin has been found to possess immunomodulatory effects. However, the molecular target responsible for the immunosuppressive effects of rifampin is not known. Herein, we show that rifampin binds to myeloid differentiation protein 2 (MD-2), the key coreceptor for innate immune TLR4. Rifampin blocked TLR4 signaling induced by LPS, including NF-κB activation and the overproduction of proinflammatory mediators nitric oxide, interleukin 1β, and tumor necrosis factor α in mouse microglia BV-2 cells and macrophage RAW 264.7 cells. Rifampin's inhibition of TLR4 signaling was also observed in immunocompetent rat primary macrophage, microglia, and astrocytes. Further, we show that rifampin (75 or 100 mg/kg b.i.d. for 3 d, intraperitoneal) suppressed allodynia induced by chronic constriction injury of the sciatic nerve and suppressed nerve injury-induced activation of microglia. Our findings indicate that MD-2 is a important target of rifampin in its inhibition of innate immune function and contributes to its clinically observed immune-suppressive effect. The results also suggest that rifampin may be repositioned as an agent for the treatment of neuropathic pain.",0
https://doi.org/10.1167/13.10.9,Modeling visual working memory with the MemToolbox,"The MemToolbox is a collection of MATLAB functions for modeling visual working memory. In support of its goal to provide a full suite of data analysis tools, the toolbox includes implementations of popular models of visual working memory, real and simulated data sets, Bayesian and maximum likelihood estimation procedures for fitting models to data, visualizations of data and fit, validation routines, model comparison metrics, and experiment scripts. The MemToolbox is released under the permissive BSD license and is available at http://memtoolbox.org.",0
https://doi.org/10.1111/j.1467-842x.2005.00405.x,HIERARCHICAL BAYESIAN MODELLING OF SOCIAL VARIATION IN THE AGE DEPENDENCE OF DISABILITY PREVALENCE,"Summary  Motivated by a study of social variation in the relationship of functional limitation prevalence to age, this paper examines methods for modelling social variation in health outcomes. It is argued that, from a Bayesian perspective, modelling the dependence of functional limitation prevalence on age separately for each social group, corresponds to an implausible prior model, in addition to leading to imprecise estimates for some groups. The alternative strategy of fitting a single model, perhaps including some age-by-group interactions but omitting higher-order interactions, requires a strong prior commitment to the absence of such effects. Hierarchical Bayesian modelling is proposed as a compromise between these two analytical approaches. Under all hierarchical Bayes analyses there is strong evidence for an ethnic group difference in limitation prevalence in early- to mid-adulthood among tertiary-qualified males. In contrast, the single-model approach largely misses this effect, while the group-specific analyses exhibit an unrealistically large degree of heterogeneity in gender-education-specific ethnicity effects. The sensitivity of posterior inferences to prior specifications is studied.",0
https://doi.org/10.1080/09644016.2011.589577,Citizens as veto players: climate change policy and the constraints of direct democracy,"The search for and implementation of effective climate change policies is one of the crucial challenges of policy-makers. One strand of literature argues that domestic factors and in particular institutional prerequisites or veto points strongly influence the quality and pace of a country's policy innovation and adaptation. Focusing on a particular institutional veto point – direct democracy – how does direct democracy influence a country's adaptive capacity in the areas of climate change and, more precisely, what kind of climate change policies have the best chances to be accepted in citizen's direct decision-making? The analyses demonstrate that direct democracy makes it difficult to implement far-reaching, but probably most effective climate change policies, while its direct and indirect impact on the policy-making process rather produces politics of small steps that are supported by a broad political elite.",0
https://doi.org/10.1002/9780470686621,Bayesian Analysis for the Social Sciences,"List of Figures. List of Tables. Preface. Acknowledgments. Introduction. Part I: Introducing Bayesian Analysis. 1. The foundations of Bayesian inference. 1.1 What is probability? 1.2 Subjective probability in Bayesian statistics. 1.3 Bayes theorem, discrete case. 1.4 Bayes theorem, continuous parameter. 1.5 Parameters as random variables, beliefs as distributions. 1.6 Communicating the results of a Bayesian analysis. 1.7 Asymptotic properties of posterior distributions. 1.8 Bayesian hypothesis testing. 1.9 From subjective beliefs to parameters and models. 1.10 Historical note. 2. Getting started: Bayesian analysis for simple models. 2.1 Learning about probabilities, rates and proportions. 2.2 Associations between binary variables. 2.3 Learning from counts. 2.4 Learning about a normal mean and variance. 2.5 Regression models. 2.6 Further reading. Part II: Simulation Based Bayesian Analysis. 3. Monte Carlo methods. 3.1 Simulation consistency. 3.2 Inference for functions of parameters. 3.3 Marginalization via Monte Carlo integration. 3.4 Sampling algorithms. 3.5 Further reading. 4. Markov chains. 4.1 Notation and definitions. 4.2 Properties of Markov chains. 4.3 Convergence of Markov chains. 4.4 Limit theorems for Markov chains. 4.5 Further reading. 5. Markov chain Monte Carlo. 5.1 Metropolis-Hastings algorithm. 5.2 Gibbs sampling. 6. Implementing Markov chain Monte Carlo. 6.1 Software for Markov chain Monte Carlo. 6.2 Assessing convergence and run-length. 6.3 Working with BUGS/JAGS from R. 6.4 Tricks of the trade. 6.5 Other examples. 6.6 Further reading. Part III: Advanced Applications in the Social Sciences. 7. Hierarchical Statistical Models. 7.1 Data and parameters that vary by groups: the case for hierarchical modeling. 7.2 ANOVA as a hierarchical model. 7.3 Hierarchical models for longitudinal data. 7.4 Hierarchical models for non-normal data. 7.5 Multi-level models. 8. Bayesian analysis of choice making. 8.1 Regression models for binary responses. 8.2 Ordered outcomes. 8.3 Multinomial outcomes. 8.4 Multinomial probit. 9. Bayesian approaches to measurement. 9.1 Bayesian inference for latent states. 9.2 Factor analysis. 9.3 Item-response models. 9.4 Dynamic measurement models. Part IV: Appendices. Appendix A: Working with vectors and matrices. Appendix B: Probability review. B.1 Foundations of probability. B.2 Probability densities and mass functions. B.3 Convergence of sequences of random variabales. Appendix C: Proofs of selected propositions. C.1 Products of normal densities. C.2 Conjugate analysis of normal data. C.3 Asymptotic normality of the posterior density. References. Topic index. Author index.",0
https://doi.org/10.1080/10705511.2014.935749,Modeling Nonlinear Structural Equation Models: A Comparison of the Two-Stage Generalized Additive Models and the Finite Mixture Structural Equation Model,"Researchers have devoted some time and effort to developing methods for fitting nonlinear relationships among latent variables. In particular, most of these have focused on correctly modeling interactions between 2 exogenous latent variables, and quadratic relationships between exogenous and endogenous variables. All of these approaches require prespecification of the nonlinearity by the researcher, and are limited to fairly simple nonlinear relationships. Other work has been done using mixture structural equation models (SEMM) in an attempt to fit more complex nonlinear relationships. This study expands on this earlier work by introducing the 2-stage generalized additive model (2SGAM) approach for fitting regression splines in the context of structural equation models. The model is first described and then investigated through the use of simulated data, in which it was compared with the SEMM approach. Results demonstrate that the 2SGAM is an effective tool for fitting a variety of nonlinear relationships...",0
https://doi.org/10.1214/08-aos619,Flexible covariance estimation in graphical Gaussian models,"In this paper, we propose a class of Bayes estimators for the covariance matrix of graphical Gaussian models Markov with respect to a decomposable graph G. Working with the WPG family defined by Letac and Massam [Ann. Statist. 35 (2007) 1278–1323] we derive closed-form expressions for Bayes estimators under the entropy and squared-error losses. The WPG family includes the classical inverse of the hyper inverse Wishart but has many more shape parameters, thus allowing for flexibility in differentially shrinking various parts of the covariance matrix. Moreover, using this family avoids recourse to MCMC, often infeasible in high-dimensional problems. We illustrate the performance of our estimators through a collection of numerical examples where we explore frequentist risk properties and the efficacy of graphs in the estimation of high-dimensional covariance structures.",0
https://doi.org/10.1016/s0167-9473(02)00234-7,Computing the distribution of the product of two continuous random variables,"We present an algorithm for computing the probability density function of the product of two independent random variables, along with an implementation of the algorithm in a computer algebra system. We combine this algorithm with the earlier work on transformations of random variables to create an automated algorithm for convolutions of random variables. Some examples demonstrate the algorithm's application.",0
https://doi.org/10.2307/3001666,The Combination of Estimates from Different Experiments,"When we are trying to make the best estimate of some quantity A that is available from the research conducted to date, the problem of combining results from different experiments is encountered. The problem is often troublesome, particularly if the individual estimates were made by different workers using different procedures. This paper discusses one of the simpler aspects of the problem, in which there is sufficient uniformity of experimental methods so that the ith experiment provides an estimate xi of u, and an estimate si of the standard error of xi . The experiments may be, for example, determinations of a physical or astronomical constant by different scientists, or bioassays carried out in different laboratories, or agricultural field experiments laid out in different parts of a region. The quantity xi may be a simple mean of the observations, as in a physical determination, or the difference between the means of two treatments, as in a comparative experiment, or a median lethal dose, or a regression coefficient. The problem of making a combined estimate has been discussed previously by Cochran (1937) and Yates and Cochran (1938) for agricultural experiments, and by Bliss (1952) for bioassays in different laboratories. The last two papers give recommendations for the practical worker. My purposes in treating the subject again are to discuss it in more general terms, to take account of some recent theoretical research, and, I hope, to bring the practical recommendations to the attention of some biologists who are not acquainted with the previous papers. The basic issue with which this paper deals is as follows. The simplest method of combining estimates made in a number of different experiments is to take the arithmetic mean of the estimates. If, however, the experiments vary in size, or appear to be of different precision, the investigator may wonder whether some kind of weighted meani would be more precise. This paper gives recommendations about the kinds of weighted mean that are appropriate, the situations in which they",0
https://doi.org/10.4310/sii.2013.v6.n1.a4,A Bayesian analysis of generalized latent curve mixture models,"Latent curve models for longitudinal data have received increasing attention in medical, educational, psychological, and behavioral sciences. In these applied areas of research, heterogeneous longitudinal data are common. This paper proposes the use of generalized latent curve models for analyzing heterogenous longitudinal data. The basic model features a mixture of trajectories. It also employs a multinomial logit model for assessing the influence of fixed covariates and explanatory latent variables on the class membership probability within the mixture model. This broad class of models also handles non-normal data from the exponential family distributions. A Bayesian approach is implemented for data analysis. We report a simulation study that proves the satisfactory performance of the proposed approach. Furthermore, we analyzed a real data set extracted from the National Longitudinal Survey of Youth to illustrate the practical value of the proposed model and methodology.",0
https://doi.org/10.1037/a0013024,Central relationship themes in group psychotherapy: A social relations model analysis of transference.,"Group members (N = 55) in 11 therapy groups reported central relationship themes (CRTs) (wishes, responses of others, and responses of self) with other group members and with a romantic partner. Social relations model analyses were used to partition the variance in group member CRT ratings with other members into perceiver, target, and relationship plus error variance components. Significant perceiver variance in member CRT ratings was proposed to serve as a proxy for transference. Overall, significant perceiver variance and mostly insignificant target variance was found, and the perceiver effect accounted for substantially more variance than the target effect. As an exploratory question, the authors wondered to what extent relationship variance accounted for the total variance in member ratings of their CRT? Unfortunately, relationship variance could not be separated from error in this study. Relationship plus error variance accounted for, on average, 42% of the variance in scores. In addition, as a test of the social microcosm of the group theory, it was hypothesized that group member CRTs within the group would relate to member CRTs with a romantic partner outside of the group. Contrary to expectation, this hypothesis was not supported. Â© 2008 American Psychological Association.",0
https://doi.org/10.1037/a0032574,Multilevel confirmatory ordinal factor analysis of the Life Skills Profile–16.,"The aim of this study was to assess the factor structure of the Life skills profile-16 (LSP-16; Buckingham, Burgess, Solomon, Pirkis, & Eagar, 1998a, 1998b) with a view to meeting the assumption of statistical independence that is at significant risk of violation due to the dependency introduced to the data by pooling numerous ratings made by the same observers across independent patients. The sample consisted of 20,181 outpatients rated by 2,071 clinicians employed within 54 mental health organizations within the New South Wales public adult mental health service. To estimate the extent to which the item scores were contaminated with rater-level intraclass correlations (ICC), I fit 16 3-level multinominal ordered proportional odds intercept only models that revealed large ICCs associated with Level 2 (the rater of the LSP-16) demonstrating that a multilevel analysis was required. A multilevel confirmatory factor analysis (M-CFA) using robust weighted least squares (B. O. Muthén, du Toit, & Spisic, 1997) with polychoric correlation was used to test the fit of 2 measurement models that were hypothesized a priori. The 2 models failed to provide an acceptable fit to the sample data and within- and between-level CFAs were used to inform revisions to the 4-factor model. A 15-item version of the LSP was developed, which provided an improved approximate fit in an M-CFA. Limitations of these findings are discussed.",0
https://doi.org/10.1093/biomet/79.4.797,Characterizing the effect of matching using linear propensity score methods with normal distributions,"SUMMARY Matched sampling is a standard technique for controlling bias in observational studies due to specific covariates. Since Rosenbaum & Rubin (1983), multivariate matching methods based on estimated propensity scores have been used with increasing frequency in medical, educational, and sociological applications. We obtain analytic expressions for the effect of matching using linear propensity score methods with normal distributions. These expressions cover cases where the propensity score is either known, or estimated using either discriminant analysis or logistic regression, as is typically done in current practice. The results show that matching using estimated propensity scores not only reduces bias along the population propensity score, but also controls variation of components orthogonal to it. Matching on estimated rather than population propensity scores can therefore lead to relatively large variance reduction, as much as a factor of two in common matching settings where close matches are possible. Approximations are given for the magnitude of this variance reduction, which can be computed using estimates obtained from the matching pools. Related expressions for bias reduction are also presented which suggest that, in difficult matching situations, the use of population scores leads to greater bias reduction than the use of estimated scores.",0
https://doi.org/10.1080/10705511.2016.1186549,On Using Bayesian Methods to Address Small Sample Problems,"As Bayesian methods continue to grow in accessibility and popularity, more empirical studies are turning to Bayesian methods to model small sample data. Bayesian methods do not rely on asympotics, a property that can be a hindrance when employing frequentist methods in small sample contexts. Although Bayesian methods are better equipped to model data with small sample sizes, estimates are highly sensitive to the specification of the prior distribution. If this aspect is not heeded, Bayesian estimates can actually be worse than frequentist methods, especially if frequentist small sample corrections are utilized. We show with illustrative simulations and applied examples that relying on software defaults or diffuse priors with small samples can yield more biased estimates than frequentist methods. We discuss conditions that need to be met if researchers want to responsibly harness the advantages that Bayesian methods offer for small sample problems as well as leading small sample frequentist methods.",1
https://doi.org/10.18148/srm/2012.v6i2.5033,How few countries will do? Comparative survey analysis from a Bayesian perspective,"Meuleman and Billiet (2009) have carried out a simulation study aimed at the question how many countries are needed for accurate multilevel SEM estimation in comparative studies. The authors concluded that a sample of 50 to 100 countries is needed for accurate estimation. Recently, Bayesian estimation methods have been introduced in structural equation modeling which should work well with much lower sample sizes. The current study reanalyzes the simulation of Meuleman and Billiet using Bayesian estimation to find the lowest number of countries needed when conducting multilevel SEM. The main result of our simulations is that a sample of about 20 countries is sufficient for accurate Bayesian estimation, which makes multilevel SEM practicable for the number of countries commonly available in large scale comparative surveys.",1
https://doi.org/10.1111/j.1365-2753.2008.01010.x,Meta-analysis of repeated measures study designs,"Repeated measures studies are found in many areas of research, particularly in areas of healthcare. There is currently little information available to inform the method of meta-analysis of repeated measures studies so that the structural dependence of the data is appropriately accommodated and the findings are meaningful.Using a published meta-analysis on the impact of diet advice on weight reduction of obese or overweight individuals, we demonstrate possible approaches for repeated measures meta-analysis. These approaches differ in terms of the type of result obtained (e.g. effect at a particular time-point, trend over time, change between time-points) and the data needed for the analysis (e.g. means, regression slope estimates). Some approaches involve violating assumptions of independence in the data structure and so to investigate the impact of this violation a simulation study is carried out.The different approaches described for the meta-analyses of repeated measures studies can all provide useful effect estimates depending on the question to be addressed by the meta-analysis. However, violation of the independence assumption in some approaches can lead to biased estimates.In practice, the methods available to carry out meta-analyses of repeated measures studies will not only depend upon the question of interest, but also on the data available from the primary studies.",0
https://doi.org/10.1177/1536867x0200200101,Reliable Estimation of Generalized Linear Mixed Models using Adaptive Quadrature,"Generalized linear mixed models or multilevel regression models have become increasingly popular. Several methods have been proposed for estimating such models. However, to date there is no single method that can be assumed to work well in all circumstances in terms of both parameter recovery and computational efficiency. Stata's xt commands for two-level generalized linear mixed models (e.g., xtlogit) employ Gauss–Hermite quadrature to evaluate and maximize the marginal log likelihood. The method generally works very well, and often better than common contenders such as MQL and PQL, but there are cases where quadrature performs poorly. Adaptive quadrature has been suggested to overcome these problems in the two-level case. We have recently implemented a multilevel versionofthismethodin gllamm, a program that fits a large class of multilevel latent variable models including multilevel generalized linear mixed models. As far as we know, this is the first time that adaptive quadrature has been proposed for multilevel models. We show that adaptive quadrature works well in problems where ordinary quadrature fails. Furthermore, even when ordinary quadrature works, adaptive quadrature is often computationally more efficient since it requires fewer quadrature points to achieve the same precision.",0
https://doi.org/10.1111/j.1751-9004.2010.00303.x,The Social Relations Model: How to Understand Dyadic Processes,The social relations model (SRM) is an intriguing tool both to conceptualize and to analyze dyadic processes. We begin with explaining why interpersonal phenomena in everyday life are more complex than often considered. We then show how the SRM accounts for these complexities by decomposing interpersonal perceptions and behaviors into three independent components and describe the designs required to investigate these components. We then provide a step-by-step,0
https://doi.org/10.1111/j.1745-3984.1998.tb00537.x,Some Practical Examples of Computer-Adaptive Sequential Testing,"Computerized testing has created new challenges for the production and administration of test forms. Many testing organizations engaged in or considering computerized testing may find themselves changing from well-established procedures for handcrafting a small number of paper-and-pencil test forms to procedures for mass producing many computerized test forms. This paper describes an integrated approach to test development and administration called computer-adaptive sequential testing, or CAST. CAST is a structured approach to test construction which incorporates both adaptive testing methods with automated test assembly to allow test developers to maintain a greater degree of control over the production, quality assurance, and administration of different types of computerized tests. CAST retains much of the efficiency of traditional computer adaptive testing (CAT) and can be modified for computer mastery testing (CMT) applications. The CAST framework is described in detail and several applications are demonstrated using a medical licensure example.",0
https://doi.org/10.1111/j.1467-9248.2010.00835.x,When Deliberative Theory Meets Empirical Political Science: Theoretical and Methodological Challenges in Political Deliberation,"Re-linking deliberative theory with empirical political science has become a major theme in the discipline. But when philosophical concepts are to be integrated into positive political science, researchers confront both theoretical and methodological challenges. Focusing on deliberative democracy, a major theoretical challenge is the practical implementation of deliberative ideals. Comparative scholars have explored institutional contexts that favour deliberation, but they have largely neglected actor-centric and cultural variables that might affect deliberative quality as well. Focusing on legislatures in Switzerland and Germany, we show that political institutions as well as partisan strategies and status strongly affect deliberative action, while the effects of culture are less clear. Methodologically, one (frequently neglected) challenge concerns the appropriate statistical tools with which to study deliberation. On the one hand, analysing deliberative processes is demanding and time consuming; hence we tend to have only few and non-randomly selected cases at the group or context level. In addition, the real world of deliberation presents us with a complex matrix of cross-classified speakers. We demonstrate that Bayesian multi-level modelling provides an elegant way to tackle these methodological problems.",0
https://doi.org/10.1016/j.jmva.2009.04.013,Bayesian analysis of non-linear structural equation models with non-ignorable missing outcomes from reproductive dispersion models,"Non-linear structural equation models are widely used to analyze the relationships among outcomes and latent variables in modern educational, medical, social and psychological studies. However, the existing theories and methods for analyzing non-linear structural equation models focus on the assumptions of outcomes from an exponential family, and hence can’t be used to analyze non-exponential family outcomes. In this paper, a Bayesian method is developed to analyze non-linear structural equation models in which the manifest variables are from a reproductive dispersion model (RDM) and/or may be missing with non-ignorable missingness mechanism. The non-ignorable missingness mechanism is specified by a logistic regression model. A hybrid algorithm combining the Gibbs sampler and the Metropolis–Hastings algorithm is used to obtain the joint Bayesian estimates of structural parameters, latent variables and parameters in the logistic regression model, and a procedure calculating the Bayes factor for model comparison is given via path sampling. A goodness-of-fit statistic is proposed to assess the plausibility of the posited model. A simulation study and a real example are presented to illustrate the newly developed Bayesian methodologies.",0
https://doi.org/10.1016/j.lisr.2012.11.006,International students' everyday life information seeking: The informational value of social networking sites,"Abstract Sojourns to other countries, such as for studying abroad, are increasingly common. However, adjusting to life in a different country can be stressful and require significant effort. Sojourners need to not only maintain and expand their social networks, but they also continuously seek information about their new environment. While international students are a sizable group, their daily information behavior is not well understood. This study posits that social networking sites (SNS), such as Facebook, may play an important role in international students' everyday life information seeking (ELIS). Using descriptive statistics, ANOVA, and structural equation modeling (SEM), the study analyzed international students' everyday life information needs, their usage of SNS for ELIS, and the relationships among demographics, personality traits, SNS usage, and perceived usefulness of the acquired everyday life information. Findings indicate that a majority of the respondents frequently used SNS for ELIS. Younger students, undergraduates, and extroverts were more likely to use SNS for ELIS, while no gender difference was found. Notably, among the nine user characteristics and behavior factors, SNS usage emerged as the only positive predictor of perceived usefulness of acquired information in meeting daily needs. This indicates that SNS serve as a valuable channel for purposeful everyday life information seeking. Beyond its social support value, the ELIS value of SNS is a fruitful area for future research.",0
https://doi.org/10.1207/s15328007sem0703_2,Performance of Modified Test Statistics in Covariance and Correlation Structure Analysis Under Conditions of Multivariate Nonnormality,"Questions of whether hypothesized structure models are appropriate representations of the pattern of association among a group of variables can be addressed using a wide variety of statistical procedures. These procedures include covariance structure analysis techniques and correlation structure analysis techniques, in which covariance structure procedures are based on distribution theory for covariances, and correlation structure procedures are based on distribution theory for correlations. The present article provides an overview of standard and modified normal theory and asymptotically distribution-free covariance and correlation structure analysis techniques and also details Monte Carlo simulation results on the Type I and Type II error control as a function of structure model type, number of variables in the model, sample size, and distributional nonnormality. The present Monte Carlo simulation demonstrates clearly that the robustness and nonrobustness of structure analysis techniques vary as a funct...",0
https://doi.org/10.1167/11.5.1,Surface color perception and equivalent illumination models,"Vision provides information about the properties and identity of objects. The ease with which we perceive object properties belies the difficulty of the underlying information-processing task. In the case of object color, retinal information about object reflectance is confounded with information about the illumination as well as about the object's shape and pose. There is no obvious rule that allows transformation of the retinal image to a color representation that depends primarily on object surface reflectance. Under many circumstances, however, object color appearance is remarkably stable across scenes in which the object is viewed. Here, we review a line of experiments and theory that aim to understand how the visual system stabilizes object color appearance. Our emphasis is on models derived from explicit analysis of the computational problem of estimating the physical properties of illuminants and surfaces from the retinal image, and experiments that test these models. We argue that this approach has considerable promise for allowing generalization from simplified laboratory experiments to richer scenes that more closely approximate natural viewing. We discuss the relation between the work we review and other theoretical approaches available in the literature.",0
https://doi.org/10.1007/bf02294343,Multidimensional adaptive testing,"Maximum likelihood and Bayesian procedures for item selection and scoring of multidimensional adaptive tests are presented. A demonstration using simulated response data illustrates that multidimensional adaptive testing (MAT) can provide equal or higher reliabilities with about one-third fewer items than are required by one-dimensional adaptive testing (OAT). Furthermore, holding test-length constant across the MAT and OAT approaches, substantial improvements in reliability can be obtained from multidimensional assessment. A number of issues relating to the operational use of multidimensional adaptive testing are discussed.",0
https://doi.org/10.1177/0146621613508306,Detecting DIF With Ideal Point Models,"Ideal point models have become increasingly popular in research and practice, but little is known about how traditional methods for linking and detecting differential item functioning (DIF) perform with data satisfying ideal point assumptions. Very few studies have been conducted on this topic, and the results of some well-known area and parameter difference DIF detection methods have been inconsistent with previous research involving dominance models. Consequently, the authors conducted a Monte Carlo study to help identify sources of these discrepancies. Specifically, they compared the effectiveness of the Lord’s chi-square parameter difference method and the differential functioning of items and tests (DFIT) area method for detecting DIF with the Generalized Graded Unfolding Model (GGUM). The results clearly indicated that when DIF was simulated via parameter shifts that produced constant magnitudes of effect for all designated items, Lord’s chi-square and DFIT performed similarly well. In addition, the authors found that iterative item characteristic curve (ICC) linking outperformed iterative test characteristic curve (TCC) linking in most experimental conditions. The implications of these findings for DIF detection research and practice with ideal point models are discussed.",0
https://doi.org/10.2466/pr0.106.2.519-533,Classical and Bayesian Estimation in the Logistic Regression Model Applied to Diagnosis of Child Attention Deficit Hyperactivity Disorder,"The limitations inherent to classical estimation of the logistic regression models are known. The Bayesian approach in statistical analysis is an alternative to be considered, given that it makes it possible to introduce prior information about the phenomenon under study. The aim of the present work is to analyze binary and multinomial logistic regression simple models estimated by means of a Bayesian approach in comparison to classical estimation. To that effect, Child Attention Deficit Hyperactivity Disorder (ADHD) clinical data were analyzed. The sample included 286 participants of 6-12 years (78% boys, 22% girls) with ADHD positive diagnosis in 86.7% of the cases. The results show a reduction of standard errors associated to the coefficients obtained from the Bayesian analysis, thus bringing a greater stability to the coefficients. Complex models where parameter estimation may be easily compromised could benefit from this advantage.",0
https://doi.org/10.1038/ncomms10985,Unpredictable environments lead to the evolution of parental neglect in birds,"A nest of begging chicks invites an intuitive explanation: needy chicks want to be fed and parents want to feed them. Surprisingly, however, in a quarter of species studied, parents ignore begging chicks. Furthermore, parents in some species even neglect smaller chicks that beg more, and preferentially feed the biggest chicks that beg less. This extreme variation across species, which contradicts predictions from theory, represents a major outstanding problem for the study of animal signalling. We analyse parent-offspring communication across 143 bird species, and show that this variation correlates with ecological differences. In predictable and good environments, chicks in worse condition beg more, and parents preferentially feed those chicks. In unpredictable and poor environments, parents pay less attention to begging, and instead rely on size cues or structural signals of quality. Overall, these results show how ecological variation can lead to different signalling systems being evolutionarily stable in different species.",0
https://doi.org/10.1111/j.2044-8317.1998.tb00682.x,Normal theory based test statistics in structural equation modelling,"Even though data sets in psychology are seldom normal, the statistics used to evaluate covariance structure models are typically based on the assumption of multivariate normality. Consequently, many conclusions based on normal theory methods are suspect. In this paper, we develop test statistics that can be correctly applied to the normal theory maximum likelihood estimator. We propose three new asymptotically distribution-free (ADF) test statistics that technically must yield improved behaviour in samples of realistic size, and use Monte Carlo methods to study their actual finite sample behaviour. Results indicate that there exists an ADF test statistic that also performs quite well in finite sample situations. Our analysis shows that various forms of ADF test statistics are sensitive to model degrees of freedom rather than to model complexity. A new index is proposed for evaluating whether a rescaled statistic will be robust. Recommendations are given regarding the application of each test statistic.",0
https://doi.org/10.1007/s10162-006-0042-y,Comparison of Absolute Thresholds Derived from an Adaptive Forced-Choice Procedure and from Reaction Probabilities and Reaction Times in a Simple Reaction Time Paradigm,"An understanding of the auditory system's operation requires knowledge of the mechanisms underlying thresholds. In this work we compare detection thresholds obtained with a three-interval-three-alternative forced-choice paradigm with reaction thresholds extracted from both reaction probabilities (RP) and reaction times (RT) in a simple RT paradigm from the same listeners under otherwise nearly identical experimental conditions. Detection thresholds, RP, and RT to auditory stimuli exhibited substantial variation from session to session. Most of the intersession variation in RP and RT could be accounted for by intersession variation in a listener's absolute sensitivity. The reaction thresholds extracted from RP were very similar, if not identical, to those extracted from RT. On the other hand, reaction thresholds were always higher than detection thresholds. The difference between the two thresholds can be considered as the additional amount of evidence required by each listener to react to a stimulus in an unforced design on top of that necessary for detection in the forced-choice design. This difference is inversely related to the listener's probability of producing false alarms. We found that RT, once corrected for some irreducible minimum RT, reflects the time at which a given stimulus reaches the listener's reaction threshold. This suggests that the relationships between simple RT and loudness (reported in the literature) are probably caused by a tight relationship between temporal summation at threshold and temporal summation of loudness.",0
,Reliability analysis of supply chain in strategy synergetic networks based on hierarchical Bayesian method,"The reliability computing formula of supply chain is achieved based on the analysis and judgment of supply chain reliability within the strategy synergetic networks.Accounting to the statistical characteristic of supply chain reliability of the strategy synergetic networks,there are three estimation methods,two Bayesian estimation methods and one hierarchical Bayesian estimation method,used for the evaluation of supply chain specimen's reliability.The test of the estimation of supply chain reliability shows that the hierarchical Bayesian estimation method is more effective than others.",0
https://doi.org/10.1007/bf02291477,Factor analysis of dichotomized variables,An approach for multiple factor analysis of dichotomized variables is presented. It is based on the distribution of the first and second order joint probabilities of the binary scored items. The estimator is based on the generalized least squares principle. Standard errors and a test of the fit of the model is given. Â© 1975 Psychometric Society.,0
https://doi.org/10.1080/10618600.1995.10474663,Approximations to the Log-Likelihood Function in the Nonlinear Mixed-Effects Model,"Abstract Nonlinear mixed-effects models have received a great deal of attention in the statistical literature in recent years because of the flexibility they offer in handling the unbalanced repeated-measures data that arise in different areas of investigation, such as pharmacokinetics and economics. Several different methods for estimating the parameters in nonlinear mixed-effects model have been proposed. We concentrate here on two of them—maximum likelihood and restricted maximum likelihood. A rather complex numerical issue for (restricted) maximum likelihood estimation in nonlinear mixed-effects models is the evaluation of the log-likelihood function of the data, because it involves the evaluation of a multiple integral that, in most cases, does not have a closed-form expression. We consider here four different approximations to the log-likelihood, comparing their computational and statistical properties. We conclude that the linear mixed-effects (LME) approximation suggested by Lindstrom and Bates, t...",0
https://doi.org/10.1201/b14835-6,Introducing Markov chain Monte Carlo,,0
https://doi.org/10.1177/000312240907400502,Low-Income Students and the Socioeconomic Composition of Public High Schools,"Increasing constraints placed on race-based school diversification have shifted attention to socioeconomic desegregation. Although past research suggests that socioeconomic desegregation can produce heightened achievement, the “frog pond” perspective points to potential problems with socioeconomic desegregation in nonachievement domains. Such problems are important in their own right, and they may also chip away at the magnitude of potential achievement benefits. In this article, I report conducted propensity score analyses and robustness calculations on a sample of public high schools in the National Longitudinal Study of Adolescent Health. As the proportion of the student body with middle- or high-income parents increased, low-income students progressed less far in math and science. Moreover, as the proportion of the student body with middle- or high-income or college-educated parents increased, low-income students experienced more psychosocial problems. Such patterns were often more pronounced among African American and Latino students. These findings suggest curricular and social psychological mechanisms of oft-noted frog pond effects in schools and extend the frog pond framework beyond achievement itself to demographic statuses (e.g., race/ethnicity and SES) perceptually linked to achievement. In terms of policy, these findings indicate that socioeconomic desegregation plans should also attend to equity in course enrollments and the social integration of students more generally.",0
https://doi.org/10.3168/jds.2011-4629,Associations between udder health and reproductive performance in United Kingdom dairy cows,"The objective of this research was to evaluate the relationship between udder health and reproductive performance in UK dairy cows. Data from 80 herds were restructured such that each unit of data represented a 2-d period during lactation where a cow was at risk of becoming pregnant. Multilevel discrete-time survival models were then used within a Bayesian framework to explore associations between reproductive outcomes and a variety of potential explanatory variables. Separate models were constructed using 2 different univariate binary outcomes: a cow becoming pregnant during a risk period and a cow becoming pregnant as a result of a given service. Potential explanatory variables included occurrence of clinical mastitis and a categorical representation of individual cow somatic cell count (SCC), both at a variety of timings relative to the risk period. Posterior predictions were used to assess model fit and to check model building assumptions. These demonstrated that the model represented the data well. Within-sample Monte Carlo simulation (i.e., use of the model to predict outcomes for cases within the data set, repeated over a large number of iterations) was used to illustrate results as posterior predicted relative risks. A negative association was found between reproductive performance and cases of clinical mastitis over a wide time frame relative to the risk period (from 28 d before to 70 d after the risk period). A similar negative association with the probability of a service leading to a pregnancy (pregnancy rate) was observed over the same time frame. Higher SCC recordings (i.e., those more likely to be associated with an intramammary infection) were also associated with decreased reproductive performance, especially where an individual cow SCC of greater than 399,000/mL was recorded in the 30 d following a risk period or service. This research demonstrates that both clinical and subclinical mastitis are associated with a reduction in reproductive performance, and that this influence varies in magnitude but can be exerted over a prolonged period.",0
https://doi.org/10.1024/1010-0652/a000059,Dynamic Task Selection in Learning Arithmetic: The Role of Learner Control and Adaptation Based on a Hierarchy of Skills 1Dieser Beitrag wurde unter der geschäftsführenden Herausgeberschaft von Jens Möller angenommen,"Abstract.The effects of locus of instructional control in computer-assisted practice of arithmetic skills and word problem solving were investigated in a field experiment with 13 third grade classes. In a program-controlled condition (n = 95), the selection of practice problems was based on a hypothetical hierarchy of skills. This was expected to regulate cognitive load to a moderate level. In a condition with shared control (n = 89), subjects could select problems from a subset provided by the program. Results show that program-controlled selection of problems based on the hierarchy of skills was more successful in supporting skill development than the students’ selection. In the shared control condition, students tended to select too easy problems, regardless of their level of expertise. Both conditions with computer assisted instruction caused more progress than traditional instruction (n = 94). Ways of improving the regulation of cognitive load within a shared control approach are discussed.",0
https://doi.org/10.1186/s12955-015-0395-1,Detecting short-term change and variation in health-related quality of life: within- and between-person factor structure of the SF-36 health survey,"A major goal of much aging-related research and geriatric medicine is to identify early changes in health and functioning before serious limitations develop. To this end, regular collection of patient-reported outcome measure (PROMs) in a clinical setting may be useful to identify and monitor these changes. However, existing PROMs were not designed for repeated administration and are more commonly used as one-time screening tools; as such, their ability to detect variation and measurement properties when administered repeatedly remain unknown. In this study we evaluated the potential of the RAND SF-36 Health Survey as a repeated-use PROM by examining its measurement properties when modified for administration over multiple occasions.To distinguish between-person (i.e., average) from within-person (i.e., occasion) levels, the SF-36 Health Survey was completed by a sample of older adults (N = 122, M age = 66.28 years) daily for seven consecutive days. Multilevel confirmatory factor analysis (CFA) was employed to investigate the factor structure at both levels for two- and eight-factor solutions.Multilevel CFA models revealed that the correlated eight-factor solution provided better model fit than the two-factor solution at both the between-person and within-person levels. Overall model fit for the SF-36 Health Survey administered daily was not substantially different from standard survey administration, though both were below optimal levels as reported in the literature. However, individual subscales did demonstrate good reliability.Many of the subscales of the modified SF-36 for repeated daily assessment were found to be sufficiently reliable for use in repeated measurement designs incorporating PROMs, though the overall scale may not be optimal. We encourage future work to investigate the utility of the subscales in specific contexts, as well as the measurement properties of other existing PROMs when administered in a repeated measures design. The development and integration of new measures for this purpose may ultimately be necessary.",0
https://doi.org/10.1371/journal.pone.0039059,When One Size Does Not Fit All: A Simple Statistical Method to Deal with Across-Individual Variations of Effects,"In science, it is a common experience to discover that although the investigated effect is very clear in some individuals, statistical tests are not significant because the effect is null or even opposite in other individuals. Indeed, t-tests, Anovas and linear regressions compare the average effect with respect to its inter-individual variability, so that they can fail to evidence a factor that has a high effect in many individuals (with respect to the intra-individual variability). In such paradoxical situations, statistical tools are at odds with the researcher's aim to uncover any factor that affects individual behavior, and not only those with stereotypical effects. In order to go beyond the reductive and sometimes illusory description of the average behavior, we propose a simple statistical method: applying a Kolmogorov-Smirnov test to assess whether the distribution of p-values provided by individual tests is significantly biased towards zero. Using Monte-Carlo studies, we assess the power of this two-step procedure with respect to RM Anova and multilevel mixed-effect analyses, and probe its robustness when individual data violate the assumption of normality and homoscedasticity. We find that the method is powerful and robust even with small sample sizes for which multilevel methods reach their limits. In contrast to existing methods for combining p-values, the Kolmogorov-Smirnov test has unique resistance to outlier individuals: it cannot yield significance based on a high effect in one or two exceptional individuals, which allows drawing valid population inferences. The simplicity and ease of use of our method facilitates the identification of factors that would otherwise be overlooked because they affect individual behavior in significant but variable ways, and its power and reliability with small sample sizes (<30-50 individuals) suggest it as a tool of choice in exploratory studies.",0
https://doi.org/10.1007/bf02294461,EM and beyond,"The basic theme of the EM algorithm, to repeatedly use complete-data methods to solve incomplete data problems, is also a theme of several more recent statistical techniques. These techniques-multiple imputation, data augmentation, stochastic relaxation, and sampling importance resampling-combine simulation techniques with complete-data methods to attack problems that are difficult or impossible for EM. Ã‚Â© 1991 The Psychometric Society.",0
https://doi.org/10.1111/j.1541-0420.2007.00764.x,A Comparison of Two Bias-Corrected Covariance Estimators for Generalized Estimating Equations,"Mancl and DeRouen (2001, Biometrics57, 126-134) and Kauermann and Carroll (2001, JASA96, 1387-1398) proposed alternative bias-corrected covariance estimators for generalized estimating equations parameter estimates of regression models for marginal means. The finite sample properties of these estimators are compared to those of the uncorrected sandwich estimator that underestimates variances in small samples. Although the formula of Mancl and DeRouen generally overestimates variances, it often leads to coverage of 95% confidence intervals near the nominal level even in some situations with as few as 10 clusters. An explanation for these seemingly contradictory results is that the tendency to undercoverage resulting from the substantial variability of sandwich estimators counteracts the impact of overcorrecting the bias. However, these positive results do not generally hold; for small cluster sizes (e.g., <10) their estimator often results in overcoverage, and the bias-corrected covariance estimator of Kauermann and Carroll may be preferred. The methods are illustrated using data from a nested cross-sectional cluster intervention trial on reducing underage drinking.",0
https://doi.org/10.1080/10705511.2014.915205,Interaction Effects in Latent Growth Models: Evaluation of Alternative Estimation Approaches,"The purpose of this investigation is to compare and evaluate 2 approaches for estimating interaction effects in latent growth models (LGMs): the unconstrained approach and the latent moderated structural equations (LMS) approach. To reduce the complexity of modeling interactions in LGMs, we created difference-product indicators to replace the traditional product indicators used in the unconstrained approach because these differences in original indicators represent changes over time. Our focus was to verify the performance of this simplified interaction model of LGMs with difference-product indicators by using the unconstrained approach and comparing it with the LMS approach. Our simulation study showed that the LMS approach generally resulted in smaller biases in the estimated parameters and was consistently more precise than the unconstrained approach under normal conditions, particularly when the sample size was small. When normality assumptions were violated, however, the unconstrained approach was sh...",0
https://doi.org/10.1037/met0000073,Bayesian dynamic mediation analysis.,"Most existing methods for mediation analysis assume that mediation is a stationary, time-invariant process, which overlooks the inherently dynamic nature of many human psychological processes and behavioral activities. In this article, we consider mediation as a dynamic process that continuously changes over time. We propose Bayesian multilevel time-varying coefficient models to describe and estimate such dynamic mediation effects. By taking the nonparametric penalized spline approach, the proposed method is flexible and able to accommodate any shape of the relationship between time and mediation effects. Simulation studies show that the proposed method works well and faithfully reflects the true nature of the mediation process. By modeling mediation effect nonparametrically as a continuous function of time, our method provides a valuable tool to help researchers obtain a more complete understanding of the dynamic nature of the mediation process underlying psychological and behavioral phenomena. We also briefly discuss an alternative approach of using dynamic autoregressive mediation model to estimate the dynamic mediation effect. The computer code is provided to implement the proposed Bayesian dynamic mediation analysis. (PsycINFO Database Record",0
https://doi.org/10.1080/19345747.2014.911396,Covariate Balance in Bayesian Propensity Score Approaches for Observational Studies,"AbstractBayesian alternatives to frequentist propensity score approaches have recently been proposed. However, few studies have investigated their covariate balancing properties. This article compares a recently developed two-step Bayesian propensity score approach to the frequentist approach with respect to covariate balance. The effects of different priors on covariate balance are evaluated and the differences between frequentist and Bayesian covariate balance are discussed. Results of the case study reveal that both the Bayesian and frequentist propensity score approaches achieve good covariate balance. The frequentist propensity score approach performs slightly better on covariate balance for stratification and weighting methods, whereas the two-step Bayesian approach offers slightly better covariate balance in the optimal full matching method. Results of a comprehensive simulation study reveal that accuracy and precision of prior information on propensity score model parameters do not greatly influen...",0
https://doi.org/10.1007/bf02289028,An application of confidence intervals and of maximum likelihood to the estimation of an examinee's ability,"A mathematical definition of the theoretical relation between the examinee's actual responses to the test items and his ""true ability"" is selected. A maximum-likelihood solution is obtained for estimating the examinee's ""true ability"" from his responses to the items. The standard error of the maximum-likelihood estimate is obtained, its relation to the discriminating power of the test is pointed out, and some generalizations are drawn as to the optimum level of item difficulty. The Neyman-Pearson power function is applied to determine which of two psychological tests is the most powerful for the selection of ""successful"" examinees. Â© 1953 Psychometric Society.",0
https://doi.org/10.1016/j.healthplace.2013.07.001,A multilevel analysis of the role of the family and the state in self-rated health of elderly Chinese,"This study examines the geographical variations of self-rated health of the elderly based on the 2008 Chinese Longitudinal Healthy Longevity Survey. Multilevel logistic models are employed to estimate how individual, family, and institutional factors affect the health of the elderly at both individual and province levels. Results show that while individual characteristics help to explain self-rated health, the family remains an important determinant. Those with nobody to care for them, those in poverty and those who have to rely on medical insurance report the worst health. The role of the state is relatively limited in contributing to the health of the elderly. There are substantial between province differences.",0
https://doi.org/10.3102/10769986026001001,Generalized Appended Product Indicator Procedure for Nonlinear Structural Equation Analysis,Interest in considering nonlinear structural equation models is well documented in the behavioral and social sciences as well as in the education and marketing literature. This article considers estimation of polynomial structural models. An existing method is shown to have a limitation that the produced estimator is inconsistent for most practical situations. A new procedure is introduced and defined for a general model using products of observed indicators. The resulting estimator is consistent without assuming any distributional form for the underlying factors or errors. Identification assessment and standard error estimation are discussed. A simulation study addresses statistical issues including comparisons of discrepancy functions and the choice of appended product indicators. Application of the new procedure in a substance abuse prevention study is also reported.,0
https://doi.org/10.3758/s13414-010-0083-5,Visual apparent motion can be modulated by task-irrelevant lexical information,"Previous studies have repeatedly demonstrated the impact of Gestalt structural grouping principles upon the parsing of motion correspondence in ambiguous apparent motion. Here, by embedding Chinese characters in a visual Ternus display that comprised two stimulus frames, we showed that the perception of visual apparent motion can be modulated by activation of task-irrelevant lexical representations. Each frame had two disks, with the second disk of the first frame and the first disk of the second frame being presented at the same location. Observers could perceive either ""element motion,"" in which the endmost disk is seen as moving back and forth while the middle disk at the central position remains stationary, or ""group motion,"" in which both disks appear to move laterally as a whole. More reports of group motion, as opposed to element motion, were obtained when the embedded characters formed two-character compound words than when they formed nonwords, although this lexicality effect appeared to be attenuated by the use of the same characters at the overlapping position across the two frames. Thus, grouping of visual elements in a changing world can be guided by both structural principles and prior world knowledge, including lexical information.",0
https://doi.org/10.1016/j.jmp.2012.06.004,Approximate Bayesian computation with differential evolution,"a b s t r a c t Approximate Bayesian computation (ABC) is a simulation-based method for estimating the posterior distribution of the parameters of a model. The ABC approach is instrumental when a likelihood function for a model cannot be mathematically specified, or has a complicated form. Although difficulty in calculating a model's likelihood is extremely common, current ABC methods suffer from two problems that have largely prevented their mainstream adoption: long computation time and an inability to scale beyond a few parameters. We introduce differential evolution as a computationally efficient genetic algorithm for proposal generation in our ABC sampler. We show how using this method allows our new ABC algorithm, called ABCDE, to obtain accurate posterior estimates in fewer iterations than kernel-based ABC algorithms and to scale to high-dimensional parameter spaces that have proven difficult for current ABC methods.",0
https://doi.org/10.1126/science.277.5328.918,Neighborhoods and Violent Crime: A Multilevel Study of Collective Efficacy,"It is hypothesized that collective efficacy, defined as social cohesion among neighbors combined with their willingness to intervene on behalf of the common good, is linked to reduced violence. This hypothesis was tested on a 1995 survey of 8782 residents of 343 neighborhoods in Chicago, Illinois. Multilevel analyses showed that a measure of collective efficacy yields a high between-neighborhood reliability and is negatively associated with variations in violence, when individual-level characteristics, measurement error, and prior violence are controlled. Associations of concentrated disadvantage and residential instability with violence are largely mediated by collective efficacy.",0
https://doi.org/10.1007/bf02289400,Coefficient alpha and the reliability of composite measurements,"Following a general approach due to Guttman, coefficient Î± is rederived as a lower bound on the reliability of a test. A necessary and sufficient condition under which equality is attained in this inequality and hence that Î± is equal to the reliability of the test is derived and shown to be closely related to the recent redefinition of the concept of parallel measurements due to Novick. This condition is then also shown to be closely related to the unit rank assumption originally adopted by Kuder and Richardson in the derivation of their formula 20. The assumption later adopted by Jackson and Ferguson and the one adopted by Gulliksen are shown to be related to the necessary and sufficient condition derived here. It is then pointed out that the statement that ""coefficient Î± is equal to the mean of the split-half reliabilities"" is true only under the restricted condition assumed by Cronbach in the body of his derivation of this result. Finally some limitations on the uses of any function of Î± as a measure of internal consistency are noted. Â© 1967 Psychometric Society.",0
https://doi.org/10.1111/j.1467-985x.2008.00576.x,"Multilevel multivariate modelling of childhood growth, numbers of growth measurements and adult characteristics",Summary. A general latent normal model for multilevel data with mixtures of response types is extended in the case of ordered responses to deal with variates having a large number of categories and including count data. An example is analysed by using repeated measures data on child growth and adult measures of body mass index and glucose. Applications are described that are concerned with the flexible prediction of adult measurements from collections of growth measurements and for studying the relationship between the number of measurement occasions and growth trajectories.,0
https://doi.org/10.1177/0272431613498646,Conducting Three-Level Cross-Sectional Analyses,"Applied early adolescent researchers often sample students (Level 1) from within classrooms (Level 2) that are nested within schools (Level 3), resulting in data that requires multilevel modeling analysis to avoid Type 1 errors. Although several articles have been published to assist researchers with analyzing sample data nested at two levels, few articles are available to researchers seeking assistance with three-level data analyses. The purpose of this article is to extend the presentational logic and pedagogical flow employed in previous two-level pedagogical publications to illustrate the relevant issues researchers face, the decisions to be made, and the proper procedures needed, when analyzing cross-sectional three-level data. These procedures are demonstrated with a generated three-level data example based on the Early Childhood Longitudinal Study (ECLS-K) public use dataset. The generated data used in this article, as well as the SPSS, SAS, and Mplus model specification syntax files needed to reproduce all analyses in this article, and additional illustrative examples, are available as supplemental online materials at http://jea.sagepub.com/content/early/recent .",0
https://doi.org/10.1016/j.jmp.2010.04.001,A general latent assignment approach for modeling psychological contaminants,"Abstract Data from psychological experiments are rife with ‘contaminants’, which can generally be defined as data generated by psychological processes different from those intended as the object of study. Contaminant data can interfere with the testing of substantive psychological models and their parameters, so it is important to have methods for their identification and removal. After noting that current practices in cognitive modeling for dealing with contaminants are not completely satisfactory, we argue for a general latent mixture approach to the problem. We demonstrate the tractability and effectiveness of the approach concretely, through a series of four applications. These applications involve a simple choice problem, a diffusion model of a response time and accuracy in decision-making, a hierarchical signal detection model of recognition memory, and a reinforcement learning model of decision-making on bandit problems. We conclude that developing models of contaminant processes requires the same sort of creative effort that is needed to model substantive psychological processes, but that it is a necessary endeavour that can be coherently and usefully pursued within the latent mixture modeling approach.",0
https://doi.org/10.1093/biomet/82.3.479,Efficient parametrisations for normal linear mixed models,"SUMMARY The generality and easy programmability of modern sampling-based methods for maximisation of likelihoods and summarisation of posterior distributions have led to a tremendous increase in the complexity and dimensionality of the statistical models used in practice. However, these methods can often be extremely slow to converge, due to high correlations between, or weak identifiability of, certain model parameters. We present simple hierarchical centring reparametrisations that often give improved convergence for a broad class of normal linear mixed models. In particular, we study the two-stage hierarchical normal linear model, the Laird-Ware model for longitudinal data, and a general structure for hierarchically nested linear models. Using analytical arguments, simulation studies, and an example involving clinical markers of acquired immune deficiency syndrome (AIDS), we indicate when reparametrisation is likely to provide substantial gains in efficiency.",0
https://doi.org/10.1177/1094428107300343,Testing Mediation and Suppression Effects of Latent Variables,"Because of the importance of mediation studies, researchers have been continuously searching for the best statistical test for mediation effect. The approaches that have been most commonly employed include those that use zero-order and partial correlation, hierarchical regression models, and structural equation modeling (SEM). This study extends MacKinnon and colleagues (MacKinnon, Lockwood, Hoffmann, West, &amp; Sheets, 2002; MacKinnon, Lockwood, &amp; Williams, 2004, MacKinnon, Warsi, &amp; Dwyer, 1995) works by conducting a simulation that examines the distribution of mediation and suppression effects of latent variables with SEM, and the properties of confidence intervals developed from eight different methods. Results show that SEM provides unbiased estimates of mediation and suppression effects, and that the bias-corrected bootstrap confidence intervals perform best in testing for mediation and suppression effects. Steps to implement the recommended procedures with Amos are presented.",0
https://doi.org/10.3389/fpsyg.2014.00748,A general non-linear multilevel structural equation mixture model,"In the past 2 decades latent variable modeling has become a standard tool in the social sciences. In the same time period, traditional linear structural equation models have been extended to include non-linear interaction and quadratic effects (e.g., Klein and Moosbrugger, 2000), and multilevel modeling (Rabe-Hesketh et al., 2004). We present a general non-linear multilevel structural equation mixture model (GNM-SEMM) that combines recent semiparametric non-linear structural equation models (Kelava and Nagengast, 2012; Kelava et al., 2014) with multilevel structural equation mixture models (Muthén and Asparouhov, 2009) for clustered and non-normally distributed data. The proposed approach allows for semiparametric relationships at the within and at the between levels. We present examples from the educational science to illustrate different submodels from the general framework.",0
https://doi.org/10.1037/0022-3514.94.6.1062,Conceptualizing and assessing self-enhancement bias: A componential approach.,"Four studies implemented a componential approach to assessing self-enhancement and contrasted this approach with 2 earlier ones: social comparison (comparing self-ratings with ratings of others) and self-insight (comparing self-ratings with ratings by others). In Study 1, the authors varied the traits being rated to identify conditions that lead to more or less similarity between approaches. In Study 2, the authors examined the effects of acquaintance on the conditions identified in Study 1. In Study 3, the authors showed that using rankings renders the self-insight approach equivalent to the component-based approach but also has limitations in assessing self-enhancement. In Study 4, the authors compared the social-comparison and the component-based approaches in terms of their psychological implications; the relation between self-enhancement and adjustment depended on the self-enhancement approach used, and the positive-adjustment correlates of the social-comparison approach disappeared when the confounding influence of the target effect was controlled.",0
https://doi.org/10.1198/000313006x117837,Calibrated Bayes,"The lack of an agreed inferential basis for statistics makes life interesting for academic statisticians, but at the price of negative implications for the status of statistics in industry, science, and government. The practice of our discipline will mature only when we can come to a basic agreement about how to apply statistics to real problems. Simple and more general illustrations are given of the negative consequences of the existing schism between frequentists and Bayesians. An assessment of strengths and weaknesses of the frequentist and Bayes systems of inference suggests that calibrated Bayes-a compromise based on the works of Box, Rubin, and others-captures the strengths of both approaches and provides a roadmap for future advances. The approach asserts that inferences under a particular model should be Bayesian, but model assessment can and should involve frequentist ideas. This article also discusses some implications of this proposed compromise for the teaching and practice of statistics.",0
https://doi.org/10.1007/s10463-007-0127-3,Bayesian hierarchical linear mixed models for additive smoothing splines,"Bayesian hierarchical models have been used for smoothing splines, thin-plate splines, and L-splines. In analyzing high dimensional data sets, additive models and backfitting methods are often used. A full Bayesian analysis for such models may include a large number of random effects, many of which are not intuitive, so researchers typically use noninformative improper or nearly improper priors. We investigate propriety of the posterior for these cases. Our findings extend known results for normal linear mixed models to certain cases with Bayesian additive smoothing spline models. Â© 2007 The Institute of Statistical Mathematics.",0
https://doi.org/10.1109/tdei.2009.4784578,Fitting the three-parameter weibull distribution: review and evaluation of existing and new methods,"The three-parameter Weibull distribution is a commonly-used distribution for the study of reliability and breakage data. However, given a data set, it is difficult to estimate the parameters of the distribution and that, for many reasons: (1) the equations of the maximum likelihood estimators are not all available in closed form. These equations can be estimated using iterative methods. However, (2) they return biased estimators and the exact amount of bias is not known. (3) The Weibull distribution does not meet the regularity conditions so that in addition to being biased, the maximum likelihood estimators may also be highly variable from one sample to another (weak efficiency). The methods to estimate parameters of a distribution can be divided into three classes: a) the maximizing approaches, such as the maximum likelihood method, possibly followed by a bias-correction operation; b) the methods of moments; and c) a mixture of the previous two classes of methods. We found using Monte Carlo simulations that a mixed method was the most accurate to estimate the parameters of the Weibull distribution across many shapes and sample sizes, followed by the weighted maximum likelihood estimation method. If the shape parameter is known to be larger than 1, the maximum product of spacing method is the most accurate whereas in the opposite case, the mixed method is to be preferred. A test that can detect if the shape parameter is smaller than 1 is discussed and evaluated. Overall, the maximum likelihood estimation method was the worst, with errors of estimation almost twice as large as those of the best methods.",0
https://doi.org/10.1111/obr.12373,Neighbourhood social capital: measurement issues and associations with health outcomes,"We compared ecometric neighbourhood scores of social capital (contextual variation) to mean neighbourhood scores (individual and contextual variation), using several health-related outcomes (i.e. self-rated health, weight status and obesity-related behaviours). Data were analysed from 5,900 participants in the European SPOTLIGHT survey. Factor analysis of the 13-item social capital scale revealed two social capital constructs: social networks and social cohesion. The associations of ecometric and mean neighbourhood-level scores of these constructs with self-rated health, weight status and obesity-related behaviours were analysed using multilevel regression analyses, adjusted for key covariates. Analyses using ecometric and mean neighbourhood scores, but not mean neighbourhood scores adjusted for individual scores, yielded similar regression coefficients. Higher levels of social network and social cohesion were not only associated with better self-rated health, lower odds of obesity and higher fruit consumption, but also with prolonged sitting and less transport-related physical activity. Only associations with transport-related physical activity and sedentary behaviours were associated with mean neighbourhood scores adjusted for individual scores. As analyses using ecometric scores generated the same results as using mean neighbourhood scores, but different results when using mean neighbourhood scores adjusted for individual scores, this suggests that the theoretical advantage of the ecometric approach (i.e. teasing out individual and contextual variation) may not be achieved in practice. The different operationalisations of social network and social cohesion were associated with several health outcomes, but the constructs that appeared to represent the contextual variation best were only associated with two of the outcomes.",0
https://doi.org/10.1080/01621459.1952.10483446,A Generalization of Sampling Without Replacement from a Finite Universe,"This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions.Journal Paper No. J2139 of the Iowa Agricultural Experiment Station, Ames, Iowa, Project 1005. Presented to the Institute of Mathematical Statistics, March 17, 1951. Â© Taylor & Francis Group, LLC.",0
https://doi.org/10.1177/0193945914548229,Model-Driven Meta-Analyses for Informing Health Care,"A relatively novel type of meta-analysis, a model-driven meta-analysis, involves the quantitative synthesis of descriptive, correlational data and is useful for identifying key predictors of health outcomes and informing clinical guidelines. Few such meta-analyses have been conducted and thus, large bodies of research remain unsynthesized and uninterpreted for application in health care. We describe the unique challenges of conducting a model-driven meta-analysis, focusing primarily on issues related to locating a sample of published and unpublished primary studies, extracting and verifying descriptive and correlational data, and conducting analyses. A current meta-analysis of the research on predictors of key health outcomes in diabetes is used to illustrate our main points.",0
https://doi.org/10.1002/(sici)1097-0258(19990915/30)18:17/18<2343::aid-sim260>3.0.co;2-3,Meta-analysis by random effect modelling in generalized linear models,"The meta-analysis of multi-centre trials can be based on either fixed or random effect models. This paper argues for the general use of random effect models, and illustrates the value of non-parametric maximum likelihood (NPML) analysis of such trials. The same general approach unifies administrative 'league table' analyses in epidemiological and other studies. Several examples of the NPML analysis are given, including a 70-centre trial.",0
https://doi.org/10.1080/10705511.2013.742404,Implementing Restricted Maximum Likelihood Estimation in Structural Equation Models,"Structural equation modeling (SEM) is now a generic modeling framework for many multivariate techniques applied in the social and behavioral sciences. Many statistical models can be considered either as special cases of SEM or as part of the latent variable modeling framework. One popular extension is the use of SEM to conduct linear mixed-effects modeling (LMM) such as cross-sectional multilevel modeling and latent growth modeling. It is well known that LMM can be formulated as structural equation models. However, one main difference between the implementations in SEM and LMM is that maximum likelihood (ML) estimation is usually used in SEM, whereas restricted (or residual) maximum likelihood (REML) estimation is the default method in most LMM packages. This article shows how REML estimation can be implemented in SEM. Two empirical examples on latent growth model and meta-analysis are used to illustrate the procedures implemented in OpenMx. Issues related to implementing REML in SEM are discussed.",0
https://doi.org/10.1214/15-ba962,Posterior Propriety for Hierarchical Models with Log-Likelihoods That Have Norm Bounds,"Statisticians often use improper priors to express ignorance or to provide good frequency properties, requiring that posterior propriety be verified. This paper addresses generalized linear mixed models, GLMMs, when Level I parameters have Normal distributions, with many commonly-used hyperpriors. It provides easy-to-verify sufficient posterior propriety conditions based on dimensions, matrix ranks, and exponentiated norm bounds, ENBs, for the Level I likelihood. Since many familiar likelihoods have ENBs, which is often verifiable via log-concavity and MLE finiteness, our novel use of ENBs permits unification of posterior propriety results and posterior MGF/moment results for many useful Level I distributions, including those commonly used with multilevel generalized linear models, e.g., GLMMs and hierarchical generalized linear models, HGLMs. Those who need to verify existence of posterior distributions or of posterior MGFs/moments for a multilevel generalized linear model given a proper or improper multivariate F prior as in Section 1 should find the required results in Sections 1 and 2 and Theorem 3 (GLMMs), Theorem 4 (HGLMs), or Theorem 5 (posterior MGFs/moments).",0
https://doi.org/10.1037/1082-989x.10.1.21,Investigating population heterogeneity with factor mixture models.,"Sources of population heterogeneity may or may not be observed. If the sources of heterogeneity are observed (e.g., gender), the sample can be split into groups and the data analyzed with methods for multiple groups. If the sources of population heterogeneity are unobserved, the data can be analyzed with latent class models. Factor mixture models are a combination of latent class and common factor models and can be used to explore unobserved population heterogeneity. Observed sources of heterogeneity can be included as covariates. The different ways to incorporate covariates correspond to different conceptual interpretations. These are discussed in detail. Characteristics of factor mixture modeling are described in comparison to other methods designed for data stemming from heterogeneous populations. A step-by-step analysis of a subset of data from the Longitudinal Survey of American Youth illustrates how factor mixture models can be applied in an exploratory fashion to data collected at a single time point.",0
https://doi.org/10.1016/j.jmva.2014.06.001,Estimating high dimensional covariance matrices: A new look at the Gaussian conjugate framework,"In this paper, we describe and study a class of linear shrinkage estimators of the covariance matrix that is well-suited for high dimensional matrices, has a rather wide domain of applicability, and is rooted into the Gaussian conjugate framework of Chen (1979). We propose here a new look at this framework. The linear shrinkage estimator is thereby obtained as the posterior mean of the covariance, using a Bayesian Gaussian model with conjugate inverse Wishart prior, and deriving the shrinkage intensity and target matrix by marginal likelihood maximization. We introduce some extensions to the seminal approach by deriving a closed-form expression of the marginal likelihood as well as computationally light schemes for its maximization. Further, these developments are implemented in a variety of situations and include a simulation-based performance comparison with a recent, widely used class of linear shrinkage estimators. The Gaussian conjugate estimators are found to outperform these estimators in every tested situation where the latter are available and to be more widely and directly applicable. • We (re)introduce a class of linear shrinkage estimators of the covariance matrix. • We follow an empirical Bayesian approach to obtain shrinkage intensity and target. • The method is generally applicable to any class of target matrices. • Estimators are found to outperform those of the state-of-the-art Ledoit–Wolf class. • The implementation is computationally light.",0
https://doi.org/10.1177/1471082x1001100305,Robust statistical modelling using the multivariate skew <i>t</i> distribution with complete and incomplete data,"Missing data is inevitable in many situations that could hamper data analysis for scientific investigations. We establish flexible analytical tools for multivariate skew t models when fat-tailed, asymmetric and missing observations simultaneously occur in the input data. For the ease of computation and theoretical developments, two auxiliary indicator matrices are incorporated into the model for the determination of observed and missing components of each observation that can effectively reduce the computational complexity. Under the missing at random assumption, we present a Monte Carlo version of the expectation conditional maximization algorithm, which is performed to estimate the parameters and retrieve each missing observation with a single value. Additionally, a Metropolis–Hastings within Gibbs sampler with data augmentation is developed to account for the uncertainty of parameters as well as missing outcomes. The methodology is illustrated through two real data sets.",0
https://doi.org/10.2307/2986304,Straight Lines with a Change-Point: A Bayesian Analysis of Some Renal Transplant Data,"Y1=(X2+f2xi+ei, i=zT+l,..., (1) where the ei are independently, normally distributed, N(O, a2), 1 < T < T (al f,) $ (a2, ,B2) and all the parameters a,, f1 a25 ,B25 a2, T are unknown. The two straight lines in (1) intersect at a point with x coordinate v = (a1 a2)/(12 fl1). If we further assume that X1 < X2 <... < XT (in many applications, x will denote time), it is useful to distinguish two versions of (1) according to whether v satisfies x, < v < x+ 1 or not. When this constraint is assumed, we shall refer to the constrained case of switching straight lines. It has been found see, for example, Knapp et al. (1977)-that the constrained version provides a satisfactory statistical model for monitoring the function of renal transplants. Following a transplant, daily measurements are made of serum-creatinine, a substance which indicates the level of functioning of the patient's kidney and a plot of the reciprocal of bodyweight corrected serum-creatinine (y) is then made against time (x). Fig. 1 shows part of two typical plots obtained from patients undergoing treatment at the City Hospital, Nottingham. An increasing straight line indicates successful functioning of the transplanted kidney. A sudden switch to a decreasing straight line indicates that rejection has occurred. It might be argued that rejection is not an instantaneous process, so that the sharp intersection of the two lines is a fiction, and an approach assuming a less sharp transition, such as that of Bacon and Watts (1971), might be more reasonable. On present evidence, however, any transition period would appear to be usually very short compared with the interval between measurements, and so the switching straight line model provides a reasonable approximation. With this assumption, in the renal transplant application the intersection point, y, corresponds to the time at which a rejection occurs. This parameter is of very great interest, both in the treatment of individual patients and because inferences about v for each of a number of patients permit inferences about daily and hourly variations in the rates of rejection. This, in turn, has implications for treatment and monitoring procedures in the hospital (see Knapp et al., 1979).",0
https://doi.org/10.1037/a0014686,A randomized trial of individual and couple behavioral alcohol treatment for women.,"Although alcohol use disorders (AUDs) adversely affect women, research on efficacious treatments for women is limited. In this randomized efficacy trial of 102 heterosexual women with AUDs, the authors compared alcohol behavioral couple therapy (ABCT) and alcohol behavioral individual therapy (ABIT) on percentage of days abstinent (PDA) and percentage of days of heavy drinking (PDH) over 6 months of treatment and 12 months of posttreatment follow-up. Baseline relationship functioning and comorbid disorders were tested as moderators of outcome. Piecewise linear growth models were used to model outcomes. During treatment, women increased their PDA and decreased their PDH, with significantly greater improvements in ABCT than in ABIT (d = 0.59 for PDA; d = 0.79 for PDH). Differences favoring ABCT were maintained during follow-up. Women with poorer baseline relationship functioning improved more on PDA during treatment with ABCT than with ABIT. For PDH, results during treatment and follow-up favored ABCT for women with better baseline relationship functioning. ABCT resulted in better outcomes than ABIT for women with Axis I disorders at the end of follow-up (PDA), and for women with Axis II disorders at the end of treatment (PDA) and at the end of follow-up (PDH).",0
https://doi.org/10.3758/s13428-011-0138-0,CAML—Maximum likelihood consensus analysis,"Consensus analysis enables estimation of individual differences in competencies and response tendencies when answer keys to dichotomous forced-choice questions are unknown. CAML, a set of functions written in R, implements maximum likelihood estimation for the general Condorcet model that underlies consensus analysis. CAML avoids problems of alternative approaches that have often rendered consensus analysis impractical or unfeasible in the past. It provides (1) measures of model fit, (2) a measure of consensus, (3) point and interval estimates of competencies and response tendencies, and (4) an estimate of the unknown answer key. The present article describes the general Condorcet model, the CAML algorithms, and the handling of the software. In addition, the validity of CAML results is tested in a recognition memory study using selective experimental manipulations of the parameters. The results show that CAML works very well in practice and provides valid estimates of competencies, response tendencies, and answer keys.",0
https://doi.org/10.1023/a:1022930918859,,"Event history models typically assume that the entire population is at risk of experiencing the event of interest throughout the observation period. However, there will often be individuals, referred to as long-term survivors, who may be considered a priori to have a zero hazard throughout the study period. In this paper, a discrete-time mixture model is proposed in which the probability of long-term survivorship and the timing of event occurrence are modelled jointly. Another feature of event history data that often needs to be considered is that they may come from a population with a hierarchical structure. For example, individuals may be nested within geographical regions and individuals in the same region may have similar risks of experiencing the event of interest due to unobserved regional characteristics. Thus, the discrete-time mixture model is extended to allow for clustering in the likelihood and timing of an event within regions. The model is further extended to allow for unobserved individual heterogeneity in the hazard of event occurrence. The proposed model is applied in an analysis of contraceptive sterilization in Bangladesh. The results show that a woman's religion and education level affect her probability of choosing sterilization, but not when she gets sterilized. There is also evidence of community-level variation in sterilization timing, but not in the probability of sterilization.",0
https://doi.org/10.1207/s15328007sem1103_8,Modeling Latent Growth Curves With Incomplete Data Using Different Types of Structural Equation Modeling and Multilevel Software,"This article offers different examples of how to fit latent growth curve (LGC) models to longitudinal data using a variety of different software programs (i.e., LISREL, Mx, Mplus, AMOS, SAS). The article shows how the same model can be fitted using both structural equation modeling and multilevel software, with nearly identical results, even in the case of models of latent growth fitted to incomplete data. The general purpose of this article is to provide a demonstration that integrates programming features from different software. The most immediate goal is to help researchers implement these LGC models as a useful way to test hypotheses of growth.",0
https://doi.org/10.3102/1076998610375834,Bayesian Estimation of the Logistic Positive Exponent IRT Model,"A Bayesian inference approach using Markov Chain Monte Carlo (MCMC) is developed for the logistic positive exponent (LPE) model proposed by Samejima and for a new skewed Logistic Item Response Theory (IRT) model, named Reflection LPE model. Both models lead to asymmetric item characteristic curves (ICC) and can be appropriate because a symmetric ICC treats both correct and incorrect answers symmetrically, which results in a logical contradiction in ordering examinees on the ability scale. A data set corresponding to a mathematical test applied in Peruvian public schools is analyzed, where comparisons with other parametric IRT models also are conducted. Several model comparison criteria are discussed and implemented. The main conclusion is that the LPE and RLPE IRT models are easy to implement and seem to provide the best fit to the data set considered.",0
https://doi.org/10.1080/02664760903093625,Extended Bayesian model averaging for heritability in twin studies,"Family studies are often conducted to examine the existence of familial aggregation. Particularly, twin studies can model separately the genetic and environmental contribution. Here we estimate the heritability of quantitative traits via variance components of random-effects in linear mixed models (LMMs). The motivating example was a myopia twin study containing complex nesting data structures: twins and siblings in the same family and observations on both eyes for each individual. Three models are considered for this nesting structure. Our proposal takes into account the model uncertainty in both covariates and model structures via an extended Bayesian model averaging (EBMA) procedure. We estimate the heritability using EBMA under three suggested model structures. When compared with the results under the model with the highest posterior model probability, the EBMA estimate has smaller variation and is slightly conservative. Simulation studies are conducted to evaluate the performance of variance-componen...",0
https://doi.org/10.1111/j.1745-3984.2003.tb01150.x,Assessing Goodness of Fit of Item Response Theory Models: A Comparison of Traditional and Alternative Procedures,"Testing the goodness of fit of item response theory (IRT) models is relevant to validating IRT models, and new procedures have been proposed. These alternatives compare observed and expected response frequencies conditional on observed total scores, and use posterior probabilities for responses across 0 levels rather than cross-classifying examinees using point estimates of 0 and score responses. This research compared these alternatives with regard to their methods, properties (Type I error rates and empirical power), available research, and practical issues (computational demands, treatment of missing data, effects of sample size and sparse data, and available computer programs). Different advantages and disadvantages related to these characteristics are discussed. A simulation study provided additional information about empirical power and Type I error rates. Item response theory (IRT) involves a class of mathematical models that are used to predict examinee performance using item and person characteristics. The properties of these models offer many well-known advantages in testing applications. However, the extent to which these properties are attained is dependent on the degree to which the IRT model itself is appropriate. To validate the use of an IRT model, goodness of fit for a model, or correspondence between model predictions and observed data, is often examined (see Hambleton & Swaminathan, 1985, for a discussion of other useful validation studies). When a model is not appropriate or does not fit the data, use of estimated parameters may be compromised. Several alternative approaches to assessing IRT model-data-fit have emerged in response to using traditional methods in different testing applications. Orlando and Thissen (2000) described a fit statistic that does not use ability estimates but provides a comparison of observed and expected frequencies for score responses across total score levels. Stone, Mislevy, and Mazzeo (1994) argued that uncertainty in ability estimation was responsible for deviations in the approximation of the goodness-of-fit statistics to the null distribution, and discussed a fit statistic based on posterior expectations that addressed the issue. A scaling correction for the chi-squared fit statistic based on resampling methods was subsequently described (Stone, 2000a). Donoghue and Hombo (1999, 2001a) also used the fit statistic based on posterior probabilities to evaluate goodness of fit for National Assessment of Educational Progress (NAEP) items, but derived a distribution for the fit statistic (labeled QDH) that could be used for hypothesis testing.",0
,Lisrel 8: Structural Equation Modeling With the Simplis Command Language,"This text introduces the SIMPLIS command language for structural equation modelling. It is written for students and researchers with limited mathematical and statistical training who need to use structural equation models to analyze their data, and for those who have tried but failed to learn the LISREL command language. It is not a textbook on factor analysis, structural equations or latent variable models, although there are many examples of such in the book. Rather, it is assumed that the reader is already familiar with the basic ideas and principles of these types of analyses and techniques. The main objective is to demonstrate that structural equation modelling can be done easily without the technical jargon with which it has been associated. The SIMPLIS language shifts the focus away from the technical question How to do it, so that researchers can concentrate on the question, What does it all mean? Although the SIMPLIS language makes it easier to specify models and to carry out the analysis, the substantive specification and interpretation remain the same as with the LISREL command language.",0
https://doi.org/10.1016/j.ijresmar.2009.09.004,A simple mechanism to incentive-align conjoint experiments,"Recent literature has established the importance of incentive-aligning research participants in conjoint analysis. Pertinent studies have also proposed and validated a fairly general incentive-aligning mechanism (willingness-to-pay, or WTP) that achieves incentive alignment by using respondents' data to determine their value for a reward product (Ding, 2007). This mechanism, however, requires an estimation of the value of money and is relatively difficult for the average respondent to understand. We propose an alternative mechanism based on inferred rank order for situations where conjoint practitioners have more than one version of real products. In an empirical test of choice-based conjoint, we show that the RankOrder mechanism leads to substantial improvement in predictive performance when compared to non-aligned hypothetical choices. A second test shows that both incentive-aligned mechanisms – RankOrder and WTP – produce very similar predictive performances. RankOrder, however, dominates the WTP mechanism in user preference, an outcome shown both by perceived understanding and by the incentive-aligned money that respondents are willing to pay to switch from one mechanism to the other.",0
https://doi.org/10.1186/1745-6215-15-346,Meta-analysis of randomized phase II trials to inform subsequent phase III decisions,"If multiple Phase II randomized trials exist then meta-analysis is favorable to increase statistical power and summarize the existing evidence about an intervention's effect in order to help inform Phase III decisions. We consider some statistical issues for meta-analysis of Phase II trials for this purpose, as motivated by a real example involving nine Phase II trials of bolus thrombolytic therapy in acute myocardial infarction with binary outcomes.We propose that a Bayesian random effects logistic regression model is most suitable as it models the binomial distribution of the data, helps avoid continuity corrections, accounts for between-trial heterogeneity, and incorporates parameter uncertainty when making inferences. The model also allows predictions that inform Phase III decisions, and we show how to derive: (i) the probability that the intervention will be truly beneficial in a new trial, and (ii) the probability that, in a new trial with a given sample size, the 95% credible interval for the odds ratio will be entirely in favor of the intervention. As Phase II trials are potentially optimistic due to bias in design and reporting, we also discuss how skeptical prior distributions can reduce this optimism to make more realistic predictions.In the example, the model identifies heterogeneity in intervention effect missed by an I-squared of 0%. Prediction intervals accounting for this heterogeneity are shown to support subsequent Phase III trials. The probability of success in Phase III trials increases as the sample size increases, up to 0.82 for intracranial hemorrhage and 0.79 for reinfarction outcomes.The choice of meta-analysis methods can influence the decision about whether a trial should proceed to Phase III and thus need to be clearly documented and investigated whenever a Phase II meta-analysis is performed.",0
https://doi.org/10.1080/07481756.1988.12022886,Using Generalizability Theory in Counseling and Development,"(1988). Using Generalizability Theory in Counseling and Development. Measurement and Evaluation in Counseling and Development: Vol. 21, No. 2, pp. 81-90.",0
https://doi.org/10.1080/00273171.2015.1114911,Incorporating Mobility in Growth Modeling for Multilevel and Longitudinal Item Response Data,"Multilevel data often cannot be represented by the strict form of hierarchy typically assumed in multilevel modeling. A common example is the case in which subjects change their group membership in longitudinal studies (e.g., students transfer schools; employees transition between different departments). In this study, cross-classified and multiple membership models for multilevel and longitudinal item response data (CCMM-MLIRD) are developed to incorporate such mobility, focusing on students' school change in large-scale longitudinal studies. Furthermore, we investigate the effect of incorrectly modeling school membership in the analysis of multilevel and longitudinal item response data. Two types of school mobility are described, and corresponding models are specified. Results of the simulation studies suggested that appropriate modeling of the two types of school mobility using the CCMM-MLIRD yielded good recovery of the parameters and improvement over models that did not incorporate mobility properly. In addition, the consequences of incorrectly modeling the school effects on the variance estimates of the random effects and the standard errors of the fixed effects depended upon mobility patterns and model specifications. Two sets of large-scale longitudinal data are analyzed to illustrate applications of the CCMM-MLIRD for each type of school mobility.",0
https://doi.org/10.1080/01621459.1992.10475235,Bayesian Analysis of Constrained Parameter and Truncated Data Problems Using Gibbs Sampling,"Abstract Constrained parameter problems arise in a wide variety of applications, including bioassay, actuarial graduation, ordinal categorical data, response surfaces, reliability development testing, and variance component models. Truncated data problems arise naturally in survival and failure time studies, ordinal data models, and categorical data studies aimed at uncovering underlying continuous distributions. In many applications both parameter constraints and data truncation are present. The statistical literature on such problems is very extensive, reflecting both the problems’ widespread occurrence in applications and the methodological challenges that they pose. However, it is striking that so little of this applied and theoretical literature involves a parametric Bayesian perspective. From a technical viewpoint, this perhaps is not difficult to understand. The fundamental tool for Bayesian calculations in typical realistic models is (multidimensional) numerical integration, which often is problem...",0
https://doi.org/10.1044/2014_jslhr-h-13-0166,Simulating the Effects of Common and Specific Abilities on Test Performance: An Evaluation of Factor Analysis,"Purpose Factor analysis is a useful technique to aid in organizing multivariate data characterizing speech, language, and auditory abilities. However, knowledge of the limitations of factor analysis is essential for proper interpretation of results. The present study used simulated test scores to illustrate some characteristics of factor analysis. Method Linear models were used to simulate test scores that were determined by multiple latent variables. These simulated test scores were evaluated with principal components analysis and, in certain cases, structural equation modeling. In addition, a subset of simulated individuals characterized by poor test performance was examined. Results The number of factors recovered and their identity do not necessarily correspond to the structure of the latent variables that generated the test scores. The first principal component may represent variance from multiple uncorrelated sources. Practices such as correction or control for general cognitive ability may produce misleading results. Conclusions Inferences from the results of factor analysis should be primarily about the structure of test batteries rather than the structure of human mental abilities. Researchers and clinicians should consider multiple sources of evidence to evaluate hypotheses about the processes generating test results.",0
https://doi.org/10.1186/1471-2288-7-34,A simulation study of sample size for multilevel logistic regression models,"BackgroundMany studies conducted in health and social sciences collect individual level data as outcome measures. Usually, such data have a hierarchical structure, with patients clustered within physicians, and physicians clustered within practices. Large survey data, including national surveys, have a hierarchical or clustered structure; respondents are naturally clustered in geographical units (e.g., health regions) and may be grouped into smaller units. Outcomes of interest in many fields not only reflect continuous measures, but also binary outcomes such as depression, presence or absence of a disease, and self-reported general health. In the framework of multilevel studies an important problem is calculating an adequate sample size that generates unbiased and accurate estimates.MethodsIn this paper simulation studies are used to assess the effect of varying sample size at both the individual and group level on the accuracy of the estimates of the parameters and variance components of multilevel logistic regression models. In addition, the influence of prevalence of the outcome and the intra-class correlation coefficient (ICC) is examined.ResultsThe results show that the estimates of the fixed effect parameters are unbiased for 100 groups with group size of 50 or higher. The estimates of the variance covariance components are slightly biased even with 100 groups and group size of 50. The biases for both fixed and random effects are severe for group size of 5. The standard errors for fixed effect parameters are unbiased while for variance covariance components are underestimated. Results suggest that low prevalent events require larger sample sizes with at least a minimum of 100 groups and 50 individuals per group.ConclusionWe recommend using a minimum group size of 50 with at least 50 groups to produce valid estimates for multi-level logistic regression models. Group size should be adjusted under conditions where the prevalence of events is low such that the expected number of events in each group should be greater than one.",0
https://doi.org/10.1111/j.1467-985x.2009.00586.x,The use of simple reparameterizations to improve the efficiency of Markov chain Monte Carlo estimation for multilevel models with applications to discrete time survival models,"We consider the application of Markov chain Monte Carlo (MCMC) estimation methods to random-effects models and in particular the family of discrete time survival models. Survival models can be used in many situations in the medical and social sciences and we illustrate their use through two examples that differ in terms of both substantive area and data structure. A multilevel discrete time survival analysis involves expanding the data set so that the model can be cast as a standard multilevel binary response model. For such models it has been shown that MCMC methods have advantages in terms of reducing estimate bias. However, the data expansion results in very large data sets for which MCMC estimation is often slow and can produce chains that exhibit poor mixing. Any way of improving the mixing will result in both speeding up the methods and more confidence in the estimates that are produced. The MCMC methodological literature is full of alternative algorithms designed to improve mixing of chains and we describe three reparameterization techniques that are easy to implement in available software. We consider two examples of multilevel survival analysis: incidence of mastitis in dairy cattle and contraceptive use dynamics in Indonesia. For each application we show where the reparameterization techniques can be used and assess their performance.",0
https://doi.org/10.1177/0958928714538214,Subjective insecurity and the role of institutions,"The issue of social insecurity is high on the public and scientific agenda. Most research, however, looks at objective forms of insecurity like growing labour market volatilities or atypical employment. Less has been done with regard to the way people perceive these changes and the role of institutions therein. While recent studies have highlighted the relatively weak role of institutions in explaining different levels of subjective insecurity, they were limited in their understanding in the institutions–security interplay. This special issue aims to understand how institutions generate and moderate the outcomes of subjective insecurity, as well as to overcome some of the methodological limitations of previous studies. The introduction provides a state-of-the-art literature review and unfolds the research question addressed in the special issue. It concludes with some thoughts for future research in the field of social insecurity and institutions.",0
https://doi.org/10.1348/000711007x213963,Generalized latent variable models with non-linear effects,"Until recently, item response models such as the factor analysis model for metric responses, the two-parameter logistic model for binary responses and the multinomial model for nominal responses considered only the main effects of latent variables without allowing for interaction or polynomial latent variable effects. However, non-linear relationships among the latent variables might be necessary in real applications. Methods for fitting models with non-linear latent terms have been developed mainly under the structural equation modelling approach. In this paper, we consider a latent variable model framework for mixed responses (metric and categorical) that allows inclusion of both non-linear latent and covariate effects. The model parameters are estimated using full maximum likelihood based on a hybrid integration-maximization algorithm. Finally, a method for obtaining factor scores based on multiple imputation is proposed here for the non-linear model.",0
https://doi.org/10.1080/0924345950060201,A Multi-Level Analysis of School Improvement: Changes in Schools' Performance over Time,"ABSTRACT The improvement of schools takes place over extended periods of time. Consequently longitudinal studies which track successive cohorts of pupils through their schooling are required if estimates of the extent of improvement are to be established. To date, hardly any studies have collected the necessary data. Those studies which have had appropriate data have tended to emphasise the extent of stability of schools' effectiveness over time rather than the extent of any changes. A shift in conceptual framework is called for if improvements in schools' effectiveness are to be the central focus of concern. The study is based on three successive cohorts of pupils passing through some 30 English secondary schools. It uses examination results as the outcome measure and includes a prior attainment measure amongst the variables used to control for differences between schools' intakes. A multi‐level strategy for conceptualising and modelling data on schools' changes in performance over time is offered. In co...",0
https://doi.org/10.1123/jsep.2013-0261,Goal Striving and Well-Being in Sport: The Role of Contextual and Personal Motivation,"This investigation sought to clarify mixed results in the literature exploring coach behaviors, basic psychological needs, goal motivation, and well- and ill-being. Regional-level team sport athletes ( N = 241) completed questionnaires on the aforementioned variables at the beginning of the season. A subsample ( n = 70) provided saliva samples to assess physical ill-being. At the end of the season, athletes ( n = 98) reported their goal motivation and attainment. Structural equation modeling demonstrated that coach behaviors were related to needs satisfaction and thwarting, which were related to autonomous and controlled goal motives respectively. Autonomous motives were related to well- and ill-being; controlled motives were only related to ill-being. Over time, only end-of-season autonomous goal motives were related to goal attainment. The findings provide an insight into how coaches can facilitate optimum goal striving and well-being in their athletes.",0
https://doi.org/10.1214/12-ba729,Bayesian Graphical Lasso Models and Efficient Posterior Computation,"Recently, the graphical lasso procedure has become popular in estimating Gaussian graphical models. In this paper, we introduce a fully Bayesian treatment of graphical lasso models. We first investigate the graphical lasso prior that has been relatively unexplored. Using data augmentation, we develop a simple but highly efficient block Gibbs sampler for simulating covariance matrices. We then generalize the Bayesian graphical lasso to the Bayesian adaptive graphical lasso. Finally, we illustrate and compare the results from our approach to those obtained using the standard graphical lasso procedures for real and simulated data. In terms of both covariance matrix estimation and graphical structure learning, the Bayesian adaptive graphical lasso appears to be the top overall performer among a range of frequentist and Bayesian methods.",0
https://doi.org/10.1038/npp.2008.66,Reduced Stress-Sensitivity or Increased Reward Experience: The Psychological Mechanism of Response to Antidepressant Medication,"Depression has often been associated with increased negative affect reactivity to stress (Stress-Sensitivity) and reduced capacity to experience pleasure or positive affect (Reward Experience). To date, no studies have prospectively examined changes in Stress-Sensitivity and Reward Experience following antidepressant treatment. The sample included 83 depressed patients and 22 healthy controls. A randomized controlled trial was carried out with patients receiving either imipramine or placebo for 6 weeks. At baseline and 6 weeks, patients and controls participated in an Experience Sampling procedure, prospectively measuring ecologically valid daily life appraisals of activities and mood states. The course of depression was assessed with the Hamilton Depression Rating Scale (HDRS). Multilevel linear regression analyses showed that patients had higher negative and lower positive appraisals of activities than controls. In addition, patients showed increased Stress-Sensitivity (negative affect reactivity to negatively appraised activities). Treatment with imipramine decreased Stress-Sensitivity and increased Reward Experience (positive affect reactivity to positively appraised activities). Changes in Stress-Sensitivity and Reward Experience were in part reducible to changes in the process of activity appraisal itself. However, increase in Reward Experience, but not decrease in Stress-Sensitivity, discriminated between patients who responded and those who did not, independent of changes in the process of activity appraisal itself. Response to treatment in depression may be conditional on restoration of hedonic capacity, the cerebral substrate of which requires further study in relation to antidepressant response. A search for (synergistic) antidepressant therapies specifically targeting ability to experience reward may be warranted.",0
https://doi.org/10.2307/271070,Complex Sample Data in Structural Equation Modeling,"Large-scale surveys using complex sample designs are frequently carried out by government agencies. The statistical analysis technology available for such data is, however, limited in scope. This study investigates and further develops statistical methods that could be used in software for the analysis of data collected under complex sample designs. First, it identifies several recent methodological lines of inquiry which taken together provide a powerful and general statistical basis for a complex sample, structural equation modeling analysis. Second, it extends some of this research to new situations of interest. A Monte Carlo study that empirically evaluates these techniques on simulated data comparable to those in largescale complex surveys demonstrates that they work well in practice. Due to the generality of the approaches, the methods cover not only continuous normal variables but also continuous nonnormal variables and dichotomous variables. Two methods designed to take into account the complex sample structure were",0
https://doi.org/10.1037/a0035420,Spatial working memory in children with high-functioning autism: Intact configural processing but impaired capacity.,"Visual attention and visual working memory exert severe capacity limitations on cognitive processing. Impairments in both functions may exacerbate the social and communication deficits seen in children with an autism spectrum disorder (ASD). This study characterizes spatial working memory and visual attention in school-age children with high-functioning autism. Children with ASD, and age, gender, and IQ-matched typically developing (TD) children performed 2 tasks: a spatial working memory task and an attentive tracking task. Compared with TD children, children with ASD showed a more pronounced deficit in the spatial working memory task than the attentive tracking task, even though the latter placed significant demands on sustained attention, location updating, and distractor inhibition. Because both groups of children were sensitive to configuration mismatches between the sample and test arrays, the spatial working memory deficit was not because of atypical organization of spatial working memory. These findings show that attention and working memory are dissociable, and that children with ASD show a specific deficit in buffering visual information across temporal discontinuity.",0
https://doi.org/10.1001/jama.2014.10780,Benefits and Risks Associated With Thrombolysis for Pulmonary Embolism,,0
https://doi.org/10.1080/10705511003659318,Evaluation of Structural Equation Mixture Models: Parameter Estimates and Correct Class Assignment,"Structural Equation Mixture Models(SEMMs) are latent class models that permit the estimation of a structural equation model within each class. Fitting SEMMs is illustrated using data from one wave of the Notre Dame Longitudinal Study of Aging. Based on the model used in the illustration, SEMM parameter estimation and correct class assignment are investigated in a large scale simulation study. Design factors of the simulation study are (im)balanced class proportions, (im)balanced factor variances, sample size, and class separation. We compare the fit of models with correct and misspecified within-class structural relations. In addition, we investigate the potential to fit SEMMs with binary indicators. The structure of within-class distributions can be recovered under a wide variety of conditions, indicating the general potential and flexibility of SEMMs to test complex within-class models. Correct class assignment is limited.",0
https://doi.org/10.3758/s13428-012-0207-z,Estimating discrimination performance in two-alternative forced choice tasks: Routines for MATLAB and R,"Ulrich and Vorberg (Attention, Perception, & Psychophysics 71: 1219-1227, 2009) introduced a novel approach for estimating discrimination performance in two-alternative forced choice (2AFC) tasks. This approach avoids pitfalls that are inherent when the order of the standard and the comparison is neglected in estimating the difference limen (DL), as in traditional approaches. The present article provides MATLAB and R routines that implement this novel procedure for estimating DLs. These routines also allow to account for processing failures such as lapses or finger errors and can be applied to experimental designs in which the standard and comparison differ only along the task-relevant dimension, as well as to designs in which the stimuli differ in more than one dimension. In addition, Monte Carlo simulations were conducted to check the quality of our routines. Â© 2012 Psychonomic Society, Inc.",0
https://doi.org/10.1186/1744-8069-1-9,"Controlling Neuropathic Pain by Adeno-Associated Virus Driven Production of the Anti-Inflammatory Cytokine, Interleukin-10","Despite many decades of drug development, effective therapies for neuropathic pain remain elusive. The recent recognition of spinal cord glia and glial pro-inflammatory cytokines as important contributors to neuropathic pain suggests an alternative therapeutic strategy; that is, targeting glial activation or its downstream consequences. While several glial-selective drugs have been successful in controlling neuropathic pain in animal models, none are optimal for human use. Thus the aim of the present studies was to explore a novel approach for controlling neuropathic pain. Here, an adeno-associated viral (serotype II; AAV2) vector was created that encodes the anti-inflammatory cytokine, interleukin-10 (IL-10). This anti-inflammatory cytokine is known to suppress the production of pro-inflammatory cytokines. Upon intrathecal administration, this novel AAV2-IL-10 vector was successful in transiently preventing and reversing neuropathic pain. Intrathecal administration of an AAV2 vector encoding beta-galactosidase revealed that AAV2 preferentially infects meningeal cells surrounding the CSF space. Taken together, these data provide initial support that intrathecal gene therapy to drive the production of IL-10 may prove to be an efficacious treatment for neuropathic pain.",0
https://doi.org/10.3389/fpsyg.2014.01531,Logical-rules and the classification of integral dimensions: individual differences in the processing of arbitrary dimensions,"A variety of converging operations demonstrate key differences between separable dimensions, which can be analyzed independently, and integral dimensions, which are processed in a non-analytic fashion. A recent investigation of response time distributions, applying a set of logical rule-based models, demonstrated that integral dimensions are pooled into a single coactive processing channel, in contrast to separable dimensions, which are processed in multiple, independent processing channels. This paper examines the claim that arbitrary dimensions created by factorially morphing four faces are processed in an integral manner. In two experiments, 16 participants completed a categorization task in which either upright or inverted morph stimuli were classified in a speeded fashion. Analyses focused on contrasting different assumptions about the psychological representation of the stimuli, perceptual and decisional separability, and the processing architecture. We report consistent individual differences which demonstrate a mixture of some observers who demonstrate coactive processing with other observers who process the dimensions in a parallel self-terminating manner.",0
https://doi.org/10.1002/sim.5904,"Regression calibration for models with two predictor variables measured with error and their interaction, using instrumental variables and longitudinal data","Regression calibration provides a way to obtain unbiased estimators of fixed effects in regression models when one or more predictors are measured with error. Recent development of measurement error methods has focused on models that include interaction terms between measured-with-error predictors, and separately, methods for estimation in models that account for correlated data. In this work, we derive explicit and novel forms of regression calibration estimators and associated asymptotic variances for longitudinal models that include interaction terms, when data from instrumental and unbiased surrogate variables are available but not the actual predictors of interest. The longitudinal data are fit using linear mixed models that contain random intercepts and account for serial correlation and unequally spaced observations. The motivating application involves a longitudinal study of exposure to two pollutants (predictors) - outdoor fine particulate matter and cigarette smoke - and their association in interactive form with levels of a biomarker of inflammation, leukotriene E4 (LTE 4 , outcome) in asthmatic children. Because the exposure concentrations could not be directly observed, we used measurements from a fixed outdoor monitor and urinary cotinine concentrations as instrumental variables, and we used concentrations of fine ambient particulate matter and cigarette smoke measured with error by personal monitors as unbiased surrogate variables. We applied the derived regression calibration methods to estimate coefficients of the unobserved predictors and their interaction, allowing for direct comparison of toxicity of the different pollutants. We used simulations to verify accuracy of inferential methods based on asymptotic theory.",0
https://doi.org/10.1093/biomet/73.1.13,Longitudinal data analysis using generalized linear models,"SUMMARY This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that give consistent estimates of the regression parameters and of their variance under mild assumptions about the time dependence. The estimating equations are derived without specifying the joint distribution of a subject's observations yet they reduce to the score equations for multivariate Gaussian outcomes. Asymptotic theory is presented for the general class of estimators. Specific cases in which we assume independence, m-dependence and exchangeable correlation structures from each subject are discussed. Efficiency of the proposed estimators in two simple situations is considered. The approach is closely related to quasi-likelih ood. Some key ironh: Estimating equation; Generalized linear model; Longitudinal data; Quasi-likelihood; Repeated measures.",0
https://doi.org/10.1093/biomet/73.1.43,Multilevel mixed linear model analysis using iterative generalized least squares,"SUMMARY Models for the analysis of hierarchically structured data are discussed. An iterative generalized least squares estimation procedure is given and shown to be equivalent to maximum likelihood in the normal case. There is a discussion of applications to complex surveys, longitudinal data, and estimation in multivariate models with missing responses. An example is given using educational data.",0
https://doi.org/10.1111/risa.12157,"Risk Perception, Experience, and Objective Risk: A Cross-National Study with European Emergency Survivors","Understanding public risk perceptions and their underlying processes is important in order to learn more about the way people interpret and respond to hazardous emergency events. Direct experience with an involuntary hazard has been found to heighten the perceived risk of experiencing the same hazard and its consequences in the future, but it remains unclear if cross-over effects are possible (i.e., experience with one hazard influencing perceived risk for other hazards also). Furthermore, the impact of objective risk and country of residence on perceived risk is not well understood. As part of the BeSeCu (Behavior, Security, and Culture) Project, a sample of 1,045 survivors of emergencies from seven European countries (i.e., Germany, the Czech Republic, Poland, Sweden, Spain, Turkey, and Italy) was drawn. Results revealed heightened perceived risk for emergency events (i.e., domestic and public fires, earthquakes, floods, and terrorist attacks) when the event had been experienced previously plus some evidence of cross-over effects, although these effects were not so strong. The largest country differences in perceived risk were observed for earthquakes, but this effect was significantly reduced by taking into account the objective earthquake risk. For fires, floods, terrorist attacks, and traffic accidents, only small country differences in perceived risk were found. Further studies including a larger number of countries are welcomed.",0
https://doi.org/10.1016/j.ecoenv.2015.01.022,Hierarchical modelling of species sensitivity distribution: Development and application to the case of diatoms exposed to several herbicides,"The species sensitivity distribution (SSD) is a key tool to assess the ecotoxicological threat of contaminants to biodiversity. For a contaminant, it predicts which concentration is safe for a community of species. Widely used, this approach suffers from several drawbacks: (i) summarizing the sensitivity of each species by a single value entails a loss of valuable information about the other parameters characterizing the concentration-effect curves; (ii) it does not propagate the uncertainty on estimated sensitivities into the SSD; (iii) the hazardous concentration estimated with SSD only indicates the threat to biodiversity, without any insight about a global response of the community related to the measured endpoint. To remedy these drawbacks, we built a global hierarchical model including the concentration-effect model together with the distribution law of the SSD. We revisited the current SSD approach to account for more sources of variability and uncertainty into the prediction than the traditional analysis and to assess a global response for the community. Working within a Bayesian framework, we were able to compute an SSD taking into account the uncertainty from the original raw data. We also developed a quantitative indicator of a global response of the community to the contaminant. We applied this methodology to study the toxicity and the risk of six herbicides to benthic diatoms from Lake Geneva, based on the biomass endpoint. Our approach highlighted a wide variability within the set of diatom species for all the parameters of the concentration-effect model and a potential correlation between them. Remarkably, variability of the shape parameter of the model and correlation had not been considered before. Comparison between the SSD and the global response of the community revealed that protecting 95% of the species might preserve only 80-86% of the global response. Finally, propagating the uncertainty on the estimated sensitivity showed that building an SSD on a low level of effect, such as EC10, might be unreasonable as it induces a large uncertainty on the result.",0
https://doi.org/10.1016/j.socscimed.2011.11.023,"Psychological pathways linking social support to health outcomes: A visit with the “ghosts” of research past, present, and future","Contemporary models postulate the importance of psychological mechanisms linking perceived and received social support to physical health outcomes. In this review, we examine studies that directly tested the potential psychological mechanisms responsible for links between social support and health-relevant physiological processes (1980s-2010). Inconsistent with existing theoretical models, no evidence was found that psychological mechanisms such as depression, perceived stress, and other affective processes are directly responsible for links between support and health. We discuss the importance of considering statistical/design issues, emerging conceptual perspectives, and limitations of our existing models for future research aimed at elucidating the psychological mechanisms responsible for links between social support and physical health outcomes.",0
https://doi.org/10.1016/s0166-4115(08)62094-4,A New Derivation of the Rasch Model,"Abstract The Rasch model is usually derived from statistical requirements of estimation of its parameters. The principle of 'specific objectivity' or, equivalently, 'sample independence' requires that subject parameters can be estimated independent from the item parameters and vice versa. This in turn requires that sufficient statistics exist for the parameters, from which the Rasch model follows. Rather than using a statistical argument, the present paper presents a derivation of the Rasch model based on the requirement that the probability of inferential ordering of subjects (or items, resp.) is sample independent. The derivation parallels Ducamp & Falmagne's axiomatization of the Guttman scale (composite measurement); it is first given for the ordering of two items or subjects, and subsequently generalized to the probability of a multiple ordering. The core equation in the derivation is, of course, not different from the core equation in the statistical derivation.",0
https://doi.org/10.1037/0033-2909.103.1.111,Modeling multivariate effect sizes.,"In this article, we present a flexible approach to the modeling of multiple effect sizes in meta-analysis. The method uses generalized least squares regression to account for interdependence among multiple outcomes within studies and to allow for different numbers of effect sizes across studies. Furthermore, the approach allows great flexibility in modeling linear equations for multivariate outcomes by means of the inclusion of different sets of predictors for each outcome. We use data from studies of the effectiveness of coaching on performance on the Scholastic Aptitude Test to illustrate application of the method.",0
https://doi.org/10.1214/08-aoas191,A weakly informative default prior distribution for logistic and other regression models,"We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-$t$ prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-$t$ prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.",0
https://doi.org/10.1093/oxfordjournals.aje.a008945,"Intraclass Correlation Estimates in a School-based Smoking Prevention Study: Outcome and Mediating Variables, by Sex and Ethnicity","Most school-based smoking prevention studies employ designs in which schools or classrooms are assigned to different treatment conditions while observations are made on individual students. This design requires that the treatment effect be assessed against the between-school variance. However, the between-school variance is usually larger than the variance that would be obtained if students were individually randomized to different conditions. Consequently, the power of the test for a treatment effect is reduced, and it becomes difficult to detect important treatment effects. To assess the potential loss of power or to calculate appropriate sample sizes, investigators need good estimates of the intraclass correlations for the variables of interest. The authors calculated intraclass correlations for some common outcome variables in a school-based smoking prevention study, using a three-level model-i.e., students nested within classrooms and classrooms nested within schools. The authors present the intraclass correlation estimates for the entire data set, as well as separately by sex and ethnicity. They also illustrate the use of these estimates in the planning of future studies.",0
https://doi.org/10.1002/jrsm.1112,A multivariate model for the meta‐analysis of study level survival data at multiple times,"Motivated by our meta-analytic dataset involving survival rates after treatment for critical leg ischemia, we develop and apply a new multivariate model for the meta-analysis of study level survival data at multiple times. Our data set involves 50 studies that provide mortality rates at up to seven time points, which we model simultaneously, and we compare the results to those obtained from standard methodologies. Our method uses exact binomial within-study distributions and enforces the constraints that both the study specific and the overall mortality rates must not decrease over time. We directly model the probabilities of mortality at each time point, which are the quantities of primary clinical interest. We also present I2 statistics that quantify the impact of the between-study heterogeneity, which is very considerable in our data set. © 2014 The Authors. Research Synthesis Methods published by John Wiley & Sons, Ltd.",0
https://doi.org/10.1016/j.lindif.2011.09.006,"Prediction of self-reported knowledge with over-claiming, fluid and crystallized intelligence and typical intellectual engagement","abstract Article history:Received 19 April 2011Received in revised form 7 September 2011Accepted 9 September 2011Keywords:Over-Claiming Technique (OCT)Over-Claiming Questionnaire (OCQ)Self-reported knowledgeFluid and crystallized intelligenceIntellectual engagement We investigated the usefulness of the Over-Claiming Questionnaire (OCQ) as a measure of cognitive abilities. InOCQs respondents are asked to rate their familiarity with items of academic or everyday knowledge (Paulhus,Harms, Bruce, & Lysy, 2003). Some items exist in reality (reals), and others do not (foils). We developed fourOCQs, each consisting of 40 reals and 8 foils from the domains of Science, Humanities and Civics. The OCQswere administered in a longitudinal rotation design to 112 participants who attended the 9th school grade atthe beginning ofthestudy. Inlatent variableregression analyses53% ofvariation inthe reals couldbeexplainedbyﬂuidandcrystallizedintelligenceandover-claimingasindicatedbyresponsestofoils.Furthervariationinre-sponses to reals and foils was explained by intellectual engagement. Our results show that self-reported knowl-edge, although positively related to measures of ability, to a large extent reﬂects over-claiming.© 2011 Elsevier Inc. All rights reserved.",0
,Statistically based tests for the number of common factors,,0
https://doi.org/10.1016/j.jmva.2015.12.004,Approximate uniform shrinkage prior for a multivariate generalized linear mixed model,Multivariate generalized linear mixed models (MGLMM) are used for jointly modeling the clustered mixed outcomes obtained when there are two or more responses repeatedly measured on each individual in scientific studies. Bayesian methods are widely used techniques for analyzing MGLMM. The need for noninformative priors arises when there is insufficient prior information on the model parameters. The main aim of the present study is to propose an approximate uniform shrinkage prior for the random effect variance components in the Bayesian analysis for the MGLMM. This prior is an extension of the approximate uniform shrinkage prior proposed by Natarajan and Kass (2000). This prior is easy to apply and is shown to possess several nice properties. The use of the approximate uniform shrinkage prior is illustrated in terms of both a simulation study and osteoarthritis data.,0
https://doi.org/10.1111/j.2044-8295.1940.tb00968.x,WEIGHTING FOR BATTERY RELIABILITY AND PREDICTION,,0
https://doi.org/10.1177/0146621609359284,Modeling DIF Effects Using Distractor-Level Invariance Effects: Implications for Understanding the Causes of DIF,"In 2008, Penfield showed that measurement invariance across all response options of a multiple-choice item (correct option and the J distractors) can be modeled using a nominal response model that included a differential distractor functioning (DDF) effect for each of the J distractors. This article extends this concept to consider how the differential item functioning (DIF) effect (i.e., the conditional between-group differences in the probability of correct response) is determined by the J DDF effects. In particular, this article shows how the DIF effect can be modeled as a function of the J DDF effects and thus reveals the conditions that must hold for uniform DIF, nonuniform DIF, and crossing DIF to exist. The results provide insight into the potential item-level properties that lead to uniform, nonuniform, and crossing DIF. The findings may shed light on the etiology of different forms of DIF, which may help analysts target the particular causes of the DIF effect.",0
https://doi.org/10.1037/a0037244,"Testing the four-factor model of personality vulnerability to alcohol misuse: A three-wave, one-year longitudinal study.","The 4-factor model of personality vulnerability identifies 4 personality risk factors for alcohol misuse: hopelessness, anxiety sensitivity, impulsivity, and sensation seeking. These personality traits are associated with distinct mechanisms and motivations for alcohol misuse. Individuals high in hopelessness drink to regulate dysphoric affect, while those high in anxiety sensitivity drink to reduce anxiety and to conform to peer expectations. Individuals high in sensation seeking are highly sensitive to the rewarding properties of alcohol, and misuse alcohol to maximize enjoyment. Impulsivity is a broad risk factor contributing to all drinking motives. We hypothesized that personality vulnerabilities would indirectly predict alcohol quantity and problems through specific drinking motives theorized by the 4-factor model. The present study tested hypotheses using a 3-wave, 1-year longitudinal study of undergraduate drinkers (N = 302). Data were analyzed using multilevel path analysis. Hopelessness and impulsivity were positively related to drinking motives in the expected fashion. Anxiety sensitivity was related to coping-anxiety and conformity motives only in the between-subjects model (partially supporting hypotheses), while sensation seeking was generally unrelated to all drinking motives and alcohol outcomes (failing to support hypotheses). Enhancement motives predicted alcohol quantity and problems at both levels, coping-depression motives predicted alcohol problems at the between-subjects level only, and coping-anxiety, conformity, and social motives failed to predict alcohol outcomes beyond other motives. Overall, this study partially supports the 4-factor model, with the strongest support emerging for impulsivity and hopelessness. This study suggests that personality traits such as impulsivity and hopelessness may be important targets in prevention and treatment with undergraduate drinkers.",0
https://doi.org/10.1177/0272989x12453504,The Choice of a Noninformative Prior on Between-Study Variance Strongly Affects Predictions of Future Treatment Effect,"Purpose. Bayesian random-effects meta-analyses require the analyst to specify the prior distribution for between-study variance of the treatment effect. We assessed the sensitivity of prediction and other outputs of the meta-analysis to the choice of this prior. Methods. We reanalyzed 7 published meta-analyses (5–14 trials) with rare (event rates &lt;5%), moderate (15%–50%), and frequent binary outcomes (&gt;50%). We examined 10 noninformative priors: inverse gamma on between-study variance ( τ 2 ), 2 uniforms on each of the between-study standard deviation ( τ) and τ 2 , uniform shrinkage on τ 2 , DuMouchel shrinkage on τ, half-normal on τ 2 , and half-normal priors on τ with large and small variances. For each analysis, we calculated the posterior distributions for τ, the population treatment effect in current studies, and the predicted treatment effect in a future study. We assessed goodness of fit using total residual deviance, the deviance information criterion, and predictive deviance (by cross-validations). Results. According to total residual deviance, the best-fitting priors were uniform on τ 2 . According to predictive deviance, half-normal on τ 2 and the shrinkage priors were optimal. Across analyses with the 10 priors, there were no important differences in the posteriors for the population treatment effect, but there were substantial differences in the posteriors for τ and predictions. The priors that fitted best according to predictive deviance resulted in less uncertainty around predictions of future treatment effect. Conclusions. In this sample of Bayesian meta-analyses with binary outcomes, the choice of noninformative prior for between-study variance affected model fit and the predictions of future treatment effect. When the predictive distribution is of interest, we highly recommend examination of multiple prior distributions for between-study variance, especially the half-normal on τ 2 and the shrinkage priors.",0
https://doi.org/10.1177/0958928713507468,Public opinion and the reform of the pension systems in Europe: the influence of solidarity principles,"The demographic changes that have occurred in European countries in recent decades have made the policies of the public pension system one of the most debated issues of the welfare state. In this paper, I focus on preferences for three pension policy reforms with different distributive consequences: raising contributions, raising the age of retirement, and allowing free choice between public and private pension plans. I use multilevel models to analyse how individual attachment to different solidarity principles (universalistic, conservative, liberal and familistic) affects attitudes toward pension system reforms while controlling for institutional factors. The empirical results strongly support the hypothesis that solidarity principles have a significant influence on individual preferences. I find that individuals who adhere to universalistic or conservative principles are more in favour of increasing contributions in order to maintain the level of pensions, whereas they oppose a postponement of retirement age. In contrast, those who adhere to liberal or familistic principles are against increasing contributions and prefer extending retirement age. The findings at least partially support the ‘regime hypothesis’, as a more generous pension system appears to increase support for raising contributions while decreasing support for a raise in the age of retirement.",0
https://doi.org/10.3758/bf03206555,Fitting distributions using maximum likelihood: Methods and packages,"The most powerful tests of response time (RT) models often involve the whole shape of the RT distribution, thus avoiding mimicking that can occur at the level of RT means and variances. Nonparametric distribution estimation is, in principle, the most appropriate approach, but such estimators are sometimes difficult to obtain. On the other hand, distribution fitting, given an algebraic function, is both easy and compact. We review the general approach to performing distribution fitting with maximum likelihood (ML) and a method based on quantiles (quantile maximum probability, QMP). We show that QMP has both small bias and good efficiency when used with common distribution functions (the ex-Gaussian, Gumbel, lognormal, Wald, and Weibull distributions). In addition, we review some software packages performing ML (PASTIS, QMPE, DISFIT, and MATHEMATICA) and compare their results. In general, the differences between packages have little influence on the optimal solution found, but the form of the distribution function has: Both the lognormal and the Wald distributions have non-linear dependencies between the parameter estimates that tend to increase the overall bias in parameter recovery and to decrease efficiency. We conclude by laying out a few pointers on how to relate descriptive models of RT to cognitive models of RT. A program that generated the random deviates used in our studies may be downloaded from www.psychonomic.org/archive/.",0
https://doi.org/10.1198/016214504000000647,Full Matching in an Observational Study of Coaching for the SAT,"Among matching techniques for observational studies, full matching is in principle the best, in the sense that its alignment of comparable treated and control subjects is as good as that of any alternate method, and potentially much better. This article evaluates the practical performance of full matching for the first time, modifying it in order to minimize variance as well as bias and then using it to compare coached and uncoached takers of the SAT. In this new version, with restrictions on the ratio of treated subjects to controls within matched sets, full matching makes use of many more observations than does pair matching, but achieves far closer matches than does matching with k≥ 2 controls. Prior to matching, the coached and uncoached groups are separated on the propensity score by 1.1 SDs. Full matching reduces this separation to 1% or 2% of an SD. In older literature comparing matching and regression, Cochran expressed doubts that any method of adjustment could substantially reduce observed bias ...",0
https://doi.org/10.1214/06-ba115,The case for objective Bayesian analysis,"Bayesian statistical practice makes extensive use of versions of objective Bayesian analysis. We discuss why this is so, and address some of the criticisms that have been raised concerning objective Bayesian analysis. The dangers of treating the issue too casually are also considered. In particular, we suggest that the statistical community should accept formal objective Bayesian techniques with confidence, but should be more cautious about casual objective Bayesian techniques.",0
https://doi.org/10.1108/jmhtep-02-2015-0005,Developing a scale measuring perceived knowledge and skills dimensions for mental health promotion: a pilot test using a convenience sample,"Purpose – Against the background of rising mental health (MH) problems many practitioners and health programmers require tools to plan and implement mental health promotion (MHP). A Likert scale to measure Perceived Knowledge of Skills needed for MHP (PKSMHP) was developed and pilot tested. The paper aims to discuss these issues. Design/methodology/approach – A convenience sample of leading personnel ( n =106) in three settings (43 schools, 24 workplaces, 39 care facilities) was drawn in five European countries. A descriptive item analysis, an exploratory and confirmatory factor analysis, and a scales’ performance analysis was adopted. Findings – The validated PKSMHP scale included nine high-quality items measuring the knowledge level of three skills dimensions: MHP management/planning, MHP tools/methods/services and recognition/detection of MH problems. Taken together these can be seen to represent the overall type of skills needed for implementing MHP. Originality/value – The short scale showed very good scale performance values in this pilot study. After further testing the scale might be used as a baseline assessment of MHP needs, as a building block for MHP training and organisational capacity building.",0
https://doi.org/10.1037/0033-295x.102.2.396,A measurement-theoretic analysis of the fuzzy logic model of perception.,"The fuzzy logic model of perception (FLMP) is analyzed from a measurement-theoretic perspective. FLMP has an impressive history of fitting factorial data, suggesting that its probabilistic form is valid. The authors raise questions about the underlying processing assumptions of FLMP. Although FLMP parameters are interpreted as fuzzy logic truth values, the authors demonstrate that for several factorial designs widely used in choice experiments, most desirable fuzzy truth value properties fail to hold under permissible rescalings, suggesting that the fuzzy logic interpretation may be unwarranted. The authors show that FLMP's choice rule is equivalent to a version of G. Rasch's (1960) item response theory model, and the nature of FLMP measurement scales is transparent when stated in this form. Statistical inference theory exists for the Rasch model and its equivalent forms. In fact, FLMP can be reparameterized as a simple 2-category logit model, thereby facilitating interpretation of its measurement scales and allowing access to commercially available software for performing statistical inference.",0
https://doi.org/10.1214/07-ba221,Re-considering the variance parameterization in multiple precision models,"Recent developments in Bayesian computing allow accurate estimation of integrals, making advanced Bayesian analysis feasible. However, some problems remain difficult, such as estimating posterior distributions for variance parameters. For models with three or more variances, this paper proposes a simplex parameterization for the variance structure, which has appealing properties and eases the related burden of specifying a reference prior. This parameterization can be profitably used in several multiple-precision models, including crossed random-effect models, many linear mixed models, smoothed ANOVA, and the conditionally autoregressive (CAR) model with two classes of neighbor relations, often useful for spatial data. The simplex parameterization has at least two attractive features. First, it typically leads to simple MCMC algorithms with good mixing properties regardless of the parameterization used to specify the model's reference prior. Thus, a Bayesian analysis can take computational advantage of the simplex parameterization even if its prior was specified using another parameterization. Second, the simplex parameterization suggests a natural reference prior that is proper, invariant under multiplication of the data by a constant, and which appears to reduce the posterior correlation of smoothing parameters with the error precision. We use simulations to compare the simplex parameterization, with its reference prior, to other parameterizations with their reference priors, according to bias and mean-squared error of point estimates and coverage of posterior 95% credible intervals. The results suggest advantages for the simplex approach, particularly when the error precision is small. We offer results in the context of two real data sets from the fields of periodontics and prosthodontics.",0
https://doi.org/10.1080/13594320244000184,Comprehensive meta-analysis of the construct validity of the employment interview,"This article presents a series of meta-analyses carried out, exploring the construct validity of personnel selection interviews. Accordingly, the interviews were divided into two different groups: conventional interviews and behavior interviews. Conventional interviews are typically composed of questions directed at checking credentials, description of experience, and self-evaluative information. Behavior interviews mainly include questions concerning job knowledge, job experience, and behavior descriptions. The results showed that conventional interviews assessed general mental ability, job experience, the Big Five personality dimensions, and social skills, whereas behavior interviews mainly assessed job knowledge, job experience, situational judgment, and social skills. According to these findings, conventional and behavior interviews seem to be different interviews.",0
https://doi.org/10.1002/j.2162-6057.2010.tb01328.x,Unfolding the Measurement of the Creative Personality,"ABSTRACT Gough’s Creative Personality Scale (CPS) is a self-report personality inventoryfor creativity assessment. We investigated the undimensionality and the responseprocess on the CPS from an ideal point (unfolding) perspective. The Graded Unfold-ing Model (GUM) was used to model binary responses and participants were228 engineering students who completed a Greek version of the CPS. Resultssupport the undimensionality of the CPS construct and suggest that unfoldingmeasurement models may provide new insights to the assessment of creativity. Keywords: Creativity measurement, ideal point models, GUM, Greece INTRODUCTION Creativity assessments seek to identify the creative individual and this is one ofthe most exciting adventures in creativity research (Runco, 2007). Psychometrictests (i.e. tests of divergent thinking, attitude and interest inventories, personalityinventories, and biographical inventories) are one general approach used in thestudy of creativity (Hocevar and Bachelor, 1989). Researchers have devoted sub-stantial efforts to demonstrating the psychometric quality of creativity tests; andthe importance of understanding the psychometric properties of creativity mea-sures has been recognized (Plucker and Renzulli, 1999). Yet, the techniques usedin these efforts have not kept pace with the advances in psychometric theory andmethods. In the field of creativity, the application of modern test theory — notablyItem Response Theory (IRT) and its variants, such as unfolding models is rare.This is regrettable given that modern test theory may bring more psychometricrigor to the problem of measuring creativity, over and above traditional psycho-metric techniques.",0
https://doi.org/10.1890/07-0744.1,Accounting for uncertainty in ecological analysis: the strengths and limitations of hierarchical statistical modeling,"Analyses of ecological data should account for the uncertainty in the process(es) that generated the data. However, accounting for these uncertainties is a difficult task, since ecology is known for its complexity. Measurement and/or process errors are often the only sources of uncertainty modeled when addressing complex ecological problems, yet analyses should also account for uncertainty in sampling design, in model specification, in parameters governing the specified model, and in initial and boundary conditions. Only then can we be confident in the scientific inferences and forecasts made from an analysis. Probability and statistics provide a framework that accounts for multiple sources of uncertainty. Given the complexities of ecological studies, the hierarchical statistical model is an invaluable tool. This approach is not new in ecology, and there are many examples (both Bayesian and non-Bayesian) in the literature illustrating the benefits of this approach. In this article, we provide a baseline for concepts, notation, and methods, from which discussion on hierarchical statistical modeling in ecology can proceed. We have also planted some seeds for discussion and tried to show where the practical difficulties lie. Our thesis is that hierarchical statistical modeling is a powerful way of approaching ecological analysis in the presence of inevitable but quantifiable uncertainties, even if practical issues sometimes require pragmatic compromises.",0
https://doi.org/10.1037/a0039251,Signal detection and threshold modeling of confidence-rating ROCs: A critical test with minimal assumptions.,"An ongoing discussion in the recognition-memory literature concerns the question of whether recognition judgments reflect a direct mapping of graded memory representations (a notion that is instantiated by signal detection theory) or whether they are mediated by a discrete-state representation with the possibility of complete information loss (a notion that is instantiated by threshold models). These 2 accounts are usually evaluated by comparing their (penalized) fits to receiver operating characteristic data, a procedure that is predicated on substantial auxiliary assumptions, which if violated can invalidate results. We show that the 2 accounts can be compared on the basis of critical tests that invoke only minimal assumptions. Using previously published receiver operating characteristic data, we show that confidence-rating judgments are consistent with a discrete-state account. (PsycINFO Database Record",0
https://doi.org/10.1186/1744-859x-12-26,No role for initial severity on the efficacy of antidepressants: results of a multi-meta-analysis,"During the last decade, a number of meta-analyses questioned the clinically relevant efficacy of antidepressants. Part of the debate concerned the method used in each of these meta-analyses as well as the quality of the data set.The Kirsch data set was analysed with a number of different methods, and eight key questions were tackled. We fit random effects models in both Bayesian and frequentist statistical frameworks using raw mean difference and standardised mean difference scales. We also compare between-study heterogeneity estimates and produce treatment rank probabilities for all antidepressants. The role of the initial severity is further examined using meta-regression methods.The results suggest that antidepressants have a standardised effect size equal to 0.34 which is lower but comparable to the effect of antipsychotics in schizophrenia and acute mania. The raw HDRS difference from placebo is 2.82 with the value of 3 included in the confidence interval (2.21-3.44). No role of initial severity was found after partially controlling for the effect of structural (mathematical) coupling. Although data are not definite, even after controlling for baseline severity, there is a strong possibility that venlafaxine is superior to fluoxetine, with the other two agents positioned in the middle. The decrease in the difference between the agent and placebo in more recent studies in comparison to older ones is attributed to baseline severity alone.The results reported here conclude the debate on the efficacy of antidepressants and suggest that antidepressants are clearly superior to placebo. They also suggest that baseline severity cannot be utilized to dictate whether the treatment should include medication or not. Suggestions like this, proposed by guidelines or institutions (e.g. the NICE), should be considered mistaken.",0
https://doi.org/10.1111/bmsp.12081,Bayesian analysis of longitudinal multitrait-multimethod data with ordinal response variables,"A new multilevel latent state graded response model for longitudinal multitrait-multimethod (MTMM) measurement designs combining structurally different and interchangeable methods is proposed. The model allows researchers to examine construct validity over time and to study the change and stability of constructs and method effects based on ordinal response variables. We show how Bayesian estimation techniques can address a number of important issues that typically arise in longitudinal multilevel MTMM studies and facilitates the estimation of the model presented. Estimation accuracy and the impact of between- and within-level sample sizes as well as different prior specifications on parameter recovery were investigated in a Monte Carlo simulation study. Findings indicate that the parameters of the model presented can be accurately estimated with Bayesian estimation methods in the case of low convergent validity with as few as 250 clusters and more than two observations within each cluster. The model was applied to well-being data from a longitudinal MTMM study, assessing the change and stability of life satisfaction and subjective happiness in young adults after high-school graduation. Guidelines for empirical applications are provided and advantages and limitations of a Bayesian approach to estimating longitudinal multilevel MTMM models are discussed.",0
https://doi.org/10.1111/j.1471-4159.2012.07833.x,Harnessing pain heterogeneity and RNA transcriptome to identify blood-based pain biomarkers: a novel correlational study design and bioinformatics approach in a graded chronic constriction injury model,"A quantitative, peripherally accessible biomarker for neuropathic pain has great potential to improve clinical outcomes. Based on the premise that peripheral and central immunity contribute to neuropathic pain mechanisms, we hypothesized that biomarkers could be identified from the whole blood of adult male rats, by integrating graded chronic constriction injury (CCI), ipsilateral lumbar dorsal quadrant (iLDQ) and whole blood transcriptomes, and pathway analysis with pain behavior. Correlational bioinformatics identified a range of putative biomarker genes for allodynia intensity, many encoding for proteins with a recognized role in immune/nociceptive mechanisms. A selection of these genes was validated in a separate replication study. Pathway analysis of the iLDQ transcriptome identified Fcγ and Fcε signaling pathways, among others. This study is the first to employ the whole blood transcriptome to identify pain biomarker panels. The novel correlational bioinformatics, developed here, selected such putative biomarkers based on a correlation with pain behavior and formation of signaling pathways with iLDQ genes. Future studies may demonstrate the predictive ability of these biomarker genes across other models and additional variables.",0
https://doi.org/10.1007/bf03395630,Effective Analysis of Reaction Time Data,"Most analyses of reaction time (RT) data are conducted by using the statistical techniques with which psychologists are most familiar, such as analysis of variance on the sample mean. Unfortunately, these methods are usually inappropriate for RT data, because they have little power to detect genuine differences in RT between conditions. In addition, some statistical approaches can, under certain circumstances, result in findings that are artifacts of the analysis method itself. A corpus of research has shown more effective analytical methods, such as analyzing the whole RT distribution, although this research has had limited influence. The present article will summarize these advances in methods for analyzing RT data.",0
https://doi.org/10.3310/hta14400,Systematic review and economic modelling of the effectiveness and cost-effectiveness of non-surgical treatments for women with stress urinary incontinence,"To assess the clinical effectiveness and cost-effectiveness of non-surgical treatments for women with stress urinary incontinence (SUI) through systematic review and economic modelling.The Cochrane Incontinence Group Specialised Register, electronic databases and the websites of relevant professional organisations and manufacturers, and the following databases: CINAHL, EMBASE, BIOSIS, Science Citation Index and Social Science Citation Index, Current Controlled Trials, ClinicalTrials.gov and the UKCRN Portfolio Database.The study comprised three distinct elements. (1) A survey of 188 women with SUI to identify outcomes of importance to them (activities of daily living; sex, hygiene and lifestyle issues; emotional health; and the availability of services). (2) A systematic review and meta-analysis of non-surgical treatments for SUI to find out which are most effective by comparing results of trials (direct pairwise comparisons) and by modelling results (mixed-treatment comparisons - MTCs). A total of 88 randomised controlled trials (RCTs) and quasi-RCTs reporting data from 9721 women were identified, considering five generic interventions [pelvic floor muscle training (PFMT), electrical stimulation (ES), vaginal cones (VCs), bladder training (BT) and serotonin-noradrenaline reuptake inhibitor (SNRI) medications], in many variations and combinations. Data were available for 37 interventions and 68 treatment comparisons by direct pairwise assessment. Mixed-treatment comparison models compared 14 interventions, using data from 55 trials (6608 women). (3) Economic modelling, using a Markov model, to find out which combinations of treatments (treatment pathways) are most cost-effective for SUI.Titles and abstracts identified were assessed by one reviewer and full-text copies of all potentially relevant reports independently assessed by two reviewers. Any disagreements were resolved by consensus or arbitration by a third person.Direct pairwise comparison and MTC analysis showed that the treatments were more effective than no treatment. Delivering PFMT in a more intense fashion, either through extra sessions or with biofeedback (BF), appeared to be the most effective treatment [PFMT extra sessions vs no treatment (NT) odds ratio (OR) 10.7, 95% credible interval (CrI) 5.03 to 26.2; PFMT + BF vs NT OR 12.3, 95% CrI 5.35 to 32.7]. Only when success was measured in terms of improvement was there evidence that basic PFMT was better than no treatment (PFMT basic vs NT OR 4.47, 95% CrI 2.03 to 11.9). Analysis of cost-effectiveness showed that for cure rates, the strategy using lifestyle changes and PFMT with extra sessions followed by tension-free vaginal tape (TVT) (lifestyle advice-PFMT extra sessions-TVT) had a probability of greater than 70% of being considered cost-effective for all threshold values for willingness to pay for a QALY up to 50,000 pounds. For improvement rates, lifestyle advice-PFMT extra sessions-TVT had a probability of greater than 50% of being considered cost-effective when society's willingness to pay for an additional QALY was more than 10,000 pounds. The results were most sensitive to changes in the long-term performance of PFMT and also in the relative effectiveness of basic PFMT and PFMT with extra sessions.Although a large number of studies were identified, few data were available for most comparisons and long-term data were sparse. Challenges for evidence synthesis were the lack of consensus on the most appropriate method for assessing incontinence and intervention protocols that were complex and varied considerably across studies.More intensive forms of PFMT appear worthwhile, but further research is required to define an optimal form of more intensive therapy that is feasible and efficient for the NHS to provide, along with further definitive evidence from large, well-designed studies.",0
https://doi.org/10.1207/s15327906mbr4002_1,Maximum Likelihood Analysis of Nonlinear Structural Equation Models With Dichotomous Variables,"In this article, a maximum likelihood approach is developed to analyze structural equation models with dichotomous variables that are common in behavioral, psychological and social research. To assess nonlinear causal effects among the latent variables, the structural equation in the model is defined by a nonlinear function. The basic idea of the development is to augment the observed dichotomous data with the hypothetical missing data that involve the latent underlying continuous measurements and the latent variables in the model. An EM algorithm is implemented. The conditional expectation in the E-step is approximated via observations simulated from the appropriate conditional distributions by a Metropolis-Hastings algorithm within the Gibbs sampler, whilst the M-step is completed by conditional maximization. Convergence is monitored by bridge sampling. Standard errors are also obtained. Results from a simulation study and a real example are presented to illustrate the methodology.",0
https://doi.org/10.1177/0049124106289112,A Multilevel Factor Model for Mixed Binary and Ordinal Indicators of Women's Status,"The authors present a factor model for the analysis of categorical responses with a two-level hierarchical structure. The model allows for multiple, potentially correlated factors at each level as well as covariate effects on the responses. Estimation using Markov chain Monte Carlo (MCMC) methods is described. The methodology is applied in an analysis of women's status in Bangladesh. Two dimensions of women's status are considered—social independence and decision-making power—and the factor structure of the responses at the woman and district levels is explored.",0
https://doi.org/10.4324/9781315092614-1,A Comparative Review of Interaction and Nonlinear Modeling,,0
https://doi.org/10.1080/00031305.1987.10475509,How Do We Judge Confidence-Interval Adequacy?,"Abstract Simulation studies of confidence-interval procedures often only report coverage rates. This is not sufficient to judge whether the intervals are “unbiased,” that is, whether they are equally likely to be above as below the true value if they do not cover the true value. Most procedures suggest that they are forming such intervals.",0
https://doi.org/10.1016/0042-6989(95)00016-x,Adaptive psychophysical procedures,"Improvements in measuring thresholds, or points on a psychometric function, have advanced the field of psychophysics in the last 30 years. The arrival of laboratory computers allowed the introduction of adaptive procedures, where the presentation of the next stimulus depends on previous responses of the subject. Unfortunately, these procedures present themselves in a bewildering variety, though some of them differ only slightly. Even someone familiar with several methods cannot easily name the differences, or decide which method would be best suited for a particular application. This review tries to illuminate the historical background of adaptive procedures, explain their differences and similarities, and provide criteria for choosing among the various techniques.",0
https://doi.org/10.1007/s11121-014-0519-6,Engagement in Training as a Mechanism to Understanding Fidelity of Implementation of the Responsive Classroom Approach,"Fidelity of implementation of classroom interventions varies greatly, a reality that is concerning because higher fidelity of implementation relates to greater effectiveness of the intervention. We analyzed 126 fourth and fifth grade teachers from the treatment group of a randomized controlled trial of the Responsive ClassroomÂ® (RC) approach. Prior to training in the intervention, we assessed factors that had the potential to represent a teacherâ€™s readiness to implement with fidelity. These included teachersâ€™ observed emotional support, teacher-rated use of intervention practices, teacher-rated self-efficacy, teacher-rated collective responsibility, education level, and years of experience, and they were not directly related to observed fidelity of implementation 2Â years later. Further analyses indicated, however, that RC trainersâ€™ ratings of teachersâ€™ engagement in the initial weeklong RC training mediated the relation between initial observed emotional support and later observed fidelity of implementation. We discuss these findings as a way to advance understanding of teachersâ€™ readiness to implement new interventions with fidelity. Â© 2014, Society for Prevention Research.",0
,Cross-domain collaborative filtering via bilinear multilevel analysis,"Cross-domain collaborative filtering (CDCF), which aims to leverage data from multiple domains to relieve the data sparsity issue, is becoming an emerging research topic in recent years. However, current CDCF methods that mainly consider user and item factors but largely neglect the heterogeneity of domains may lead to improper knowledge transfer issues. To address this problem, we propose a novel CDCF model, the Bilinear Multilevel Analysis (BLMA), which seamlessly introduces multilevel analysis theory to the most successful collaborative filtering method, matrix factorization (MF). Specifically, we employ BLMA to more efficiently address the determinants of ratings from a hierarchical view by jointly considering domain, community, and user effects so as to overcome the issues caused by traditional MF approaches. Moreover, a parallel Gibbs sampler is provided to learn these effects. Finally, experiments conducted on a realworld dataset demonstrate the superiority of the BLMA over other state-of-the-art methods.",0
https://doi.org/10.1080/10705510802561279,Small-Sample Robust Estimators of Noncentrality-Based and Incremental Model Fit,"Traditional estimators of fit measures based on the noncentral chi–square distribution (root mean square error of approximation [RMSEA], Steiger's γ, etc.) tend to overreject acceptable models when the sample size is small. To handle this problem, it is proposed to employ Bartlett's (1950) Bartlett, M. S. 1950. Tests of significance in factor analysis. British Journal of Psychology (Statistical Section), 3: 77–85. [Crossref], [Web of Science ®] , [Google Scholar], Yuan's (2005) Yuan, K.-H. 2005. Fit indices versus test statistics. Multivariate Behavioral Research, 40: 115–148. [Taylor & Francis Online], [Web of Science ®] , [Google Scholar], or Swain's (1975) Swain, A. J. 1975. Analysis of parametric structures for variance matrices., Australia: Department of Statistics, University of Adelaide. Unpublished doctoral dissertation [Google Scholar] correction of the maximum likelihood chi–square statistic for the estimation of noncentrality–based fit measures. In a Monte Carlo study, it is shown that Swain's correction especially produces reliable estimates and confidence intervals for different degrees of model misspecification (RMSEA range: 0.000–0.096) and sample sizes (50, 75, 100, 150, 200). In the second part of the article, the study is extended to incremental fit indexes (Tucker–Lewis Index, Comparative Fit Index, etc.). For their small–sample robust estimation, use of Swain's correction is recommended only for the target model, not for the independence model. The Swain–corrected estimators only require a ratio of sample size to estimated parameters of about 2:1 (sometimes even less) and are thus strongly recommended for applied research. R software is provided for convenient use.",0
https://doi.org/10.1076/edre.4.1.13.13014,A Multilevel Perspective on the Design and Analysis of Intervention Studies,"The level (pupil, classroom or school) at which an educational intervention is assigned affects both the kinds of questions which can be answered in evaluation research, and the statistical methods used to answer them. This paper sets out ways of analysing different kinds of designs using multilevel models. It also considers practical issues such as the method used to allocate interventions, leakage, integrity of delivery, and cost, and how these interact with the more technical issues of model specification. These practical issues are illustrated by two recent British intervention studies. Resume Le niveau - eleve, classe ou ecole - auquel s'adresse une intervention educative affecte tout a la fois le genre de questions qui peuvent trouver reponse dans la recherche evaluative et les methodes statistiques pour y repondre. Cet article presente des voies pour analyser differents types de design qui utilisent des modeles a plusieurs niveaux. Il traite aussi de certains aspects pratiques tels que la methode...",0
https://doi.org/10.1080/00031305.1992.10475878,Explaining the Gibbs Sampler,"Abstract Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.",0
https://doi.org/10.1016/j.csda.2013.07.036,"Robust growth mixture models with non-ignorable missingness: Models, estimation, selection, and application","Challenges in the analyses of growth mixture models include missing data, outliers, estimation, and model selection. Four non-ignorable missingness models to recover the information due to missing data, and three robust models to reduce the effect of non-normality are proposed. A full Bayesian method is implemented by means of data augmentation algorithm and Gibbs sampling procedure. Model selection criteria are also proposed in the Bayesian context. Simulation studies are then conducted to evaluate the performances of the models, the Bayesian estimation method, and selection criteria under different situations. The application of the models is demonstrated through the analysis of education data on children's mathematical ability development. The models can be widely applied to longitudinal analyses in medical, psychological, educational, and social research. Four non-ignorable missingness models are proposed.Three robust models to deal with outliers are proposed.A full Bayesian method is implemented.Model selection criteria are proposed in a Bayesian context.Three simulation studies and one real data case study are conducted.",0
https://doi.org/10.1108/tpm-03-2015-0014,Measuring adaptive performance in individuals and teams,"Purpose – While scales were developed to measure individual adaptive performance (IAP), fewer contributions have been done to assess the construct at the team level of analysis. This issue is addressed through two related studies: Study 1 builds on Pulakos et al. (2000) to develop a measure of IAP. Study 2 follows from the results in Study 1 and tests a measure of team adaptive performance (Chan, 1998). Design/methodology/approach – Scale development was done adopting a single level (Study 1) and multi-level (Study 2) structural equations modeling approach. Findings – Results suggest that both measures of individual and team adaptive performance are reliable and show evidence supporting the adequacy of adopting referent-shift methodologies to the measurement and aggregation of team members’ rating of team adaptive performance. Originality/value – The study offers a reliable, parsimonious and easy to apply measure of individual and team adaptive performance in organizational work environments.",0
https://doi.org/10.1080/10888438.2011.618153,Fluency Has a Role in the Simple View of Reading,"The Simple View of Reading (SVR) suggests that the components of reading comprehension are decoding and linguistic comprehension. Given research that suggests that fluency is a separate construct from decoding and linguistic comprehension in fourth grade, the aim of this study was to examine the role of fluency in the SVR model. Analyses of data from 248 fourth-grade children explored whether the influence of fluency on reading comprehension is direct or whether fluency plays an indirect role on reading comprehension as a mediator or moderator of decoding. Structural equation modeling and latent regression analyses revealed that reading fluency plays a mediating role in explaining the relation between decoding and reading comprehension. This novel finding is placed in the context of studies that reported either a direct effect or no effect of reading fluency in SVR.",0
https://doi.org/10.1214/06-ba128,A skew item response model,"We introduce a new skew-probit link for item response theory (IRT) by considering an accumulated skew-normal distribution. The model extends the symmetric probit-normal IRT model by considering a new item (or skewness) parameter for the item characteristic curve. A special interpretation is given for this parameter, and a latent linear structure is indicated for the model when an augmented likelihood is considered. Bayesian MCMC inference approach is developed and an efficiency study in the estimation of the model parameters is undertaken for a data set from (Tanner 1996, pg. 190) by using the notion of effective sample size (ESS) as defined in Kass et al. (1999) and the sample size per second (ESS/s) as considered in Sahu (2002). The methodology is illustrated using a data set corresponding to a Mathematical Test applied in Peruvian schools for which a sensitivity analysis of the chosen priors is conducted and also a comparison with seven parametric IRT models is conducted. The main conclusion is that the skew-probit item response model seems to provide the best fit.",0
https://doi.org/10.3758/s13428-018-1054-3,On the importance of avoiding shortcuts in applying cognitive models to hierarchical data,"Psychological experiments often yield data that are hierarchically structured. A number of popular shortcut strategies in cognitive modeling do not properly accommodate this structure and can result in biased conclusions. To gauge the severity of these biases, we conducted a simulation study for a two-group experiment. We first considered a modeling strategy that ignores the hierarchical data structure. In line with theoretical results, our simulations showed that Bayesian and frequentist methods that rely on this strategy are biased towards the null hypothesis. Secondly, we considered a modeling strategy that takes a two-step approach by first obtaining participant-level estimates from a hierarchical cognitive model and subsequently using these estimates in a follow-up statistical test. Methods that rely on this strategy are biased towards the alternative hypothesis. Only hierarchical models of the multilevel data lead to correct conclusions. Our results are particularly relevant for the use of hierarchical Bayesian parameter estimates in cognitive modeling.",0
https://doi.org/10.1037/0022-3514.86.1.148,"Academic Performance, Career Potential, Creativity, and Job Performance: Can One Construct Predict Them All?","This meta-analysis addresses the question of whether 1 general cognitive ability measure developed for predicting academic performance is valid for predicting performance in both educational and work domains. The validity of the Miller Analogies Test (MAT; W. S. Miller, 1960) for predicting 18 academic and work-related criteria was examined. MAT correlations with other cognitive tests (e.g., Raven's Matrices [J. C. Raven, 1965]; Graduate Record Examinations) also were meta-analyzed. The results indicate that the abilities measured by the MAT are shared with other cognitive ability instruments and that these abilities are generalizably valid predictors of academic and vocational criteria, as well as evaluations of career potential and creativity. These findings contradict the notion that intelligence at work is wholly different from intelligence at school, extending the voluminous literature that supports the broad importance of general cognitive ability (g).",0
https://doi.org/10.1007/s11336-008-9088-6,Identifying Variables Responsible for Data not Missing at Random,"When data are not missing at random (NMAR), maximum likelihood (ML) procedure will not generate consistent parameter estimates unless the missing data mechanism is correctly modeled. Understanding NMAR mechanism in a data set would allow one to better use the ML methodology. A survey or questionnaire may contain many items; certain items may be responsible for NMAR values in other items. The paper develops statistical procedures to identify the responsible items. By comparing ML estimates (MLE), statistics are developed to test whether the MLEs are changed when excluding items. The items that cause a significant change of the MLEs are responsible for the NMAR mechanism. Normal distribution is used for obtaining the MLEs; a sandwich-type covariance matrix is used to account for distribution violations. The class of nonnormal distributions within which the procedure is valid is provided. Both saturated and structural models are considered. Effect sizes are also defined and studied. The results indicate that more missing data in a sample does not necessarily imply more significant test statistics due to smaller effect sizes. Knowing the true population means and covariances or the parameter values in structural equation models may not make things easier either. Ã‚Â© 2008 The Psychometric Society.",0
https://doi.org/10.1177/1094428107308906,Estimating Statistical Power and Required Sample Sizes for Organizational Research Using Multilevel Modeling,"The use of multilevel modeling to investigate organizational phenomena is rapidly increasing. Unfortunately, little advice is readily available for organizational researchers attempting to determine statistical power when using multilevel models or when determining sample sizes for each level that will maximize statistical power. This article presents an introduction to statistical power in multilevel models. The unique factors influencing power in multilevel models and calculations for estimating power for simple fixed effects, variance components, and cross-level interactions are presented. The results of simulation studies and the existing general rules of thumb are discussed, and the available power analysis software is reviewed.",0
https://doi.org/10.1186/1471-2288-13-2,Modelling heterogeneity variances in multiple treatment comparison meta-analysis – Are informative priors the better solution?,"BackgroundMultiple treatment comparison (MTC) meta-analyses are commonly modeled in a Bayesian framework, and weakly informative priors are typically preferred to mirror familiar data driven frequentist approaches. Random-effects MTCs have commonly modeled heterogeneity under the assumption that the between-trial variance for all involved treatment comparisons are equal (i.e., the ‘common variance’ assumption). This approach ‘borrows strength’ for heterogeneity estimation across treatment comparisons, and thus, ads valuable precision when data is sparse. The homogeneous variance assumption, however, is unrealistic and can severely bias variance estimates. Consequently 95% credible intervals may not retain nominal coverage, and treatment rank probabilities may become distorted. Relaxing the homogeneous variance assumption may be equally problematic due to reduced precision. To regain good precision, moderately informative variance priors or additional mathematical assumptions may be necessary.MethodsIn this paper we describe four novel approaches to modeling heterogeneity variance - two novel model structures, and two approaches for use of moderately informative variance priors. We examine the relative performance of all approaches in two illustrative MTC data sets. We particularly compare between-study heterogeneity estimates and model fits, treatment effect estimates and 95% credible intervals, and treatment rank probabilities.ResultsIn both data sets, use of moderately informative variance priors constructed from the pair wise meta-analysis data yielded the best model fit and narrower credible intervals. Imposing consistency equations on variance estimates, assuming variances to be exchangeable, or using empirically informed variance priors also yielded good model fits and narrow credible intervals. The homogeneous variance model yielded high precision at all times, but overall inadequate estimates of between-trial variances. Lastly, treatment rankings were similar among the novel approaches, but considerably different when compared with the homogenous variance approach.ConclusionsMTC models using a homogenous variance structure appear to perform sub-optimally when between-trial variances vary between comparisons. Using informative variance priors, assuming exchangeability or imposing consistency between heterogeneity variances can all ensure sufficiently reliable and realistic heterogeneity estimation, and thus more reliable MTC inferences. All four approaches should be viable candidates for replacing or supplementing the conventional homogeneous variance MTC model, which is currently the most widely used in practice.",0
https://doi.org/10.1177/0013164407301544,Estimating the Standard Error of the Maximum Likelihood Ability Estimator in Adaptive Testing Using the Posterior-Weighted Test Information Function,"The standard error of the maximum likelihood ability estimator is commonly estimated by evaluating the test information function at an examinee's current maximum likelihood estimate (a point estimate) of ability. Because the test information function evaluated at the point estimate may differ from the test information function evaluated at an examinee's true ability value, the estimated standard error may be biased under certain conditions. This is of particular concern in adaptive testing because the height of the test information function is expected to be higher at the current estimate of ability than at the actual value of ability. This article proposes using the posterior-weighted test information function in computing the standard error of the maximum likelihood ability estimator for adaptive test sessions. A simulation study showed that the proposed approach provides standard error estimates that are less biased and more efficient than those provided by the traditional point estimate approach.",0
https://doi.org/10.1037/0022-006x.72.2.288,"Structural Ecosystems Therapy for HIV-Seropositive African American Women: Effects on Psychological Distress, Family Hassles, and Family Support.","This study tests the efficacy of Structural Ecosystems Therapy (SET), a family-ecological intervention, in improving psychosocial functioning when compared with an attention-comparison person-centered condition and a community control condition. A sample of 209 HIV-seropositive, urban, low-income, African American women was randomized into 1 of the 3 conditions. Results of growth curve analyses over 5 time points revealed that SET was more efficacious than either of the control conditions in reducing psychological distress and family-related hassles. However, contrary to hypotheses, SET was not more efficacious in increasing family support. Latent growth mixture modeling analyses indicated that SET was most efficacious for women who, on average, were at or near the clinical threshold for psychological distress and for women with high levels of family hassles. Implications for further intervention development are discussed.",0
https://doi.org/10.1002/(sici)1098-2272(1999)17:2<118::aid-gepi3>3.0.co;2-v,Genetic variance components analysis for binary phenotypes using generalized linear mixed models (GLMMs) and Gibbs sampling,"The common complex diseases such as asthma are an important focus of genetic research, and studies based on large numbers of simple pedigrees ascertained from population-based sampling frames are becoming commonplace. Many of the genetic and environmental factors causing these diseases are unknown and there is often a strong residual covariance between relatives even after all known determinants are taken into account. This must be modelled correctly whether scientific interest is focused on fixed effects, as in an association analysis, or on the covariances themselves. Analysis is straightforward for multivariate Normal phenotypes, but difficulties arise with other types of trait. Generalized linear mixed models (GLMMs) offer a potentially unifying approach to analysis for many classes of phenotype including multivariate Normal traits, binary traits, and censored survival times. Markov Chain Monte Carlo methods, including Gibbs sampling, provide a convenient framework within which such models may be fitted. In this paper, Bayesian inference Using Gibbs Sampling (a generic Gibbs sampler; BUGS) is used to fit GLMMs for multivariate Normal and binary phenotypes in nuclear families. BUGS is easy to use and readily available. We motivate a suitable model structure for Normal phenotypes and show how the model extends to binary traits. We discuss parameter interpretation and statistical inference and show how to circumvent a number of important theoretical and practical problems that we encountered. Using simulated data we show that model parameters seem consistent and appear unbiased in smaller data sets. We illustrate our methods using data from an ongoing cohort study.",0
https://doi.org/10.1056/nejm198010303031804,Influence of Adherence to Treatment and Response of Cholesterol on Mortality in the Coronary Drug Project,"The Coronary Drug Project was carried out to evaluate the efficacy and safety of several lipid-influencing drugs in the long-term treatment of coronary heart disease. The five-year mortality in 1103 men treated with clofibrate was 20.0 per cent, as compared with 20.9 per cent in 2789 men given placebo (P = 0.55). Good adherers to clofibrate, i.e., patients who took 80 per cent of more of the protocol prescription during the five-year follow-up period, had a substantially lower five-year mortality than did poor adherers to clofibrate (15.0 vs. 24.6 per cent; P = 0.00011). However, similar findings were noted in the placebo group, i.e., 15.1 per cent mortality for good adherers and 28.3 per cent for poor adherers (P = 4.7x10-16). These findings and various other analyses of mortality in the clofibrate and placebo groups of the project show the serious difficulty, if not impossibility, of evaluating treatment efficacy in subgroups determined by patient responses (e.g., adherence or cholesterol change) to the treatment protocol after randomization.",0
https://doi.org/10.1093/annonc/mdv238,Statistical controversies in clinical research: scientific and ethical problems with adaptive randomization in comparative clinical trials,"In recent years, various outcome adaptive randomization (AR) methods have been used to conduct comparative clinical trials. Rather than randomizing patients equally between treatments, outcome AR uses the accumulating data to unbalance the randomization probabilities in favor of the treatment arm that currently is superior empirically. This is motivated by the idea that, on average, more patients in the trial will be given the treatment that is truly superior, so AR is ethically more desirable than equal randomization. AR remains controversial, however, and some of its properties are not well understood by the clinical trials community.Computer simulation was used to evaluate properties of a 200-patient clinical trial conducted using one of four Bayesian AR methods and compare them to an equally randomized group sequential design.Outcome AR has several undesirable properties. These include a high probability of a sample size imbalance in the wrong direction, which might be surprising to nonstatisticians, wherein many more patients are assigned to the inferior treatment arm, the opposite of the intended effect. Compared with an equally randomized design, outcome AR produces less reliable final inferences, including a greatly overestimated actual treatment effect difference and smaller power to detect a treatment difference. This estimation bias becomes much larger if the prognosis of the accrued patients either improves or worsens systematically during the trial.AR produces inferential problems that decrease potential benefit to future patients, and may decrease benefit to patients enrolled in the trial. These problems should be weighed against its putative ethical benefit. For randomized comparative trials to obtain confirmatory comparisons, designs with fixed randomization probabilities and group sequential decision rules appear to be preferable to AR, scientifically, and ethically.",0
https://doi.org/10.1080/03610920701826427,On Mean Squared Prediction Error Estimation in Small Area Estimation Problems,"For a general linear mixed normal model, a new linearized weighted jackknife method is proposed to estimate the mean squared prediction error (MSPE) of an empirical best linear unbiased predictor (EBLUP) of a general mixed effect. Different MSPE estimators are compared using a Monte Carlo simulation study.",0
https://doi.org/10.1080/01621459.1981.10477731,Bayes Empirical Bayes,"Abstract A Bayesian approach is given for various kinds of empirical Bayes problems. In particular it is shown that empirical Bayes procedures are really non-Bayesian, asymptotically optimal, classical procedures for mixtures. In some situations these procedures are Bayes with respect to some prior and in other situations, there is no prior for which they are Bayes. Several examples of these concepts are given as well as a general theory showing the difference between an empirical Bayes model and a Bayes empirical Bayes model.",0
https://doi.org/10.1080/10705511.2014.935256,Moderated Mediation Analysis Using Bayesian Methods,"Conventionally, moderated mediation analysis is conducted through adding relevant interaction terms into a mediation model of interest. In this study, we illustrate how to conduct moderated mediation analysis by directly modeling the relation between the indirect effect components including a and b and the moderators, to permit easier specification and interpretation of moderated mediation. With this idea, we introduce a general moderated mediation model that can be used to model many different moderated mediation scenarios including the scenarios described in Preacher, Rucker, and Hayes (2007). Then we discuss how to estimate and test the conditional indirect effects and to test whether a mediation effect is moderated using Bayesian approaches. How to implement the estimation in both BUGS and Mplus is also discussed. Performance of Bayesian methods is evaluated and compared to that of frequentist methods including maximum likelihood (ML) with 1st-order and 2nd-order delta method standard errors and mL with bootstrap (percentile or bias-corrected confidence intervals) via a simulation study. The results show that Bayesian methods with diffuse (vague) priors implemented in both BUGS and Mplus yielded unbiased estimates, higher power than the ML methods with delta method standard errors, and the ML method with bootstrap percentile confidence intervals, and comparable power to the ML method with bootstrap bias-corrected confidence intervals. We also illustrate the application of these methods with the real data example used in Preacher et al. (2007). Advantages and limitations of applying Bayesian methods to moderated mediation analysis are also discussed.",0
https://doi.org/10.3758/bf03214307,Simple adaptive testing with the weighted up-down method,"This paper proposes a method for adaptive testing that is less complicated than the commonly used transformed up-down methods (1 up 2 down, 1 up 3 down, etc.). In addition, the weighted up-down method can converge to any desired point of the psychometric function. The rule is very simple: Each correct response leads to a decrease in signal level, each incorrect response to an increase. The only difference from the simple up-down method (1 up 1 down) is that the steps upward and the steps downward are of a different size. The straightforward construction of the novel procedure pays off in efficiency and stability: A Monte Carlo simulation reveals a definite advantage, though small, of the weighted up-down method over the 1-up-2-down rule.",0
https://doi.org/10.1002/sim.3478,"Fixed effects, random effects and GEE: What are the differences?","For analyses of longitudinal repeated-measures data, statistical methods include the random effects model, fixed effects model and the method of generalized estimating equations. We examine the assumptions that underlie these approaches to assessing covariate effects on the mean of a continuous, dichotomous or count outcome. Access to statistical software to implement these models has led to widespread application in numerous disciplines. However, careful consideration should be paid to their critical assumptions to ascertain which model might be appropriate in a given setting. To illustrate similarities and differences that might exist in empirical results, we use a study that assessed depressive symptoms in low-income pregnant women using a structured instrument with up to five assessments that spanned the pre-natal and post-natal periods. Understanding the conceptual differences between the methods is important in their proper application even though empirically they might not differ substantively. The choice of model in specific applications would depend on the relevant questions being addressed, which in turn informs the type of design and data collection that would be relevant. Copyright © 2008 John Wiley & Sons, Ltd.",0
https://doi.org/10.1207/s15328007sem0902_3,Reliability of Scales With General Structure: Point and Interval Estimation Using a Structural Equation Modeling Approach,"A method for obtaining point and interval estimates of reliability for composites of measures with a general structure is discussed. The approach is based on fitting a correspondingly constrained structural equation model and generalizes earlier covariance structure analysis methods for scale reliability estimation with congeneric tests. The procedure can be used with weighted or unweighted composites, in which the weights need not be known in advance but may be estimated simultaneously. The approach allows one also to obtain an approximate standard error and confidence interval for scale reliability using the bootstrap methodology.",0
https://doi.org/10.1016/j.jrp.2010.03.003,The General Factor of Personality: A meta-analysis of Big Five intercorrelations and a criterion-related validity study,"Recently, it has been proposed that a General Factor of Personality (GFP) occupies the top of the hierarchical personality structure. We present a meta-analysis (K = 212, total N = 144,117) on the intercorrelations among the Big Five personality factors (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) to test for the existence of a GFP. In addition, we report a multi-method validity study testing the relationship between the GFP and supervisor-rated job performance. The meta-analysis provided supporting evidence for the two meta-factors Stability and Plasticity (ora and b, respectively) and a GFP at the highest hierarchal level. The validity study indicated that the GFP has a substantive component as it is related to supervisor-rated job performance.",0
https://doi.org/10.1177/1740774511430714,An approach to combining parallel and cross-over trials with and without run-in periods using individual patient data,"Background In active run-in trials, where patients may be excluded after a run-in period based on their response to the treatment, it is implicitly assumed that patients have individual treatment effects. If individual patient data are available, active run-in trials can be modelled using patient-specific random effects. With more than one trial on the same medication available, one can obtain a more precise overall treatment effect estimate. Methods We present a model for joint analysis of a two-sequence, four-period cross-over trial (AABB/BBAA) and a three-sequence, two-period active run-in trial (AB/AA/A), where the aim is to investigate the effect of a new treatment for patients with pain due to osteoarthritis. Results Our approach enables us to separately estimate the direct treatment effect for all patients, for the patients excluded after the active run-in trial prior to randomisation, and for the patients who completed the active run-in trial. A similar model approach can be used to analyse other types of run-in trials, but this depends on the data and type of other trials available. Limitations We assume equality of the various carry-over effects over time. Conclusions The proposed approach is flexible and can be modified to handle other designs. Our results should be encouraging for those responsible for planning cost-efficient clinical development programmes.",0
https://doi.org/10.1080/01621459.1949.10483310,The Monte Carlo Method,"Abstract We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.",0
https://doi.org/10.1037/a0015857,Modeling life-span growth curves of cognition using longitudinal data with multiple samples and changing scales of measurement.,"The authors use multiple-sample longitudinal data from different test batteries to examine propositions about changes in constructs over the life span. The data come from 3 classic studies on intellectual abilities in which, in combination, 441 persons were repeatedly measured as many as 16 times over 70 years. They measured cognitive constructs of vocabulary and memory using 8 age-appropriate intelligence test batteries and explore possible linkage of these scales using item response theory (IRT). They simultaneously estimated the parameters of both IRT and latent curve models based on a joint model likelihood approach (i.e., NLMIXED and WINBUGS). They included group differences in the model to examine potential interindividual differences in levels and change. The resulting longitudinal invariant Rasch test analyses lead to a few new methodological suggestions for dealing with repeated constructs based on changing measurements in developmental studies.",0
https://doi.org/10.3389/fnint.2012.00024,Modulation of tactile duration judgments by emotional pictures,"Judging the duration of emotional stimuli is known to be influenced by their valence and arousal values. However, whether and how perceiving emotion in one modality affects time perception in another modality is still unclear. To investigate this, we compared the influence of different types of emotional pictures-a picture of threat, disgust, or a neutral picture presented at the start of a trial-on temporal bisection judgments of the duration of a subsequently presented vibrotactile stimulus. We found an overestimation of tactile duration following exposure to pictures of threat, but not pictures of disgust (even though these scored equally high on arousal), in a short-range temporal bisection task (range 300/900 ms). Follow-up experiments revealed that this duration lengthening effect was abolished when the range to be bisected was increased (1000/1900 ms). However, duration overestimation was maintained in the short-range bisection task regardless of whether the interval between the visual and tactile events was short or long. This pattern is inconsistent with a general arousal interpretation of duration distortion and suggests that crossmodal linkages in the processing of emotions and emotional regulation are two main factors underlying the manifestation of crossmodal duration modulation.",0
https://doi.org/10.1016/j.socscimed.2012.09.028,"An exploratory multilevel analysis of income, income inequality and self-rated health of the elderly in China","In the last three decades, China has experienced rapid economic development and growing economic inequality, such that economic disparities between rural and urban areas, as well as coastal and interior areas have deepened. Since the late 1990s China has also experienced an ageing population which has attracted attention to the wellbeing of the rapidly growing number of elderly. This research aims to characterise province differences in health and to explore the effects of individual income and economic disparity in the form of income inequality on health outcomes of the elderly. The study is based on the Chinese Longitudinal Healthy Longevity Survey data collected in 2008 for 23 provinces. Multilevel logistic models are employed to investigate the relationship between income, income inequality and self-rated health for the elderly using both individual and province-level variables. Results are presented as relative odds ratios, and for province differentials as Median Odds Ratios. The analysis is deliberately exploratory so as to find evidence of income effects if they exist and particular attention is placed on how province-level inequality (contemporaneous and lagged) may moderate individual relationships. The results show that the health of the elderly is not only affected by individual income (the odds of poor health are 3 times greater for the elderly with the lowest income compared to those at the upper quartile) but also by a small main effect for province-level income inequality (odds ratio of 1.019). There are significant cross-level interactions such that where inequality is high there are greater differences between those with and without formal education, and between men and women with the latter experiencing poorer health.",0
https://doi.org/10.1002/sim.6188,A design‐by‐treatment interaction model for network meta‐analysis with random inconsistency effects,"Network meta-analysis is becoming more popular as a way to analyse multiple treatments simultaneously and, in the right circumstances, rank treatments. A difficulty in practice is the possibility of ‘inconsistency’ or ‘incoherence’, where direct evidence and indirect evidence are not in agreement. Here, we develop a random-effects implementation of the recently proposed design-by-treatment interaction model, using these random effects to model inconsistency and estimate the parameters of primary interest. Our proposal is a generalisation of the model proposed by Lumley and allows trials with three or more arms to be included in the analysis. Our methods also facilitate the ranking of treatments under inconsistency. We derive R and I2 statistics to quantify the impact of the between-study heterogeneity and the inconsistency. We apply our model to two examples. © 2014 The Authors. Statistics in Medicine published by John Wiley & Sons, Ltd.",0
https://doi.org/10.3758/bf03206553,SPSS and SAS procedures for estimating indirect effects in simple mediation models,"Researchers often conduct mediation analysis in order to indirectly assess the effect of a proposed cause on some outcome through a proposed mediator. The utility of mediation analysis stems from its ability to go beyond the merely descriptive to a more functional understanding of the relationships among variables. A necessary component of mediation is a statistically and practically significant indirect effect. Although mediation hypotheses are frequently explored in psychological research, formal significance tests of indirect effects are rarely conducted. After a brief overview of mediation, we argue the importance of directly testing the significance of indirect effects and provide SPSS and SAS macros that facilitate estimation of the indirect effect with a normal theory approach and a bootstrap approach to obtaining confidence intervals, as well as the traditional approach advocated by Baron and Kenny (1986). We hope that this discussion and the macros will enhance the frequency of formal mediation tests in the psychology literature. Electronic copies of these macros may be downloaded from the Psychonomic Society’s Web archive atwww.psychonomic.org/archive/.",0
https://doi.org/10.1007/s10654-007-9145-y,A Bayesian multinomial model to analyse spatial patterns of childhood co-morbidity in Malawi,"Children in less developed countries die from relatively small number of infectious disease, some of which epidemiologically overlap. Using self-reported illness data from the 2000 Malawi Demographic and Health Survey, we applied a random effects multinomial model to assess risk factors of childhood co-morbidity of fever, diarrhoea and pneumonia, and quantify area-specific spatial effects. The spatial structure was modelled using the conditional autoregressive prior. Various models were fitted and compared using deviance information criterion. Inference was Bayesian and was based on Markov Chain Monte Carlo simulation techniques. We found spatial variation in childhood co-morbidity and determinants of each outcome category differed. Specifically, risk factors associated with child co-morbidity included age of the child, place of residence, undernutrition, bednet use and Vitamin A. Higher residual risk levels were identified in the central and southern-eastern regions, particularly for fever, diarrhoea and pneumonia; fever and pneumonia; and fever and diarrhoea combinations. This linkage between childhood health and geographical location warrants further research to assess local causes of these clusters. More generally, although each disease has its own mechanism, overlapping risk factors suggest that integrated disease control approach may be cost-effective and should be employed. Ã‚Â© 2007 Springer Science+Business Media B.V.",0
https://doi.org/10.1068/p6134,A Contextual Effect of 2nd-Order Configural Processing of Non-Face Objects by Non-Experts,"We investigated here the detection of 2nd-order configural relations both in faces and in non-face objects. In experiment 1 it was shown that observers were more sensitive to feature displacements in upright faces and houses than in inverted faces and houses. The presence of an inversion effect in the house stimuli suggested that 2nd-order relational processing was applied to the non-face stimuli. In experiment 2, the inversion effect for houses was absent when only houses were presented. In experiment 3, face and house stimuli were once again presented in the same task and inversion effects were again seen for both types of stimuli. Together, these results suggest that 2nd-order relational processing can be flexibly applied to non-face objects when they are presented in the context of faces.",0
https://doi.org/10.1167/11.10.1,The horizontal tuning of face perception relies on the processing of intermediate and high spatial frequencies,"It was recently shown that expert face perception relies on the extraction of horizontally oriented visual cues. Picture-plane inversion was found to eliminate horizontal, suggesting that this tuning contributes to the specificity of face processing. The present experiments sought to determine the spatial frequency (SF) scales supporting the horizontal tuning of face perception. Participants were instructed to match upright and inverted faces that were filtered both in the frequency and orientation domains. Faces in a pair contained horizontal or vertical ranges of information in low, middle, or high SF (LSF, MSF, or HSF). Our findings confirm that upright (but not inverted) face perception is tuned to horizontal orientation. Horizontal tuning was the most robust in the MSF range, next in the HSF range, and absent in the LSF range. Moreover, face inversion selectively disrupted the ability to process horizontal information in MSF and HSF ranges. This finding was replicated even when task difficulty was equated across orientation and SF at upright orientation. Our findings suggest that upright face perception is tuned to horizontally oriented face information carried by intermediate and high SF bands. They further indicate that inversion alters the sampling of face information both in the orientation and SF domains.",0
https://doi.org/10.3758/s13428-011-0181-x,Four applications of permutation methods to testing a single-mediator model,"Four applications of permutation tests to the single-mediator model are described and evaluated in this study. Permutation tests work by rearranging data in many possible ways in order to estimate the sampling distribution for the test statistic. The four applications to mediation evaluated here are the permutation test of ab, the permutation joint significance test, and the noniterative and iterative permutation confidence intervals for ab. A Monte Carlo simulation study was used to compare these four tests with the four best available tests for mediation found in previous research: the joint significance test, the distribution of the product test, and the percentile and bias-corrected bootstrap tests. We compared the different methods on Type I error, power, and confidence interval coverage. The noniterative permutation confidence interval for ab was the best performer among the new methods. It successfully controlled Type I error, had power nearly as good as the most powerful existing methods, and had better coverage than any existing method. The iterative permutation confidence interval for ab had lower power than do some existing methods, but it performed better than any other method in terms of coverage. The permutation confidence interval methods are recommended when estimating a confidence interval is a primary concern. SPSS and SAS macros that estimate these confidence intervals are provided.",0
https://doi.org/10.1080/104852502310001652629,Nonparametric Bayesian analysis of a proportion for a small area under nonignorable nonresponse,"In small area estimation, it is a standard practice to assume that the area effects are exchangeable. This is obtained by assuming that the area effects have a common parametric distribution, and a Bayesian approach is attractive. The Dirichlet process prior (DPP) has been used to provide a nonparametric version of this approach. The DPP is useful because it makes the procedure more robust, and the Bayesian approach helps to reduce the effect of nonidentifiability prominent in nonignorable nonresponse models. Using the DPP, we develop a Bayesian methodology for the analysis of nonignorable nonresponse binary data from many small areas, and for each area, we estimate the proportion of individuals with a particular characteristic. Our DPP model is centered on a baseline model, a standard parametric model. We use Markov chain Monte Carlo methods to fit the DPP model and the baseline model, and our methodology is illustrated using data on victimization in ten domains from the National Crime Survey. Our compar...",0
https://doi.org/10.1146/annurev.clinpsy.1.102803.144239,"Structural Equation Modeling: Strengths, Limitations, and Misconceptions","Because structural equation modeling (SEM) has become a very popular data-analytic technique, it is important for clinical scientists to have a balanced perception of its strengths and limitations. We review several strengths of SEM, with a particular focus on recent innovations (e.g., latent growth modeling, multilevel SEM models, and approaches for dealing with missing data and with violations of normality assumptions) that underscore how SEM has become a broad data-analytic framework with flexible and unique capabilities. We also consider several limitations of SEM and some misconceptions that it tends to elicit. Major themes emphasized are the problem of omitted variables, the importance of lower-order model components, potential limitations of models judged to be well fitting, the inaccuracy of some commonly used rules of thumb, and the importance of study design. Throughout, we offer recommendations for the conduct of SEM analyses and the reporting of results.",0
https://doi.org/10.1016/j.csda.2008.03.030,Structural equation modeling with near singular covariance matrices,"Conventional structural equation modeling involves fitting a structural model to the sample covariance matrix S. Due to collinearity or small samples with practical data, nonconvergences often occur in the estimation process. For a small constant a, this paper proposes to fit the structural model to the covariance matrix Sa=S+aI. When treating Sa as the sample covariance matrix in the maximum likelihood (ML) procedure, consistent parameter estimates are still obtained. The asymptotic distributions of the parameter estimates and the corresponding likelihood ratio statistic are studied and compared to those by the conventional ML. Two rescaled statistics for the overall model evaluation with modeling Sa are constructed. Empirical results imply that the estimates from modeling Sa are more efficient than those of fitting the structural model to S even when data are normally distributed. Simulations and real data examples indicate that modeling Sa allows us to evaluate the overall model structure even when S is literally singular. Implications of modeling Sa in a broader context are discussed.",0
https://doi.org/10.1177/0146167206287721,A Procedure for Evaluating Sensitivity to Within-Person Change: Can Mood Measures in Diary Studies Detect Change Reliably?,"The recent growth in diary and experience sampling research has increased research attention on how people change over time in natural settings. Often however, the measures in these studies were originally developed for studying between-person differences, and their sensitivity to within-person changes is usually unknown. Using a Generalizability Theory framework, the authors illustrate a procedure for developing reliable measures of change using a version of the Profile of Mood States (POMS; McNair, Lorr, &amp; Droppleman, 1992) shortened for diary studies. Analyzing two data sets, one composed of 35 daily reports from 68 persons experiencing a stressful examination and another composed of daily reports from 164 persons over a typical 28-day period, we demonstrate that three-item measures of anxious mood, depressed mood, anger, fatigue, and vigor have appropriate reliability to detect within-person change processes.",0
https://doi.org/10.1080/1743727x.2014.885012,A methodological review of statistical methods for handling multilevel non-nested longitudinal data in educational research,"As applications of multilevel modelling in educational research increase, researchers realize that multilevel data collected in many educational settings are often not purely nested. The most common multilevel non-nested data structure is one that involves student mobility in longitudinal studies. This article provides a methodological review of three statistical methods for handling student mobility in longitudinal studies: a multilevel approach, a cross-classified approach, and a cross-classified multiple membership approach. The strengths and weaknesses of each approach and the essential differences between the three approaches are discussed. The Early Childhood Longitudinal Study Kindergarten Cohort data are analysed to demonstrate the differences in parameter estimates and statistical inference between the three approaches. Potential applications of the three approaches in educational research and beyond and directions for further methodological investigations are discussed.",0
https://doi.org/10.3102/1076998611422912,Utilizing Response Time Distributions for Item Selection in CAT,"Traditional methods for item selection in computerized adaptive testing only focus on item information without taking into consideration the time required to answer an item. As a result, some examinees may receive a set of items that take a very long time to finish, and information is not accrued as efficiently as possible. The authors propose two item-selection criteria that utilize information from a lognormal model for response times. The first modifies the maximum information criterion to maximize information per time unit. The second is an inverse time-weighted version of a-stratification that takes advantage of the response time model, but achieves more balanced item exposure than the information-based techniques. Simulations are conducted to compare these procedures against their counterparts that ignore response times, and efficiency of estimation, time-required, and item exposure rates are assessed.",0
https://doi.org/10.1093/biostatistics/kxv036,Modeling agreement on categorical scales in the presence of random scorers,"Kappa coefficients are often used to assess agreement between two fixed scorers on categorical scales. Cohen's version is popular for nominal scales and the weighted version for ordinal scales. In the present paper, similar agreement coefficients are defined for random scorers. A partial-Bayesian methodology is then developed to directly relate these agreement coefficients to predictors through a multilevel model. Statistical properties of the proposed approach are studied using simulations. Finally, the approach is applied to gynecological and medical imaging data.",0
https://doi.org/10.1111/j.1467-9868.2004.00438.x,Likelihood ratio tests in linear mixed models with one variance component,Summary. We consider the problem of testing null hypotheses that include restrictions on the variance component in a linear mixed model with one variance component and we derive the finite sample and asymptotic distribution of the likelihood ratio test and the restricted likelihood ratio test. The spectral representations of the likelihood ratio test and the restricted likelihood ratio test statistics are used as the basis of efficient simulation algorithms of their null distributions. The large sample χ2 mixture approximations using the usual asymptotic theory for a null hypothesis on the boundary of the parameter space have been shown to be poor in simulation studies. Our asymptotic calculations explain these empirical results. The theory of Self and Liang applies only to linear mixed models for which the data vector can be partitioned into a large number of independent and identically distributed subvectors. One-way analysis of variance and penalized splines models illustrate the results.,0
https://doi.org/10.1214/06-ba117c,Comment on article by Browne and Draper,http://projecteuclid.org/euclid.ba/1340371050#ui-tabs-1,0
https://doi.org/10.1007/s11336-005-1297-7,Are unshifted distributional models appropriate for response time?,"Van Breukelen (this issue) provides an approach to using both response time (RT) and accuracy for (1) measuring latent abilities of participants even when they may trade speed for accuracy, and for (2) providing insight into the psychological processes underlying task performance. In this commentary, I focus on the second of these aims and assess how useful this approach is for exploring cognition. The approach is based on formulating statistical rather than substantive models on accuracy and RT. I focus here on the appropriateness of the RT component model. This RT model joins a family of recent statistical approaches to RT including those of Glickman, Gray, and Morales (in press); Peruggia, Van Zandt, and Chen (2002); Rouder, Lu, Speckman, Sun, and Jiang (in press); Schnipke and Scrams (1997); and Wenger and Gibson (2004). The advantage of these statistical rather than substantive models is robust estimation and sound inference when data are collected across disparate individuals and items. It is not hard to see how these statistical developments will lead to better substantive theory testing and development in cognitive psychology. Van Breukelen’s model for the ith participant’s RT to the j th item is given by:",0
https://doi.org/10.1167/15.15.6,Contextual effects in visual working memory reveal hierarchically structured memory representations,"Influential slot and resource models of visual working memory make the assumption that items are stored in memory as independent units, and that there are no interactions between them. Consequently, these models predict that the number of items to be remembered (the set size) is the primary determinant of working memory performance, and therefore these models quantify memory capacity in terms of the number and quality of individual items that can be stored. Here we demonstrate that there is substantial variance in display difficulty within a single set size, suggesting that limits based on the number of individual items alone cannot explain working memory storage. We asked hundreds of participants to remember the same sets of displays, and discovered that participants were highly consistent in terms of which items and displays were hardest or easiest to remember. Although a simple grouping or chunking strategy could not explain this individual-display variability, a model with multiple, interacting levels of representation could explain some of the display-by-display differences. Specifically, a model that includes a hierarchical representation of items plus the mean and variance of sets of the colors on the display successfully accounts for some of the variability across displays. We conclude that working memory representations are composed only in part of individual, independent object representations, and that a major factor in how many items are remembered on a particular display is interitem representations such as perceptual grouping, ensemble, and texture representations.",0
https://doi.org/10.1016/j.cogpsych.2009.07.001,Uncovering mental representations with Markov chain Monte Carlo,"A key challenge for cognitive psychology is the investigation of mental representations, such as object categories, subjective probabilities, choice utilities, and memory traces. In many cases, these representations can be expressed as a non-negative function defined over a set of objects. We present a behavioral method for estimating these functions. Our approach uses people as components of a Markov chain Monte Carlo (MCMC) algorithm, a sophisticated sampling method originally developed in statistical physics. Experiments 1 and 2 verified the MCMC method by training participants on various category structures and then recovering those structures. Experiment 3 demonstrated that the MCMC method can be used estimate the structures of the real-world animal shape categories of giraffes, horses, dogs, and cats. Experiment 4 combined the MCMC method with multidimensional scaling to demonstrate how different accounts of the structure of categories, such as prototype and exemplar models, can be tested, producing samples from the categories of apples, oranges, and grapes.",0
https://doi.org/10.1016/j.neuroscience.2010.02.011,Possible involvement of toll-like receptor 4/myeloid differentiation factor-2 activity of opioid inactive isomers causes spinal proinflammation and related behavioral consequences,"Opioid-induced glial activation and its proinflammatory consequences have been associated with both reduced acute opioid analgesia and the enhanced development of tolerance, hyperalgesia and allodynia following chronic opioid administration. Intriguingly, recent evidence demonstrates that these effects can result independently from the activation of classical, stereoselective opioid receptors. Here, a structurally disparate range of opioids cause activation of signaling by the innate immune receptor toll like receptor 4 (TLR4), resulting in proinflammatory glial activation. In the present series of studies, we demonstrate that the (+)-isomers of methadone and morphine, which bind with negligible affinity to classical opioid receptors, induced upregulation of proinflammatory cytokine and chemokine production in rat isolated dorsal spinal cord. Chronic intrathecal (+)-methadone produced hyperalgesia and allodynia, which were associated with significantly increased spinal glial activation (TLR4 mRNA and protein) and the expression of multiple chemokines and cytokines. Statistical analysis suggests that a cluster of cytokines and chemokines may contribute to these nociceptive behavioral changes. Acute intrathecal (+)-methadone and (+)-morphine were also found to induce microglial, interleukin-1 and TLR4/myeloid differentiation factor-2 (MD-2) dependent enhancement of pain responsivity. In silico docking analysis demonstrated (+)-naloxone sensitive docking of (+)-methadone and (+)-morphine to human MD-2. Collectively, these data provide the first evidence of the pro-nociceptive consequences of small molecule xenobiotic activation of spinal TLR4 signaling independent of classical opioid receptor involvement.",0
https://doi.org/10.2307/2533558,Small Sample Inference for Fixed Effects from Restricted Maximum Likelihood,"Restricted maximum likelihood (REML) is now well established as a method for estimating the parameters of the general Gaussian linear model with a structured covariance matrix, in particular for mixed linear models. Conventionally, estimates of precision and inference for fixed effects are based on their asymptotic distribution, which is known to be inadequate for some small-sample problems. In this paper, we present a scaled Wald statistic, together with an F approximation to its sampling distribution, that is shown to perform well in a range of small sample settings. The statistic uses an adjusted estimator of the covariance matrix that has reduced small sample bias. This approach has the advantage that it reproduces both the statistics and F distributions in those settings where the latter is exact, namely for Hotelling T2 type statistics and for analysis of variance F-ratios. The performance of the modified statistics is assessed through simulation studies of four different REML analyses and the methods are illustrated using three examples.",0
https://doi.org/10.1177/0049124103260187,Estimating Causal Effects With Matching Methods in the Presence and Absence of Bias Cancellation,"This article explores the implications of bias cancellation on the estimate of average treatment effects using ordinary least squares (OLS) and Rubin-style matching methods. Bias cancellation (offsetting biases at high and low propensities for treatment in estimates of treatment effects that are uncorrected for nonrandom selection) has been observed when job training is the treatment variable and earnings is the outcome variable. Contrary to published assertions in the literature, bias cancellation is not explainable in terms of the standard selection model, which assumes a symmetric distribution for the errors in the structural and assignment equations. A substantive rationale for bias cancellation is offered, which conceptualizes bias cancellation as the result of a mixture process based on two distinct individual-level decision-making models. While the general properties are unknown, the existence of bias cancellation appears to reduce the average bias in both OLS and matching methods relative to the symmetric distribution case.",0
https://doi.org/10.1051/vetres/2009013,Use of posterior predictive assessments to evaluate model fit in multilevel logistic regression,"Assessing the fit of a model is an important final step in any statistical analysis, but this is not straightforward when complex discrete response models are used. Cross validation and posterior predictions have been suggested as methods to aid model criticism. In this paper a comparison is made between four methods of model predictive assessment in the context of a three level logistic regression model for clinical mastitis in dairy cattle; cross validation, a prediction using the full posterior predictive distribution and two “mixed” predictive methods that incorporate higher level random effects simulated from the underlying model distribution. Cross validation is considered a gold standard method but is computationally intensive and thus a comparison is made between posterior predictive assessments and cross validation. The analyses revealed that mixed prediction methods produced results close to cross validation whilst the full posterior predictive assessment gave predictions that were over-optimistic (closer to the observed disease rates) compared with cross validation. A mixed prediction method that simulated random effects from both higher levels was best at identifying the outlying level two (farm-year) units of interest. It is concluded that this mixed prediction method, simulating random effects from both higher levels, is straightforward and may be of value in model criticism of multilevel logistic regression, a technique commonly used for animal health data with a hierarchical structure.",0
https://doi.org/10.1348/000711010x497442,Ridge structural equation modelling with correlation matrices for ordinal and continuous data,"This paper develops a ridge procedure for structural equation modelling (SEM) with ordinal and continuous data by modelling the polychoric/polyserial/product-moment correlation matrix R. Rather than directly fitting R, the procedure fits a structural model to R(a) =R+aI by minimizing the normal distribution-based discrepancy function, where a > 0. Statistical properties of the parameter estimates are obtained. Four statistics for overall model evaluation are proposed. Empirical results indicate that the ridge procedure for SEM with ordinal data has better convergence rate, smaller bias, smaller mean square error, and better overall model evaluation than the widely used maximum likelihood procedure.",0
https://doi.org/10.1080/03610920903145154,A Hybrid Approximation Bayesian Test of Variance Components for Longitudinal Data,"The test of variance components of possibly correlated random effects in generalized linear mixed models (GLMMs) can be used to examine if there exists heterogeneous effects. The Bayesian test with Bayes factors offers a flexible method. In this article, we focus on the performance of Bayesian tests under three reference priors and a conjugate prior: an approximate uniform shrinkage prior, modified approximate Jeffreys' prior, half-normal unit information prior and Wishart prior. To compute Bayes factors, we propose a hybrid approximation approach combining a simulated version of Laplace's method and importance sampling techniques to test the variance components in GLMMs.",0
https://doi.org/10.1016/s0165-0270(00)00173-4,Assessing spatial vision — automated measurement of the contrast-sensitivity function in the hooded rat,"The contrast-sensitivity function (CSF) provides a concise and thorough description of an organism’s spatial vision; it is widely used to describe vision in animals and humans, to track developmental changes in vision, and to compare vision among different species. Despite the predominance of rats in neuroscience research, their vision is not thoroughly studied due to the complexity of psychophysical measurement and a generally held notion that rat vision is poor. We therefore designed an economical and rapid method to assess the hooded rat’s CSF, using a computer monitor to display stimuli and an infrared touch screen to record responses. A six-alternative forced-choice task presented trials in which a sine-wave grating (S+), varying in spatial frequency and contrast, was displayed at different locations along with five gray stimuli (S−). Nose pokes to the S+ but not the S− produced water reinforcers. Contrasts were tested at each spatial frequency with a simple adaptive procedure until stimulus detection fell below chance. Psychometric functions were obtained by maximum-likelihood fitting of a logistic function to the raw data, obtaining the threshold as the function’s point of inflection. As in previous studies with rats, CSFs showed an inverse-U shape with peak sensitivity at 0.12 cyc/deg and acuity just under 1 cyc/deg. The results indicate the present computer-controlled behavioral testing device is a precise and efficient instrument to assess spatial visual function in rats.",0
https://doi.org/10.1037/fam0000058,Getting the most out of family data with the R package fSRM.,"Family research aims to explore family processes, but is often limited to the examination of unidirectional processes. As the behavior of 1 person has consequences that go beyond that individual, family functioning should be investigated in its full complexity. The social relations model (SRM; Kenny & La Voie, 1984) is a conceptual and analytical model that can disentangle family data from a round-robin design at 3 different levels: the individual level (actor and partner effects), the dyadic level (relationship effects), and the family level (family effect). Its statistical complexity may however be a hurdle for family researchers. The user-friendly R package fSRM performs almost automatically those rather complex SRM analyses and introduces new possibilities for assessing differences between SRM means and between SRM variances, both within and between groups of families. Using family data on negative processes, different type of research questions are formulated and corresponding analyses with fSRM are presented.",0
https://doi.org/10.1038/nn.3655,Changing concepts of working memory,"Working memory is widely considered to be limited in capacity, holding a fixed, small number of items, such as Miller's 'magical number' seven or Cowan's four. It has recently been proposed that working memory might better be conceptualized as a limited resource that is distributed flexibly among all items to be maintained in memory. According to this view, the quality rather than the quantity of working memory representations determines performance. Here we consider behavioral and emerging neural evidence for this proposal.",0
https://doi.org/10.1037/0022-006x.73.5.924,Empirically supported treatments or type I errors? Problems with the analysis of data from group-administered treatments.,"When treatments are administered in groups, clients interact in ways that lead to violations of a key assumption of most statistical analyses-the assumption of independence of observations. The resulting dependencies, when not properly accounted for, can increase Type I errors dramatically. Of the 33 studies of group-administered treatment on the empirically supported treatments list, none appropriately analyzed their data. The current authors provide corrections that can be applied to improper analyses. After the corrections, only 12.4% to 68.2% of tests that were originally reported as significant remained significant, depending on what assumptions were made about how large the dependencies among observations really are. Of the 33 studies, 6-19 studies no longer had any significant results after correction. The authors end by providing recommendations for researchers planning group-administered treatment research.",0
https://doi.org/10.1093/biomet/81.4.633,The ECME algorithm: A simple extension of EM and ECM with faster monotone convergence,"A generalisation of the ECM algorithm (Meng & Rubin, 1993), which is itselfan extension of the EM algorithm (Dempster, Laird & Rubin, 1977), can be obtained by replacing some CM-steps of ECM, which maximise the constrained expected complete-data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. This algorithm, which we call ECME algorithm, for Expectation /Conditional Maximisation Either, shares with both EM and ECM their stable monotone convergence and basic simplicity of implementation relative to competing faster converging methods. Moreover, ECME can have a substantially faster convergence rate than either EM or ECM, measured using either the number of iterations or actual computer time",0
https://doi.org/10.3102/1076998609359788,MCMC Sampling for a Multilevel Model With Nonindependent Residuals Within and Between Cluster Units,"In this article, we discuss the effect of removing the independence assumptions between the residuals in two-level random effect models. We first consider removing the independence between the Level 2 residuals and instead assume that the vector of all residuals at the cluster level follows a general multivariate normal distribution. We demonstrate how this assumption can allow us to fit higher levels of clustering and school competition effects via an example from education. We then consider removing the assumption of independence between Level 1 residuals within clusters. We show how this extension can allow time series type models. Both normal and binary responses are considered.",0
https://doi.org/10.1023/a:1017529427433,Joint Bayesian Analysis of Factor Scores and Structural Parameters in the Factor Analysis Model,"A Bayesian approach is developed to assess the factor analysis model. Joint Bayesian estimates of the factor scores and the structural parameters in the covariance structure are obtained simultaneously. The basic idea is to treat the latent factor scores as missing data and augment them with the observed data in generating a sequence of random observations from the posterior distributions by the Gibbs sampler. Then, the Bayesian estimates are taken as the sample means of these random observations. Expressions for implementing the algorithm are derived and some statistical properties of the estimates are presented. Some aspects of the algorithm are illustrated by a real example and the performance of the Bayesian procedure is studied using simulation.",0
https://doi.org/10.1007/978-3-319-19977-1_17,A General SEM Framework for Integrating Moderation and Mediation: The Constrained Approach,"Modeling the combination of latent moderating and mediating effects is a significant issue in the social and behavioral sciences. Chen and Cheng (Structural Equation Modeling: A Multidisciplinary Journal 21: 94-101, 2014) generalized JÃ¶reskog and Yangâ€™s (Advanced structural equation modeling: Issues and techniques (pp. 57-88). Mahwah, NJ: Lawrence Erlbaum, 1996) constrained approach to allow for the concurrent modeling of moderation and mediation within the context of SEM. Unfortunately, due to restrictions related to Chen and Chengâ€™s partitioning scheme, their framework cannot completely conceptualize and interpret moderation of indirect effects in a mediated model. In the current study, the Chen and Cheng (abbreviated below as C & C) framework is extended to accommodate situations in which any two pathways that constitute a particular indirect effect in a mediated model can be differentially or collectively moderated by the moderator variable(s). By preserving the inherent advantage of the C & C framework, i.e., the matrix partitioning technique, while at the same time further generalizing its applicability, it is expected that the current framework enhances the potential usefulness of the constrained approach as well as the entire class of the product indicator approaches. Â© Springer International Publishing Switzerland 2015.",0
https://doi.org/10.1037/h0021740,Inferred components of reaction times as functions of foreperiod duration.,"A distribution function representing simple reaction-time distributions was derived, assuming RT is the sum of 2 component variables with exponential and normal distributions. 4 Ss each gave 100 RTs to an auditory stimulus following each of 4 foreperiods, under each of 2 conditions: (a) foreperiod constant within sessions but varied over sessions, and (b) foreperiods appearing in a random sequence. The derived distribution function provided completely satisfactory representations of all 32 RT distributions, and the relations of the fitted parameters of this function to foreperiod suggest that the variation of RT as a function of foreperiod is due to variation in the normally distributed component. (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1965 American Psychological Association.",0
https://doi.org/10.1177/1740774509339977,"Effects of sources of variability on sample sizes required for RCTs, applied to trials of lipid-altering therapies on carotid artery intima-media thickness","Objective Studies measuring progression of carotid artery intima-media thickness (cIMT) have been used to estimate the effect of lipid-modifying therapies cardiovascular event risk. The likelihood that future cIMT clinical trials will detect a true treatment effect is estimated by leveraging results from prior studies. The present analyses assess the impact of between- and within-study variability based on currently published data from prior clinical studies on the likelihood that ongoing or future cIMT trials will detect the true treatment effect of lipid-modifying therapies. Methods Published data from six contemporary cIMT studies (ASAP, ARBITER 2, RADIANCE 1, RADIANCE 2, ENHANCE, and METEOR) including data from a total of 3563 patients were examined. Bayesian and frequentist methods were used to assess the impact of between study variability on the likelihood of detecting true treatment effects on 1-year cIMT progression/regression and to provide a sample size estimate that would specifically compensate for the effect of between-study variability. Results In addition to the well-described within-study variability, there is considerable between-study variability associated with the measurement of annualized change in cIMT. Accounting for the additional between-study variability decreases the power for existing study designs. In order to account for the added between-study variability, it is likely that future cIMT studies would require a large increase in sample size in order to provide substantial probability (≥90%) to have 90% power of detecting a true treatment effect. Limitation Analyses are based on study level data. Future meta-analyses incorporating patient-level data would be useful for confirmation. Conclusion Due to substantial within- and between-study variability in the measure of 1-year change of cIMT, as well as uncertainty about progression rates in contemporary populations, future study designs evaluating the effect of new lipid-modifying therapies on atherosclerotic disease progression are likely to be challenged by large sample sizes in order to demonstrate a true treatment effect.",0
https://doi.org/10.1037/a0030642,Bayesian methods for the analysis of small sample multilevel data with a complex variance structure.,"Inferences from multilevel models can be complicated in small samples or complex data structures. When using (restricted) maximum likelihood methods to estimate multilevel models, standard errors and degrees of freedom often need to be adjusted to ensure that inferences for fixed effects are correct. These adjustments do not address problems in estimating variance/covariance components. An alternative to the adjusted likelihood method is to use Bayesian methods, which can produce accurate inferences about fixed effects and variance/covariance parameters. In this article, the authors contrast the benefits and limitations of likelihood and Bayesian methods in the estimation of multilevel models. The issues are discussed in the context of a partially clustered intervention study, a common intervention design that has been shown to require an adjusted likelihood analysis. The authors report a Monte Carlo study that compares the performance of an adjusted restricted maximum likelihood (REML) analysis to a Bayesian analysis. The results suggest that for fixed effects, the models perform equally well with respect to bias, efficiency, and coverage of interval estimates. Bayesian models with a carefully selected gamma prior for the variance components were more biased but also more efficient with respect to estimation of the variance components than the REML model. However, the results also show that the inferences about the variance components in partially clustered studies are sensitive to the prior distribution when sample sizes are small. Finally, the authors compare the results of a Bayesian and adjusted likelihood model using data from a partially clustered intervention trial.",1
https://doi.org/10.1016/j.jmp.2012.02.005,A tutorial on approximate Bayesian computation,"Abstract This tutorial explains the foundation of approximate Bayesian computation (ABC), an approach to Bayesian inference that does not require the specification of a likelihood function, and hence that can be used to estimate posterior distributions of parameters for simulation-based models. We discuss briefly the philosophy of Bayesian inference and then present several algorithms for ABC. We then apply these algorithms in a number of examples. For most of these examples, the posterior distributions are known, and so we can compare the estimated posteriors derived from ABC to the true posteriors and verify that the algorithms recover the true posteriors accurately. We also consider a popular simulation-based model of recognition memory (REM) for which the true posteriors are unknown. We conclude with a number of recommendations for applying ABC methods to solve real-world problems.",0
https://doi.org/10.1177/0146621613513494,General Test Overlap Control,"This article proposed a new online test overlap control algorithm that is an improvement of Chen’s algorithm in controlling general test overlap rate for item pooling among a group of examinees. Chen’s algorithm is not very efficient in that not only item pooling between current examinee and prior examinees is controlled for but also item pooling between previous examinees, which would have been controlled for when they were current examinees. The proposed improvement increases efficiency by only considering item pooling between current and previous examinees, and its improved performance over Chen is demonstrated in a simulated computerized adaptive testing (CAT) environment. Moreover, the proposed algorithm is adapted for computerized classification testing (CCT) using the sequential probability ratio test procedure and is evaluated against some existing exposure control procedures. The proposed algorithm appears to work best in controlling general test overlap rate among the exposure control procedures examined without sacrificing much classification precision, though longer tests might be required for more stringent control of item pooling among larger groups. Given the capability of the proposed algorithm in controlling item pooling among a group of examinees of any size and its ease of implementation, it appears to be a good test overlap control method.",0
https://doi.org/10.1037/a0031609,Mixture class recovery in GMM under varying degrees of class separation: Frequentist versus Bayesian estimation.,"Growth mixture modeling (GMM) represents a technique that is designed to capture change over time for unobserved subgroups (or latent classes) that exhibit qualitatively different patterns of growth. The aim of the current article was to explore the impact of latent class separation (i.e., how similar growth trajectories are across latent classes) on GMM performance. Several estimation conditions were compared: maximum likelihood via the expectation maximization (EM) algorithm and the Bayesian framework implementing diffuse priors, ""accurate"" informative priors, weakly informative priors, data-driven informative priors, priors reflecting partial-knowledge of parameters, and ""inaccurate"" (but informative) priors. The main goal was to provide insight about the optimal estimation condition under different degrees of latent class separation for GMM. Results indicated that optimal parameter recovery was obtained though the Bayesian approach using ""accurate"" informative priors, and partial-knowledge priors showed promise for the recovery of the growth trajectory parameters. Maximum likelihood and the remaining Bayesian estimation conditions yielded poor parameter recovery for the latent class proportions and the growth trajectories.",1
https://doi.org/10.1080/01621459.1969.10501064,Mean Square Efficiency of Estimators of Variance Components,Abstract Various estimators of components of variance for the balanced one way layout are compared using mean squared error as a measure of performance. Several modifications of the maximum likelihood estimator and several formal Bayes estimators are compared and mean square error relationships are given. Inadmissibility is established for a large class of translation and scale invariant estimators. Numerical results are tabled and graphed to exhibit the relative efficiency of the estimators considered.,0
https://doi.org/10.1080/00273171.2011.625310,Conceptualizing and Estimating Process Speed in Studies Employing Ecological Momentary Assessment Designs: A Multilevel Variance Decomposition Approach,"Researchers have been making use of ecological momentary assessment (EMA) and other study designs that sample feelings and behaviors in real time and in naturalistic settings to study temporal dynamics and contextual factors of a wide variety of psychological, physiological, and behavioral processes. As EMA designs become more widespread, questions are arising about the frequency of data sampling, with direct implications for participants' burden and researchers' ability to capture and study dynamic processes. Traditionally, spectral analytic techniques are used for time series data to identify process speed. However, the nature of EMA data, often collected with fewer than 100 measurements per person, sampled at randomly spaced intervals, and replete with planned and unplanned missingness, precludes application of traditional spectral analytic techniques. Building on principles of variance partitioning used in the generalizability theory of measurement and spectral analysis, we illustrate the utility of multilevel variance decompositions for isolating process speed in EMA-type data. Simulation and empirical data from a smoking-cessation study are used to demonstrate the method and to evaluate the process speed of smoking urges and quitting self-efficacy. Results of the multilevel variance decomposition approach can inform process-oriented theory and future EMA study designs.",0
https://doi.org/10.1037/a0035147,Linear and nonlinear growth models: Describing a Bayesian perspective.,"Conventional estimation of longitudinal growth models can produce inaccurate parameter estimates under certain research scenarios (e.g., smaller sample sizes and nonlinear growth patterns) and thus lead to potentially misleading interpretations of results (i.e., interpreting growth patterns that do not reflect the population patterns). The current article used patterns of change in cigarette and alcohol abuse prevalence and depression levels to demonstrate an alternative method for estimating growth models more accurately under these conditions, namely, via the Bayesian estimation framework. This article acts as an introduction and tutorial for implementing Bayesian methods when examining growth or change over time, particularly nonlinear growth.The National Longitudinal Survey of Youth 1997 database was used to highlight different linear and nonlinear (quadratic and logistic) growth models via growth curve modeling (GCM) and growth mixture modeling (GMM). The specific focus was on changes in cigarette/alcohol consumption and depression throughout adolescence and young adulthood. Specifically, a nationally representative group of individuals between the ages of 12 and 16 years were assessed at 4 time-points for levels of cigarette consumption, alcohol use, and depression.The results for each example illustrated different patterns of linear and nonlinear growth via GCM and GMM through the versatile Bayesian estimation framework.Growth models may benefit from the Bayesian perspective by incorporating prior information or knowledge into the model, especially when sample sizes are small or growth is nonlinear. A step-by-step tutorial for assessing various growth models via the Bayesian perspective is provided as online supplemental material.",0
https://doi.org/10.1080/01621459.1987.10478458,The Calculation of Posterior Distributions by Data Augmentation,"Abstract The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea ...",0
https://doi.org/10.1111/j.2044-8317.1994.tb01039.x,"Factor analysis of variables with 2, 3, 5 and 7 response categories: A comparison of categorical variable estimators using simulated data","Two estimators in the factor analysis of categorical items are studied, the weighted least squares function implemented in the tandem PRELIS-LISREL 7 and a generalized least squares function implemented in LISCOMP. Of main interest is the performance of these estimators in relatively small samples (200 to 400) and the comparison of their performance with the normal theory maximum likelihood estimator given an increasing number of response categories. The evaluation of the performance of these estimators concerns the variability of the parameter estimates, the bias of the parameter estimates, the distribution of the parameter estimates and the χ2 goodness-of-fit statistics. The model used in the simulation is an 8-indicator single common factor model. The effect of model size (12- and 16-indicator models) on the categorical item estimator of LISREL 7 is investigated briefly.    The results indicate that in the ideal circumstances of the simulation study, 200 is too small a sample size to justify the use of large sample statistics associated with these estimators.",0
https://doi.org/10.1080/1359432x.2015.1028378,The perceived value of team players: a longitudinal study of how group identification affects status in work groups,"Theory and research on status attainment in work groups primarily focuses on members’ abilities and characteristics that make them appear competent as predictors of their status in the group. We complement the abilities perspective with a social identity perspective by arguing that another important determinant of a member’s status is based on the extent to which the member serves the group’s interests. Specifically, we assert that a member’s identification with the group affects performance on behalf of the group, which in turn affects other members’ assessment of the member’s status. We test this social identity perspective on status attainment by studying the influence of members’ group identification on their performance and status in the group, while controlling for the members’ abilities and status characteristics. In a three-wave longitudinal field study following 33 work groups during a six-month group project, we find that members’ identification enhances their performance on behalf of the group,...",0
https://doi.org/10.1080/10705511.2015.1057284,Monte Carlo Confidence Intervals for Complex Functions of Indirect Effects,"One challenge in mediation analysis is to generate a confidence interval (CI) with high coverage and power that maintains a nominal significance level for any well-defined function of indirect and direct effects in the general context of structural equation modeling (SEM). This study discusses a proposed Monte Carlo extension that finds the CIs for any well-defined function of the coefficients of SEM such as the product of k coefficients and the ratio of the contrasts of indirect effects, using the Monte Carlo method. Finally, we conduct a small-scale simulation study to compare CIs produced by the Monte Carlo, nonparametric bootstrap, and asymptotic-delta methods. Based on our simulation study, we recommend researchers use the Monte Carlo method to test a complex function of indirect effects.",0
https://doi.org/10.1109/soli.2008.4682982,Judge neural networks for multi-channel retailing competition,"With the development of online commerce, its becoming more and more important to deal with multi-channel problem, especially when there is an e-channel. In this paper, we establish neural networks to describe customers' decision process. Based on the neural networks, a new price setting game is proposed, which can simulate the competition in multichannel. Since the networks can reflect the real situation well, in simulation experiments, the game and neural networks have shown their ability of describing the multi-channel competition. This provides a powerful tool for retail companies' decision making.",0
https://doi.org/10.1111/biom.12278,Generalized multilevel function-on-scalar regression and principal component analysis,"This manuscript considers regression models for generalized, multilevel functional responses: functions are generalized in that they follow an exponential family distribution and multilevel in that they are clustered within groups or subjects. This data structure is increasingly common across scientific domains and is exemplified by our motivating example, in which binary curves indicating physical activity or inactivity are observed for nearly 600 subjects over 5 days. We use a generalized linear model to incorporate scalar covariates into the mean structure, and decompose subject-specific and subject-day-specific deviations using multilevel functional principal components analysis. Thus, functional fixed effects are estimated while accounting for within-function and within-subject correlations, and major directions of variability within and between subjects are identified. Fixed effect coefficient functions and principal component basis functions are estimated using penalized splines; model parameters are estimated in a Bayesian framework using Stan, a programming language that implements a Hamiltonian Monte Carlo sampler. Simulations designed to mimic the application have good estimation and inferential properties with reasonable computation times for moderate datasets, in both cross-sectional and multilevel scenarios; code is publicly available. In the application we identify effects of age and BMI on the time-specific change in probability of being active over a 24-hour period; in addition, the principal components analysis identifies the patterns of activity that distinguish subjects and days within subjects.",0
https://doi.org/10.1080/00949658408810713,On maximum likelihood and restricted maximum likelihood approaches to estimation of variance components,"Two variations of the dispersion-mean correspondence model for varince components lead to the ML and REML equations. This formulation provides for addition of a nonnegativity constraint to the computational method of T. W. Anderson (1971, 1973), an iterative procedure for obtaining ML and REML estimates, which not only assures that estimates will be within the parameter space, but contributes to the stability of the algorithm as well. Estimates of the variance components obtained in this way (which includes MINQUE) are simulated for four balanced and two unbalanced random effects designs. The effect of the nonnegativity constraint on mean squared error is compared to the alternative of replacing negative values of unconstrained estimates to zero, for both the REML and ML approaches. For the two unbalanced designs, the effect of initial starting values and the significance of iterating are also assessed. The simulation results are an extension of those reported by Rich and Brown (1979) for the REML approac...",0
https://doi.org/10.1001/archgenpsychiatry.2011.132,Trajectories of Depression Severity in Clinical Trials of Duloxetine,"The high percentage of failed clinical trials in depression may be due to high placebo response rates and the failure of standard statistical approaches to capture heterogeneity in treatment response.To assess whether growth mixture modeling can provide insights into antidepressant and placebo responses in clinical trials of patients with major depression.We reanalyzed clinical trials of duloxetine to identify distinct trajectories of Hamilton Scale for Depression (HAM-D) scores during treatment. We analyzed the trajectories in the entire sample and then separately in all active arms and in all placebo arms. Effects of duloxetine hydrochloride, selective serotonin reuptake inhibitor (SSRI), and covariates on the probability of following a particular trajectory were assessed. Outcomes in different trajectories were compared using mixed-effects models.Seven randomized double-blind clinical trials of duloxetine vs placebo and comparator SSRI. Patients A total of 2515 patients with major depression.Duloxetine and comparator SSRI. Main Outcome Measure Total score on the HAM-D.In the entire sample and in the antidepressant-treated subsample, we identified trajectories of responders (76.3% of the sample) and nonresponders (23.7% of the sample). However, placebo-treated patients were characterized by a single response trajectory. Duloxetine and SSRI did not differ in efficacy, and compared with placebo they significantly decreased the odds of following the nonresponder trajectory. Antidepressant responders had significantly better HAM-D scores over time than placebo-treated patients, but antidepressant nonresponders had significantly worse HAM-D scores over time than the placebo-treated patients.Most patients treated with serotonergic antidepressants showed a clinical trajectory over time that is superior to that of placebo-treated patients. However, some patients receiving these medications did more poorly than patients receiving placebo. These data highlight the importance of ongoing monitoring of medication risks and benefits during serotonergic antidepressant treatment. They should further stimulate the search for biomarkers or other predictors of responder status in guiding antidepressant treatment.",0
https://doi.org/10.1093/ijpor/edv039,Informal Political Conversation in Old and New Democracies in a Comparative Perspective,,0
https://doi.org/10.1093/jssam/smu003,Design Effects for a Regression Slope in a Cluster Sample,,0
https://doi.org/10.1002/sim.3858,Bayesian bivariate meta-analysis of diagnostic test studies using integrated nested Laplace approximations,"For bivariate meta-analysis of diagnostic studies, likelihood approaches are very popular. However, they often run into numerical problems with possible non-convergence. In addition, the construction of confidence intervals is controversial. Bayesian methods based on Markov chain Monte Carlo (MCMC) sampling could be used, but are often difficult to implement, and require long running times and diagnostic convergence checks. Recently, a new Bayesian deterministic inference approach for latent Gaussian models using integrated nested Laplace approximations (INLA) has been proposed. With this approach MCMC sampling becomes redundant as the posterior marginal distributions are directly and accurately approximated. By means of a real data set we investigate the influence of the prior information provided and compare the results obtained by INLA, MCMC, and the maximum likelihood procedure SAS PROC NLMIXED. Using a simulation study we further extend the comparison of INLA and SAS PROC NLMIXED by assessing their performance in terms of bias, mean-squared error, coverage probability, and convergence rate. The results indicate that INLA is more stable and gives generally better coverage probabilities for the pooled estimates and less biased estimates of variance parameters. The user-friendliness of INLA is demonstrated by documented R-code.",0
https://doi.org/10.1002/sim.6471,Summarising and validating test accuracy results across multiple studies for use in clinical practice,"Following a meta-analysis of test accuracy studies, the translation of summary results into clinical practice is potentially problematic. The sensitivity, specificity and positive (PPV) and negative (NPV) predictive values of a test may differ substantially from the average meta-analysis findings, because of heterogeneity. Clinicians thus need more guidance: given the meta-analysis, is a test likely to be useful in new populations, and if so, how should test results inform the probability of existing disease (for a diagnostic test) or future adverse outcome (for a prognostic test)? We propose ways to address this. Firstly, following a meta-analysis, we suggest deriving prediction intervals and probability statements about the potential accuracy of a test in a new population. Secondly, we suggest strategies on how clinicians should derive post-test probabilities (PPV and NPV) in a new population based on existing meta-analysis results and propose a cross-validation approach for examining and comparing their calibration performance. Application is made to two clinical examples. In the first example, the joint probability that both sensitivity and specificity will be >80% in a new population is just 0.19, because of a low sensitivity. However, the summary PPV of 0.97 is high and calibrates well in new populations, with a probability of 0.78 that the true PPV will be at least 0.95. In the second example, post-test probabilities calibrate better when tailored to the prevalence in the new population, with cross-validation revealing a probability of 0.97 that the observed NPV will be within 10% of the predicted NPV.",0
https://doi.org/10.1002/(sici)1097-0258(19981130)17:22<2537::aid-sim953>3.0.co;2-c,Meta-analysis of multiple outcomes by regression with random effects,"Earlier work showed how to perform fixed-effects meta-analysis of studies or trials when each provides results on more than one outcome per patient and these multiple outcomes are correlated. That fixed-effects generalized-least-squares approach analyzes the multiple outcomes jointly within a single model, and it can include covariates, such as duration of therapy or quality of trial, that may explain observed heterogeneity of results among the trials. Sometimes the covariates explain all the heterogeneity, and the fixed-effects regression model is appropriate. However, unexplained heterogeneity may often remain, even after taking into account known or suspected covariates. Because fixed-effects models do not make allowance for this remaining unexplained heterogeneity, the potential exists for bias in estimated coefficients, standard errors and p-values. We propose two random-effects approaches for the regression meta-analysis of multiple correlated outcomes. We compare their use with fixed-effects models and with separate-outcomes models in a meta-analysis of periodontal clinical trials. A simulation study shows the advantages of the random-effects approach. These methods also facilitate meta-analysis of trials that compare more than two treatments.",0
https://doi.org/10.1177/1073191113509004,Cross-Validation Study Using Item Response Theory,"The Health-Related Quality of Life for Eating Disorder–Short questionnaire is one of the most suitable existing instruments for measuring quality of life in patients with eating disorders. The objective of the study was to evaluate its reliability, validity, and responsiveness in a cohort of 377 patients. A comprehensive validation process was performed, including confirmatory factor analysis and a graded response model, and assessments of reliability and responsiveness at 1 year of follow-up. The confirmatory factor analysis confirmed the two second-order latent traits, social maladjustment, and mental health and functionality. The graded response model results showed that all items were good for discriminating their respective latent traits. Cronbach’s alpha coefficients were high, and responsiveness parameters showed moderate changes. In conclusion, this short questionnaire has good psychometric properties. Its simplicity and ease of application further enhance its acceptability and usefulness in clinical research and trials, as well as in routine practice.",0
https://doi.org/10.1016/j.jmp.2012.02.001,A tutorial on the Bayesian approach for analyzing structural equation models,"Abstract In this paper, we provide a tutorial exposition on the Bayesian approach in analyzing structural equation models (SEMs). SEMs, which can be regarded as regression models with observed and latent variables, have been widely applied to substantive research. However, the classical methods and most commercial software in this area are based on the covariance structure approach, which would encounter serious difficulties when dealing with complicated models and/or data structures. In contrast, the Bayesian approach has much more flexibility in handling complex situations. We give a brief introduction to SEMs and a detailed description of how to apply the Bayesian approach to this kind of model. Advantages of the Bayesian approach are discussed, and results obtained from a simulation study are provided for illustration. The intended audience is statisticians/methodologists who either know about SEMs or simple Bayesian statistics, and Ph.D. students in statistics, psychometrics, or mathematical psychology.",0
https://doi.org/10.1111/1467-937x.00044,Matching As An Econometric Evaluation Estimator,"This paper develops the method of matching as an econometric evaluation estimator. A rigorous distribution theory for kernel-based matching is presented. The method of matching is extended to more general conditions than the ones assumed in the statistical literature on the topic. We focus on the method of propensity score matching and show that it is not necessarily better, in the sense of reducing the variance of the resulting estimator, to use the propensity score method even if propensity score is known. We extend the statistical literature on the propensity score by considering the case when it is estimated both parametrically and nonparametrically. We examine the benefits of separability and exclusion restrictions in improving the efficiency of the estimator. Our methods also apply to the econometric selection bias estimator. Matching is a widely-used method of evaluation. It is based on the intuitively attractive idea of contrasting the outcomes of programme participants (denoted Y1) with the outcomes of comparable nonparticipants (denoted Y0). Differences in the outcomes between the two groups are attributed to the programme. Let 1 and 11 denote the set of indices for nonparticipants and participants, respectively. The following framework describes conventional matching methods as well as the smoothed versions of these methods analysed in this paper. To estimate a treatment effect for each treated person iecI, outcome Yli is compared to an average of the outcomes Yoj for matched persons je10 in the untreated sample. Matches are constructed on the basis of observed characteristics X in Rd. Typically, when the observed characteristics of an untreated person are closer to those of the treated person ieI1, using a specific distance measure, the untreated person gets a higher weight in constructing the match. The estimated gain for each person i in the treated sample is",0
https://doi.org/10.1080/10705511003659375,Small Sample Statistics for Incomplete Nonnormal Data: Extensions of Complete Data Formulae and a Monte Carlo Comparison,"Incomplete nonnormal data are common occurrences in applied research. Although these 2 problems are often dealt with separately by methodologists, they often cooccur. Very little has been written about statistics appropriate for evaluating models with such data. This article extends several existing statistics for complete nonnormal data to incomplete data and evaluates their performance via a Monte Carlo study. The focus is on statistics that also perform well in small samples. The following statistics are defined and studied: corrected residual-based statistic, residual-based F statistic, scaled chi-square, adjusted chi-square, Bartlett-corrected scaled chi-square, and Swain-corrected scaled chi-square. Both Type I error rates and power are studied with missing completely at random nonnnormally distributed data and varying degrees of nonnormality. Sample size, model size, and number of variables containing missingness are also varied. For power comparisons, both minor and major model misspecifications a...",0
https://doi.org/10.1111/bmsp.12020,Semi-parametric proportional hazards models with crossed random effects for psychometric response times,"The semi-parametric proportional hazards model with crossed random effects has two important characteristics: it avoids explicit specification of the response time distribution by using semi-parametric models, and it captures heterogeneity that is due to subjects and items. The proposed model has a proportionality parameter for the speed of each test taker, for the time intensity of each item, and for subject or item characteristics of interest. It is shown how all these parameters can be estimated by Markov chain Monte Carlo methods (Gibbs sampling). The performance of the estimation procedure is assessed with simulations and the model is further illustrated with the analysis of response times from a visual recognition task.",0
https://doi.org/10.2307/3316112,A prior for the variance in hierarchical models,"The choice of prior distributions for the variances can be important and quite difficult in Bayesian hierarchical and variance component models. For situations where little prior information is available, a ‘nonin-formative’ type prior is usually chosen. ‘Noninformative’ priors have been discussed by many authors and used in many contexts. However, care must be taken using these prior distributions as many are improper and thus, can lead to improper posterior distributions. Additionally, in small samples, these priors can be ‘informative’. In this paper, we investigate a proper ‘vague’ prior, the uniform shrinkage prior (Strawder-man 1971; Christiansen & Morris 1997). We discuss its properties and show how posterior distributions for common hierarchical models using this prior lead to proper posterior distributions. We also illustrate the attractive frequentist properties of this prior for a normal hierarchical model including testing and estimation. To conclude, we generalize this prior to the multivariate situation of a covariance matrix.    Le choix d'une loi a priori pour les variances peut s'averer a la fois difficile et important dans le cadre d'une analyse bayesienne hierarchique ou d'un modele des composantes de la variance. En l'absence totale ou quasi-totale d'information a priori, l'emploi d'une loi ‘non informative’ est de mise. Plusieurs lois de ce type ont ete proposees dans differents contextes, mais leur utilisation est delicate, puisque certaines d'entre elles sont impropres et peuvent conduire a des lois a posteriori non integrables. Dans de petits echantillons, ces lois peuvent aussi se reveler ‘informatives’. Cet article est consacre a l'etude d'une loi a priori a la fois vague et integrable, la loi a priori a retrecissement uniforme (Strawderman 1971; Christiansen & Morris, 1997). Certaines de ses proprietes sont evoquees, notamment le fait que les lois a posteriori auxquelles elle conduit dans certains modeles hierarchiques classiques sont bel et bien integrables. Ses proprietes frequentistes sont egalement mises en valeur dans des situations d'estimation et de test au sein du modele hierarchique gaussien. On montre en outre comment elle peut ětre generalisee au cas multivarie d'une matrice de covariance.",0
,Florida State University Libraries,,0
https://doi.org/10.7275/qkqa-9k50,Comparing Propensity Score Methods in Balancing Covariates and Recovering Impact in Small Sample Educational Program Evaluations.,"Propensity score applications are often used to evaluate educational program impact. However, various options are available to estimate both propensity scores and construct comparison groups. This study used a student achievement dataset with commonly available covariates to compare different propensity scoring estimation methods (logistic regression, boosted regression, and Bayesian logistic regression) in combination with different methods for constructing comparison groups (nearest-neighbor matching, optimal matching, weighting) relative to balancing pre-existing differences and recovering a simulated treatment effect in small samples. Results indicated that applied researchers evaluating program impact should first consider use of standard logistic regression methods with nearest-neighbor or optimal matching or boosted regression in combination with propensity score weighting. Advantages and disadvantages of the methods are discussed.",0
https://doi.org/10.1007/bf02291262,Fitting a response model forn dichotomously scored items,"A method of estimating the parameters of the normal ogive model for dichotomously scored item-responses by maximum likelihood is demonstrated. Although the procedure requires numerical integration in order to evaluate the likelihood equations, a computer implemented Newton-Raphson solution is shown to be straightforward in other respects. Empirical tests of the procedure show that the resulting estimates are very similar to those based on a conventional analysis of item ""difficulties"" and first factor loadings obtained from the matrix of tetrachoric correlation coefficients. Problems of testing the fit of the model, and of obtaining invariant parameters are discussed. Â© 1970 Psychometric Society.",0
https://doi.org/10.1080/13803611.2012.702991,Educational choice in secondary school in Flanders: the relative impact of occupational interests on option choice,"The present study aims at unravelling the myriad of student-level (i.e., gender, socioeconomic status [SES], academic self-concept, achievement, ability, and occupational interests) and school-level (i.e., gender composition, maths composition, and SES composition) determinants of option choice in the academic track of secondary school in Flanders. We focused on 2 decisional thresholds in Flemish secondary education, namely, the transition from Grade 8 to Grade 9 (N = 2518) and from Grade 10 to Grade 11 (N = 2871). Data were analyzed through multinomial multilevel analysis. Our results strongly confirm Lent's (2005) jigsaw puzzle metaphor in that different factors go into a complex and dynamic interplay. Especially in the first grades, prior achievement is a major predictor of option choice in secondary education, whereas in the last years occupational interests become increasingly important. From a gender perspective, boys rather choose math/sciences-oriented options than girls. Option choice is mainly d...",0
https://doi.org/10.3402/ejpt.v6.27882,Mobile mental health: a challenging research agenda,"The field of mobile health (""m-Health"") is evolving rapidly and there is an explosive growth of psychological tools on the market. Exciting high-tech developments may identify symptoms, help individuals manage their own mental health, encourage help seeking, and provide both preventive and therapeutic interventions. This development has the potential to be an efficient cost-effective approach reducing waiting lists and serving a considerable portion of people globally (""g-Health""). However, few of the mobile applications (apps) have been rigorously evaluated. There is little information on how valid screening and assessment tools are, which of the mobile intervention apps are effective, or how well mobile apps compare to face-to-face treatments. But how feasible is rigorous scientific evaluation with the rising demands from policy makers, business partners, and users for their quick release? In this paper, developments in m-Health tools-targeting screening, assessment, prevention, and treatment-are reviewed with examples from the field of trauma and posttraumatic stress disorder. The academic challenges in developing and evaluating m-Health tools are being addressed. Evidence-based guidance is needed on appropriate research designs that may overcome some of the public and ethical challenges (e.g., equity, availability) and the market-driven wish to have mobile apps in the ""App Store"" yesterday rather than tomorrow.",0
,Large sample estimation and hypothesis testing,"Asymptotic distribution theory is the primary method used to examine the properties of econometric estimators and tests. We present conditions for obtaining cosistency and asymptotic normality of a very general class of estimators (extremum estimators). Consistent asymptotic variance estimators are given to enable approximation of the asymptotic distribution. Asymptotic efficiency is another desirable property then considered. Throughout the chapter, the general results are also specialized to common econometric estimators (e.g. MLE and GMM), and in specific examples we work through the conditions for the various results in detail. The results are also extended to two-step estimators (with finite-dimensional parameter estimation in the first step), estimators derived from nonsmooth objective functions, and semiparametric two-step estimators (with nonparametric estimation of an infinite-dimensional parameter in the first step). Finally, the trinity of test statistics is considered within the quite general setting of GMM estimation, and numerous examples are given.",0
https://doi.org/10.1080/01621459.1988.10478561,An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data,"Abstract Knowledge of the area under different crops is important to the U.S. Department of Agriculture. Sample surveys have been designed to estimate crop areas for large regions, such as crop-reporting districts, individual states, and the United States as a whole. Predicting crop areas for small areas such as counties has generally not been attempted, due to a lack of available data from farm surveys for these areas. The use of satellite data in association with farm-level survey observations has been the subject of considerable research in recent years. This article considers (a) data for 12 Iowa counties, obtained from the 1978 June Enumerative Survey of the U.S. Department of Agriculture and (b) data obtained from land observatory satellites (LANDSAT) during the 1978 growing season. Emphasis is given to predicting the area under corn and soybeans in these counties. A linear regression model is specified for the relationship between the reported hectares of corn and soybeans within sample segments in...",0
,Key advances in the history of structural equation modeling.,,0
https://doi.org/10.1214/aos/1176344845,Estimation of the Inverse Covariance Matrix: Random Mixtures of the Inverse Wishart Matrix and the Identity,"Let $S_{p \times p}$ have a nonsingular Wishart distribution with unknown matrix $\Sigma$ and $k$ degrees of freedom. For two different loss functions, estimators of $\Sigma^{-1}$ are given which dominate the obvious estimators $aS^{-1}, 0 < a \leqslant k - p - 1$. Our class of estimators $\mathscr{C}$ includes random mixtures of $S^{-1}$ and $I$. A subclass $\mathscr{C}_0 \subset \mathscr{C}$ was given by Haff. Here, we show that any member of $\mathscr{C}_0$ is dominated in $\mathscr{C}$. Some troublesome aspects of the estimation problem are discussed, and the theory is supplemented by simulation results.",0
https://doi.org/10.1016/b978-0-12-742780-5.50022-6,Reliability and Validity of Adaptive Ability Tests in a Military Setting,"Publisher Summary This chapter presents a research to discuss the reliability and validity of adaptive ability tests in a military setting. It discusses a research with the purpose to report the results of the first in a series of live-testing studies investigating the psychometric characteristics of computer-administered adaptive testing in comparison with a conventional test. A pilot study based on a small sample of Marine recruits was followed by a larger study based on the same research design. The two studies were designed in part to address two psychometric research questions: (1) whether a computer-administered adaptive test is more reliable than a conventional test, holding test length constant and (2) whether a computer-administered test is more valid than a comparable conventional test with test length constant. These questions were motivated by the results of previous research. The question of the advantages of adaptive tests over conventional ones in terms of reliability has a clear and positive theoretical answer: Holding test length and all else constant, a good adaptive test is superior, provided that highly discriminating test items are available. The general method used in both studies was that of equivalent tests administered to independent examinee groups. One group took two equivalent computer-administered adaptive tests. The other group took two equivalent conventional tests, also administered by computer. To control for item quality, both test types were made up of items from the same source—a common pool of 150 verbal ability items, which had previously been calibrated using item response theory methods in large samples of Marine recruits.",0
,An empirical comparison of methods to measure willingness to pay by examining the hypothetical bias,"In the literature, several methods to measure willingness to pay (WTP) have been proposed. However, there is still little knowledge about their reliability. We empirically test the appropriateness of two methods - open-ended contingent valuation (CV) and limit conjoint analysis (LCA) - for measuring willingness to pay, by examining their hypothetical bias. Significant differences of the WTP values were found between the two methods. Comparing the respective hypothetical biases, LCA performs better than CV, yielding non-significant differences in different purchase situations. Nevertheless, our results may depend on the product category analysed. As a consequence, further studies are needed to determine which factors influence the appropriateness of methods for measuring WTP. Ã‚Â© 2005 The Market Research Society.",0
https://doi.org/10.2307/2983404,An Assessment of Estimation Procedures for Multilevel Models with Binary Responses,"We evaluate two software packages that are available for fitting multilevel models to binary response data, namely VARCL and ML3, by using a Monte Carlo study designed to represent quite closely the actual structure of a data set used in an analysis of health care utilization in Guatemala. We find that the estimates of fixed effects and variance components produced by the software packages are subject to very substantial downward bias when the random effects are sufficiently large to be interesting. In fact, the fixed effect estimates are no better than the estimates obtained by using standard logit models that ignore the hierarchical structure of the data. The estimates of standard errors appear to be reasonably accurate and superior to those obtained by ignoring clustering, although one might question their utility in the presence of large biases. We conclude that alternative estimation procedures need to be developed and implemented for the binary response case",0
https://doi.org/10.3102/1076998606298039,Bayesian Methods for Scalable Multivariate Value-Added Assessment,There is increased interest in value-added models relying on longitudinal student-level test score data to isolate teachers’ contributions to student achievement. The complex linkage of students to...,0
https://doi.org/10.1002/wics.1218,A Bayesian model for inference on population proportions,"In recent times and even up to now, traffic congestion and parking difficulties, especially during morning and evening rush hours, have become a major concern to members of the University of Lagos (UNILAG) community and visitors alike. UNILAG has witnessed unprecedented growth in student enrollment during the past 10 years or so culminating in the current total enrollment of more than 35,000 students, of which about 25,000 are undergraduates. In order to study the worrisome traffic situation at UNILAG in the wake of these large numbers, independent, although similar, sample surveys of undergraduate students of the eight faculties on the main campus of UNILAG were conducted in 2007. The purpose of the surveys was to collect data on undergraduate students who owned or used motor vehicles on campus. Furthermore, to investigate possible temporal trends, the surveys were repeated in 2009. The types of data obtained from the surveys provided veritable impetus for the application of empirical Bayes (EB) analysis to estimate the proportions of students of individual faculties who used motor vehicles on campus roads during the periods under reference. The EB technique, being a Bayesian method, combines prior information and sample information in a manner that ‘shrinks’ an EB estimator toward the sample estimator if a vague prior (proper prior with a large variance) is used. The main result is that in 2007 about one in four students used motor vehicles, and this result held almost across the eight faculties. Although results of the 2009 surveys were generally similar there were faculties that recorded some reduction in the estimated proportions of students who used motor vehicles. WIREs Comput Stat 2012 doi: 10.1002/wics.1218",0
https://doi.org/10.1007/s11336-003-1108-y,Multilevel Model Prediction,"Multilevel models are proven tools in social research for modeling complex, hierarchical systems. In multilevel modeling, statistical inference is based largely on quantification of random variables. This paper distinguishes among three types of random variables in multilevel modeling - model disturbances, random coefficients, and future response outcomes - and provides a unified procedure for predicting them. These predictors are best linear unbiased and are commonly known via the acronym BLUP; they are optimal in the sense of minimizing mean square error and are Bayesian under a diffuse prior. For parameter estimation purposes, a multilevel model can be written as a linear mixed-effects model. In this way, parameters of the many equations can be estimated simultaneously and hence efficiently. For prediction purposes, we show that it is more convenient to retain the multiple equation feature of multilevel models. In this way, the efficient BLUPs are easy to compute and retain their intuitively appealing recursive form. We also derive explicit equations for standard errors of these different types of predictors. Prediction in multilevel modeling is important in a wide range of applications. To demonstrate the applicability of our results, this paper discusses prediction in the context of a study of school effectiveness. Â© 2006 The Psychometric Society.",0
https://doi.org/10.1007/bf02294492,A generalized rasch model for manifest predictors,"A logistic regression model is suggested for estimating the relation between a set of manifest predictors and a latent trait assumed to be measured by a set of k dichotomous items. Usually the estimated subject parameters of latent trait models are biased, especially for short tests. Therefore, the relation between a latent trait and a set of predictors should not be estimated with a regression model in which the estimated subject parameters are used as a dependent variable. Direct estimation of the relation between the latent trait and one or more independent variables is suggested instead. Estimation methods and test statistics for the Rasch model are discussed and the model is illustrated with simulated and empirical data. Â© 1991 The Psychometric Society.",0
https://doi.org/10.1016/j.ejphar.2008.07.053,Crotalphine induces potent antinociception in neuropathic pain by acting at peripheral opioid receptors,"Neuropathic pain is an important clinical problem and it is usually resistant to the current therapy. We have recently characterized a novel analgesic peptide, crotalphine, from the venom of the South American rattlesnake Crotalus durissus terrificus. In the present work, the antinociceptive effect of crotalphine was evaluated in an experimental model of neuropathic pain induced in rats by chronic constriction of sciatic nerve. The effect of the peptide was compared to that induced by the crude venom, which confirmed that crotalphine is responsible for the antinociceptive effect of the crotalid venom on neuropathic pain. For characterization of neuropathic pain, the presence of hyperalgesia, allodynia and spontaneous pain was assessed at different times after nerve constriction. These phenomena were detected 24 h after surgery and persisted at least for 14 days. The pharmacological treatments were performed on day 14 after surgery. Crotalphine (0.2-5 microg/kg) and the crude venom (400-1600 microg/kg) administered p.o. inhibited hyperalgesia, allodynia and spontaneous pain induced by nerve constriction. The antinociceptive effect of the peptide and crude venom was long lasting, since it was detected up to 3 days after treatment. Intraplantar injection of naloxone (1 microg/paw) blocked the antinociceptive effect, indicating the involvement of opioid receptors in this phenomenon. Gabapentin (200 mg/kg, p.o.), and morphine (5 mg/kg, s.c.), used as positive controls, blocked hyperalgesia and partially inhibited allodynia induced by nerve constriction. These data indicate that crotalphine induces a potent and long lasting opioid antinociceptive effect in neuropathic pain that surpasses that observed with standard analgesic drugs.",0
https://doi.org/10.1586/14737167.2015.1011131,A Bayesian sensitivity study of risk difference in the meta-analysis of binary outcomes from sparse data,"In most cases, including those of discrete random variables, statistical meta-analysis is carried out using the normal random effect model. The authors argue that normal approximation does not always properly reflect the underlying uncertainty of the original discrete data. Furthermore, in the presence of rare events the results from this approximation can be very poor. This review proposes a Bayesian meta-analysis to address binary outcomes from sparse data and also introduces a simple way to examine the sensitivity of the quantities of interest in the meta-analysis with respect to the structure dependence selected. The findings suggest that for binary outcomes data it is possible to develop a Bayesian procedure, which can be directly applied to sparse data without ad hoc corrections. By choosing a suitable class of linking distributions, the authors found that a Bayesian robustness study can be easily implemented. For illustrative purposes, an example with real data is analyzed using the proposed Bayesian meta-analysis for binomial sparse data.",0
https://doi.org/10.1093/biomet/70.1.41,The central role of the propensity score in observational studies for causal effects,"Abstract : The results of observational studies are often disputed because of nonrandom treatment assignment. For example, patients at greater risk may be overrepresented in some treatment group. This paper discusses the central role of propensity scores and balancing scores in the analysis of observational studies. The propensity score is the (estimated) conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: matched sampling on the univariate propensity score which is equal percent bias reducing under more general conditions than required for discriminant matching, multivariate adjustment by subclassification on balancing scores where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and visual representation of multivariate adjustment by a two-dimensional plot. (Author)",0
https://doi.org/10.1177/096228029600500202,An introduction to finite mixture distributions,"Finite mixture densities can be used to model data from populations known or suspected to contain a number of separate subpopulations. Most commonly used are mixture densities with Gaussian (univariate or multivariate) components, but mixtures with other types of component are also increas ingly used to model, for example, survival times. This paper gives a general introduction to the topic which should help when considering the other more specialized papers in this issue.",0
https://doi.org/10.1111/jopy.12146,Extraversion and Agreeableness: Divergent Routes to Daily Satisfaction With Social Relationships,"We examined the unique effects of extraversion and agreeableness (and honesty-humility) on everyday satisfaction with family, friends, romantic life, and acquaintances, and explored potential mediators of these effects. Three diary studies (Ns = 206, 139, 185) were conducted on Singaporean university students. In Studies 1 and 2, participants rated their satisfaction with different relationship categories. In Study 3, participants rated their satisfaction and social interactions with 10 target individuals each day for a 1-week period. Both extraversion and agreeableness predicted relationship satisfaction. However, the effect of extraversion was mediated by greater levels of trust in others, whereas the effect of agreeableness was mediated by less frequent negative exchanges (e.g., criticism, perceived anger, and perceived neglect). The effect of honesty-humility on negative exchanges was similar to agreeableness. When both were entered as predictors, only the effect of honesty-humility was significant. We discuss how the processes by which personality affect relationship satisfaction vary depending on the trait as well as the particular measure that is used (IPIP NEO PI-R, California Q-Set, and IPIP-HEXACO).",0
https://doi.org/10.1207/s15327906mbr3101_6,Latent Variable Regression: A Technique for Estimating Interaction and Quadratic Coefficients,"The article proposes a technique to estimate regression coefficients for interaction and quadratic latent variables that combines regression analysis with the measurement model portion of structural equation analysis (e.g., analysis involving CALLS, EQS or LISREL). The measurement model provides parameter estimates that can be combined to correct the variance-covariance matrix used in regression, as Heise (1986) and others recommend. The proposed technique will provide coefficient estimates for regression models involving existing measures, or new measures for which a priori error estimates are not available.",0
https://doi.org/10.1198/jbes.2010.07136,Robust Inference With Multiway Clustering,"In this article we propose a variance estimator for the OLS estimator as well as for nonlinear estimators such as logit, probit, and GMM. This variance estimator enables cluster-robust inference when there is two-way or multiway clustering that is nonnested. The variance estimator extends the standard cluster-robust variance estimator or sandwich estimator for one-way clustering (e.g., Liang and Zeger 1986; Arellano 1987) and relies on similar relatively weak distributional assumptions. Our method is easily implemented in statistical packages, such as Stata and SAS, that already offer cluster-robust standard errors when there is one-way clustering. The method is demonstrated by a Monte Carlo analysis for a two-way random effects model; a Monte Carlo analysis of a placebo law that extends the state–year effects example of Bertrand, Duflo, and Mullainathan (2004) to two dimensions; and by application to studies in the empirical literature where two-way clustering is present.",0
https://doi.org/10.1002/dev.20262,On the implications of the classical ergodic theorems: Analysis of developmental processes has to focus on intra-individual variation,"It is argued that general mathematical-statistical theorems imply that standard statistical analysis techniques of inter-individual variation are invalid to investigate developmental processes. Developmental processes have to be analyzed at the level of individual subjects, using time series data characterizing the patterns of intra-individual variation. It is shown that standard statistical techniques based on the analysis of inter-individual variation appear to be insensitive to the presence of arbitrary large degrees of inter-individual heterogeneity in the population. An important class of nonlinear epigenetic models of neural growth is described which can explain the occurrence of such heterogeneity in brain structures and behavior. Links with models of developmental instability are discussed. A simulation study based on a chaotic growth model illustrates the invalidity of standard analysis of inter-individual variation, whereas time series analysis of intra-individual variation is able to recover the true state of affairs.",0
https://doi.org/10.1509/jmkr.40.2.235.19225,A Comparison of Segment Retention Criteria for Finite Mixture Logit Models,"Despite the widespread application of finite mixture models in marketing research, the decision of how many segments to retain in the models is an important unresolved issue. Almost all applications of the models in marketing rely on segment retention criteria such as Akaike's information criterion, Bayesian information criterion, consistent Akaike's information criterion, and information complexity to determine the number of latent segments to retain. Because these applications employ real-world data in which the true number of segments is unknown, it is not clear whether these criteria are effective. Retaining the true number of segments is crucial because many product design and marketing decisions depend on it. The purpose of this extensive simulation study is to determine how well commonly used segment retention criteria perform in the context of simulated multinomial choice data, as obtained from supermarket scanner panels, in which the true number of segments is known. The authors find that an Akaike's information criterion with a penalty factor of three rather than the traditional value of two has the highest segment retention success rate across nearly all experimental conditions. Currently, this criterion is rarely, if ever, applied in the marketing literature. Experimental factors of particular interest in marketing contexts, such as the number of choices per household, the number of choice alternatives, the error variance of the choices, and the minimum segment size, have not been considered in the statistics literature. The authors show that they, among other factors, affect the performance of segment retention criteria.",0
https://doi.org/10.1037/0033-2909.108.3.551,Truth and consequences of ordinal differences in statistical distributions: Toward a theory of hierarchical inference.,"A theory is presented that establishes a dominance hierarchy of potential distinctions (order relations) between two distributions. It is proposed that it is worthwhile for researchers to ascertain the strongest possible distinction, because all weaker distinctions are logically implied. Implications of the theory for hypothesis testing, theory construction, and scales of measurement are considered. Open problems for future research are outlined.",0
https://doi.org/10.1177/01466216980223001,Optimal Assembly of Psychological and Educational Tests,"The advent of computers in psychological and educational measurement has led to the need for algorithms for optimal assembly of tests from item banks. This paper reviews optimal test assembly literature and introduces the contributions to this Special Issue. Four approaches to computerized test assembly are discussed: heuristic-based test assembly, 0-1 linear programming, network-flow programming, and an optimal design approach. Ap-plications of these methods to a variety of problems are examined, including IRT-based test assembly, classical test assembly, assembling multiple test forms, item matching, observed-score equating, constrained adaptive testing, assembling tests with item sets, item bank design, and assembling tests with multiple traits. A bibliography on optimal test assembly is provided.",0
https://doi.org/10.1207/s15327906mbr3901_2,Comparison of Approaches in Estimating Interaction and Quadratic Effects of Latent Variables,"Various approaches using the maximum likelihood (ML) option of the LISREL program and products of indicators have been proposed to analyze structural equation models with non-linear latent effects on the basis of Kenny and Judd's formulation. Recently, some methods based on the Bayesian approach and the exact ML approaches have been developed. This article reviews, elaborates and compares several approaches for analyzing nonlinear models with interaction and/or quadratic effects. A total of four approaches are examined, including the product indicator ML approaches proposed by Jaccard and Wan (1995) and Joreskog and Yang (1996), a Bayesian approach and an exact ML approach. The empirical performances of these approaches are assessed using simulation studies in terms of their capabilities in producing reliable parameter and standard error estimates. It is found that whilst the Bayesian and the exact ML approaches produce satisfactory results in all the settings under consideration, and are in general very reliable; the product indicator ML approaches can only produce reasonable results in simple models with large sample sizes.",0
https://doi.org/10.1073/pnas.1117465109,Variability in encoding precision accounts for visual short-term memory limitations,"It is commonly believed that visual short-term memory (VSTM) consists of a fixed number of ""slots"" in which items can be stored. An alternative theory in which memory resource is a continuous quantity distributed over all items seems to be refuted by the appearance of guessing in human responses. Here, we introduce a model in which resource is not only continuous but also variable across items and trials, causing random fluctuations in encoding precision. We tested this model against previous models using two VSTM paradigms and two feature dimensions. Our model accurately accounts for all aspects of the data, including apparent guessing, and outperforms slot models in formal model comparison. At the neural level, variability in precision might correspond to variability in neural population gain and doubly stochastic stimulus representation. Our results suggest that VSTM resource is continuous and variable rather than discrete and fixed and might explain why subjective experience of VSTM is not all or none.",0
https://doi.org/10.1037/a0032222,A method for efficiently sampling from distributions with correlated dimensions.,"Bayesian estimation has played a pivotal role in the understanding of individual differences. However, for many models in psychology, Bayesian estimation of model parameters can be difficult. One reason for this difficulty is that conventional sampling algorithms, such as Markov chain Monte Carlo (MCMC), can be inefficient and impractical when little is known about the target distribution--particularly the target distribution's covariance structure. In this article, we highlight some reasons for this inefficiency and advocate the use of a population MCMC algorithm, called differential evolution Markov chain Monte Carlo (DE-MCMC), as a means of efficient proposal generation. We demonstrate in a simulation study that the performance of the DE-MCMC algorithm is unaffected by the correlation of the target distribution, whereas conventional MCMC performs substantially worse as the correlation increases. We then show that the DE-MCMC algorithm can be used to efficiently fit a hierarchical version of the linear ballistic accumulator model to response time data, which has proven to be a difficult task when conventional MCMC is used.",0
https://doi.org/10.1034/j.1600-0420.2001.790304.x,An intervention trial on efficacy of atropine and multi-focal glasses in controlling myopic progression,"This randomized clinical trial assessed the treatment effects of atropine and/or multi-focal lenses in decreasing the progression rate of myopia in children.Two hundred and twenty-seven schoolchildren with myopia, aged from 6 to 13 years, who were stratified based on gender, age and the initial amount of myopia were randomly assigned to three treatment groups: 0.5% atropine with multi-focal glasses, multi-focal glasses, and single vision spectacles. Each subject was followed for at least eighteen months. These results report on the 188 patients available for the follow-up.The mean progression of myopia in atropine with multi-focal glasses group (0.41 D) was significantly less than the multi-focal (1.19 D) and single vision group (1.40 D) (p < 0.0001). But no significant difference was noted between the last two groups (p = 0.44). The progression of myopia was significantly correlated with the increases of axial length (r = 0.65, p = 0.0001), but not with the changes of corneal power (r=-0.09), anterior chamber depth (r = -0.023), lens thickness (r = -0.08), or intra-ocular pressure (r = -0.008).The 0.5% atropine with multi-focal lenses can slow down the progression rate of myopia. However, multi-focal lenses alone showed no difference in effect compared to control.",0
https://doi.org/10.1080/00273171.2011.589274,Formulation and Application of the Hierarchical Generalized Random-Situation Random-Weight MIRID,"The process-component approach has become quite popular for examining many psychological concepts. A typical example is the model with internal restrictions on item difficulty (MIRID) described by Butter (1994) and Butter, De Boeck, and Verhelst (1998). This study proposes a hierarchical generalized random-situation random-weight MIRID. The proposed model is more flexible for formulating endogenous latent variables within a multilevel framework, allowing the analysis of polytomous data with complex models (e.g., including item discriminations, random situations, random weights, and heteroskedasticity). The parameters in the proposed model can be estimated using the computer program WinBUGS, which adopts Markov Chain Monte Carlo algorithms. To illustrate the application of the proposed model, a real data set about guilt is analyzed and a comparison of MIRIDs for various conditions is conducted.",0
https://doi.org/10.1109/tac.1974.1100705,A new look at the statistical model identification,The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.,0
https://doi.org/10.1111/j.1745-3984.2011.00132.x,Assessing Fit of Unidimensional Graded Response Models Using Bayesian Methods,"The posterior predictive model checking method is a flexible Bayesian model-checking tool and has recently been used to assess fit of dichotomous IRT models. This paper extended previous research to polytomous IRT models. A simulation study was conducted to explore the performance of posterior predictive model checking in evaluating different aspects of fit for unidimensional graded response models. A variety of discrepancy measures (test-level, item-level, and pair-wise measures) that reflected different threats to applications of graded IRT models to performance assessments were considered. Results showed that posterior predictive model checking exhibited adequate power in detecting different aspects of misfit for graded IRT models when appropriate discrepancy measures were used. Pair-wise measures were found more powerful in detecting violations of the unidimensionality and local independence assumptions.",0
https://doi.org/10.1016/j.jmp.2005.11.006,Modeling individual differences using Dirichlet processes,"Abstract We introduce a Bayesian framework for modeling individual differences, in which subjects are assumed to belong to one of a potentially infinite number of groups. In this model, the groups observed in any particular data set are not viewed as a fixed set that fully explains the variation between individuals, but rather as representatives of a latent, arbitrarily rich structure. As more people are seen, and more details about the individual differences are revealed, the number of inferred groups is allowed to grow. We use the Dirichlet process—a distribution widely used in nonparametric Bayesian statistics—to define a prior for the model, allowing us to learn flexible parameter distributions without overfitting the data, or requiring the complex computations typically required for determining the dimensionality of a model. As an initial demonstration of the approach, we present three applications that analyze the individual differences in category learning, choice of publication outlets, and web-browsing behavior.",0
https://doi.org/10.4324/9780203879313,The Routledge Companion to Philosophy of Psychology,21 page(s,0
https://doi.org/10.1214/13-aoas702,Small area estimation of general parameters with application to poverty indicators: A hierarchical Bayes approach,"Poverty maps are used to aid important political decisions such as allocation of development funds by governments and international organizations. Those decisions should be based on the most accurate poverty figures. However, often reliable poverty figures are not available at fine geographical levels or for particular risk population subgroups due to the sample size limitation of current national surveys. These surveys cannot cover adequately all the desired areas or population subgroups and, therefore, models relating the different areas are needed to 'borrow strength"" from area to area. In particular, the Spanish Survey on Income and Living Conditions (SILC) produces national poverty estimates but cannot provide poverty estimates by Spanish provinces due to the poor precision of direct estimates, which use only the province specific data. It also raises the ethical question of whether poverty is more severe for women than for men in a given province. We develop a hierarchical Bayes (HB) approach for poverty mapping in Spanish provinces by gender that overcomes the small province sample size problem of the SILC. The proposed approach has a wide scope of application because it can be used to estimate general nonlinear parameters. We use a Bayesian version of the nested error regression model in which Markov chain Monte Carlo procedures and the convergence monitoring therein are avoided. A simulation study reveals good frequentist properties of the HB approach. The resulting poverty maps indicate that poverty, both in frequency and intensity, is localized mostly in the southern and western provinces and it is more acute for women than for men in most of the provinces.",0
https://doi.org/10.1027/1015-5759/a000232,How Test Takers See Test Examiners,Abstract. We addressed potential test takers’ preferences for women or men as examiners as well as how examiners were perceived depending on their gender. We employed an online design with 375 students who provided preferences for and ratings of examiners based on short video clips. The clips showed four out of 15 psychologists who differed in age (young vs. middle-aged) and gender giving an introduction to a fictional intelligence test session. Employing multivariate multilevel analyses we found female examiners to be perceived as more social competent and middle-aged examiners being perceived as more competent. Data analyses revealed a significant preference for choosing women as examiners. Results were discussed with reference to test performance and fairness.,0
https://doi.org/10.1177/1525822x07303502,Cultural Consensus Theory: Applications and Frequently Asked Questions,"In the ethnographic context, where answers to questions are unknown, consensus theory estimates the culturally appropriate or correct answers to the questions and individual differences in cultural knowledge. The cultural consensus model is a formal model of the process for asking and answering questions and is limited to categorical response data. An informal version of the model is available as a set of analytic procedures and obtains similar information with fewer assumptions. This article describes the assumptions, appropriate interview materials, and analytic procedures for carrying out a consensus analysis. Finally, issues that sometimes arise during the application of a consensus analysis are discussed.",0
https://doi.org/10.1016/j.jpain.2006.04.001,Spinal Cord Glia and Interleukin-1 Do Not Appear to Mediate Persistent Allodynia Induced by Intramuscular Acidic Saline in Rats,"Spinal glial activation and consequent interleukin-1 (IL-1) release are implicated in pain facilitation induced by inflammation/damage to skin and peripheral nerves. It is unclear whether pain facilitation induced at deep tissue sites also depends on these. We investigated whether spinal IL-1 and/or glial activation mediates bilateral allodynia induced by repeated unilateral intramuscular injections of acidic saline to rats. Given the prominent role of spinal IL-1 in various bilateral pain models, we predicted that intrathecal IL-1 receptor antagonist (IL-1ra) would suppress bilateral allodynia in this model as well. Surprisingly, neither single nor repeated intrathecal injections of IL-1ra affected allodynia, measured by the von Frey test, induced by prior intramuscular acidic saline compared with vehicle-injected controls. In addition, we tested the effect of 2 additional intrathecal manipulations that are broadly efficacious in suppressing glially mediated pain facilitation: (1) a glial metabolic inhibitor (fluorocitrate) and (2) the anti-inflammatory cytokine, interleukin-10 (IL-10). Like IL-1ra, fluorocitrate and IL-10 each failed to reverse allodynia. Finally, we observed no significant activation of glial cells, as assessed by immunohistochemistry of glial activation markers, in the lumbar spinal cord in response to intramuscular acidic saline. Taken together, the present data suggest that acidic saline-induced bilateral allodynia is created independently of glial activation.From converging lines of evidence, the current studies suggest that persistent bilateral allodynia induced by repeated intramuscular acidic saline is not mediated by spinal IL-1 and/or spinal glial activation. As such, this might represent the first evidence for pain facilitation occurring in the absence of glial involvement.",0
https://doi.org/10.2307/3316080,Statistical analyses for round robin interaction data,"This paper considers the analysis of round robin interaction data whereby individuals from a group of subjects interact with one another, producing a pair of outcomes, one for each individual. The authors provide an overview of the various analyses applied to these types of data and extend the work in several directions. In particular, they provide a fully Bayesian analysis for such data and use a real data example for illustration purposes.",0
https://doi.org/10.1002/sim.4072,The analysis of very small samples of repeated measurements II: A modified Box correction,"There is a need for appropriate methods for the analysis of very small samples of continuous repeated measurements. A key feature of such analyses is the role played by the covariance matrix of the repeated observations. When subjects are few it can be difficult to assess the fit of parsimonious structures for this matrix, while the use of an unstructured form may lead to a serious lack of power. The Kenward-Roger adjustment is now widely adopted as a means of providing an appropriate inferences in small samples, but does not perform adequately in very small samples. Adjusted tests based on the empirical sandwich estimator can be constructed that have good nominal properties, but are seriously underpowered. Further, when such data are incomplete, or unbalanced, or non-saturated mean models are used, exact distributional results do not exist that justify analyses with any sample size. In this paper, a modification of Box's correction applied to a linear model-based F-statistic is developed for such small sample settings and is shown to have both the required nominal properties and acceptable power across a range of settings for repeated measurements.",0
https://doi.org/10.1123/jsep.19.1.36,Application of the Theories of Reasoned Action and Planned Behavior to Exercise Behavior: A Meta-Analysis,"The primary purpose of this study was to use meta-analysis to statistically examine the utility of the theory of reasoned action (TRA) and the theory of planned behavior (TPB) for the explanation and prediction of exercise behavior. The results showed that the effect size for the relationships (a) between intention and exercise behavior, attitude and intention, attitude and exercise behavior, perceived behavioral control and intention, and perceived behavioral control and exercise behavior was large; (b) between subjective norm and intention was moderate; and (c) between subjective norm and exercise behavior was zero-order. The results also supported the conclusions that (a) TPB is superior to TRA in accounting for exercise behavior, (b) there is no differences in the ability to predict exercise behavior from proximal and distal measures of intention, and (c) expectation is a better predictor of exercise behavior than intention.",0
https://doi.org/10.1111/spsr.12179,A Decline in the Quality of Debate? The Evolution of Cognitive Complexity in Swiss Parliamentary Debates on Immigration (1968-2014),"This article explores the evolution of debate quality in the Swiss parliament. Focusing on immigration debates, we employ a psychological construct—cognitive complexity (CC)—which captures both epistemic and accommodative dimensions of political argumentation. We find a decrease in CC in parliamentary immigration debates over time, but this decrease was driven by the rise of the SVP (Swiss People's Party). However, there was almost no “spillover” of this new communication style to other parties. Moreover, we also find a constant difference between the Standerat and the Nationalrat, with the former scoring higher on CC and thus asserting its role as a “chambre de reflexion” in immigration debates. Our diachronic focus on the quality of political debate takes a novel perspective on the dynamics of consensus democracy as well as on elite political culture. While our results indicate that the rise of the SVP has transformed the traditional consensual and deliberative pattern of Swiss policy-making style into one which is geared towards less accommodation and a higher simplicity of political talk, there is still remarkable resilience against this new style of political interaction.",0
https://doi.org/10.1111/j.1540-5907.2006.00166.x,The Political Economy of Gender: Explaining Cross-National Variation in the Gender Division of Labor and the Gender Voting Gap,"Mainstream political economy has tended to treat the family as a unit when examining the distributional consequences of labor market institutions and of public policy. In a world with high divorce rates, we argue that this simplification is more likely to obscure than to instruct. We find that labor market opportunities for women, which vary systematically with the position of countries in the international division of labor and with the structure of the welfare state, affect women's bargaining power within the family and as a result, can explain much of the cross country variation in the gender division of labor as well as the gender gap in political preferences.",0
https://doi.org/10.1136/bmj.i1777,Methotrexate monotherapy and methotrexate combination therapy with traditional and biologic disease modifying antirheumatic drugs for rheumatoid arthritis: abridged Cochrane systematic review and network meta-analysis,"To compare methotrexate based disease modifying antirheumatic drug (DMARD) treatments for rheumatoid arthritis in patients naive to or with an inadequate response to methotrexate.Systematic review and Bayesian random effects network meta-analysis of trials assessing methotrexate used alone or in combination with other conventional synthetic DMARDs, biologic drugs, or tofacitinib in adult patients with rheumatoid arthritis.Trials were identified from Medline, Embase, and Central databases from inception to 19 January 2016; abstracts from two major rheumatology meetings from 2009 to 2015; two trial registers; and hand searches of Cochrane reviews.Randomized or quasi-randomized trials that compared methotrexate with any other DMARD or combination of DMARDs and contributed to the network of evidence between the treatments of interest.American College of Rheumatology (ACR) 50 response (major clinical improvement), radiographic progression, and withdrawals due to adverse events. A comparison between two treatments was considered statistically significant if its credible interval excluded the null effect, indicating >97.5% probability that one treatment was superior.158 trials were included, with between 10 and 53 trials available for each outcome. In methotrexate naive patients, several treatments were statistically superior to oral methotrexate for ACR50 response: sulfasalazine and hydroxychloroquine (""triple therapy""), several biologics (abatacept, adalimumab, etanercept, infliximab, rituximab, tocilizumab), and tofacitinib. The estimated probability of ACR50 response was similar between these treatments (range 56-67%), compared with 41% with methotrexate. Methotrexate combined with adalimumab, etanercept, certolizumab, or infliximab was statistically superior to oral methotrexate for inhibiting radiographic progression, but the estimated mean change over one year with all treatments was less than the minimal clinically important difference of 5 units on the Sharp-van der Heijde scale. Triple therapy had statistically fewer withdrawals due to adverse events than methotrexate plus infliximab. After an inadequate response to methotrexate, several treatments were statistically superior to oral methotrexate for ACR50 response: triple therapy, methotrexate plus hydroxychloroquine, methotrexate plus leflunomide, methotrexate plus intramuscular gold, methotrexate plus most biologics, and methotrexate plus tofacitinib. The probability of response was 61% with triple therapy and ranged widely (27-70%) with other treatments. No treatment was statistically superior to oral methotrexate for inhibiting radiographic progression. Methotrexate plus abatacept had a statistically lower rate of withdrawals due to adverse events than several treatments.Triple therapy (methotrexate plus sulfasalazine plus hydroxychloroquine) and most regimens combining biologic DMARDs with methotrexate were effective in controlling disease activity, and all were generally well tolerated in both methotrexate naive and methotrexate exposed patients.",0
https://doi.org/10.1038/sj.jea.7500182,Improved non-negative estimation of variance components for exposure assessment,"Hygiene surveys of pollutants exposure data can be analyzed by analysis of variance (ANOVA) model with a random worker effect. Typically, workers are classified into homogeneous exposure groups, so it is very common to obtain a zero or negative ANOVA estimate of the between-worker variance (Ã Æ’B2). Negative estimates are not sensible and also pose problems for estimating the probability (ÃŽËœ) that in a job group, a randomly selected worker's mean exposure exceeds the occupational exposure standard. Therefore, it was suggested by Rappaport et al. to replace a non-positive estimate with an approximate one-sided 60% upper confidence bound. This article develops an alternative estimator, based on the upper tolerance interval suggested by Wang and Iyer. We compared the performance of the two methods using real data and simulations with respect to estimating both the between-worker variance and the probability of overexposure in balanced designs. We found that the method of Rappaport et al. has three main disadvantages: (i) the estimated Ã Æ’B2 remains negative for some data sets; (ii) the estimator performs poorly in estimating Ã Æ’B2 and ÃŽËœ with two repeated measures per worker and when true Ã Æ’B2 is quite small, which are quite common situations when studying exposure; (iii) the estimator can be extremely sensitive to small changes in the data. Our alternative estimator offers a solution to these problems.",0
https://doi.org/10.1185/03007995.2012.734798,Efficacy of telaprevir and boceprevir in treatment-naïve and treatment-experienced genotype 1 chronic hepatitis C patients: an indirect comparison using Bayesian network meta-analysis,"To indirectly compare the efficacy of telaprevir (TVR) and boceprevir (BOC) combined with peginterferon/ribavirin α-2a/2b (PR) in achieving sustained viral response (SVR) in treatment-naïve and treatment-experienced patients with genotype 1 chronic hepatitis C virus (HCV) infection.A systematic literature review was conducted to identify randomized controlled trials reporting the efficacy of PR-based treatment in genotype 1 chronic HCV patients. A Bayesian network meta-analysis was performed on the endpoint of SVR, assuming fixed study effects. For treatment-experienced patients, only previous relapsers and partial responders were included, as no results in prior null responders were available for boceprevir.Eleven publications were included. In treatment-naïve patients, the odds ratios (OR) (posterior median [95% credible interval]) for telaprevir (12 weeks + response guided treatment [RGT] 24/48 weeks PR) and boceprevir (24 weeks + RGT 28/48 weeks PR) versus PR were respectively 3.80 (2.78-5.22) and 2.99 (2.23-4.01). The OR for telaprevir versus boceprevir was 1.42 (0.89-2.25), with a probability for telaprevir being more effective (P[OR > 1]) of 0.93. In treatment-experienced patients, the OR of telaprevir (12 weeks + 48 weeks PR) and boceprevir (32 weeks + RGT 36/48 weeks PR) versus PR were respectively 13.11 (7.30-24.43) and 5.36 (2.90-10.30). The OR for telaprevir versus boceprevir was 2.45 (1.02-5.80), with telaprevir having a probability of 0.98 of being more effective.The main limitation of this study is the low number of trials included in the analysis, especially for the treatment-experienced patient population, which only allowed random-effect models to be explored. We tried to identify potential biases due to study heterogeneity.In the absence of direct comparative head-to-head studies between telaprevir and boceprevir for the treatment of chronic HCV genotype 1 patients, an indirect comparison based on Bayesian network meta-analysis suggests better efficacy for telaprevir than boceprevir in both treatment-naïve and treatment-experienced patients.",0
https://doi.org/10.1037/1082-989x.9.4.466,An Empirical Evaluation of Alternative Methods of Estimation for Confirmatory Factor Analysis With Ordinal Data.,"Confirmatory factor analysis (CFA) is widely used for examining hypothesized relations among ordinal variables (e.g., Likert-type items). A theoretically appropriate method fits the CFA model to polychoric correlations using either weighted least squares (WLS) or robust WLS. Importantly, this approach assumes that a continuous, normal latent process determines each observed variable. The extent to which violations of this assumption undermine CFA estimation is not well-known. In this article, the authors empirically study this issue using a computer simulation study. The results suggest that estimation of polychoric correlations is robust to modest violations of underlying normality. Further, WLS performed adequately only at the largest sample size but led to substantial estimation difficulties with smaller samples. Finally, robust WLS performed well across all conditions.",0
https://doi.org/10.1111/j.1541-0420.2010.01463.x,Fixed and Random Effects Selection in Mixed Effects Models,"We consider selecting both fixed and random effects in a general class of mixed effects models using maximum penalized likelihood (MPL) estimation along with the smoothly clipped absolute deviation (SCAD) and adaptive least absolute shrinkage and selection operator (ALASSO) penalty functions. The MPL estimates are shown to possess consistency and sparsity properties and asymptotic normality. A model selection criterion, called the IC(Q) statistic, is proposed for selecting the penalty parameters (Ibrahim, Zhu, and Tang, 2008, Journal of the American Statistical Association 103, 1648-1658). The variable selection procedure based on IC(Q) is shown to consistently select important fixed and random effects. The methodology is very general and can be applied to numerous situations involving random effects, including generalized linear mixed models. Simulation studies and a real data set from a Yale infant growth study are used to illustrate the proposed methodology.",0
https://doi.org/10.1214/aos/1176349736,Calibration-Based Empirical Probability,"Probability forecasts for a sequence of uncertain events may be compared with the outcomes of those events by means of a natural criterion of empirical validity, calibration. It is shown that any two sequences of forecasts which both meet this criterion must be in asymptotic agreement. These agreed values can then be considered as correct objective probability forecasts for the particular sequence of outcome results obtained. However, the objective forecasts vary with the extent of the information taken into account when they are formulated. We thus obtain a general theory of empirical probability, relative to an information base. This theory does not require that such probabilities be interpreted in terms of repeated trials of the same event. Some implications of this theory are discussed.",0
https://doi.org/10.1002/(sici)1097-0258(19980430)17:8<841::aid-sim781>3.0.co;2-d,Detecting and describing heterogeneity in meta-analysis,"The investigation of heterogeneity is a crucial part of any meta-analysis. While it has been stated that the test for heterogeneity has low power, this has not been well quantified. Moreover the assumptions of normality implicit in the standard methods of meta-analysis are often not scrutinized in practice. Here we simulate how the power of the test for heterogeneity depends on the number of studies included, the total information (that is total weight or inverse variance) available and the distribution of weights among the different studies. We show that the power increases with the total information available rather than simply the number of studies, and that it is substantially lowered if, as is quite common in practice, one study comprises a large proportion of the total information. We also describe normal plots that are useful in assessing whether the data conform to a fixed effect or random effects model, together with appropriate tests, and give an application to the analysis of a multi-centre trial of blood pressure reduction. We conclude that the test of heterogeneity should not be the sole determinant of model choice in meta-analysis, and inspection of relevant normal plots, as well as clinical insight, may be more relevant to both the investigation and modelling of heterogeneity.",0
https://doi.org/10.1007/978-3-642-15387-7_32,A Computer Adaptive Testing Method for Intelligent Tutoring Systems,"AbstractThe growth of popularity of computers increases interest of adaptive testing in tutoring systems. Computer adaptive testing is a form of educational measurement that is adaptable to examine proficiency. In a procedure of adaptive testing it is required to determine a selection of the first item, a method of estimation of student’s proficiency, a method of selection of the next item and a termination criterion. In this paper the original algorithm of adaptive testing with all basic steps is proposed. The level of difficulty of the first item is set using user’s profile. Such solution allows to start a test, where the first item is suitable for student’s preferences. In our method 2-parameter IRT model is applied to choose the next item.KeywordsTest ItemItem Response TheoryItem Response Theory ModelIntelligent Tutor SystemProficiency LevelThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.",0
https://doi.org/10.3758/brm.41.4.1083,Statistical power analysis for growth curve models using SAS,"Power analysis is critical in research designs. This study discusses a simulation-based approach utilizing the likelihood ratio test to estimate the power of growth curve analysis. The power estimation is implemented through a set of SAS macros. The application of the SAS macros is demonstrated through several examples, including missing data and nonlinear growth trajectory situations. The results of the examples indicate that the power of growth curve analysis increases with the increase of sample sizes, effect sizes, and numbers of measurement occasions. In addition, missing data can reduce power. The SAS macros can be modified to accommodate more complex power analysis for both linear and nonlinear growth curve models.",0
https://doi.org/10.1016/0042-6989(81)90115-2,On the psychometric function for contrast detection,"Abstract The frequent current use in probability summation calculations of equations of the form. P = 1 − (1−γ)exp[−(I/α)β] to represent the psychometric function for contrast detection is based on two assumptions: (1) γ can be changed without affecting α and β (the high-threshold assumption) and (2) β is the same for all pattern-detecting mechnisms (the homogeneity assumption). Results of yes-no, rating-scale, and forced-choice experiments contradict the high-threshold assumption: estimates of α and β covary with γ. Contrary to the homogeneity assumption, bipartite fields yield lower values of β than do 12 c/deg gratings. Some consequences of these findings for probability summation calculations are discussed.",0
https://doi.org/10.1121/1.3598448,Psychometric functions for pure-tone frequency discrimination,"The form of the psychometric function (PF) for auditory frequency discrimination is of theoretical interest and practical importance. In this study, PFs for pure-tone frequency discrimination were measured for several standard frequencies (200-8000 Hz) and levels [35-85 dB sound pressure level (SPL)] in normal-hearing listeners. The proportion-correct data were fitted using a cumulative-Gaussian function of the sensitivity index, d', computed as a power transformation of the frequency difference, Δf. The exponent of the power function corresponded to the slope of the PF on log(d')-log(Δf) coordinates. The influence of attentional lapses on PF-slope estimates was investigated. When attentional lapses were not taken into account, the estimated PF slopes on log(d')-log(Δf) coordinates were found to be significantly lower than 1, suggesting a nonlinear relationship between d' and Δf. However, when lapse rate was included as a free parameter in the fits, PF slopes were found not to differ significantly from 1, consistent with a linear relationship between d' and Δf. This was the case across the wide ranges of frequencies and levels tested in this study. Therefore, spectral and temporal models of frequency discrimination must account for a linear relationship between d' and Δf across a wide range of frequencies and levels.",0
https://doi.org/10.2307/2988546,Analysis of Clinical Data Using Imprecise Prior Probabilities,"This paper describes a new method based on the theory of imprecise probabilities, for analysing clinical data in the form of a contingency table. The method is applied to a well-known set of statistical data from randomized clinical trials of two treatments for severe cardiorespiratory failure in newborn babies. Two problems are distinguished. The inference problem is to draw conclusions about which treatment is more effective. The decision problem is to determine whether one treatment should be preferred to another for the next patient, or whether it is ethical to select the treatment by randomization. The two problems are analysed using three possible models for prior ignorance about the statistical parameters, and one of the models is modified to take account of earlier clinical data. In this example the four models produce essentially the same conclusions.",0
https://doi.org/10.1111/ajps.12200,"IMF Conditionality, Government Partisanship, and the Progress of Economic Reforms","The International Monetary Fund (IMF) often seeks to influence countries' domestic public policy via varying levels of conditionality—linking financial support to borrowing governments' commitment to policy reforms. When does extensive conditionality encourage domestic economic reforms and when does it impede them? We argue that, rather than universally benefiting or harming reforms, the effects of stricter IMF conditionality depend on domestic partisan politics. More IMF conditions can pressure left-wing governments into undertaking more ambitious reforms with little resistance from partisan rivals on the right; under right governments, however, more conditions hinder reform implementation by heightening resistance from the left while simultaneously reducing leaders' ability to win their support through concessions or compromise. Using data on post-communist IMF programs for the period 1994–2010, we find robust evidence supporting these claims, even after addressing the endogeneity of IMF programs via instrumental variables analysis.",0
https://doi.org/10.3102/10769986029002177,Evaluation of the CATSIB DIF Procedure in a Pretest Setting,"A new procedure, CATSIB, for assessing differential item functioning (DIF) on computerized adaptive tests (CATs) is proposed. CATSIB, a modified SIBTEST procedure, matches test takers on estimated ability and controls for impact-induced Type 1 error inflation by employing a CAT version of the SIBTEST “regression correction.” The performance of CATSIB in terms of detection of DIF in pretest items was evaluated in a simulation study. Simulated test takers were adaptively administered 25 operational items from a pool of 1,000 and were linearly administered 16 pretest items that were evaluated for DIF. Sample size varied from 250 to 500 in each group. Simulated impact levels ranged from a 0- to 1-standard-deviation difference in mean ability levels. The results showed that CATSIB with the regression correction displayed good control over Type 1 error, whereas CATSIB without the regression correction displayed impact-induced Type 1 error inflation. With 500 test takers in each group, power rates were exceptionally high (84% to 99%) for values of DIF at the boundary between moderate and large DIF. For smaller samples of 250 test takers in each group, the corresponding power rates ranged from 47% to 95%. In addition, in all cases, CATSIB was very accurate in estimating the true values of DIF, displaying at most only minor estimation bias.",0
https://doi.org/10.3758/s13428-013-0351-0,An alternative approach to analysis of mental states in experimental social cognition research,"Establishing the mental states that affect human behavior is a primary goal of experiments on social cognitive processes. Such mental states can be manipulated only indirectly; therefore, after delivering a manipulation, researchers attempt to verify that the mental state of interest, the representation of a mental state, was in fact changed by the manipulation and that this change caused the observed effect. The usual procedure is to examine mean differences in a measure of the mental state of interest (a manipulation check) among experimental conditions and to infer whether the manipulation was effective. We describe a procedure that strengthens the construct validity of manipulations and, hence, causal inferences in experiments that focus on mental states using analyses familiar to most researchers. This procedure employs a traditional manipulation check that assesses the relationship between manipulations and mental states but, additionally, tests the relationship between the manipulation check and dependent measure. Â© 2013 Psychonomic Society, Inc.",0
https://doi.org/10.1016/s0148-2963(00)00127-2,The ability of ratings and choice conjoint to predict market shares: a Monte Carlo simulation,"We use a Monte Carlo simulation with many synthetic data sets to compare ratings and choice conjoint analysis in their ability to correctly predict market shares under varying market conditions. Our results provide guidance to researchers seeking to use conjoint analysis for managerial decisions. Our recommendations are quite different from the recommendations of prior researchers who compared conjoint methods using single empirical data sets. Our results indicate that one must, at least roughly, assess the degree of consumers' heterogeneity in preferences, product similarity in the marketplace, typical consumers' choice-rule (probabilistic or deterministic), and magnitude of error in measurement of utilities in order to make a prudent choice between ratings and choice conjoint analysis.",0
https://doi.org/10.1038/srep19203,Evaluating and excluding swap errors in analogue tests of working memory,"Abstract When observers retrieve simple visual features from working memory, two kinds of error are typically confounded in their recall. First, responses reflect noise or variability within the feature dimension they were asked to report. Second, responses are corrupted by “swap errors”, in which a different item from the memory set is reported in place of the one that was probed. Independent evaluation of these error sources is vital for understanding the structure of internal representations and their binding. However, previous methods for disentangling these errors have been critically dependent on assumptions about the noise distribution, which is a priori unknown. Here I address this question with novel non-parametric (NP) methods, which estimate swap frequency and feature variability with fewer prior assumptions and without a fitting procedure. The results suggest that swap errors are considerably more prevalent than previously appreciated (accounting for more than a third of responses at set size 8). These methods also identify which items are swapped in for targets: when the target item is cued by location, the items in closest spatial proximity are most likely to be incorrectly reported, thus implicating noise in the probe feature dimension as a source of swap errors.",0
https://doi.org/10.1111/j.1745-3992.1998.tb00632.x,Protecting the Integrity of Computerized Testing Item Pools,What are the issues and techniques involved in protecting the integrity of item pools in computerized testing? How can item exposure be limited? How do security issues differ in computerized testing and paper-and-pencil testing?,0
https://doi.org/10.1214/10-sts321,"Identification, Inference and Sensitivity Analysis for Causal Mediation Effects","Causal mediation analysis is routinely conducted by applied researchers in a variety of disciplines. The goal of such an analysis is to investigate alternative causal mechanisms by examining the roles of intermediate variables that lie in the causal paths between the treatment and outcome variables. In this paper we first prove that under a particular version of sequential ignorability assumption, the average causal mediation effect (ACME) is nonparametrically identified. We compare our identification assumption with those proposed in the literature. Some practical implications of our identification result are also discussed. In particular, the popular estimator based on the linear structural equation model (LSEM) can be interpreted as an ACME estimator once additional parametric assumptions are made. We show that these assumptions can easily be relaxed within and outside of the LSEM framework and propose simple nonparametric estimation strategies. Second, and perhaps most importantly, we propose a new sensitivity analysis that can be easily implemented by applied researchers within the LSEM framework. Like the existing identifying assumptions, the proposed sequential ignorability assumption may be too strong in many applied settings. Thus, sensitivity analysis is essential in order to examine the robustness of empirical findings to the possible existence of an unmeasured confounder. Finally, we apply the proposed methods to a randomized experiment from political psychology. We also make easy-to-use software available to implement the proposed methods.",0
https://doi.org/10.1198/jasa.2011.tm09680,Bayesian Inference for the Spatial Random Effects Model,"Spatial statistical analysis of massive amounts of spatial data can be challenging because computation of optimal procedures can break down. The Spatial Random Effects (SRE) model uses a fixed number of known but not necessarily orthogonal (multiresolutional) spatial basis functions, which gives a flexible family of nonstationary covariance functions, results in dimension reduction, and yields optimal spatial predictors whose computations are scalable. By modeling spatial data in a hierarchical manner with a process model that includes the SRE model, the choice is whether to estimate the SRE model’s parameters or to take a Bayesian approach and put a prior distribution on them. In this article, we develop Bayesian inference for the SRE model when the spatial basis functions are multiresolutional. Then the covariance matrix of the random effects decomposes naturally in terms of Givens angles and eigenvalues, for which a new class of prior distributions is developed. This approach to prior specification of ...",0
https://doi.org/10.1214/10-sts318,"Calibrated Bayes, for Statistics in General, and Missing Data in Particular","It is argued that the Calibrated Bayesian (CB) approach to statistical inference capitalizes on the strength of Bayesian and frequentist approaches to statistical inference. In the CB approach, inferences under a particular model are Bayesian, but frequentist methods are useful for model development and model checking. In this article the CB approach is outlined. Bayesian methods for missing data are then reviewed from a CB perspective. The basic theory of the Bayesian approach, and the closely related technique of multiple imputation, is described. Then applications of the Bayesian approach to normal models are described, both for monotone and nonmonotone missing data patterns. Sequential Regression Multivariate Imputation and Penalized Spline of Propensity Models are presented as two useful approaches for relaxing distributional assumptions.",0
https://doi.org/10.1186/2046-4053-1-44,Drugs commonly associated with weight change: umbrella systematic review and meta-analysis (Protocol),"Abstract Background Many drugs and treatments given to patients for various reasons affect their weight. This side effect is of great importance to patients and is also a concern for the treating physician because weight change may lead to the emergence or worsening of other health conditions. Objective The aim of this study is to summarize the evidence about commonly prescribed drugs and their association with weight change. Methods/Design Umbrella systematic review and meta-analysis of randomized controlled trials. We will use an umbrella approach to identify eligible randomized controlled trials (RCTs). We will search for systematic reviews of RCTs that compare any of the drugs that have been associated with weight gain (obesogenic) or weight loss (leptogenic); these have been summarized by our experts’ panel in a predefined list. Two reviewers will independently determine RCT eligibility. Disagreement will be solved by consensus and arbitrated by a third reviewer. We will extract descriptive, methodological, and efficacy data in duplicate. Our primary continuous outcomes will be weight loss or gain expressed as a mean difference (MD) for weight (kg) or BMI (kg/m 2 ). We will calculate the MD considering the mean difference in weight or BMI between baseline and the last available follow-up in both study arms (drugs and placebo). Our primary dichotomous outcome, presented as a relative risk, will compare the ratio of the incidence of weight change in each trial arm. When possible, results will be pooled using classic random-effects meta-analyses and a summary estimate with 95% confidence interval will provided. We will use the I 2 statistic and Cochran’s Q test to assess heterogeneity. The risk of bias will be assessed using the Cochrane risk of bias tool. Publication bias, if appropriate, will be evaluated, as well as overall strength of the evidence. Discussion This systematic review will offer the opportunity to generate a ranking of commonly prescribed drugs in terms of their effect on weight, allowing guideline developers and patient-physician dyad to choose between available therapies.",0
https://doi.org/10.2307/3151320,Alternative Estimation Methods for Conjoint Analysis: A Monte Carlo Study,,0
https://doi.org/10.4324/9781410606747-5,Nonlinear Multilevel Models for Repeated Measures Data,,0
https://doi.org/10.1177/0146621608327801,Comparison of CAT Item Selection Criteria for Polytomous Items,"Item selection is a core component in computerized adaptive testing (CAT). Several studies have evaluated new and classical selection methods; however, the few that have applied such methods to the use of polytomous items have reported conflicting results. To clarify these discrepancies and further investigate selection method properties, six different selection methods are compared systematically. The results showed no clear benefit from more sophisticated selection criteria and showed one method previously believed to be superior—the maximum expected posterior weighted information (MEPWI)—to be mathematically equivalent to a simpler method, the maximum posterior weighted information (MPWI).",0
https://doi.org/10.3168/jds.2015-10264,Use of early lactation milk recording data to predict the calving to conception interval in dairy herds,"Economic success in dairy herds is heavily reliant on obtaining pregnancies at an early stage of lactation. Our objective in this study was to attempt to predict the likelihood of conception occurring by d 100 and 150 of lactation (days in milk, DIM) by Markov chain Monte Carlo analysis using test day milk recording data and reproductive records gathered retrospectively from 8,750 cows from 33 dairy herds located in the United Kingdom. Overall, 65% of cows recalved with 30, 46, and 65% of cows conceiving by 100 DIM, 150 DIM, and beyond 150 DIM, respectively. Overall conception rate (total cows pregnant/total number of inseminations) was 27.47%. Median and mean calving to conception intervals were 123 and 105 d, respectively. The probability of conception by both 100 DIM and 150 DIM was positively associated with the average daily milk weight produced during the fourth week of lactation (W4MK) and protein percentage for test day samples collected between 0 to 30 and 31 to 60 DIM. Butterfat percentage at 0 to 30 DIM was negatively associated with the probability of conception by 100 DIM but not at 150 DIM. High somatic cell count (SCC) at both 0 to 30 and 31 to 60 DIM was negatively associated with the probability of conception by 100 DIM, whereas high SCC at 31 to 60 DIM was associated with a reduced probability of conception by 150 DIM. Increasing parity was associated with a reduced odds of pregnancy. Posterior predictions of the likelihood of conception for cows categorized as having ""good"" (W4MK >30kg and protein percentage at 0 to 30 and 31 to 60 DIM >3.2%) or ""poor"" (W4MK <25kg and protein percentage at 0 to 30 and 31 to 60 DIM <3.0%) early lactation attributes with actual observed values indicated model fit was good. The predicted likelihood of a ""good"" cow conceiving by 100 and 150 DIM was 0.39 and 0.57, respectively (actual observed values 0.40 and 0.59). The corresponding values for a ""poor"" cow were 0.28 and 0.42 (actual observed values 0.26 and 0.37). Predictions of the future reproductive success of cows may be possible using a limited number of early lactation attributes.",0
https://doi.org/10.1037/xge0000038,The effect of horizontal eye movements on free recall: A preregistered adversarial collaboration.,"A growing body of research has suggested that horizontal saccadic eye movements facilitate the retrieval of episodic memories in free recall and recognition memory tasks. Nevertheless, a minority of studies have failed to replicate this effect. This article attempts to resolve the inconsistent results by introducing a novel variant of proponent-skeptic collaboration. The proposed approach combines the features of adversarial collaboration and purely confirmatory preregistered research. Prior to data collection, the adversaries reached consensus on an optimal research design, formulated their expectations, and agreed to submit the findings to an academic journal regardless of the outcome. To increase transparency and secure the purely confirmatory nature of the investigation, the 2 parties set up a publicly available adversarial collaboration agreement that detailed the proposed design and all foreseeable aspects of the data analysis. As anticipated by the skeptics, a series of Bayesian hypothesis tests indicated that horizontal eye movements did not improve free recall performance. The skeptics suggested that the nonreplication may partly reflect the use of suboptimal and questionable research practices in earlier eye movement studies. The proponents countered this suggestion and used a p curve analysis to argue that the effect of horizontal eye movements on explicit memory did not merely reflect selective reporting.",0
https://doi.org/10.1037/1082-989x.10.2.227,Meta-Analytic Methods of Pooling Correlation Matrices for Structural Equation Modeling Under Different Patterns of Missing Data.,"Three methods of synthesizing correlations for meta-analytic structural equation modeling (SEM) under different degrees and mechanisms of missingness were compared for the estimation of correlation and SEM parameters and goodness-of-fit indices by using Monte Carlo simulation techniques. A revised generalized least squares (GLS) method for synthesizing correlations, weighted-covariance GLS (W-COV GLS), was compared with univariate weighting with untransformed correlations (univariate r) and univariate weighting with Fisher's z-transformed correlations (univariate z). These 3 methods were crossed with listwise and pairwise deletion. Univariate z and W-COV GLS performed similarly, with W-COV GLS providing slightly better estimation of parameters and more correct model rejection rates. Missing not at random data produced high levels of relative bias in correlation and model parameter estimates and higher incorrect SEM model rejection rates. Pairwise deletion resulted in inflated standard errors for all synthesis methods and higher incorrect rejection rates for the SEM model with univariate weighting procedures.",0
https://doi.org/10.1214/14-ba933,Objective Bayesian Inference for a Generalized Marginal Random Effects Model,"An objective Bayesian inference is proposed for the generalized marginal random effects model p(x|μ,σλ)=f((x−μ1)T(V+σλ2I)−1(x−μ1))/det(V+σλ2I). The matrix V is assumed to be known, and the goal is to infer μ given the observations x=(x1,…,xn)T, while σλ is a nuisance parameter. In metrology this model has been applied for the adjustment of inconsistent data x1,…,xn, where the matrix V contains the uncertainties quoted for x1,…,xn. We show that the reference prior for grouping {μ,σλ} is given by π(μ,σλ)∝F22, where F22 denotes the lower right element of the Fisher information matrix F. We give an explicit expression for the reference prior, and we also prove propriety of the resulting posterior as well as the existence of mean and variance of the marginal posterior for μ. Under the additional assumption of normality, we relate the resulting reference analysis to that known for the conventional balanced random effects model in the asymptotic case when the number of repeated within-class observations for that model tends to infinity. We investigate the frequentist properties of the proposed inference for the generalized marginal random effects model through simulations, and we also study its robustness when the underlying distributional assumptions are violated. Finally, we apply the model to the adjustment of current measurements of the Planck constant.",0
https://doi.org/10.1080/14616696.2015.1072226,Credential and Skill Mismatches Among Tertiary Graduates: The effect of labour market institutions on the differences between fields of study in 18 countries,"ABSTRACTThis study provides new empirical evidence about tertiary graduates’ overeducation, by analysing the influence of labour market institutions on the incidence and distribution of the phenomenon across fields of study. In particular, the analyses focus on the level of employment protection, the regulation of access to the so-called liberal professions, and the propensity of welfare states to hire skilled workers. Data from two comparative surveys – REFLEX and HEGESCO – are used, and a wide set of information is employed to split overeducation in two forms of suboptimal allocation of individuals in the labour market: credential and skill mismatches. The first term refers to the mismatch between formal educational credentials and job requirements, whereas the second term refers to the mismatch between the skills acquired through education and those needed to perform a job. Results suggest that field of study differentials vary by country and that welfare and labour market institutions illuminate these...",0
https://doi.org/10.1214/13-sts418,Variational Inference for Generalized Linear Mixed Models Using Partially Noncentered Parametrizations,"The effects of different parametrizations on the convergence of Bayesian computational algorithms for hierarchical models are well explored. Techniques such as centering, noncentering and partial noncentering can be used to accelerate convergence in MCMC and EM algorithms but are still not well studied for variational Bayes (VB) methods. As a fast deterministic approach to posterior approximation, VB is attracting increasing interest due to its suitability for large high-dimensional data. Use of different parametrizations for VB has not only computational but also statistical implications, as different parametrizations are associated with different factorized posterior approximations. We examine the use of partially noncentered parametrizations in VB for generalized linear mixed models (GLMMs). Our paper makes four contributions. First, we show how to implement an algorithm called nonconjugate variational message passing for GLMMs. Second, we show that the partially noncentered parametrization can adapt to the quantity of information in the data and determine a parametrization close to optimal. Third, we show that partial noncentering can accelerate convergence and produce more accurate posterior approximations than centering or noncentering. Finally, we demonstrate how the variational lower bound, produced as part of the computation, can be useful for model selection.",0
https://doi.org/10.1007/s10457-013-9622-0,Fertilizer type and species composition affect leachate nutrient concentrations in coffee agroecosystems,"Intensification of coffee (Coffea arabica) production is associated with increases in inorganic fertilizer application and decreases in species diversity. Both the use of organic fertilizers and the incorporation of trees on farms can, in theory, reduce nutrient loss in comparison with intensified practices. To test this, we measured nutrient concentrations in leachate at 15 and 100 cm depths on working farms. We examined (1) organically managed coffee agroforests (38 kg N ha -1 - year -1 ; n = 4), (2) conventionally managed coffee agroforests (96 kg N ha -1 year -1 ; n = 4), and (3) one conventionally managed monoculture coffee farm in Costa Rica (300 kg N ha -1 year -1 ). Concentrations of nitrate (NO3 - -N) and phosphate (PO4 3- -P) were higher in the monoculture compared to agroforests at both depths. Nitrate concentrations were higher in conven- tional than organic agroforests at 15 cm only. Soil solutions collected under nitrogen (N)-fixing Erythrina poeppigiana had elevated NO3 - -N concentrations at 15 cm compared to Musa acuminata (banana) or Coffea. Total soil N and carbon (C) were also higher under Erythrina. This research shows that both fertilizer type and species affect concentrations of N and P in leachate in coffee agroecosystems.",0
https://doi.org/10.1016/j.ijforecast.2010.08.002,Forecasting television ratings,"Abstract Despite the state of flux in media today, television remains the dominant player globally for advertising spending. Since television advertising time is purchased on the basis of projected future ratings, and ad costs have skyrocketed, there is increasingly pressure to forecast television ratings accurately. The forecasting methods that have been used in the past are not generally very reliable, and many have not been validated; also, even more distressingly, none have been tested in today’s multichannel environment. In this study we compare eight different forecasting models, ranging from a naive empirical method to a state-of-the-art Bayesian model-averaging method. Our data come from a recent time period, namely 2004–2008, in a market with over 70 channels, making the data more typical of today’s viewing environment. The simple models that are commonly used in industry do not forecast as well as any econometric models. Furthermore, time series methods are not applicable, as many programs are broadcast only once. However, we find that a relatively straightforward random effects regression model often performs as well as more sophisticated Bayesian models in out-of-sample forecasting. Finally, we demonstrate that making improvements in ratings forecasts could save the television industry between $250 and $586 million per year.",0
https://doi.org/10.1002/sim.6062,A multivariate multilevel Gaussian model with a mixed effects structure in the mean and covariance part,"A traditional Gaussian hierarchical model assumes a nested multilevel structure for the mean and a constant variance at each level. We propose a Bayesian multivariate multilevel factor model that assumes a multilevel structure for both the mean and the covariance matrix. That is, in addition to a multilevel structure for the mean we also assume that the covariance matrix depends on covariates and random effects. This allows to explore whether the covariance structure depends on the values of the higher levels and as such models heterogeneity in the variances and correlation structure of the multivariate outcome across the higher level values. The approach is applied to the three-dimensional vector of burnout measurements collected on nurses in a large European study to answer the research question whether the covariance matrix of the outcomes depends on recorded system-level features in the organization of nursing care, but also on not-recorded factors that vary with countries, hospitals, and nursing units. Simulations illustrate the performance of our modeling approach.",0
https://doi.org/10.3758/s13428-013-0368-4,Tracking of nociceptive thresholds using adaptive psychophysical methods,"Psychophysical thresholds reflect the state of the underlying nociceptive mechanisms. For example, noxious events can activate endogenous analgesic mechanisms that increase the nociceptive threshold. Therefore, tracking thresholds over time facilitates the investigation of the dynamics of these underlying mechanisms. Threshold tracking techniques should use efficient methods for stimulus selection and threshold estimation. This study compares, in simulation and in human psychophysical experiments, the performance of different combinations of adaptive stimulus selection procedures and threshold estimation methods. Monte Carlo simulations were first performed to compare the bias and precision of threshold estimates produced by three different stimulus selection procedures (simple staircase, random staircase, and minimum entropy procedure) and two estimation methods (logistic regression and Bayesian estimation). Logistic regression and Bayesian estimations resulted in similar precision only when the prior probability distributions (PDs) were chosen appropriately. The minimum entropy and simple staircase procedures achieved the highest precision, while the random staircase procedure was the least sensitive to different procedure-specific settings. Next, the simple staircase and random staircase procedures, in combination with logistic regression, were compared in a human subject study (n = 30). Electrocutaneous stimulation was used to track the nociceptive perception threshold before, during, and after a cold pressor task, which served as the conditioning stimulus. With both procedures, habituation was detected, as well as changes induced by the conditioning stimulus. However, the random staircase procedure achieved a higher precision. We recommend using the random staircase over the simple staircase procedure, in combination with logistic regression, for nonstationary threshold tracking experiments. © 2013 Psychonomic Society, Inc.",0
https://doi.org/10.1002/sim.4040,Random effects meta-analysis of event outcome in the framework of the generalized linear mixed model with applications in sparse data,"We consider random effects meta-analysis where the outcome variable is the occurrence of some event of interest. The data structures handled are where one has one or more groups in each study, and in each group either the number of subjects with and without the event, or the number of events and the total duration of follow-up is available. Traditionally, the meta-analysis follows the summary measures approach based on the estimates of the outcome measure(s) and the corresponding standard error(s). This approach assumes an approximate normal within-study likelihood and treats the standard errors as known. This approach has several potential disadvantages, such as not accounting for the standard errors being estimated, not accounting for correlation between the estimate and the standard error, the use of an (arbitrary) continuity correction in case of zero events, and the normal approximation being bad in studies with few events. We show that these problems can be overcome in most cases occurring in practice by replacing the approximate normal within-study likelihood by the appropriate exact likelihood. This leads to a generalized linear mixed model that can be fitted in standard statistical software. For instance, in the case of odds ratio meta-analysis, one can use the non-central hypergeometric distribution likelihood leading to mixed-effects conditional logistic regression. For incidence rate ratio meta-analysis, it leads to random effects logistic regression with an offset variable. We also present bivariate and multivariate extensions. We present a number of examples, especially with rare events, among which an example of network meta-analysis.",0
https://doi.org/10.1037/met0000078,On the unnecessary ubiquity of hierarchical linear modeling.,"In psychology and the behavioral sciences generally, the use of the hierarchical linear model (HLM) and its extensions for discrete outcomes are popular methods for modeling clustered data. HLM and its discrete outcome extensions, however, are certainly not the only methods available to model clustered data. Although other methods exist and are widely implemented in other disciplines, it seems that psychologists have yet to consider these methods in substantive studies. This article compares and contrasts HLM with alternative methods including generalized estimating equations and cluster-robust standard errors. These alternative methods do not model random effects and thus make a smaller number of assumptions and are interpreted identically to single-level methods with the benefit that estimates are adjusted to reflect clustering of observations. Situations where these alternative methods may be advantageous are discussed including research questions where random effects are and are not required, when random effects can change the interpretation of regression coefficients, challenges of modeling with random effects with discrete outcomes, and examples of published psychology articles that use HLM that may have benefitted from using alternative methods. Illustrative examples are provided and discussed to demonstrate the advantages of the alternative methods and also when HLM would be the preferred method. (PsycINFO Database Record",0
https://doi.org/10.1177/0146621606292213,IRT Model Selection Methods for Dichotomous Items,"Fit of the model to the data is important if the benefits of item response theory (IRT) are to be obtained. In this study, the authors compared model selection results using the likelihood ratio test, two information-based criteria, and two Bayesian methods. An example illustrated the potential for inconsistency in model selection depending on which of the indices was used. Results from a simulation study indicated that the inconsistencies among the indices were common but that model selection was relatively accurate for longer tests administered to larger sample of examinees. The cross-validation log-likelihood (CVLL) appeared to work the best of the five models for the conditions simulated in this study.",0
https://doi.org/10.1037/a0037011,A dynamic Thurstonian item response theory of motive expression in the picture story exercise: Solving the internal consistency paradox of the PSE.,"The measurement of implicit or unconscious motives using the picture story exercise (PSE) has long been a target of debate in the psychological literature. Most debates have centered on the apparent paradox that PSE measures of implicit motives typically show low internal consistency reliability on common indices like Cronbach's alpha but nevertheless predict behavioral outcomes. I describe a dynamic Thurstonian item response theory (IRT) model that builds on dynamic system theories of motivation, theorizing on the PSE response process, and recent advancements in Thurstonian IRT modeling of choice data. To assess the models' capability to explain the internal consistency paradox, I first fitted the model to archival data (Gurin, Veroff, & Feld, 1957) and then simulated data based on bias-corrected model estimates from the real data. Simulation results revealed that the average squared correlation reliability for the motives in the Thurstonian IRT model was .74 and that Cronbach's alpha values were similar to the real data (<.35). These findings suggest that PSE motive measures have long been reliable and increase the scientific value of extant evidence from motivational research using PSE motive measures.",0
https://doi.org/10.1207/s15327906mbr3901_4,Confidence Limits for the Indirect Effect: Distribution of the Product and Resampling Methods,"The most commonly used method to test an indirect effect is to divide the estimate of the indirect effect by its standard error and compare the resulting z statistic with a critical value from the standard normal distribution. Confidence limits for the indirect effect are also typically based on critical values from the standard normal distribution. This article uses a simulation study to demonstrate that confidence limits are imbalanced because the distribution of the indirect effect is normal only in special cases. Two alternatives for improving the performance of confidence limits for the indirect effect are evaluated: (a) a method based on the distribution of the product of two normal random variables, and (b) resampling methods. In Study 1, confidence limits based on the distribution of the product are more accurate than methods based on an assumed normal distribution but confidence limits are still imbalanced. Study 2 demonstrates that more accurate confidence limits are obtained using resampling methods, with the bias-corrected bootstrap the best method overall.",0
https://doi.org/10.3758/bf03195458,A framework for ML estimation of parameters of (mixtures of) common reaction time distributions given optional truncation or censoring,"We present a framework for distributional reaction time (RT) analysis, based on maximum likelihood (ML) estimation. Given certain information relating to chosen distribution functions, one can estimate the parameters of these distributions and of finite mixtures of these distributions. In addition, left and/or right censoring or truncation may be imposed. Censoring and truncation are useful methods by which to accommodate outlying observations, which are a pervasive problem in RT research. We consider five RT distributions: the Weibull, the ex-Gaussian, the gamma, the log-normal, and the Wald. We employ quasi-Newton optimization to obtain ML estimates. Multicase distributional analyses can be carried out, which enable one to conduct detailed (across or within subjects) comparisons of RT data by means of loglikelihood difference tests. Parameters may be freely estimated, estimated subject to boundary constraints, constrained to be equal (within or over cases), or fixed. To demonstrate the feasibility of ML estimation and to illustrate some of the possibilities offered by the present approach, we present three small simulation studies. In addition, we present three illustrative analyses of real data.",0
,Assessment of auditory temporal-order thresholds - a comparison of different measurement procedures and the influences of age and gender.,"The relationship between auditory temporal-order perception and phoneme discrimination has been discussed for several years, based on findings, showing that patients with cerebral damage in the left hemisphere and aphasia, as well as children with specific language impairments, show deficits in temporal-processing and phoneme discrimination. Over the last years several temporal-order measurement procedures and training batteries have been developed. However, there exists no standard diagnostic tool for adults that could be applied to patients with aphasia. Therefore, our study aimed at identifying a feasible, reliable and efficient measurement procedure to test for auditory-temporal processing in healthy young and elderly adults, which in a further step can be applied to patients with aphasia.The tasks varied according to adaptive procedures (staircase vs. maximum-likelihood), stimuli (tones vs. clicks) and stimulation modes (binaural- vs. alternating monaural) respectively. A phoneme-discrimination task was also employed to assess the relationship between temporal and language processing.The results show that auditory temporal-order thresholds are stimulus dependent, age related, and influenced by gender. Furthermore, the cited relationship between temporal-order threshold and phoneme discrimination can only be confirmed for measurements with pairs of tones.Our results indicate, that different norms have to be established for different gender and age groups. Furthermore, temporal-order measurements with tones seem to be more suitable for clinical intervention studies than measurements with clicks, as they show higher re-test reliabilities, and only for measurements with tones an association with phoneme-discrimination abilities was found.",0
https://doi.org/10.1037/0021-9010.92.2.297,Updating meta-analytic research findings: Bayesian approaches versus the medical model.,"The authors examine 3 methods of combining new studies into existing meta-analyses: (a) adding the new study or studies to the database and recalculating the meta-analysis (the medical model); (b) using the Bayesian procedure advocated by F. L. Schmidt and J. E. Hunter (1977) and F. L. Schmidt, J. E. Hunter, K. Pearlman, and G. S. Shane (1979) to update the meta-analysis; and (c) using the Bayesian methods advocated by these authors and M. T. Brannick (2001) and M. T. Brannick, S. M. Hall, and Y. Liu (2002) to estimate study-specific parameters. Method b was found to severely overweight new studies relative to the previous studies contained in the meta-analysis, and Method c was found to do the same while also requiring an assumption with a low prior probability of being correct, causing the method to violate Bayesian principles. The authors present an alternative Bayesian procedure that does not suffer from these drawbacks and yields meta-analytic results very similar to those obtained with the medical model. They recommend use of the medical model or this alternative Bayesian procedure.",0
https://doi.org/10.1177/1094428107303155,Psychometric Accuracy and (the Continuing Need for) Quality Thinking in Meta-Analysis,"The four feature-topic articles advance the accuracy of meta-analytic techniques. As might be expected, most articles focus on more precise ways to aggregate all available relationships in a given topic domain. Such aggregation results in overall estimates of relationships and estimates conditional on particular moderators. However, scholars and practitioners can expect more than empirical aggregation from meta-analysis. Careful attention to underlying theory, application, and important methodological issues will result in clearer understanding and explanation. Using the situational judgment testing literature as an example, the current analysis suggests the need for much more upfront, reflective thinking about each meta-analytic study's purpose and how this thinking relates to the inclusion or choice of primary studies, analytic method, coding, and so on. For example, when the focus is on prediction, frequent use of concurrent designs may bias aggregated parameter estimates. Also, it is noted that methods and constructs continue to be confounded in the research literatures.",0
,Giving up Linearity : Absorptive Capacity and Performance,"INTRODUCTION Absorptive capacity (ACAP) is defined as one of the firm's key learning processes with regard to identifying, assimilating, and exploiting knowledge from the environment (Cohen and Levinthal, 1989; Lane et al., 2006; Lane and Lubatkin, 1998). This process arises from the seminal work of Cohen and Levinthal (1989, 1990, and 1994) and enables firms to profit from external knowledge. ACAP has received broad attention in research as innovative capabilities have become more and more important in sustaining a competitive advantage (Bower and Christensen, 1995; Eisenhardt and Martin, 2000; Helfat, 2000; Li and Atuahene-Gima, 2001) and, particularly, as innovation from external sources is on the rise (Jansen et al., 2005; Zollo et al., 2002). Even though the construct of ACAP has been studied widely during the past two decades (Easterby-Smith et al., 2008; Lane et al., 2006), there have not been any findings yet on curvilinear relationships between ACAP and performance. The existing literature has provided considerable empirical work on ACAP and its relationship to innovation (Tsai, 2001), interorganizational learning (Lane and Lubatkin, 1998; Lane et al., 2001; Lichtenthaler, 2009), intra-organizational transfer of knowledge (Gupta et al., 2006; Szulanski, 1996), and firm performance (Lane et al., 2001; Lichtenthaler, 2009; Tsai, 2001). However, in its analysis of the relationship between ACAP and a second variable, current research has relied primarily on the assumption of linearity and has not taken into account the extent to which the variables are developed. Nevertheless, non-linear relationships could exist in many cases, e.g., when a dependent variable develops positively to a certain optimum with respect to an independent variable and then decreases after having reached this optimal point (Ahuja and Lampert, 2001; Tang et al., 2008). Curvilinear relationships between ACAP and a certain dependent variable could, for example, reflect decreasing or even negative marginal growth. Thus, by assuming a linear relationship, recent research may have neglected U-shaped or inverted U-shaped relationships and, therefore, may have led practitioners to nonoptimal resource allocations--maybe even to the point of decreasing marginal growth (Herold et al., 2006). To put it more concretely, as the development of ACAP requires resources, it would be entirely possible that the positive effects of ACAP do not outweigh their respective investments at a certain level of development. Although some scholars engaged in ACAP research have pointed out the possibility of meaningful curvilinear relationships (Lane et al., 2006; Lichtenthaler, 2009), no empirical studies of ACAP and performance have examined this phenomenon explicitly. This deficiency in the field of ACAP research is rooted in two causes. First, recent research on ACAP has usually built upon linear structural equation models (e.g., Flatten et al., 2009, 2011; Lichtenthaler, 2009; Jansen et al., 2005). However, even though these second-generation methods (e.g., Lisrel and PLS) offer a wide range of advantages (e.g., depiction of complex models, evaluation of models, capture of measurement errors) compared to the first generation (Fornell and Bookstein, 1982), curvilinear effects can be pictured in a limited way only (Lee et al., 2004). In the past, most researchers captured ACAP via research and development (RD Szulanski, 1996; Therin, 2007). Now that validated operationalizations of ACAP (Flatten et al., 2009; Lichtenthaler, 2009) have been published, a basis for further understanding ACAP and its effect on performance has been established. The purpose of this paper is to address the described research gap and to contribute to existing literature in the following way. …",0
https://doi.org/10.3102/1076998611432173,Modeling Differential Item Functioning Using a Generalization of the Multiple-Group Bifactor Model,"The authors present a generalization of the multiple-group bifactor model that extends the classical bifactor model for categorical outcomes by relaxing the typical assumption of independence of the specific dimensions. In addition to the means and variances of all dimensions, the correlations among the specific dimensions are allowed to differ between groups. By including group-specific difficulty parameters, the model can be used to assess differential item functioning (DIF) for testlet-based tests. The model encompasses various item response models for polytomous data by allowing for different link functions, and it includes testlet and second-order models as special cases. Importantly, by assuming that the testlet dimensions are conditionally independent given the general dimension, the authors show, using a graphical model framework, that the integration over all latent variables can be carried out through a sequence of computations in two-dimensional subspaces, making full-information maximum likelihood estimation feasible for high-dimensional problems and large datasets. The importance of relaxing the orthogonality assumption and allowing for a different covariance structure of the dimensions for each group is demonstrated in the context of the assessment of DIF. Through a simulation study, it is shown that ignoring between-group differences in the structure of the multivariate latent space can result in substantially biased estimates of DIF.",0
https://doi.org/10.1002/0470857005.ch1,Optimal Design in Educational Testing,,0
https://doi.org/10.1037/1082-989x.12.3.298,Hierarchical modeling of sequential behavioral data: Examining complex association patterns in mediation models.,"This article presents new methods for modeling the strength of association between multiple behaviors in a behavioral sequence, particularly those involving substantively important interaction patterns. Modeling and identifying such interaction patterns becomes more complex when behaviors are assigned to more than two categories, as is the case for most observational research. The authors propose multilevel empirical Bayes methods to overcome the challenges inherent in such data. Furthermore, these methods allow the study of how variation in interaction patterns can mediate the effects of antecedents or intervention on distal outcomes. New procedures are developed to compare alternative mediation models and pinpoint which random effects operate as mediators. These models are then applied to observational data taken from a study of the behavioral interactions of 254 couples.",0
https://doi.org/10.1016/j.ecolmodel.2012.08.024,Bayesian hierarchical modeling of Pacific geoduck growth increment data and climate indices,"Growth increment widths from hard structures of marine and freshwater fish and bivalve species are increasingly used to model growth and elucidate relationships with environmental variability. Fully characterizing the intrinsic age-related growth variation among individuals within and between populations, while estimating the extrinsic environmental effects simultaneously, can be challenging. Using the long-lived bivalve Pacific geoduck (Panopea generosa), we develop an integrated approach to analyze the relationship between growth increment data and climate indices using Bayesian hierarchical methods. Fitting models to growth increment data from multiple individuals over two sites, we examined different covariance structures related to random individual effects, long- and short-term environmental effects and unexplained errors. The best fitting hierarchical model accounted for a site-specific mean growth response, individual growth variability through random parameter effects, and site-specific error variances. Extrinsic environmental effects on growth were also significant and included a random year effect and the Pacific Decadal Oscillation (PDO) as a predictor of mean growth across both individuals and sites. Once intrinsic age-related growth was accounted for, PDO accounted for 18% to total variability in growth increment data; geoduck shell size was predicted to increase as a function of larger PDO anomalies. However, the greatest variability in growth increment data was explained by random year effects (∼60–70%), and while largely unexplained, sea surface temperature (SST) is a likely determinant on geoduck growth rates showing a positive growth–SST response.",0
https://doi.org/10.1016/j.jecp.2014.02.001,Explaining numeracy development in weak performing kindergartners,"• 990 Children were screened for weak early numeracy at the start of kindergarten. • Early numeracy development was monitored at four points throughout kindergarten. • Growth was predicted by several predictors together in one multilevel model. • Math language is underestimated as important factor in numeracy development. Gaining better insight into precursors of early numeracy in young children is important, especially in those with inadequate numeracy skills. Therefore, in the current study, visual and verbal working memory, non-symbolic and symbolic comparison skills, and specific math-related language were used to explain early numeracy performance and development of weak performing children throughout kindergarten. The early numeracy ability of both weak performers and typical performers was measured at four time points during 2 years of kindergarten to compare growth rates. Results show a significant faster development of early numeracy in the weak performers. The development of weak performers’ numeracy was influenced by verbal working memory, symbolic comparison skills, and math language, whereas only math language was positively related to the slope of typical performers’ numeracy. In the weak performers, visual working memory, non-symbolic comparison skills, and math language showed an effect on the initial early numeracy level of these children. The intercept of the typical performers was predicted by five covariates, all except non-symbolic comparison.",0
https://doi.org/10.1177/0022022113492892,Cultural Variation in the Minimal Group Effect,"The minimal group effect (MGE) is one of the most robust psychological findings in studies of intergroup conflict, yet there is little evidence comparing its magnitude across cultures. Recent evidence suggests that the MGE is due in part to a projection of one’s own perceived characteristics onto the novel in-group. Because of cultural variability in self-enhancement motivations, we thus expected that those from East Asian cultures would exhibit a diminished MGE relative to Westerners. A large and diverse sample of Japanese and American participants completed a traditional minimal group study. American participants were more likely to show an in-group bias in group identification, perceived group intelligence, perceived group personality traits, and resource allocation. Furthermore, these cultural differences were partially mediated by self-esteem. We discuss the implication of these findings for theories of intergroup conflict and suggest multiple directions for future cross-cultural research on the MGE.",0
https://doi.org/10.1016/j.artmed.2012.12.007,Multilevel Bayesian networks for the analysis of hierarchical health care data,"Large health care datasets normally have a hierarchical structure, in terms of levels, as the data have been obtained from different practices, hospitals, or regions. Multilevel regression is the technique commonly used to deal with such multilevel data. However, for the statistical analysis of interactions between entities from a domain, multilevel regression yields little to no insight. While Bayesian networks have proved to be useful for analysis of interactions, they do not have the capability to deal with hierarchical data. In this paper, we describe a new formalism, which we call multilevel Bayesian networks; its effectiveness for the analysis of hierarchically structured health care data is studied from the perspective of multimorbidity.Multilevel Bayesian networks are formally defined and applied to analyze clinical data from family practices in The Netherlands with the aim to predict interactions between heart failure and diabetes mellitus. We compare the results obtained with multilevel regression.The results obtained by multilevel Bayesian networks closely resembled those obtained by multilevel regression. For both diseases, the area under the curve of the prediction model improved, and the net reclassification improvements were significantly positive. In addition, the models offered considerable more insight, through its internal structure, into the interactions between the diseases.Multilevel Bayesian networks offer a suitable alternative to multilevel regression when analyzing hierarchical health care data. They provide more insight into the interactions between multiple diseases. Moreover, a multilevel Bayesian network model can be used for the prediction of the occurrence of multiple diseases, even when some of the predictors are unknown, which is typically the case in medicine.",0
https://doi.org/10.1093/arclin/acv033,The Mediating Role of Visuospatial Planning Skills on Adaptive Function Among Young–Adult Survivors of Childhood Brain Tumor,"The Boston Qualitative Scoring System (BQSS) was used as a method to examine executive skills on the Rey-Osterrieth complex figure (ROCF). Young-adult survivors of childhood brain tumor (N = 31) and a demographically-matched comparison group (N = 33) completed the ROCF copy version and Grooved Pegboard, and informants were administered the Scales of Independent Behavior-Revised (SIB-R) and Behavior Rating Inventory of Executive Function (BRIEF). Survivors had significantly lower BQSS planning and SIB-R community living skills and greater perseveration. Mediation analyses found that BQSS planning skills mediate the relationship between group and community living skills. Convergent findings of the BRIEF Planning, and discriminant findings with the BQSS Fragmentation, BRIEF Emotional Control, and Grooved Pegboard support the planning construct as the specific mediator in this model. Together, these findings highlight the role of planning skills in adaptive functions of young-adult survivors of childhood brain tumor.",0
https://doi.org/10.17485/ijst/2015/v8i15/70774,An Empirical Study to Develop a Decision Support System (DSS) for Measuring the Impact of Quality Measurements over Agile Software Development (ASD),"Background/Objectives: Primarily, this quantitative research aims to study the impact of integrating quality measurements with ASD, quantify it, and develop a DSS for predicting its outcome. Methods/Statistical Analysis: Included within a survey, the population sample is represented by project managers, who were divided into two independent groups: The first one adopts an explicit quality measurement framework while the second group does not apply quality measurements. After that, the researcher tested both groups in an independent samples t-test, and analysed results statistically. After experimenting different machine learning models, the researcher developed a DSS based on Linear Regression. Findings: Only 150 responded out of 200 respondents. The research dataset passed the “independent t-test” validity test with the fulfilment of the six assumptions. After conducting the independent t-test design, the researcher found that the value of Sig. (2-tailed) is less than .05, which means that the differences between the experimented groups are statistically significant. After that, the researcher utilized WEKA experimenter with 10-folds cross validation to test the dataset fitness with four different machine learning algorithms, which are Linear Regression (base), Multilayer Perceptron, KStar, and Decision Stump. The results showed that Linear Regression (base) provides better fitness with the dataset. Moreover, The R Square for it is .836. Based on Linear Regression, the researcher developed web and windows version of the DSS using VB.NET. In summary, research results shows that there is empirical evidence to support the proposition that quality measurements integration with ASD presents a strategic value to organizations. The contribution of these findings is materialized in its empirical nature and the scariness of research in this domain. Application/Improvements: Henceforward, the researcher are planning to expand the population sample, publishing the developed DSS online with integrated feedback, and developing other DSSs for supporting integrating quality measurements with ASD.",0
https://doi.org/10.1080/16506073.2010.520731,Intraclass Correlation Associated with Therapists: Estimates and Applications in Planning Psychotherapy Research,"It is essential that outcome research permit clear conclusions to be drawn about the efficacy of interventions. The common practice of nesting therapists within conditions can pose important methodological challenges that affect interpretation, particularly if the study is not powered to account for the nested design. An obstacle to the optimal design of these studies is the lack of data about the intraclass correlation coefficient (ICC), which measures the statistical dependencies introduced by nesting. To begin the development of a public database of ICC estimates, the authors investigated ICCs for a variety outcomes reported in 20 psychotherapy outcome studies. The magnitude of the 495 ICC estimates varied widely across measures and studies. The authors provide recommendations regarding how to select and aggregate ICC estimates for power calculations and show how researchers can use ICC estimates to choose the number of patients and therapists that will optimize power. Attention to these recommendations will strengthen the validity of inferences drawn from psychotherapy studies that nest therapists within conditions.",0
https://doi.org/10.1016/j.jval.2011.01.011,Conducting Indirect-Treatment-Comparison and Network-Meta-Analysis Studies: Report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: Part 2,"Evidence-based health care decision making requires comparison of all relevant competing interventions. In the absence of randomized controlled trials involving a direct comparison of all treatments of interest, indirect treatment comparisons and network meta-analysis provide useful evidence for judiciously selecting the best treatment(s). Mixed treatment comparisons, a special case of network meta-analysis, combine direct evidence and indirect evidence for particular pairwise comparisons, thereby synthesizing a greater share of the available evidence than traditional meta-analysis. This report from the International Society for Pharmacoeconomics and Outcomes Research Indirect Treatment Comparisons Good Research Practices Task Force provides guidance on technical aspects of conducting network meta-analyses (our use of this term includes most methods that involve meta-analysis in the context of a network of evidence). We start with a discussion of strategies for developing networks of evidence. Next we briefly review assumptions of network meta-analysis. Then we focus on the statistical analysis of the data: objectives, models (fixed-effects and random-effects), frequentist versus Bayesian approaches, and model validation. A checklist highlights key components of network meta-analysis, and substantial examples illustrate indirect treatment comparisons (both frequentist and Bayesian approaches) and network meta-analysis. A further section discusses eight key areas for future research.",0
https://doi.org/10.1207/s15328007sem0802_2,Piecewise Growth Mixture Modeling of Adolescent Alcohol Use Data,"This article addresses issues of heterogeneity in multiple-stage development as it corresponds to qualitatively different development in alcohol use during adolescence. Using a piecewise growth mixture modeling methodology proposed by Muthen (in press), a 2-piece linear growth model capturing growth trajectories in adolescent alcohol use during the transition from middle school (ages 11 to 13) to high school (ages 14 to 17; N = 81) was examined. It was hypothesized that 2 stages of alcohol use development with varying trajectories would exist in these data, the 1st corresponding to development during middle school (Growth Rate 1), followed by a 2nd stage of continuing growth during high school (Growth Rate 2). Results suggested the tenability of the 2-piece linear development in alcohol use and the emergence of 2 latent classes with individually varying transition points. Class 1 showed linear increases only during high school, whereas Class 2 showed a continued, linear growth throughout the middle and hi...",0
https://doi.org/10.1007/bf02295640,Higher-order latent trait models for cognitive diagnosis,"Higher-order latent traits are proposed for specifying the joint distribution of binary attributes in models for cognitive diagnosis. This approach results in a parsimonious model for the joint distribution of a high-dimensional attribute vector that is natural in many situations when specific cognitive information is sought but a less informative item response model would be a reasonable alternative. This approach stems from viewing the attributes as the specific knowledge required for examination performance, and modeling these attributes as arising from a broadly-defined latent trait resembling theϑ of item response models. In this way a relatively simple model for the joint distribution of the attributes results, which is based on a plausible model for the relationship between general aptitude and specific knowledge. Markov chain Monte Carlo algorithms for parameter estimation are given for selected response distributions, and simulation results are presented to examine the performance of the algorithm as well as the sensitivity of classification to model misspecification. An analysis of fraction subtraction data is provided as an example.",0
https://doi.org/10.2307/1421337,The Measurement and Prediction of Judgment and Choice,,0
https://doi.org/10.1037/tra0000096,Applying Bayesian statistics to the study of psychological trauma: A suggestion for future research.,"Several contemporary researchers have noted the virtues of Bayesian methods of data analysis. Although debates continue about whether conventional or Bayesian statistics is the ""better"" approach for researchers in general, there are reasons why Bayesian methods may be well suited to the study of psychological trauma in particular. This article describes how Bayesian statistics offers practical solutions to the problems of data non-normality, small sample size, and missing data common in research on psychological trauma.After a discussion of these problems and the effects they have on trauma research, this article explains the basic philosophical and statistical foundations of Bayesian statistics and how it provides solutions to these problems using an applied example.Results of the literature review and the accompanying example indicates the utility of Bayesian statistics in addressing problems common in trauma research.Bayesian statistics provides a set of methodological tools and a broader philosophical framework that is useful for trauma researchers. Methodological resources are also provided so that interested readers can learn more.",0
https://doi.org/10.1214/ss/1177011136,Inference from Iterative Simulation Using Multiple Sequences,"The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.",0
https://doi.org/10.1016/j.cognition.2015.10.005,Organization principles in visual working memory: Evidence from sequential stimulus display,"Although the mechanisms of visual working memory (VWM) have been studied extensively in recent years, the active property of VWM has received less attention. In the current study, we examined how VWM integrates sequentially presented stimuli by focusing on the role of Gestalt principles, which are important organizing principles in perceptual integration. We manipulated the level of Gestalt cues among three or four sequentially presented objects that were memorized. The Gestalt principle could not emerge unless all the objects appeared together. We distinguished two hypotheses: a perception-alike hypothesis and an encoding-specificity hypothesis. The former predicts that the Gestalt cue will play a role in information integration within VWM; the latter predicts that the Gestalt cue will not operate within VWM. In four experiments, we demonstrated that collinearity (Experiment 1) and closure (Experiment 2) cues significantly improved VWM performance, and this facilitation was not affected by the testing manner (Experiment 3) or by adding extra colors to the memorized objects (Experiment 4). Finally, we re-established the Gestalt cue benefit with similarity cues (Experiment 5). These findings together suggest that VWM realizes and uses potential Gestalt principles within the stored representations, supporting a perception-alike hypothesis.",0
https://doi.org/10.1163/156856897x00159,Efficient estimation of sensory thresholds with ML-PEST,"A set of C and C+2 routines are described that allow the efficient estimation of sensory thresholds in psychophysical experiments using a maximum-likelihood staircase procedure. They have been used effectively in visual, auditory, gustatory, and olfactory psychophysics.",0
https://doi.org/10.1287/mksc.1040.0088,Dynamic Models Incorporating Individual Heterogeneity: Utility Evolution in Conjoint Analysis,"It has been shown in the behavioral decision making, marketing research, and psychometric literature that the structure underlying preferences can change during the administration of repeated measurements (e.g., conjoint analysis) and data collection because of effects from learning, fatigue, boredom, and so on. In this research note, we propose a new class of hierarchical dynamic Bayesian models for capturing such dynamic effects in conjoint applications, which extend the standard hierarchical Bayesian random effects and existing dynamic Bayesian models by allowing for individual-level heterogeneity around an aggregate dynamic trend. Using simulated conjoint data, we explore the performance of these new dynamic models, incorporating individual-level heterogeneity across a number of possible types of dynamic effects, and demonstrate the derived benefits versus static models. In addition, we introduce the idea of an unbiased dynamic estimate, and demonstrate that using a counterbalanced design is important from an estimation perspective when parameter dynamics are present.",0
https://doi.org/10.1198/jasa.2010.tm09757,Estimability and Likelihood Inference for Generalized Linear Mixed Models Using Data Cloning,"Maximum likelihood estimation for Generalized Linear Mixed Models (GLMM), an important class of statistical models with substantial applications in epidemiology, medical statistics, and many other fields, poses significant computational difficulties. In this article, we use data cloning, a simple computational method that exploits advances in Bayesian computation, in particular the Markov Chain Monte Carlo method, to obtain maximum likelihood estimators of the parameters in these models. This method also leads to a simple estimator of the asymptotic variance of the maximum likelihood estimators. Determining estimability of the parameters in a mixed model is, in general, a very difficult problem. Data cloning provides a simple graphical test to not only check if the full set of parameters is estimable but also, and perhaps more importantly, if a specified function of the parameters is estimable. One of the goals of mixed models is to predict random effects. We suggest a frequentist method to obtain predict...",0
,R2WinBUGS: A Package for Running WinBUGS from R,"The R2WinBUGS package provides convenient functions to call WinBUGS from R. It automatically writes the data and scripts in a format readable by WinBUGS for processing in batch mode, which is possible since version 1.4. After the WinBUGS process has finished, it is possible either to read the resulting data into R by the package itself-which gives a compact graphical summary of inference and convergence diagnostics-or to use the facilities of the coda package for further analyses of the output. Examples are given to demonstrate the usage of this package.",0
https://doi.org/10.1348/000711008x292343,Non-linear structural equation models with correlated continuous and discrete data,"Structural equation models (SEMs) have been widely applied to examine interrelationships among latent and observed variables in social and psychological research. Motivated by the fact that correlated discrete variables are frequently encountered in practical applications, a non-linear SEM that accommodates covariates, and mixed continuous, ordered, and unordered categorical variables is proposed. Maximum likelihood methods for estimation and model comparison are discussed. One real-life data set about cardiovascular disease is used to illustrate the methodologies.",0
https://doi.org/10.1016/j.stamet.2012.08.003,Bayesian predictive inference of a finite population proportion under selection bias,"Abstract We show how to infer about a finite population proportion using data from a possibly biased sample. In the absence of any selection bias or survey weights, a simple ignorable selection model, which assumes that the binary responses are independent and identically distributed Bernoulli random variables, is not unreasonable. However, this ignorable selection model is inappropriate when there is a selection bias in the sample. We assume that the survey weights (or their reciprocals which we call ‘selection’ probabilities) are available, but there is no simple relation between the binary responses and the selection probabilities. To capture the selection bias, we assume that there is some correlation between the binary responses and the selection probabilities (e.g., there may be a somewhat higher/lower proportion of positive responses among the sampled units than among the nonsampled units). We use a Bayesian nonignorable selection model to accommodate the selection mechanism. We use Markov chain Monte Carlo methods to fit the nonignorable selection model. We illustrate our method using numerical examples obtained from NHIS 1995 data.",0
https://doi.org/10.1002/cjs.5550340302,Conservative prior distributions for variance parameters in hierarchical models,"Bayesian hierarchical models typically involve specifying prior distributions for one or more variance components. This is rather removed from the observed data, so specification based on expert knowledge can be difficult. While there are suggestions for default priors in the literature, often a condi tionally conjugate inverse-gamma specification is used, despite documented drawbacks of this choice. The authors suggest conservative prior distributions for variance components, which deliberately give more weight to smaller values. These are appropriate for investigators who are skeptical about the presence of variability in the second-stage parameters (random effects) and want to particularly guard against inferring more structure than is really present. The suggested priors readily adapt to various hierarchical modelling settings, such as fitting smooth curves, modelling spatial variation and combining data from multiple sites. Lois a priori pour les parametres de variance de modeles hierarchiques Rgsum6: Les modeles bay6siens hierarchiques comportent g6n6ralement une ou des composantes de va riance que l'on doit doter de lois a priori. Le choix de ces lois est delicat car la variation est un aspect des donn6es difficile a cemer. De toutes les lois a priori par defaut, une loi conjuguee inverse-gamma con ditionnelle est la plus souvent employ6e, malgr6 ses inconvenients. Les auteurs proposent des lois a priori conservatrices pour les composantes de la variance qui privilegient les petites valeurs. Elles conviennent bien aux situations oiu le chercheur s'interroge sur la presence r6elle de variabilit6 dans les parametres de deuxieme degre (effets aleatoires) et qu'il veut eviter d'imposer une structure artificielle. Les lois a priori sugg6rdes s'adaptent A diverses situations propices a la mod6lisation hierarchique, notamment l'ajustement de courbes lisses et la modelisation de variation spatiale ou de donn6es issues de nombreux sites.",0
https://doi.org/10.1007/s10071-012-0582-y,The psychophysics of sugar concentration discrimination and contrast evaluation in bumblebees,"The capacity to discriminate between choice options is crucial for a decision-maker to avoid unprofitable options. The physical properties of rewards are presumed to be represented on context-dependent, nonlinear cognitive scales that may systematically influence reward expectation and thus choice behavior. In this study, we investigated the discrimination performance of free-flying bumblebee workers (Bombus impatiens) in a choice between sucrose solutions with different concentrations. We conducted two-alternative free choice experiments on two B. impatiens colonies containing some electronically tagged bumblebees foraging at an array of computer-automated artificial flowers that recorded individual choices. We mimicked natural foraging conditions by allowing uncertainty in the probability of reward delivery while maintaining certainty in reward concentration. We used a Bayesian approach to fit psychometric functions, relating the strength of preference for the higher concentration option to the relative intensity of the presented stimuli. Psychometric analysis was performed on visitation data from individually marked bumblebees and pooled data from unmarked individuals. Bumblebees preferred the more concentrated sugar solutions at high stimulus intensities and showed no preference at low stimulus intensities. The obtained psychometric function is consistent with reward evaluation based on perceived concentration contrast between choices. We found no evidence that bumblebees reduce reward expectations upon experiencing non-rewarded visits. We compare psychometric function parameters between the bumblebee B. impatiens and the flower bat Glossophaga commissarisi and discuss the relevance of psychophysics for pollinator-exerted selection pressures on plants.",0
https://doi.org/10.1111/1467-9892.00222,A Hierarchical Approach to Covariance Function Estimation for Time Series,"The covariance function in time series models is typically modelled via a parametric family. This ensures straightforward best linear prediction while maintaining positive-definiteness of the covariance function. We suggest an alternative approach, which will result in data-determined shrinkage towards this parametric model. Positive-definiteness is maintained by carrying out the shrinkage in the spectral domain. We offer both a fully Bayesian hierarchical approach and an approximate hierarchical approach that will be much simpler computationally. These are implemented on the frequently analysed Canadian lynx data and compared to other models that have been fitted to these data.",0
https://doi.org/10.1016/j.cognition.2011.11.014,Temporal expectation and information processing: A model-based analysis,"People are able to use temporal cues to anticipate the timing of an event, enabling them to process that event more efficiently. We conducted two experiments, using the fixed-foreperiod paradigm (Experiment 1) and the temporal-cueing paradigm (Experiment 2), to assess which components of information processing are speeded when subjects use such temporal cues to predict the onset of a target stimulus. We analyzed the observed temporal expectation effects on task performance using sequential-sampling models of decision making: the Ratcliff diffusion model and the shifted-Wald model. The results from the two experiments were consistent: temporal expectation affected the duration of nondecision processes (target encoding and/or response preparation) but had little effect on the two main components of the decision process: response-threshold setting and the rate of evidence accumulation. Our findings provide novel evidence about the psychological processes underlying temporal-expectation effects on reaction time.",0
https://doi.org/10.1007/bf01065485,Latent variable growth within behavior genetic models,"The purpose of this paper is to introduce one kind of latent-variable structural-equation model for multivariate longitudinal data which includes behavioral genetic components. A generic structural-equation model termed RAM (McArdle, J. J. and McDonald, R. P. (1984). Br. J. Math. Stat. Psychol., 37:239-251.) is used to define the univariate twin design, including both covariances and means. This model is extended to multivariate form using a latent-variable growth-curve model recently presented by W. Meredith and J. Tisak [(1984). ""Tuckerizing"" curves. Psychometric Society Annual Meetings]. The model presented herein further permits hypothesis testing of various biometric models of the sources of these individual differences in latent growth. Aspects of this model are illustrated using the LISREL algorithm [JÃƒÂ¶reskog, K. G. and SÃƒÂ¶rbom, D. (1979). Advances in Factor Analysis and Structural Equation Models, Abt Books, Cambridge, Mass.] and longitudinal twin data on early childhood abilities [Wilson, R. S. (1983). Child Dev.54:298-316]. Ã‚Â© 1986 Plenum Publishing Corporation.",0
https://doi.org/10.1007/s11135-007-9133-z,Evaluating estimation methods for ordinal data in structural equation modeling,"This study examined the performance of two alternative estimation approaches in structural equation modeling for ordinal data under different levels of model misspecification, score skewness, sample size, and model size. Both approaches involve analyzing a polychoric correlation matrix as well as adjusting standard error estimates and model chi-squared, but one estimates model parameters with maximum likelihood and the other with robust weighted least-squared. Relative bias in parameter estimates and standard error estimates, Type I error rate, and empirical power of the model test, where appropriate, were evaluated through Monte Carlo simulations. These alternative approaches generally provided unbiased parameter estimates when the model was correctly specified. They also provided unbiased standard error estimates and adequate Type I error control in general unless sample size was small and the measured variables were moderately skewed. Differences between the methods in convergence problems and the evaluation criteria, especially under small sample and skewed variable conditions, were discussed. Â© 2007 Springer Science + Business Media B.V.",0
https://doi.org/10.1111/j.1745-3984.2008.00063.x,An Odds Ratio Approach for Assessing Differential Distractor Functioning Effects under the Nominal Response Model,"Investigations of differential distractor functioning (DDF) can provide valuable information concerning the location and possible causes of measurement invariance within a multiple-choice item. In this article, I propose an odds ratio estimator of the DDF effect as modeled under the nominal response model. In addition, I propose a simultaneous distractor-level (SDL) test of invariance based on the results of the distractor-level tests of DDF. The results of a simulation study indicated that the DDF effect estimator maintained good statistical properties under a variety of conditions, and the SDL test displayed substantially higher power than the traditional Mantel-Haenszel test of no DIF when the DDF effect varied in magnitude and/or size across the distractors.",0
https://doi.org/10.1016/j.fishres.2014.02.036,Accounting for vessel effects when standardizing catch rates from cooperative surveys,"Interpretation of fishery-dependent and independent-survey data requires accounting for changes in the proportion of local individuals that are caught by fishing gear (“catchability”). Catchability may be influenced by measured characteristics of fishing gear, and even standardized fishing techniques may experience changing catchability over time due to changes in fishing vessel characteristics and personnel. The importance of vessel power has long been recognized in the analysis of fishery dependent catch per unit effort data, but less-studied in the analysis of fishery independent data collected by research vessel surveys. Here we demonstrate how differences in catchability among vessels (“vessel effects”), as well as random variation in vessel-specific catchability over time (“vessel-year effects”) can be incorporated into generalized linear mixed models through their treatment as random effects. We apply these methods to data for 28 groundfish species caught in a standardized survey using contracted fishery vessels and personnel in the Northeast Pacific. Model selection shows that vessel, vessel-year, and both effects simultaneously are supported by available data for at least a few species. However, vessel-year effects generally have a larger effect on catch rates than vessel-effects and hence abundance indices estimated using both vessel- and vessel-year effects are generally similar to estimates when using just vessel-year effects. Additionally, models indicate little support for the hypothesis that characteristics such as length and displacement of the contracted vessels used in this survey have a substantial impact on catch rates. Finally, inclusion of vessel- or vessel-year effects generally results in wider estimates of credible intervals for resulting indices of abundance. This increased credible interval width is consistent with statistical theory, because vessel effects will result in non-independence of different sampling occasions, thus decreasing effective sample sizes. For this reason, we advocate that future analyses include vessel- and/or vessel-year effects when standardizing survey data from cooperative research programs.",0
https://doi.org/10.1177/0146621610367788,A Procedure for Controlling General Test Overlap in Computerized Adaptive Testing,"To date, exposure control procedures that are designed to control test overlap in computerized adaptive tests (CATs) are based on the assumption of item sharing between pairs of examinees. However, in practice, examinees may obtain test information from more than one previous test taker. This larger scope of information sharing needs to be considered in conducting test overlap control. The purpose of this study is to propose a test overlap control method such that the proportion of overlapping items encountered by an examinee with a group of previous examinees (described as general test overlap rate) can be controlled. Results indicated that item exposure rate and general test overlap rate could be simultaneously controlled by implementing the procedure. In addition, these two indices were controlled on the fly without any iterative simulations conducted prior to operational CATs. Thus, the proposed procedure would be an efficient method for controlling both the item exposure and general test overlap in CATs.",0
https://doi.org/10.1002/9780470998137,Semiparametric Regression for the Social Sciences,List of Tables. List of Figures. Preface. 1 Introduction: Global versus Local Statistics. 1.1 The Consequences of Ignoring Nonlinearity. 1.2 Power Transformations. 1.3 Nonparametric and Semiparametric Techniques. 1.4 Outline of the Text. 2 Smoothing and Local Regression. 2.1 Simple Smoothing. 2.1.1 Local Averaging. 2.1.2 Kernel Smoothing. 2.2 Local Polynomial Regression. 2.3 Nonparametric Modeling Choices. 2.3.1 The Span. 2.3.2 Polynomial Degree and Weight Function. 2.3.3 A Note on Interpretation. 2.4 Statistical Inference for Local Polynomial Regression. 2.5 Multiple Nonparametric Regression. 2.6 Conclusion. 2.7 Exercises. 3 Splines. 3.1 Simple Regression Splines. 3.1.1 Basis Functions. 3.2 Other Spline Models and Bases. 3.2.1 Quadratic and Cubic Spline Bases. 3.2.2 Natural Splines. 3.2.3 B-splines. 3.2.4 Knot Placement and Numbers. 3.2.5 Comparing Spline Models. 3.3 Splines and Overfitting. 3.3.1 Smoothing Splines. 3.3.2 Splines as Mixed Models. 3.3.3 Final Notes on Smoothing Splines. 3.3.4 Thin Plate Splines. 3.4 Inference for Splines. 3.5 Comparisons and Conclusions. 3.6 Exercises. 4 Automated Smoothing Techniques. 4.1 Span by Cross-Validation. 4.2 Splines and Automated Smoothing. 4.2.1 Estimating Smoothing Through the Likelihood. 4.2.2 Smoothing Splines and Cross-Validation. 4.3 Automated Smoothing in Practice. 4.4 Automated Smoothing Caveats. 4.5 Exercises. 5 Additive and Semiparametric Regression Models. 5.1 Additive Models. 5.2 Semiparametric Regression Models. 5.3 Estimation. 5.3.1 Backfitting. 5.4 Inference. 5.5 Examples. 5.5.1 Congressional Elections. 5.5.2 Feminist Attitudes. 5.6 Discussion. 5.7 Exercises. 6 Generalized Additive Models. 6.1 Generalized Linear Models. 6.2 Estimation of GAMS. 6.3 Statistical Inference. 6.4 Examples. 6.4.1 Logistic Regression: The Liberal Peace. 6.4.2 Ordered Logit: Domestic Violence. 6.4.3 Count Models: Supreme Court Overrides. 6.4.4 Survival Models: Race Riots. 6.5 Discussion. 6.6 Exercises. 7 Extensions of the Semiparametric Regression Model. 7.1 Mixed Models. 7.2 Bayesian Smoothing. 7.3 Propensity Score Matching. 7.4 Conclusion. 8 Bootstrapping. 8.1 Classical Inference. 8.2 Bootstrapping - An Overview. 8.2.1 Bootstrapping. 8.2.2 An Example: Bootstrapping the Mean. 8.2.3 Bootstrapping Regression Models. 8.2.4 An Example: Presidential Elections. 8.3 Bootstrapping Nonparametric and Semiparametric Regression Models. 8.3.1 Bootstrapping Nonparametric Fits. 8.3.2 Bootstrapping Nonlinearity Tests. 8.4 Conclusion. 8.5 Exercises. 9 Epilogue. Appendix: Software. Bibliography. Author Index. Subject Index.,0
,R functions for quantifying nonindependence in standard dyadic and SRM designs,"Interdependence is the main feature of dyadic relationships and, in recent years, various statistical procedures have been proposed for quantifying and testing this social attribute in different dyadic designs. The purpose of this paper is to develop several functions for this kind of statistical tests in an R package, known as nonindependence, for use by applied social researchers. A Graphical User Interface (GUI) is also developed to facilitate the use of the functions included in this package. Examples drawn from psychological research and simulated data are used to illustrate how the software works.",0
https://doi.org/10.1111/j.2517-6161.1996.tb02105.x,Hierarchical Generalized Linear Models,"We consider hierarchical generalized linear models which allow extra error components in the linear predictors of generalized linear models. The distribution of these components is not restricted to be normal; this allows a broader class of models, which includes generalized linear mixed models. We use a generalization of Henderson's joint likelihood, called a hierarchical or h-likelihood, for inferences from hierarchical generalized linear models. This avoids the integration that is necessary when marginal likelihood is used. Under appropriate conditions maximizing the h-likelihood gives fixed effect estimators that are asymptotically equivalent to those obtained from the use of marginal likelihood; at the same time we obtain the random effect estimates that are asymptotically best unbiased predictors. An adjusted profile h-likelihood is shown to give the required generalization of restricted maximum likelihood for the estimation of dispersion components. A scaled deviance test for the goodness of fit, a model selection criterion for choosing between various dispersion models and a graphical method for checking the distributional assumption of random effects are proposed. The ideas of quasi-likelihood and extended quasi-likelihood are generalized to the new class. We give examples of the Poisson-gamma, binomial-beta and gamma-inverse gamma hierarchical generalized linear models. A resolution is proposed for the apparent difference between population-averaged and subject-specific models. A unified framework is provided for viewing and extending many existing methods.",0
https://doi.org/10.4324/9781410601858-10,Bayesian Structural Equation Models for Multilevel Data,,0
https://doi.org/10.1177/0143624414566244,"Is CO<sub>2</sub> a good proxy for indoor air quality in classrooms? Part 1: The interrelationships between thermal conditions, CO<sub>2</sub> levels, ventilation rates and selected indoor pollutants","Current indoor air quality (IAQ) guidelines in school buildings are framed around thermal conditions, carbon dioxide (CO 2 ) levels and corresponding ventilation rates without considering specific indoor pollution levels. Drawing on detailed monitoring data from a sample of 18 classrooms from six London schools, the aim of this paper is to highlight behavioural and environmental factors that affect pollution levels in classrooms, and evaluate the adequacy of CO 2 as an overall predictor for IAQ using multilevel modelling. Together with elimination of indoor emission sources, keeping the temperatures below 26℃, and preferably below 22℃ depending on season, may limit total volatile organic compounds below thresholds associated with sensory irritations. The models suggested that after removing dust reservoirs from the classrooms, lowering average indoor CO 2 levels below 1000 ppm by increasing ventilation rates can limit indoor airborne particulate matter concentrations below recommended annual WHO 2010 guidelines. Uncontrolled infiltration rates may increase indoor NO 2 levels and microbial counts of fungal and bacterial groups, whose presence is associated with wet and moist materials. Overall, indoor CO 2 levels were a useful proxy for indoor investigations as they can prevent overheating, dilute pollutants with indoor sources and purge concentrations of airborne particles; however, they were a poor predictor of traffic related pollutants. Practical implications of the findings on the UK policy and building design industry are discussed. Practical application: Driven by the growing population, and many years of intensive use, the UK building stock is in need of rapid expanding, extensive refurbishment and maintenance. However, local authorities lack the money for comprehensive and specialist renovations. The recommendations presented in this paper take into account specific needs and possibilities, and target building designers, engineers and occupants involved with daily operation and management of school buildings. Timely control of ventilation and heating systems, informed selection of construction materials, interior finishing and elimination of indoor sources may improve IAQ of school classrooms.",0
https://doi.org/10.1214/009053605000000075,Posterior propriety and admissibility of hyperpriors in normal hierarchical models,"Hierarchical modeling is wonderful and here to stay, but hyperparameter priors are often chosen in a casual fashion. Unfortunately, as the number of hyperparameters grows, the effects of casual choices can multiply, leading to considerably inferior performance. As an extreme, but not uncommon, example use of the wrong hyperparameter priors can even lead to impropriety of the posterior. For exchangeable hierarchical multivariate normal models, we first determine when a standard class of hierarchical priors results in proper or improper posteriors. We next determine which elements of this class lead to admissible estimators of the mean under quadratic loss; such considerations provide one useful guideline for choice among hierarchical priors. Finally, computational issues with the resulting posterior distributions are addressed.",0
https://doi.org/10.1017/s0003055409090248,Who Wants To Revise Privatization? The Complementarity of Market Skills and Institutions,"Using survey data from 28 transition countries, we test for the complementarity and substitutability of market-relevant skills and institutions. We show that democracy and good governance complement market skills in transition economies. Under autocracy and weak governance institutions, there is no significant difference in support for revising privatization between high- and low-skilled respondents. As the level of democracy and the quality of governance increases, the difference in the level of support for revising privatization between the high and low skilled grows dramatically. This finding contributes to our understanding of microfoundations of the politics of economic reform.",0
https://doi.org/10.1504/ijstm.2007.013922,Random effect modelling using Bayesian methods,"Bayesian methods are increasingly used in a variety of academic disciplines, and important applications in ecology, medicine, management science, operations research and finance. The continued use of these methods depends on understanding the strengths and weaknesses of current Bayesian modelling practice. In this paper, we examine important implementation issues within the context of data analysis with statistical models which contain random effects or unmeasured heterogeneity. Random effect models provide an effective way to incorporate sources of variation not able to be modelled by covariate information, and these models lead naturally to Bayesian formulations using prior distributions for the variance components. We examine implications for specifying scale parameters in hierarchical models.",0
https://doi.org/10.1093/biomet/82.4.711,Reversible jump Markov chain Monte Carlo computation and Bayesian model determination,"Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.",0
https://doi.org/10.1167/5.5.8,Bayesian inference for psychometric functions,"In psychophysical studies, the psychometric function is used to model the relation between physical stimulus intensity and the observer's ability to detect or discriminate between stimuli of different intensities. In this study, we propose the use of Bayesian inference to extract the information contained in experimental data to estimate the parameters of psychometric functions. Because Bayesian inference cannot be performed analytically, we describe how a Markov chain Monte Carlo method can be used to generate samples from the posterior distribution over parameters. These samples are used to estimate Bayesian confidence intervals and other characteristics of the posterior distribution. In addition, we discuss the parameterization of psychometric functions and the role of prior distributions in the analysis. The proposed approach is exemplified using artificially generated data and in a case study for real experimental data. Furthermore, we compare our approach with traditional methods based on maximum likelihood parameter estimation combined with bootstrap techniques for confidence interval estimation and find the Bayesian approach to be superior.",0
https://doi.org/10.1007/s11336-006-1478-z,A Hierarchical Framework for Modeling Speed and Accuracy on Test Items,"Current modeling of response times on test items has been strongly influenced by the paradigm of experimental reaction-time research in psychology. For instance, some of the models have a parameter structure that was chosen to represent a speed-accuracy tradeoff, while others equate speed directly with response time. Also, several response-time models seem to be unclear as to the level of parametrization they represent. A hierarchical framework for modeling speed and accuracy on test items is presented as an alternative to these models. The framework allows a plug-and-play approach with alternative choices of models for the response and response-time distributions as well as the distributions of their parameters. Bayesian treatment of the framework with Markov chain Monte Carlo (MCMC) computation facilitates the approach. Use of the framework is illustrated for the choice of a normal-ogive response model, a lognormal model for the response times, and multivariate normal models for their parameters with Gibbs sampling from the joint posterior distribution.",0
https://doi.org/10.1177/0962280210372453,A spatial beta-binomial model for clustered count data on dental caries,"One of the most important indicators of dental caries prevalence is the total count of decayed, missing or filled surfaces in a tooth. These count data are often clustered in nature (several count responses clustered within a subject), over-dispersed as well as spatially referenced (a diseased tooth might be positively influencing the decay process of a set of neighbouring teeth). In this article, we develop a multivariate spatial betabinomial (BB) model for these data that accommodates both over-dispersion as well as latent spatial associations. Using a Bayesian paradigm, the re-parameterised marginal mean (as well as variance) under the BB framework are modelled using a regression on subject/tooth-specific co-variables and a conditionally autoregressive prior that models the latent spatial process. The necessity of exploiting spatial associations to model count data arising in dental caries research is demonstrated using a small simulation study. Real data confirms that our spatial BB model provides a superior estimation and model fit as compared to other sub-models that do not consider modelling spatial associations.",0
https://doi.org/10.1007/bf00143942,Simulation of truncated normal variables,We provide simulation algorithms for one-sided and two-sided truncated normal distributions. These algorithms are then used to simulate multivariate normal variables with convex restricted parameter space for any covariance structure.,0
https://doi.org/10.1016/j.jsr.2013.03.003,Bayesian road safety analysis: Incorporation of past evidence and effect of hyper-prior choice,"This paper aims to address two related issues when applying hierarchical Bayesian models for road safety analysis, namely: (a) how to incorporate available information from previous studies or past experiences in the (hyper) prior distributions for model parameters and (b) what are the potential benefits of incorporating past evidence on the results of a road safety analysis when working with scarce accident data (i.e., when calibrating models with crash datasets characterized by a very low average number of accidents and a small number of sites).A simulation framework was developed to evaluate the performance of alternative hyper-priors including informative and non-informative Gamma, Pareto, as well as Uniform distributions. Based on this simulation framework, different data scenarios (i.e., number of observations and years of data) were defined and tested using crash data collected at 3-legged rural intersections in California and crash data collected for rural 4-lane highway segments in Texas.This study shows how the accuracy of model parameter estimates (inverse dispersion parameter) is considerably improved when incorporating past evidence, in particular when working with the small number of observations and crash data with low mean. The results also illustrates that when the sample size (more than 100 sites) and the number of years of crash data is relatively large, neither the incorporation of past experience nor the choice of the hyper-prior distribution may affect the final results of a traffic safety analysis.As a potential solution to the problem of low sample mean and small sample size, this paper suggests some practical guidance on how to incorporate past evidence into informative hyper-priors. By combining evidence from past studies and data available, the model parameter estimates can significantly be improved. The effect of prior choice seems to be less important on the hotspot identification.The results show the benefits of incorporating prior information when working with limited crash data in road safety studies.",0
https://doi.org/10.1007/0-306-47531-6_9,Methods of Controlling the Exposure of Items in CAT,,0
https://doi.org/10.1080/01621459.1984.10477102,Approximations for Standard Errors of Estimators of Fixed and Random Effects in Mixed Linear Models,"Abstract Best linear unbiased estimators of the fixed and random effects of mixed linear models are available when the true values of the variance ratios are known. If the true values are replaced by estimated values, the mean squared errors of the estimators of the fixed and random effects increase in size. The magnitude of this increase is investigated, and a general approximation is proposed. The performance of this approximation is investigated in the context of (a) the estimation of the effects of the balanced one-way random model and (b) the estimation of treatment contrasts for balanced incomplete block designs.",0
https://doi.org/10.1214/aoms/1177730442,The Probability Function of the Product of Two Normally Distributed Variables,"Let $x$ and $y$ follow a normal bivariate probability function with means $\bar X, \bar Y$, standard deviations $\sigma_1, \sigma_2$, respectively, $r$ the coefficient of correlation, and $\rho_1 = \bar X/\sigma_1, \rho_2 = \bar Y/\sigma_2$. Professor C. C. Craig [1] has found the probability function of $z = xy/\sigma_1\sigma_2$ in closed form as the difference of two integrals. For purposes of numerical computation he has expanded this result in an infinite series involving powers of $z, \rho_1, \rho_2$, and Bessel functions of a certain type; in addition, he has determined the moments, semin-variants, and the moment generating function of $z$. However, for $\rho_1$ and $\rho_2$ large, as Craig points out, the series expansion converges very slowly. Even for $\rho_1$ and $\rho_2$ as small as 2, the expansion is unwieldy. We shall show that as $\rho_1$ and $\rho_2 \rightarrow \infty$, the probability function of $z$ approaches a normal curve and in case $r = 0$ the Type III function and the Gram-Charlier Type A series are excellent approximations to the $z$ distribution in the proper region. Numerical integration provides a substitute for the infinite series wherever the exact values of the probability function of $z$ are needed. Some extensions of the main theorem are given in section 5 and a practical problem involving the probability function of $z$ is solved.",0
https://doi.org/10.4135/9781446247600.n22,Robust Methods for Multilevel Analysis,,0
https://doi.org/10.4061/2011/759170,"Personality and Longevity: Knowns, Unknowns, and Implications for Public Health and Personalized Medicine","We review evidence for links between personality traits and longevity. We provide an overview of personality for health scientists, using the primary organizing framework used in the study of personality and longevity. We then review data on various aspects of personality linked to longevity. In general, there is good evidence that higher level of conscientiousness and lower levels of hostility and Type D or “distressed” personality are associated with greater longevity. Limited evidence suggests that extraversion, openness, perceived control, and low levels of emotional suppression may be associated with longer lifespan. Findings regarding neuroticism are mixed, supporting the notion that many component(s) of neuroticism detract from life expectancy, but some components at some levels may be healthy or protective. Overall, evidence suggests various personality traits are significant predictors of longevity and points to several promising directions for further study. We conclude by discussing the implications of these links for epidemiologic research and personalized medicine and lay out a translational research agenda for integrating the psychology of individual differences into public health and medicine.",0
,New Algorithms for Item Selection and Exposure Control with Computerized Adaptive Testing.,"Computerized adaptive testing (CAT) offers the prospect of both reducing testing time and increasing measurement precision when compared to conventional pencil-and-paper tests. Although adaptive tests acquire their efficiency by successively selecting items that provide optimal` *. measurement at each examinee's estimated level of ability, operational testing programs will typically consider additional factors in item selection. In practice, items are generally selected with regard to at least three, often conflicting goals: 1) to maximize test efficiency by measuring examinees as quickly and as accurately as possible, 2) to protect the security of the item pool by controlling the rates at which popular items can be administered, and 3) to assure that the test measures the same composite of multiple traits for each examinee by balancing the rates at which items with different content properties are administered. This paper focuses on the goals of maximizing test efficiency and controlling item exposure rates, avoiding discussion of content balance. While a number of algorithms for accomplishing these goals have been developed, all are problematic to some extent. We briefly sketch the nature of these problems, and then present alternative algorithms that offer at least a partial solution. U.S. DEPARTMENT OF EDUCATION Office of Educational Research and Improvement EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) Crfhis document has been reproduced as received from the person or organization originating it. Minor changes have heen made to improve reproduction quality. Points of view or opinions stated in this CO document do not necessarily represent official OERI position or policy. Co tN 2 PERMISSION TO REPRODUCE AND DISSEMINATE THIS MATERIAL HAS BEEN GRANTED BY iclhAc511A.IL TO THE EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) 1 Paper presented at the annual meeting of the American Educational Research Association, April 18-22, 1995, San Francisco New Algorithms for CAT 2 New Algorithms for Item Selection and Exposure Control with Computerized Adaptive Testing",0
https://doi.org/10.1016/j.csda.2010.05.003,Bayesian inference for a skew-normal IRT model under the centred parameterization,"Item response theory (IRT) comprises a set of statistical models which are useful in many fields, especially when there is interest in studying latent variables. These latent variables are directly considered in the Item Response Models (IRM) and they are usually called latent traits. A usual assumption for parameter estimation of the IRM, considering one group of examinees, is to assume that the latent traits are random variables which follow a standard normal distribution. However, many works suggest that this assumption does not apply in many cases. Furthermore, when this assumption does not hold, the parameter estimates tend to be biased and misleading inference can be obtained. Therefore, it is important to model the distribution of the latent traits properly. In this paper we present an alternative latent traits modeling based on the so-called skew-normal distribution; see Genton (2004) . We used the centred parameterization, which was proposed by Azzalini (1985) . This approach ensures the model identifiability as pointed out by Azevedo et al. (2009b) . Also, a Metropolis–Hastings within Gibbs sampling (MHWGS) algorithm was built for parameter estimation by using an augmented data approach. A simulation study was performed in order to assess the parameter recovery in the proposed model and the estimation method, and the effect of the asymmetry level of the latent traits distribution on the parameter estimation. Also, a comparison of our approach with other estimation methods (which consider the assumption of symmetric normality for the latent traits distribution) was considered. The results indicated that our proposed algorithm recovers properly all parameters. Specifically, the greater the asymmetry level, the better the performance of our approach compared with other approaches, mainly in the presence of small sample sizes (number of examinees). Furthermore, we analyzed a real data set which presents indication of asymmetry concerning the latent traits distribution. The results obtained by using our approach confirmed the presence of strong negative asymmetry of the latent traits distribution.",0
https://doi.org/10.1121/1.1970490,Maximum‐Likelihood Sequential Procedure for Estimation of Psychometric Functions,"This paper describes a computer‐oriented sequential procedure for the estimation of parameters of a psychometric function. In common with other sequential procedures (BUDTIF, PEST, UDTR), the goal is to concentrate observations in the region of interest. The experimenter assumes a parametric form of the psychometric function (cumulative normal from 0 to 1, for example, but any other desired form could be used). After each response, values of the parameters are computed that maximize the probability of the set of responses that have been obtained, given the set of stimuli that have been presented. These values are then used to determine the placement of the next stimulus. A compromise between fidelity of tracking a time‐varying threshold and accuracy can be effected by appropriate choice of the weighting function for previous observations. The procedure has been simulated on a GE‐645 computer and applied in the laboratory with a DDP‐516 computer.",0
https://doi.org/10.1177/0146621608320760,The Multiple-Choice Model,"This article deals with some of the problems that have hindered the application of Samejima's and Thissen and Steinberg's multiple-choice models: (a) parameter estimation difficulties owing to the large number of parameters involved, (b) parameter identifiability problems in the Thissen and Steinberg model, and (c) their treatment of omitted responses. The authors propose a new multiple-choice model (Restricted Samejima Multiple-Choice Model) that is more explicit about the assumed omitting mechanisms and takes advantage of this knowledge to improve the estimation of parameters. The three above-mentioned models and the Nominal Response Model were fitted to a 3,224-subject sample that took a Written English Test. Fit plots, X 2 statistics, and information-based fit indexes were obtained to assess the goodness of fit. Results show that the new model proposed fits as well as the other two multiple-choice models and better than the Nominal model. A parameter recovery simulation study was also carried out to confirm that its estimation features are indeed improved. Parameters are better recovered in the new model than in the other two multiple-choice models.",0
https://doi.org/10.1007/bf02291411,Estimating item parameters and latent ability when responses are scored in two or more nominal categories,"A multivariate logistic latent trait model for items scored in two or more nominal categories is proposed. Statistical methods based on the model provide 1) estimation of two item parameters for each response alternative of each multiple choice item and 2) recovery of information from “wrong” responses when estimating latent ability. An application to a large sample of data for twenty vocabulary items shows excellent fit of the model according to a chi-square criterion. Item and test information curves are compared for estimation of ability assuming multiple category and dichotomous scoring of these items. Multiple scoring proves substantially more precise for subjects of less than median ability, and about equally precise for subjects above the median.",0
https://doi.org/10.1214/aos/1176343345,Multivariate Empirical Bayes and Estimation of Covariance Matrices,"The problem of estimating several normal mean vectors in an empirical Bayes situation is considered. In this case, it reduces to the problem of estimating the inverse of a covariance matrix in the standard multivariate normal situation using a particular loss function. Estimators which dominate any constant multiple of the inverse sample covariance matrix are presented. These estimators work by shrinking the sample eigenvalues toward a central value, in much the same way as the James-Stein estimator for a mean vector shrinks the maximum likelihood estimators toward a common value. These covariance estimators then lead to a class of multivariate estimators of the mean, each of which dominates the maximum likelihood estimator.",0
https://doi.org/10.1037/a0025813,Fitting multilevel models with ordinal outcomes: Performance of alternative specifications and methods of estimation.,"Previous research has compared methods of estimation for fitting multilevel models to binary data, but there are reasons to believe that the results will not always generalize to the ordinal case. This article thus evaluates (a) whether and when fitting multilevel linear models to ordinal outcome data is justified and (b) which estimator to employ when instead fitting multilevel cumulative logit models to ordinal data, maximum likelihood (ML), or penalized quasi-likelihood (PQL). ML and PQL are compared across variations in sample size, magnitude of variance components, number of outcome categories, and distribution shape. Fitting a multilevel linear model to ordinal outcomes is shown to be inferior in virtually all circumstances. PQL performance improves markedly with the number of ordinal categories, regardless of distribution shape. In contrast to binary data, PQL often performs as well as ML when used with ordinal data. Further, the performance of PQL is typically superior to ML when the data include a small to moderate number of clusters (i.e., ≤ 50 clusters).",0
https://doi.org/10.3758/s13428-017-0870-1,Statistical properties of four effect-size measures for mediation models,"This project examined the performance of classical and Bayesian estimators of four effect size measures for the indirect effect in a single-mediator model and a two-mediator model. Compared to the proportion and ratio mediation effect sizes, standardized mediation effect-size measures were relatively unbiased and efficient in the single-mediator model and the two-mediator model. Percentile and bias-corrected bootstrap interval estimates of ab/s Y , and ab(s X )/s Y in the single-mediator model outperformed interval estimates of the proportion and ratio effect sizes in terms of power, Type I error rate, coverage, imbalance, and interval width. For the two-mediator model, standardized effect-size measures were superior to the proportion and ratio effect-size measures. Furthermore, it was found that Bayesian point and interval summaries of posterior distributions of standardized effect-size measures reduced excessive relative bias for certain parameter combinations. The standardized effect-size measures are the best effect-size measures for quantifying mediated effects.",1
https://doi.org/10.1177/1354068814549339,Corruption as an obstacle to women’s political representation,"This article presents evidence from 18 European countries showing that where levels of corruption are high, the proportion of women elected is low. We hypothesize that corruption indicates the presence of ‘shadowy arrangements’ that benefit the already privileged and pose a direct obstacle to women when male-dominated networks influence political parties’ candidate selection. There is also an indirect signal effect derived from citizen’s experiences with a broad range of government authorities. The article uses data that are more fine-grained than usual in this literature. We conduct an empirical test on a new dataset on locally elected councilors in 167 regions in Europe. Using a novel measure of regional quality of government and corruption we perform a multi-level analysis with several regional- and national-level controls. This study provides a unique picture of the proportion of women in locally elected assemblies throughout Europe and a new way of understanding the variations found.",0
https://doi.org/10.1080/02664763.2013.785491,Bayesian structural equation modeling for the health index,"There are many factors which could influence the level of health of an individual. These factors are interactive and their overall effects on health are usually measured by an index which is called as health index. The health index could also be used as an indicator to describe the health level of a community. Since the health index is important, many research have been done to study its determinant. The main purpose of this study is to model the health index of an individual based on classical structural equation modeling (SEM) and Bayesian SEM. For estimation of the parameters in the measurement and structural equation models, the classical SEM applies the robust-weighted least-square approach, while the Bayesian SEM implements the Gibbs sampler algorithm. The Bayesian SEM approach allows the user to use the prior information for updating the current information on the parameter. Both methods are applied to the data gathered from a survey conducted in Hulu Langat, a district in Malaysia. Based on the cl...",0
https://doi.org/10.1080/01621459.1986.10478292,Judging Inference Adequacy in Logistic Regression,"Abstract Inference for logistic regression based on the information matrix may be poor. This is noted in two examples in which confidence regions are examined. A measure to detect such inadequacies is presented; it judges the quadratic approximation to the likelihood surface, which justifies the usual procedure.",0
https://doi.org/10.1111/j.0006-341x.1999.00044.x,Flexible Parametric Measurement Error Models,"Inferences in measurement error models can be sensitive to modeling assumptions. Specifically, if the model is incorrect, the estimates can be inconsistent. To reduce sensitivity to modeling assumptions and yet still retain the efficiency of parametric inference, we propose using flexible parametric models that can accommodate departures from standard parametric models. We use mixtures of normals for this purpose. We study two cases in detail: a linear errors-in-variables model and a change-point Berkson model.",0
https://doi.org/10.1093/rfs/hhn053,Estimating Standard Errors in Finance Panel Data Sets: Comparing Approaches,"In both corporate finance and asset pricing empirical work, researchers are often confronted with panel data. In these data sets, the residuals may be correlated across firms and across time, and OLS standard errors can be biased. Historically, the two literatures have used different solutions to this problem. Corporate finance has relied on clustered standard errors, while asset pricing has used the Fama-MacBeth procedure to estimate standard errors. This paper examines the different methods used in the literature and explains when the different methods yield the same (and correct) standard errors and when they diverge. The intent is to provide intuition as to why the different approaches sometimes give different answers and give researchers guidance for their use.",0
https://doi.org/10.1111/j.1744-6570.1985.tb00565.x,FORTY QUESTIONS ABOUT VALIDITY GENERALIZATION AND META-ANALYSIS.,,0
https://doi.org/10.1177/0013164413517503,A Comparison of Four Item-Selection Methods for Severely Constrained CATs,"This study compared four item-selection procedures developed for use with severely constrained computerized adaptive tests (CATs). Severely constrained CATs refer to those adaptive tests that seek to meet a complex set of constraints that are often not conclusive to each other (i.e., an item may contribute to the satisfaction of several constraints at the same time). The procedures examined in the study included the weighted deviation model (WDM), the weighted penalty model (WPM), the maximum priority index (MPI), and the shadow test approach (STA). In addition, two modified versions of the MPI procedure were introduced to deal with an edge case condition that results in the item selection procedure becoming dysfunctional during a test. The results suggest that the STA worked best among all candidate methods in terms of measurement accuracy and constraint management. For the other three heuristic approaches, they did not differ significantly in measurement accuracy and constraint management at the lower bound level. However, the WPM method appears to perform considerably better in overall constraint management than either the WDM or MPI method. Limitations and future research directions were also discussed.",0
https://doi.org/10.1016/s0304-4076(00)00034-8,A Bayesian analysis of the multinomial probit model with fully identified parameters,"We present a new prior and corresponding algorithm for Bayesian analysis of the multinomial probit model. Our new approach places a prior directly on the identified parameter space. The key is the specification of a prior on the covariance matrix so that the (1,1) element if fixed at 1 and it is possible to draw from the posterior using standard distributions. Analytical results are derived which can be used to aid in assessment of the prior.",0
https://doi.org/10.1080/00273170701540537,The Impact of Misspecifying the Within-Subject Covariance Structure in Multiwave Longitudinal Multilevel Models: A Monte Carlo Study,"This Monte Carlo study examined the impact of misspecifying the Σ matrix in longitudinal data analysis under both the multilevel model and mixed model frameworks. Under the multilevel model approach, under-specification and general-misspecification of the Σ matrix usually resulted in overestimation of the variances of the random effects (e.g., τ00, ττ11 ) and standard errors of the corresponding growth parameter estimates (e.g., SEβ 0, SEβ 1). Overestimates of the standard errors led to lower statistical power in tests of the growth parameters. An unstructured Σ matrix under the mixed model framework generally led to underestimates of standard errors of the growth parameter estimates. Underestimates of the standard errors led to inflation of the type I error rate in tests of the growth parameters. Implications of the compensatory relationship between the random effects of the growth parameters and the longitudinal error structure for model specification were discussed.",0
https://doi.org/10.1016/b978-0-08-044894-7.01370-1,Structural Equation Models,"In this article, we introduce structural equation models which are powerful multivariate tools in analyzing interrelationships among observed and latent variables. Useful models, including the standard linear model and its generalizations such as the nonlinear models, multilevel models, and models with ordered categorical data are discussed. In addition, the maximum likelihood approach, a Bayesian approach for estimation and model comparison, as well as the freely available software WinBUGS for obtaining the Bayesian results are described.",0
https://doi.org/10.1002/wics.1311,Bayesian structural equation model,"Latent variables that should be measured by multiple observed variable are common in substantive research. Structural equation models SEMs, which can be regarded as regression models with observed and latent variables, are useful models to assess interrelationships among these variables and have been widely applied to many fields. When applied with data augmentation and recent techniques in statistical computing, the Bayesian approach has been found to be a powerful tool for analysing many important extensions of the basic SEMs. Here, we introduce the basic SEM, present a brief discussion on the Bayesian approach and illustrate it with a simulation study, and review some recent extension, such as two-level SEMs, transformation SEMs, and nonparametric SEMs. WIREs Comput Stat 2014, 6:276-287. doi: 10.1002/wics.1311",0
https://doi.org/10.1016/j.ijresmar.2010.08.001,Enhancing marketing with engineering: Optimal product line design for heterogeneous markets,"article i nfo Successful product line design and development often require a balance of technical and market tradeoffs. Quantitative methods for optimizing product attribute levels using preference elicitation (e.g., conjoint) data are useful for many product types. However, products with substantial engineering content involve critical tradeoffs in the ability to achieve those desired attribute levels. Technical tradeoffs in product design must be made with an eye toward market consequences, particularly when heterogeneous market preferences make differentiation and strategic positioning critical to capturing a range of market segments and avoiding cannibalization. We present a unified methodology for product line optimization that coordinates positioning and design models to achieve realizable firm-level optima. The approach overcomes several shortcomings of prior product line optimization models by incorporating a general Bayesian account of consumer preference heterogeneity, managing attributes over a continuous domain to alleviate issues of combinatorial complexity, and avoiding solutions that are impossible to realize. The method is demonstrated for a line of dial-readout scales, using physical models and conjoint-based consumer choice data. The results show that the optimal number of products in the line is not necessarily equal to the number of market segments, that an optimal single product for a heterogeneous market differs from that for a homogeneous one, and that the representational form for consumer heterogeneity has a substantial impact on the design and profitability of the resulting optimal product line — even for the design of a single product. The method is managerially valuable because it yields product line solutions efficiently, accounting for marketing-based preference heterogeneity as well as engineering-based constraints with which product attributes can be realized.",0
https://doi.org/10.1214/aos/1065705116,Enriched conjugate and reference priors for the Wishart family on symmetric cones,"A general Wishart family on a symmetric cone is a natural exponential family (NEF) having a homogeneous quadratic variance function. Using results in the abstract theory of Euclidean Jordan algebras, the structure of conditional reducibility is shown to hold for such a family, and we identify the associated parameterization $\phi$ and analyze its properties. The enriched standard conjugate family for $\phi$ and the mean parameter $\mu$ are defined and discussed. This family is considerably more flexible than the standard conjugate one. The reference priors for $\phi$ and $\mu$ are obtained and shown to belong to the enriched standard conjugate family; in particular, this allows us to verify that reference posteriors are always proper. The above results extend those available for NEFs having a simple quadratic variance function. Specifications of the theory to the cone of real symmetric and positive-definite matrices are discussed in detail and allow us to perform Bayesian inference on the covariance matrix $\Sigma$ of a multivariate normal model under the enriched standard conjugate family. In particular, commonly employed Bayes estimates, such as the posterior expectation of $\Sigma$ and $\Sigma^{-1}$, are provided in closed form.",0
https://doi.org/10.1198/016214506000000753,Spatial Analyses of Periodontal Data Using Conditionally Autoregressive Priors Having Two Classes of Neighbor Relations,"Attachment loss, the extent of a tooth's root (in millimeters) that is no longer attached to surrounding bone by periodontal ligament, is often used to measure the current state of a patient's periodontal disease and monitor disease progression. Attachment loss data can be analyzed using a conditionally autoregressive (CAR) prior distribution that smooths fitted values toward neighboring values. However, it may be desirable to have more than one class of neighbor relation in the spatial structure, so the different classes of neighbor relations can induce different degrees of smoothing. For example, we may wish to allow smoothing of neighbor pairs bridging the gap between teeth to differ from smoothing of pairs that do not bridge such gaps. Adequately modeling the spatial structure may improve the monitoring of periodontal disease progression. This article develops a two-neighbor-relation CAR model to handle this situation and presents associated theory to help explain the sometimes unusual posterior distr...",0
https://doi.org/10.1007/bf02294773,An estimating equations approach for the LISCOMP model,"Maximum likelihood estimation is computationally infeasible for latent variable models involving multivariate categorical responses, in particular for the LISCOMP model. A three-stage generalized least squares approach introduced by MuthÃƒÂ©n (1983, 1984) can experience problems of instability, bias, non-convergence, and non-positive definiteness of weight matrices in situations of low prevalence, small sample size and large numbers of observed indicator variables. We propose a quadratic estimating equations approach that only requires specification of the first two moments. By performing simultaneous estimation of parameters, this method does not encounter the problems mentioned above and experiences gains in efficiency. Methods are compared through a numerical study and an application to a study of life-events and neurotic illness.",0
https://doi.org/10.3758/s13428-016-0739-8,Default “Gunel and Dickey” Bayes factors for contingency tables,"The analysis of R×C contingency tables usually features a test for independence between row and column counts. Throughout the social sciences, the adequacy of the independence hypothesis is generally evaluated by the outcome of a classical p-value null-hypothesis significance test. Unfortunately, however, the classical p-value comes with a number of well-documented drawbacks. Here we outline an alternative, Bayes factor method to quantify the evidence for and against the hypothesis of independence in R×C contingency tables. First we describe different sampling models for contingency tables and provide the corresponding default Bayes factors as originally developed by Gunel and Dickey (Biometrika, 61(3):545-557 (1974)). We then illustrate the properties and advantages of a Bayes factor analysis of contingency tables through simulations and practical examples. Computer code is available online and has been incorporated in the ""BayesFactor"" R package and the JASP program ( jasp-stats.org ).",0
https://doi.org/10.1037/1082-989x.9.2.250,The Role of Parametric Assumptions in Adaptive Bayesian Estimation.,"Variants of adaptive Bayesian procedures for estimating the 5% point on a psychometric function were studied by simulation. Bias and standard error were the criteria to evaluate performance. The results indicated a superiority of (a) uniform priors, (b) model likelihood functions that are odd symmetric about threshold and that have parameter values larger than their counterparts in the psychometric function, (c) stimulus placement at the prior mean, and (d) estimates defined as the posterior mean. Unbiasedness arises in only 10 trials, and 20 trials ensure constant standard errors. The standard error of the estimates equals 0.617 times the inverse of the square root of the number of trials. Other variants yielded bias and larger standard errors.",0
https://doi.org/10.1093/her/17.5.586,Promoting Healthy Lifestyles: Alternative Models' Effects (PHLAME),"The Promoting Healthy Lifestyles: Alternative Models' Effects (PHLAME) study evaluates the efficacy of two intervention strategies for improving nutrition and physical activity practices in fire fighters: a team-centered program and a one-on-one format targeting the individual. PHLAME compares these two behavior change models (the team-based versus the one-on-one approaches) against a usual-care control group. As a group, fire fighters have a concentration of the same harmful behaviors and health risks commonly afflicting the US population. Fire fighters have a unique work structure which is ideal for a team-centered model of behavior change. This strategy, based on Social Learning Theory, focuses on a team of fire fighters who work together on the same shift. If this team-centered model proves successful, it could provide a cost-effective method to impact behavior, and be disseminated among fire bureaus and in other team settings. The one-on-one intervention incorporates the Transtheoretical Model of behavior change, uses Motivational Interviewing for its counseling strategy and could be used in the more typical provider-client clinic setting. Findings from PHLAME will provide information about the process and outcomes of these models' ability to achieve health behavior change.",0
https://doi.org/10.1007/978-0-387-73186-5_9,Multilevel Generalized Linear Models,"Two of the most influential papers in applied statistics published in the last few decades are Nelder and Wedderburn [65], introducing generalized linear models (GLMs), and Cox [20], the seminal paper introducing life tables with regression, better known as proportional hazard models. As we will see, these two developments are closely related. Nelder and Wedderburn?s unique contribution was to provide a unified conceptual framework for studying a large range of statistical models, including not only classical linear models, but also logit and probit models for binary data, log-linear Poisson models for count data, and others. The unification was not only conceptual, but also led to common estimation procedures in the form of an iteratively re-weighted least squares (IRLS) algorithm. The first implementation of these procedures appeared in the highly successful program GLIM [3], which for many statisticians became synonymous with GLMs. In this chapter we follow Wong and Mason [94], Longford [54, 56], Goldstein [30], Breslow and Clayton [11], and others in exploring extensions of GLMs to include random effects in a multilevel setting. Chapter 1 in this handbook has described multilevel models for continuous outcomes, while Chapter 6 has focused on multilevel models for categorical outcomes. Here we adopt a unified approach that views the general linear mixed model and many of the random-effects models for categorical data discussed in earlier chapters as special cases of the Multilevel Generalized Linear Model (MGLM). This approach has conceptual merit in emphasizing the similarities among these models, and provides a common framework to study and evaluate estimation methods. Alas, we do not have a single estimation procedure that can be applied to all MGLMs with the same measure of success that IRLS achieved for GLMs. Instead, we must choose between quick but sometimes biased approximations, and more accurate but often compute-intensive maximum likelihood and Bayesian approaches. Part of our task in this chapter is to describe and illustrate the alternatives. Section 9.2 develops the modeling framework. We introduce generalized linear models (GLMs) as an extension of linear models, and proceed to an analogous derivation of multilevel generalized linear models (MGLMs) as an extension of multilevel linear models. The ideas discussed apply more generally to generalized linear mixed models (GLMMs) and our notation reflects this broader applicability, but we tend to focus the narrative on the multilevel case. We review survival models, note their close connection with GLMs, and describe a natural extension to the multilevel case. We draw an important distinction between conditional and marginal models that is significant in the generalized linear case. Finally, we introduce non-linear mixed models and contrast them with MGLMs. Section 9.3 is devoted to a discussion of estimation procedures. It turns out that calculation of the likelihood function for MGLMs involves intractable integrals. We discuss several alternatives and assess their performance in realistic situations, referring to some of our earlier work using simulated data and a case study [81, 82] and introducing new results.We review a range of approximate estimation procedures that, unfortunately, can be severely biased when random effects are substantial. We describe maximum likelihood estimation using Gauss-Hermite quadrature, a method that appears to work remarkably well, but is limited to relatively low-dimensional models. We also discuss Bayesian estimation procedures focusing on the Gibbs sampler, a Markov Chain Monte Carlo (MCMC) method that can be pplied to more complex models involving high-dimensional integrals, albeit not without difficulty. We close this section with a brief discussion of other approaches to estimation, an active area of current research. Section 9.4 is devoted to an application of MGLMs to the study of infant and child mortality in Kenya, using data from a national survey conducted in 1998. We use a three-level piece-wise exponential survival model that allows for clustering of infant and child deaths at both the family and community levels, and fit it to data using the equivalent MGLM with Poisson errors and log link. We compare estimates that ignore clustering, and estimates obtained by approximate quasi-likelihood and by full maximum likelihood. The discussion emphasizes interpretation of the results, particularly the family and community random parameters. Finally, we show how the model can be used to estimate measures of intra-family and intra-community correlation in infant and child deaths. Section 9.5 is a brief discussion and summary of our conclusions. Ã‚Â© 2008 Springer Science+Business Media, LLC.",0
https://doi.org/10.3168/jds.2008-1497,Factors affecting cure when treating bovine clinical mastitis with cephalosporin-based intramammary preparations,"Data were collated for an independent scientific analysis from 2 international, multicenter studies that had compared the efficacy of 3 different cephalosporin-containing intramammary preparations in the treatment of clinical mastitis in dairy cattle [cefalexin (first generation) in combination with kanamycin; cefquinome (fourth generation); and cefoperazone (third generation)]. Quarters were assessed using standard bacteriological techniques before treatment and at 16 and 25 d posttreatment. Additional data were also available on individual cows and study farms, including parity, breed, and cow somatic cell count histories, herd bulk milk somatic cell counts, and farm management regimens. Sufficient data for analysis were available from a total of 491 cases on 192 farms in 3 countries (United Kingdom, France, and Germany) with up to 16 cases being recruited from any one farm. Clinical cases were of diverse etiology, representing both contagious and environmental pathogens. Univariable analysis demonstrated that quarters in the cefalexin + kanamycin and cefquinome treatment groups were not significantly different from each other, but were both significantly more likely to be pathogen free posttreatment than quarters in the cefoperazone group. Multivariable analysis was undertaken using conventional random effects models. Two models were built, with the first incorporating only information available to the practitioner at the time of treatment and the second including all information collected during the study. These models indicated that country, pretreatment rectal temperature (above-normal temperature associated with an increased chance of being pathogen free posttreatment), individual cow somatic cell count (increased somatic cell count associated with a decreased chance of being pathogen free posttreatment), and pathogen (Staphylococcus aureus isolation associated with a decreased chance of being pathogen free posttreatment) were useful predictors of pathogen free status; parity, yield, bulk milk somatic cell counts, and other farm management factors were not. The importance of country in the analysis demonstrates the need to generate local data when assessing treatment regimens. In addition, these results suggest that the factors important in predicting the outcome of treatment of clinical mastitis cases may be dissimilar to those reported to affect the likelihood of cure when treating subclinical intramammary infections.",0
https://doi.org/10.2307/2094445,The Decomposition of Effects in Path Analysis,"This paper is about the logic of interpreting recursive causal theories in sociology. We review the distinction between associations and effects and discuss the decomposition of effects into direct and indirect components. We then describe a general method for decomposing effects into their components by the systematic application of ordinary least squares regression. The method involves successive computation of reduced-form equations, beginning with an equation containing only exogenous variables, then computing equations which add intervening variables in sequence from cause to effect. This generates all the information required to decompose effects into their various direct and indirect parts. This method is a substitute for the often more cumbersome computation of indirect effects from the structural coefficients (direct effects) of the causal model Finally, we present a way of summarizing this information in tabular form and illustrate the procedures using an empirical example.",0
https://doi.org/10.1177/0049124199028002001,Assessing Direct and Indirect Effects in Multilevel Designs with Latent Variables,"Researchers commonly ask whether relationships between exogenous predictors, X, and outcomes, Y, are mediated by a third set of variables, Z. Simultaneous equations decompose the relationship between X and Y into an indirect component, operating through Z, and a direct component, the relationship between X and Y given Z. Often, X, Y, and/or Z are measured with error. Structural equation modeling is widely used in this scenario. However, sociological data commonly have a nested structure (students within schools, residents within local areas). Hierarchical linear models represent such multilevel data well and can handle errors of measurement, but have not incorporated simultaneous equations for direct and indirect effects. This article incorporates the study of such mediated effects into the hierarchical linear model, naturally extending the analysis to include unbalanced, multilevel designs and missing data. The authors illustrate the approach by examining the extent to which neighborhood social control mediates the relationship between neighborhood social composition and violence in Chicago.",0
https://doi.org/10.1037/1082-989x.11.2.127,Structural equation modeling with interchangeable dyads.,"Structural equation modeling (SEM) can be adapted in a relatively straightforward fashion to analyze data from interchangeable dyads (i.e., dyads in which the 2 members cannot be differentiated). The authors describe a general strategy for SEM model estimation, comparison, and fit assessment that can be used with either dyad-level or pairwise (double-entered) dyadic data. They present applications illustrating this approach with the actor-partner interdependence model, confirmatory factor analysis, and latent growth curve analysis.",0
https://doi.org/10.1002/sim.5677,Level-adjusted funnel plots based on predicted marginal expectations: an application to prophylactic antibiotics in gallstone surgery,"Funnel plots are widely used to visualize grouped data, for example, in institutional comparison. This paper extends the concept to a multi-level setting, displaying one level at a time, adjusted for the other levels, as well as for covariates at all levels. These level-adjusted funnel plots are based on a Markov chain Monte Carlo fit of a random effects model, translating the estimated model parameters to predicted marginal expectations. Working within the estimation framework, we accommodate outlying institutions using heavy-tailed random effects distributions. We also develop computer-efficient methods to compute predicted probabilities in the case of dichotomous outcome data and various random effect distributions. We apply the method to a data set on prophylactic antibiotics in gallstone surgery.",0
https://doi.org/10.1007/s11336-013-9360-2,Additive Multilevel Item Structure Models with Random Residuals: Item Modeling for Explanation and Item Generation,"An additive multilevel item structure (AMIS) model with random residuals is proposed. The model includes multilevel latent regressions of item discrimination and item difficulty parameters on covariates at both item and item category levels with random residuals at both levels. The AMIS model is useful for explanation purposes and also for prediction purposes as in an item generation context. The parameters can be estimated with an alternating imputation posterior algorithm that makes use of adaptive quadrature, and the performance of this algorithm is evaluated in a simulation study. Ã‚Â© 2013 The Psychometric Society.",0
https://doi.org/10.1177/0146621612446170,Recovery of Graded Response Model Parameters,"Markov chain Monte Carlo (MCMC) methods enable a fully Bayesian approach to parameter estimation of item response models. In this simulation study, the authors compared the recovery of graded response model parameters using marginal maximum likelihood (MML) and Gibbs sampling (MCMC) under various latent trait distributions, test lengths, and sample sizes. Sample size and test length explained the largest amount of variance in item and person parameter estimates, respectively. There was little difference in item parameter recovery between MML and MCMC in samples with 300 or more respondents. MCMC recovered some item threshold parameters better in samples with 75 or 150 respondents. Bias in threshold parameter estimates depended on the generating value and the type of threshold. Person parameters were comparable between MCMC and MML/expected a posteriori for all test lengths.",0
https://doi.org/10.1177/0958928713511280,What explains ‘generosity’ in the public financing of high-tech drugs? An empirical investigation of 25 OECD countries and 11 controversial drugs,"In times of increasing cost pressures, public healthcare systems in Organisation for Economic Co-operation and Development (OECD) countries face the question of whether and to which extent new high-tech drugs are to be financed within their public healthcare systems. Systematic empirical research that explains across-country variation in these decisions is, however, almost non-existent. We analyse an original dataset that contains coverage decisions for 11 controversial drugs in 25 OECD countries using multilevel modelling. Our results indicate that the ‘generosity’ with which controversial new drugs are publicly financed is unrelated to a country’s wealth and general expenditure levels for healthcare. However, healthcare systems financed through social insurance contributions tend to be more generous than tax-financed ones. Moreover, we uncover evidence suggesting that the institutional characteristics of the decision-making process matter systematically for decisions on whether to finance controversial drugs.",0
https://doi.org/10.1016/j.socnet.2014.02.002,Food sharing networks in lowland Nicaragua: An application of the social relations model to count data,"Abstract Previous research on food sharing in small-scale societies provides support for multiple evolutionary hypotheses, but evolutionary anthropologists have devoted relatively little attention to the broader relational context of inter-household transfers of food. The present research observes transfers of meat over a yearlong period among 25 households of indigenous Mayangna and Miskito horticulturalists in Nicaragua. To analyze these data, we extend the multilevel formulation of the social relations model to count data, namely the number of portions of meat exchanged between households. Along with other covariates, we examine the effect of an “association index,” which reflects the amount of time that households interact with one another. The association index exhibits a positive effect on sharing, and our overall results indicate that food sharing networks largely correspond to kin-based networks of social interaction, suggesting that food sharing is embedded in broader social relationships between households. We discuss possible extensions of our methodological approach, as appropriate for research on food sharing and social network analysis more broadly.",0
https://doi.org/10.1177/0146621608324023,I've Fallen and I Can't Get Up: Can High-Ability Students Recover From Early Mistakes in CAT?,"A difficult result to interpret in Computerized Adaptive Tests (CATs) occurs when an ability estimate initially drops and then ascends continuously until the test ends, suggesting that the true ability may be higher than implied by the final estimate. This study explains why this asymmetry occurs and shows that early mistakes by high-ability students can lead to considerable underestimation, even in tests with 45 items. The opposite response pattern, where low-ability students start with lucky guesses, leads to much less bias. The authors show that using Barton and Lord's four-parameter model (4PM) and a less informative prior can lower bias and root mean square error (RMSE) for high-ability students with a poor start, as the CAT algorithm ascends more quickly after initial underperformance. Results also show that the 4PM slightly outperforms a CAT in which less discriminating items are initially used. The practical implications and relevance for psychological measurement more generally are discussed.",0
https://doi.org/10.1007/bf00144234,"The problem of multicollinearity in a multistage causal alienation model: A comparison of ordinary least squares, maximum-likelihood and ridge estimators",,0
https://doi.org/10.1080/01966324.2007.10737689,Bayesian Analysis of Dyadic Data,"SYNOPTIC ABSTRACTThis paper considers the Bayesian analysis of dyadic data with particular emphasis on applications in social psychology. Various existing models are extended and unified under a class of models where a single value is elicited to complete the prior specification. Certain situations which have sometimes been problematic (e.g. incomplete data, non-standard covariates, missing data, unbalanced data) are easily handled under the proposed class of Bayesian models. Inference is straightforward using software that is based on Markov chain Monte Carlo methods. Examples are provided which highlight the variety of data sets that can be entertained and the ease in which they can now be analyzed.",0
https://doi.org/10.1093/pan/mpi033,Two-Step Hierarchical Estimation: Beyond Regression Analysis,"Two-step estimators for hierarchical models can be constructed even when neither stage is a conventional linear regression model. For example, the first stage might consist of probit models, or duration models, or event count models. The second stage might be a nonlinear regression specification. This note sketches some of the considerations that arise in ensuring that two-step estimators are consistent in such cases.",0
https://doi.org/10.1080/00273171.2014.977429,Detecting Misspecified Multilevel Structural Equation Models with Common Fit Indices: A Monte Carlo Study,"This study investigated the sensitivity of common fit indices (i.e., RMSEA, CFI, TLI, SRMR-W, and SRMR-B) for detecting misspecified multilevel SEMs. The design factors for the Monte Carlo study were numbers of groups in between-group models (100, 150, and 300), group size (10, 20, 30, and 60), intra-class correlation (low, medium, and high), and the types of model misspecification (Simple and Complex). The simulation results showed that CFI, TLI, and RMSEA could only identify the misspecification in the within-group model. Additionally, CFI, TLI, and RMSEA were more sensitive to misspecification in pattern coefficients while SRMR-W was more sensitive to misspecification in factor covariance. Moreover, TLI outperformed both CFI and RMSEA in terms of the hit rates of detecting the within-group misspecification in factor covariance. On the other hand, SRMR-B was the only fit index sensitive to misspecification in the between-group model and more sensitive to misspecification in factor covariance than misspecification in pattern coefficients. Finally, we found that the influence of ICC on the performance of targeted fit indices was trivial.",0
https://doi.org/10.1007/bf02296338,Maximum likelihood estimation of latent interaction effects with the LMS method,"In the context of structural equation modeling, a general interaction model with multiple latent interaction effects is introduced. A stochastic analysis represents the nonnormal distribution of the joint indicator vector as a finite mixture of normal distributions. The Latent Moderated Structural Equations (LMS) approach is a new method developed for the analysis of the general interaction model that utilizes the mixture distribution and provides a ML estimation of model parameters by adapting the EM algorithm. The finite sample properties and the robustness of LMS are discussed. Finally, the applicability of the new method is illustrated by an empirical example.",0
https://doi.org/10.1037/a0036635,Small sample mediation testing: Misplaced confidence in bootstrapped confidence intervals.,"Bootstrapping is an analytical tool commonly used in psychology to test the statistical significance of the indirect effect in mediation models. Bootstrapping proponents have particularly advocated for its use for samples of 20-80 cases. This advocacy has been heeded, especially in the Journal of Applied Psychology, as researchers are increasingly utilizing bootstrapping to test mediation with samples in this range. We discuss reasons to be concerned with this escalation, and in a simulation study focused specifically on this range of sample sizes, we demonstrate not only that bootstrapping has insufficient statistical power to provide a rigorous hypothesis test in most conditions but also that bootstrapping has a tendency to exhibit an inflated Type I error rate. We then extend our simulations to investigate an alternative empirical resampling method as well as a Bayesian approach and demonstrate that they exhibit comparable statistical power to bootstrapping in small samples without the associated inflated Type I error. Implications for researchers testing mediation hypotheses in small samples are presented. For researchers wishing to use these methods in their own research, we have provided R syntax in the online supplemental materials.",1
https://doi.org/10.1111/j.1540-5907.2010.00452.x,Who Learns from What in Policy Diffusion Processes?,"The idea that policy makers in different states or countries may learn from one another has fascinated scholars for a long time, but little systematic evidence has been produced so far. This article improves our understanding of this elusive argument by distinguishing between the policy and political consequences of reforms and by emphasizing the conditional nature of learning processes. Using a directed dyadic approach and multilevel methods, the analysis of unemployment benefits retrenchment in OECD countries demonstrates that policy makers learn selectively from the experience of others. Right governments tend to be more sensitive to information on the electoral consequences of reforms, while left governments are more likely to be influenced by their policy effects.",0
https://doi.org/10.1016/s0376-8716(02)00216-8,Mediation designs for tobacco prevention research,"This paper describes research designs and statistical analyses to investigate how tobacco prevention programs achieve their effects on tobacco use. A theoretical approach to program development and evaluation useful for any prevention program guides the analysis. The theoretical approach focuses on action theory for how the program affects mediating variables and on conceptual theory for how mediating variables are related to tobacco use. Information on the mediating mechanisms by which tobacco prevention programs achieve effects is useful for the development of efficient programs and provides a test of the theoretical basis of prevention efforts. Examples of these potential mediating mechanisms are described including mediated effects through attitudes, social norms, beliefs about positive consequences, and accessibility to tobacco. Prior research provides evidence that changes in social norms are a critical mediating mechanism for successful tobacco prevention. Analysis of mediating variables in single group designs with multiple mediators are described as well as multiple group randomized designs which are the most likely to accurately uncover important mediating mechanisms. More complicated dismantling and constructive designs are described and illustrated based on current findings from tobacco research. Mediation analysis for categorical outcomes and more complicated statistical methods are outlined.",0
https://doi.org/10.1080/00273171.2016.1167008,Modeling Clustered Data with Very Few Clusters,"Small-sample inference with clustered data has received increased attention recently in the methodological literature, with several simulation studies being presented on the small-sample behavior of many methods. However, nearly all previous studies focus on a single class of methods (e.g., only multilevel models, only corrections to sandwich estimators), and the differential performance of various methods that can be implemented to accommodate clustered data with very few clusters is largely unknown, potentially due to the rigid disciplinary preferences. Furthermore, a majority of these studies focus on scenarios with 15 or more clusters and feature unrealistically simple data-generation models with very few predictors. This article, motivated by an applied educational psychology cluster randomized trial, presents a simulation study that simultaneously addresses the extreme small sample and differential performance (estimation bias, Type I error rates, and relative power) of 12 methods to account for clustered data with a model that features a more realistic number of predictors. The motivating data are then modeled with each method, and results are compared. Results show that generalized estimating equations perform poorly; the choice of Bayesian prior distributions affects performance; and fixed effect models perform quite well. Limitations and implications for applications are also discussed.",1
https://doi.org/10.1177/01466219922031130,Item Parameter Recovery for the Nominal Response Model,"Establishing guidelines for reasonable item parameter estimation is fundamental to use of the nominal response model. Factors studied were the sample size ratio (SSR), latent trait distribution (LD), and amount of item information. Results showed that the LD accounted for 42.5% of the variability in the accuracy of estimating the slope parameter; the SSR and the maximum item information factors accounted for 29.5% and 3.5% of the accuracy, respectively. In general, as the LD departed from a normal distribution, a larger number of examinees was required to accurately estimate the slope and intercept parameters. Results indicated that an SSR of 10:1 can produce reasonably accurate item parameter estimates when the LD is normal.",0
https://doi.org/10.1348/000711007x227067,Controlling item exposure and test overlap on the fly in computerized adaptive testing,"This paper proposes an on-line version of the Sympson and Hetter procedure with test overlap control (SHT) that can provide item exposure control at both the item and test levels on the fly without iterative simulations. The on-line procedure is similar to the SHT procedure in that exposure parameters are used for simultaneous control of item exposure rates and test overlap rate. The exposure parameters for the on-line procedure, however, are updated sequentially on the fly, rather than through iterative simulations conducted prior to operational computerized adaptive tests (CATs). Unlike the SHT procedure, the on-line version can control item exposure rate and test overlap rate without time-consuming iterative simulations even when item pools or examinee populations have been changed. Moreover, the on-line procedure was found to perform better than the SHT procedure in controlling item exposure and test overlap for examinees who take tests earlier. Compared with two other on-line alternatives, this proposed on-line method provided the best all-around test security control. Thus, it would be an efficient procedure for controlling item exposure and test overlap in CATs.",0
https://doi.org/10.1017/s0003055409990050,Gay Rights in the States: Public Opinion and Policy Responsiveness,"We study the effects of policy-specific public opinion on state adoption of policies affecting gays and lesbians, and the factors that condition this relationship. Using national surveys and advances in opinion estimation, we create new estimates of state-level support for eight policies, including civil unions and nondiscrimination laws. We differentiate between responsiveness to opinion and congruence with opinion majorities. We find a high degree of responsiveness, controlling for interest group pressure and the ideology of voters and elected officials. Policy salience strongly increases the influence of policy-specific opinion (directly and relative to general voter ideology). There is, however, a surprising amount of noncongruence—for some policies, even clear supermajority support seems insufficient for adoption. When noncongruent, policy tends to be more conservative than desired by voters; that is, there is little progay policy bias. We find little to no evidence that state political institutions affect policy responsiveness or congruence.",0
https://doi.org/10.1002/(sici)1097-0258(19990715)18:13<1707::aid-sim138>3.0.co;2-h,Longitudinal data analysis (repeated measures) in clinical trials,"Longitudinal data is often collected in clinical trials to examine the effect of treatment on the disease process over time. This paper reviews and summarizes much of the methodological research on longitudinal data analysis from the perspective of clinical trials. We discuss methodology for analysing Gaussian and discrete longitudinal data and show how these methods can be applied to clinical trials data. We illustrate these methods with five examples of clinical trials with longitudinal outcomes. We also discuss issues of particular concern in clinical trials including sequential monitoring and adjustments for missing data. A review of current software for analysing longitudinal data is also provided. Published in 1999 by John Wiley & Sons, Ltd. This article is a US Government work and is the public domain in the United States.",0
https://doi.org/10.1152/jn.00206.2011,Attention to baseline: does orienting visuospatial attention really facilitate target detection?,"Standard protocols testing the orientation of visuospatial attention usually present spatial cues before targets and compare valid-cue trials with invalid-cue trials. The valid/invalid contrast results in a relative behavioral or physiological difference that is generally interpreted as a benefit of attention orientation. However, growing evidence suggests that inhibitory control of response is closely involved in this kind of protocol that requires the subjects to withhold automatic responses to cues, probably biasing behavioral and physiological baselines. Here, we used two experiments to disentangle the inhibitory control of automatic responses from orienting of visuospatial attention in a saccadic reaction time task in humans, a variant of the classical cue-target detection task and a sustained visuospatial attentional task. Surprisingly, when referring to a simple target detection task in which there is no need to refrain from reacting to avoid inappropriate responses, we found no consistent evidence of facilitation of target detection at the attended location. Instead, we observed a cost at the unattended location. Departing from the classical view, our results suggest that reaction time measures of visuospatial attention probably relie on the attenuation of elementary processes involved in visual target detection and saccade initiation away from the attended location rather than on facilitation at the attended location. This highlights the need to use proper control conditions in experimental designs to disambiguate relative from absolute cueing benefits on target detection reaction times, both in psychophysical and neurophysiological studies.",0
https://doi.org/10.2307/2648877,Econometric Issues in the Analysis of Regressions with Generated Regressors,,0
https://doi.org/10.1177/0049124106289164,Matching Estimators of Causal Effects,"As the counterfactual model of causality has increased in popularity, sociologists have returned to matching as a research methodology. In this article, advances over the past two decades in matching estimators are explained, and the practical limitations of matching techniques are emphasized. The authors introduce matching methods by focusing first on ideal scenarios in which stratification and weighting procedures warrant causal inference. Then, they discuss how matching is often undertaken in practice, offering an overview of the most prominent data analysis routines. With four hypothetical examples, they demonstrate how the assumptions behind matching estimators often break down in practice. Even so, the authors argue that matching techniques can be used effectively to strengthen the prosecution of causal questions in sociology.",0
https://doi.org/10.1093/ijpor/edp021,An Application of the Estimated Dependent Variable Approach: Trade Union Members' Support for Active Labor Market Policies and Insider-Outsider Politics,,0
https://doi.org/10.1007/bf02294842,Maximum likelihood estimation of nonlinear structural equation models,"The existing maximum likelihood theory and its computer software in structural equation modeling are established based on linear relationships among manifest variables and latent variables. However, models with nonlinear relationships are often encountered in social and behavioral sciences. In this article. an EM type algorithm is developed for maximum likelihood estimation of a general nonlinear structural equation model. To avoid computation of the complicated multiple integrals involved, the E-step is completed by a Metropolis-Hastings algorithm. It is shown that the M-step can be completed efficiently by simple conditional maximization. Standard errors of the maximum likelihood estimates are obtained via Louis's formula. The methodology is illustrated with results from a simulation study and two real examples.",0
https://doi.org/10.1016/j.yrtph.2009.01.011,Interspecies extrapolation in environmental exposure standard setting: A Bayesian synthesis approach,"Currently the extrapolation of evidence from studies of non-human species to the setting of environmental exposure standards for humans includes the imposition of a variety of uncertainty factors reflecting unknown aspects of the procedure, including the relevance of evidence from one species to impacts in another. This paper develops and explores more flexible modelling of aspects of this extrapolation, using models proposed by DuMouchel [DuMouchel, W.H., Harris, J.E., 1983. Bayes methods for combining the results of cancer studies in humans and other species (with comment). J. Am. Statist. Assoc. 78, 293–308.] The approaches are based on Bayesian meta-analysis methods involving explicit modelling of relevance in the prior distributions, estimated using Markov chain Monte Carlo (MCMC) methods. The methods are applied to evidence relating chlorinated by-products exposure to adverse reproductive health effects. The relative merits of various approaches are discussed, and developments and next steps are outlined.",0
https://doi.org/10.1002/9781119970583,Latent Variable Models and Factor Analysis,"Latent Variable Models and Factor Analysis provides a comprehensive and unified approach to factor analysis and latent variable modeling from a statistical perspective. This book presents a general framework to enable the derivation of the commonly used models, along with updated numerical examples. Nature and interpretation of a latent variable is also introduced along with related techniques for investigating dependency. This book: Provides a unified approach showing how such apparently diverse methods as Latent Class Analysis and Factor Analysis are actually members of the same family. Presents new material on ordered manifest variables, MCMC methods, non-linear models as well as a new chapter on related techniques for investigating dependency. Includes new sections on structural equation models (SEM) and Markov Chain Monte Carlo methods for parameter estimation, along with new illustrative examples. Looks at recent developments on goodness-of-fit test statistics and on non-linear models and models with mixed latent variables, both categorical and continuous. No prior acquaintance with latent variable modelling is pre-supposed but a broad understanding of statistical theory will make it easier to see the approach in its proper perspective. Applied statisticians, psychometricians, medical statisticians, biostatisticians, economists and social science researchers will benefit from this book. Â© 2011 John Wiley & Sons, Ltd. All rights reserved.",0
https://doi.org/10.1016/j.jmva.2014.12.013,Joint prior distributions for variance parameters in Bayesian analysis of normal hierarchical models,"In random effect models, error variance (stage 1 variance) and scalar random effect variance components (stage 2 variances) are a priori modeled independently. Considering the intrinsic link between the stages 1 and 2 variance components and their interactive effect on the parameter draws in Gibbs sampling, we propose modeling the variances of the two stages a priori jointly in a multivariate fashion. We use random effects linear growth model for illustration and consider multivariate distributions to model the variance components jointly including the recently developed generalized multivariate log gamma (G-MVLG) distribution. We discuss these variance priors as well as the independent variance priors exercised in the literature in different aspects including noninformativeness and propriety of the associated posterior density. We show through an extensive simulation experiment that modeling the variance components of different stages multivariately results in better estimation properties for the response and random effect model parameters compared to independent modeling. We scrutinize the sensitivity of response model coefficient estimates to the parameters of considered noninformative variance priors and find that their full conditional expectations are insensitive to noninformative G-MVLG prior parameters. We apply independent and joint models for analysis of a real dataset and find that multivariate priors for variance components lead to better fitted hierarchical model than the univariate variance priors.",0
https://doi.org/10.1037/1082-989x.10.4.477,Inequality Constrained Analysis of Variance: A Bayesian Approach.,"Researchers often have one or more theories or expectations with respect to the outcome of their empirical research. When researchers talk about the expected relations between variables if a certain theory is correct, their statements are often in terms of one or more parameters expected to be larger or smaller than one or more other parameters. Stated otherwise, their statements are often formulated using inequality constraints. In this article, a Bayesian approach to evaluate analysis of variance or analysis of covariance models with inequality constraints on the (adjusted) means is presented. This evaluation contains two issues: estimation of the parameters given the restrictions using the Gibbs sampler and model selection using Bayes factors in the case of competing theories. The article concludes with two illustrations: a one-way analysis of covariance and an analysis of a three-way table of ordered means.",0
https://doi.org/10.1016/j.csda.2003.10.005,Small sample inference for the fixed effects in the mixed linear model,"The small sample performance of several procedures for testing a given fixed effect in a mixed linear model is investigated. Using simulations, constructed on the basis of a study of growth of children with Gaucher's disease, standard normal-theory Wald tests for both ML and REML estimates, the likelihood ratio test (LRT), a modified LRT based on Bartlett correction, and a number of adjusted tests based on t and F distributions are evaluated. Methods used for determining the denominator degrees of freedom in the t and F tests include the residual degrees of freedom method, the between and within degrees of freedom, the containment method, the naive method and the Satterthwaite method. A test based on a sandwich-type estimator of the variance of the parameter estimate is evaluated as well and the effect of mis-specifying the random-effects distribution is considered. Results show that Type I error rates for the Wald-type test with chi-square approximation are substantially inflated, though less so with REML estimates than with ML estimates. The LRT based on ML estimates yielded Type I error rates similar to those observed for the Wald-type chi-square test with REML estimates. A substantial improvement in Type I error rates for testing on both the intercept and slope is provided by each of the three following modifications: the Satterthwaite and naive methods with REML-based estimates and the Bartlett-corrected LRT.",0
https://doi.org/10.1207/s15328007sem0703_3,Bayesian Analysis of Structural Equation Model With Fixed Covariates,"This article extends the LISREL model to incorporate fixed covariates at both the measurement and the structural equations of the model. A Bayesian procedure with conjugate type prior distributions is established. The joint Bayesian estimates of the latent variables and the structural parameters that involve the regression coefficients of the covariates, the variances, covariances and causations among the manifest and latent variables are obtained via the Gibbs sampler algorithm. It is shown that the conditional distributions required in the Gibbs sampler are familiar distributions, hence the algorithm is very efficient. A goodness of fit statistic for assessing the proposed model is presented. An illustrative example with some real data is presented.",0
https://doi.org/10.1002/jrsm.1129,Multivariate meta‐analysis using individual participant data,"When combining results across related studies, a multivariate meta-analysis allows the joint synthesis of correlated effect estimates from multiple outcomes. Joint synthesis can improve efficiency over separate univariate syntheses, may reduce selective outcome reporting biases, and enables joint inferences across the outcomes. A common issue is that within-study correlations needed to fit the multivariate model are unknown from published reports. However, provision of individual participant data (IPD) allows them to be calculated directly. Here, we illustrate how to use IPD to estimate within-study correlations, using a joint linear regression for multiple continuous outcomes and bootstrapping methods for binary, survival and mixed outcomes. In a meta-analysis of 10 hypertension trials, we then show how these methods enable multivariate meta-analysis to address novel clinical questions about continuous, survival and binary outcomes; treatment–covariate interactions; adjusted risk/prognostic factor effects; longitudinal data; prognostic and multiparameter models; and multiple treatment comparisons. Both frequentist and Bayesian approaches are applied, with example software code provided to derive within-study correlations and to fit the models. © 2014 The Authors. Research Synthesis Methods published by John Wiley & Sons, Ltd.",0
https://doi.org/10.1177/0049124107301947,Latent Variable Models Under Misspecification: Two-Stage Least Squares (2SLS) and Maximum Likelihood (ML) Estimators,"This article compares maximum likelihood (ML) estimation to three variants of two-stage least squares (2SLS) estimation in structural equation models. The authors use models that are both correctly and incorrectly specified. Simulated data are used to assess bias, efficiency, and accuracy of hypothesis tests. Generally, 2SLS with reduced sets of instrumental variables performs similarly to ML when models are correctly specified. Under correct specification, both estimators have little bias except at the smallest sample sizes and are approximately equally efficient. As predicted, when models are incorrectly specified, 2SLS generally performs better, with less bias and more accurate hypothesis tests. Unless a researcher has tremendous confidence in the correctness of his or her model, these results suggest that a 2SLS estimator should be considered.",0
https://doi.org/10.1037/a0012320,"Where similarity beats redundancy: The importance of context, higher order similarity, and response assignment.","People are especially efficient in processing certain visual stimuli such as human faces or good configurations. It has been suggested that topology and geometry play important roles in configural perception. Visual search is one area in which configurality seems to matter. When either of 2 target features leads to a correct response and the sequence includes trials in which either or both targets are present, the result is a redundant-target paradigm. It is common for such experiments to find faster performance with the double target than with either alone, something that is difficult to explain with ordinary serial models. This redundant-targets study uses figures that can be dissimilar in their topology and geometry and manipulates the stimulus set and the stimulus?response assignments. The authors found that the combination of higher order similarity (e.g., topological) among the features in the stimulus set and response assignment can effectively overpower or facilitate the redundant-target effect, depending on the exact nature of the former characteristics. Several reasonable models of redundant-targets performance are falsified. Parallel models with the potential for channel interactions are supported by the data.",0
https://doi.org/10.1093/bioinformatics/btt633,Testing multiple biological mediators simultaneously,"Abstract Motivation: Modern biomedical and epidemiological studies often measure hundreds or thousands of biomarkers, such as gene expression or metabolite levels. Although there is an extensive statistical literature on adjusting for ‘multiple comparisons’ when testing whether these biomarkers are directly associated with a disease, testing whether they are biological mediators between a known risk factor and a disease requires a more complex null hypothesis, thus offering additional methodological challenges. Results: We propose a permutation approach that tests multiple putative mediators and controls the family wise error rate. We demonstrate that, unlike when testing direct associations, replacing the Bonferroni correction with a permutation approach that focuses on the maximum of the test statistics can significantly improve the power to detect mediators even when all biomarkers are independent. Through simulations, we show the power of our method is 2–5× larger than the power achieved by Bonferroni correction. Finally, we apply our permutation test to a case-control study of dietary risk factors and colorectal adenoma to show that, of 149 test metabolites, docosahexaenoate is a possible mediator between fish consumption and decreased colorectal adenoma risk. Availability and implementation: R-package included in online Supplementary Material. Contact: joshua.sampson@nih.gov Supplementary information: Supplementary materials are available at Bioinformatics online.",0
https://doi.org/10.1037/0022-0663.90.1.153,Individual differences in dyadic cooperative learning.,"The impact of individual differences on the performance of 2 roles-learner and learning facilitator-was assessed during dyadic cooperative learning. Eighty university students, 40 men and 40 women, participated in same-sex groups of 4. Each student cooperatively learned a text passage with 1 partner and then learned a 2nd passage with another partner. It a later session, the students recalled the information contained within both text passages and completed several personality measures. A social relations analysis (D. A. Kenny & L. LaVoie, 1984) was used to partition the variability in recall for the passages into various sources. Variability in recall depended strongly on individual differences in learning ability and (to a lesser extent) on individual differences in the ability to facilitate others' learning. Differences in the ability to learn text passages were independent of individual differences in the ability to facilitate others' learning. Effective learners were high in verbal ability, whereas effective learning facilitators were low in public self-consciousness and in self-monitoring. The influence of cognitive and rapport factors on the performance of the learner role and the learning facilitator role is discussed.",0
https://doi.org/10.1016/j.neuroimage.2015.05.041,Dynamic causal modelling of brain–behaviour relationships,"In this work, we expose a mathematical treatment of brain-behaviour relationships, which we coin behavioural Dynamic Causal Modelling or bDCM. This approach aims at decomposing the brain's transformation of stimuli into behavioural outcomes, in terms of the relative contribution of brain regions and their connections. In brief, bDCM places the brain at the interplay between stimulus and behaviour: behavioural outcomes arise from coordinated activity in (hidden) neural networks, whose dynamics are driven by experimental inputs. Estimating neural parameters that control network connectivity and plasticity effectively performs a neurobiologically-constrained approximation to the brain's input-outcome transform. In other words, neuroimaging data essentially serves to enforce the realism of bDCM's decomposition of input-output relationships. In addition, post-hoc artificial lesions analyses allow us to predict induced behavioural deficits and quantify the importance of network features for funnelling input-output relationships. This is important, because this enables one to bridge the gap with neuropsychological studies of brain-damaged patients. We demonstrate the face validity of the approach using Monte-Carlo simulations, and its predictive validity using empirical fMRI/behavioural data from an inhibitory control task. Lastly, we discuss promising applications of this work, including the assessment of functional degeneracy (in the healthy brain) and the prediction of functional recovery after lesions (in neurological patients).",0
https://doi.org/10.1080/00273170701710338,Observations on the Use of Growth Mixture Models in Psychological Research,"Psychologists are applying growth mixture models at an increasing rate. This article argues that most of these applications are unlikely to reproduce the underlying taxonic structure of the population. At a more fundamental level, in many cases there is probably no taxonic structure to be found. Latent growth classes then categorically approximate the true continuum of individual differences in change. This approximation, although in some cases potentially useful, can also be problematic. The utility of growth mixture models for psychological science thus remains in doubt. Some ways in which these models might be more profitably used are suggested.",0
https://doi.org/10.1080/03610929508831616,Eliciting prior information to enhance the predictive performance of bayesian graphical models,"Both knowledge-based systems and statistical models are typically concerned with making predictions about future observables. Here we focus on assessment of predictive performance and provide two techniques for improving the predictive performance of Bayesian graphical models. First, we present Bayesian model averaging, a technique for accounting for model uncertainty. Second, we describe a technique for eliciting a prior distribution for competing models from domain experts. We explore the predictive performance of both techniques in the context of a urological diagnostic problem.",0
https://doi.org/10.1080/01621459.1987.10478441,Model-Based Direct Adjustment,"Abstract Direct adjustment or standardization applies population weights to subclass means in an effort to estimate population quantities from a sample that is not representative of the population. Direct adjustment has several attractive features, but when there are many subclasses it can attach large weights to small quantities of data, often in a fairly erratic manner. In the extreme, direct adjustment can attach infinite weight to nonexistent data, a noticeable inconvenience in practice. This article proposes a method of model-based direct adjustment that preserves the attractive features of conventional direct adjustment while stabilizing the weights attached to small subclasses. The sample mean and conventional direct adjustment are both special cases of model-based direct adjustment under two different extreme models for the subclass-specific selection probabilities. The discussion of this method provides some insights into the behavior of true and estimated propensity scores: the estimated scores ...",0
https://doi.org/10.1111/j.1368-423x.2007.00212.x,Propensity score matching without conditional independence assumption—with an application to the gender wage gap in the United Kingdom,"revised version of Discussion paper 2002-08#### Propensity score matching is frequently used for estimating average treatment effects. Its applicability, however, is not confined to treatment evaluation. In this paper it is shown that propensity score matching does not hinge on a selection on observables assumption and can be used to estimate not only adjusted means but also their distributions, even with non-iid sampling. Propensity score matching is used to analyze the gender wage gap among graduates in the UK. It is found that subject of degree contributes substantially to explaining the gender wage gap, particularly at higher quantiles of the wage distribution. Download Discussion Paper: (pdf, 910 kb)",0
https://doi.org/10.1016/s0169-7161(07)27002-6,"2 Statistical Inference for Causal Effects, With Emphasis on Applications in Epidemiology and Medical Statistics","Abstract A central problem in epidemiology and medical statistics is how to draw inferences about the causal effects of treatments (i.e., interventions) from randomized and nonrandomized data. For example, does the new drug really reduce heart disease, or does exposure to that chemical in drinking water increase cancer rates relative to drinking water without that chemical? This chapter provides an overview of the approach to the estimation of such causal effects based on the concept of potential outcomes. We discuss randomization-based approaches and the Bayesian posterior predictive approach.",0
https://doi.org/10.3758/s13414-014-0825-x,Modeling visual search using three-parameter probability functions in a hierarchical Bayesian framework,"In this study, we applied Bayesian-based distributional analyses to examine the shapes of response time (RT) distributions in three visual search paradigms, which varied in task difficulty. In further analyses we investigated two common observations in visual search-the effects of display size and of variations in search efficiency across different task conditions-following a design that had been used in previous studies (Palmer, Horowitz, Torralba, & Wolfe, Journal of Experimental Psychology: Human Perception and Performance, 37, 58-71, 2011; Wolfe, Palmer, & Horowitz, Vision Research, 50, 1304-1311, 2010) in which parameters of the response distributions were measured. Our study showed that the distributional parameters in an experimental condition can be reliably estimated by moderate sample sizes when Monte Carlo simulation techniques are applied. More importantly, by analyzing trial RTs, we were able to extract paradigm-dependent shape changes in the RT distributions that could be accounted for by using the EZ2 diffusion model. The study showed that Bayesian-based RT distribution analyses can provide an important means to investigate the underlying cognitive processes in search, including stimulus grouping and the bottom-up guidance of attention.",0
https://doi.org/10.1080/00273171.2016.1208074,"A Comparison of ML, WLSMV, and Bayesian Methods for Multilevel Structural Equation Models in Small Samples: A Simulation Study","Multilevel structural equation models are increasingly applied in psychological research. With increasing model complexity, estimation becomes computationally demanding, and small sample sizes pose further challenges on estimation methods relying on asymptotic theory. Recent developments of Bayesian estimation techniques may help to overcome the shortcomings of classical estimation techniques. The use of potentially inaccurate prior information may, however, have detrimental effects, especially in small samples. The present Monte Carlo simulation study compares the statistical performance of classical estimation techniques with Bayesian estimation using different prior specifications for a two-level SEM with either continuous or ordinal indicators. Using two software programs (Mplus and Stan), differential effects of between- and within-level sample sizes on estimation accuracy were investigated. Moreover, it was tested to which extent inaccurate priors may have detrimental effects on parameter estimates in categorical indicator models. For continuous indicators, Bayesian estimation did not show performance advantages over ML. For categorical indicators, Bayesian estimation outperformed WLSMV solely in case of strongly informative accurate priors. Weakly informative inaccurate priors did not deteriorate performance of the Bayesian approach, while strong informative inaccurate priors led to severely biased estimates even with large sample sizes. With diffuse priors, Stan yielded better results than Mplus in terms of parameter estimates.",1
https://doi.org/10.1111/1467-9280.00067,Modeling Response Times for Two-Choice Decisions,"The diffusion model for two-choice real-time decisions is applied to four psychophysical tasks. The model reveals how stimulus information guides decisions and shows how the information is processed through time to yield sometimes correct and sometimes incorrect decisions. Rapid two-choice decisions yield multiple empirical measures: response times for correct and error responses, the probabilities of correct and error responses, and a variety of interactions between accuracy and response time that depend on instructions and task difficulty. The diffusion model can explain all these aspects of the data for the four experiments we present. The model correctly accounts for error response times, something previous models have failed to do. Variability within the decision process explains how errors are made, and variability across trials correctly predicts when errors are faster than correct responses and when they are slower.",0
https://doi.org/10.1177/0146621615574694,Marginalized Maximum Likelihood Estimation for the 1PL-AG IRT Model,Marginal maximum likelihood estimation based on the expectation–maximization algorithm (MML/EM) is developed for the one-parameter logistic model with ability-based guessing (1PL-AG) item response theory (IRT) model. The use of the MML/EM estimator is cross-validated with estimates from NLMIXED procedure (PROC NLMIXED) in Statistical Analysis System. Numerical data are provided for comparisons of results from MML/EM and PROC NLMIXED.,0
https://doi.org/10.1093/biostatistics/kxq016,Surface shape analysis with an application to brain surface asymmetry in schizophrenia,"Some methods for the statistical analysis of surface shapes and asymmetry are introduced. We focus on a case study where magnetic resonance images of the brain are available from groups of 30 schizophrenia patients and 38 controls, and we investigate large-scale brain surface shape differences. Key aspects of shape analysis are to remove nuisance transformations by registration and to identify which parts of one object correspond with the parts of another object. We introduce maximum likelihood and Bayesian methods for registering brain images and providing large-scale correspondences of the brain surfaces. Brain surface size-and-shape analysis is considered using random field theory, and also dimension reduction is carried out using principal and independent components analysis. Some small but significant differences are observed between the the patient and control groups. We then investigate a particular type of asymmetry called torque. Differences in asymmetry are observed between the control and patient groups, which add strength to other observations in the literature. Further investigations of the midline plane location in the 2 groups and the fitting of nonplanar curved midlines are also considered.",0
https://doi.org/10.3102/10769986027003223,A New Maximum Likelihood Estimator for the Population Squared Multiple Correlation,"Using maximum likelihood estimation as described by R. A. Fisher (1912) , a new estimator for the population squared multiple correlation was developed. This estimator ( ρcirc; 2 (ML) ) was derived by examining all possible values of the population squared multiple correlation for a given sample size and number of predictors, and finding the one for which the observed sample value had the highest probability of occurring. The new estimator is shown to have greater accuracy than other estimators and to generate values that always fall within the parameter space. The utility of ρcirc; 2 (ML) in terms of providing the basis for the development of small sample significance tests is demonstrated. A Microsoft Excel workbook for computing ρcirc; 2 (ML) and its regions of nonsignificance and for computing a normal transformation for R 2 is offered.",0
https://doi.org/10.1177/0146621609349804,A Comparison of Item Selection Techniques for Testlets,"This study examined the performance of the maximum Fisher’s information, the maximum posterior weighted information, and the minimum expected posterior variance methods for selecting items in a computerized adaptive testing system when the items were grouped in testlets. A simulation study compared the efficiency of ability estimation among the item selection techniques under varying conditions of local-item dependency when the response model was either the three-parameter-logistic item response theory or the three-parameter-logistic testlet response theory. The item selection techniques performed similarly within any particular condition, the practical implications of which are discussed within the article.",0
https://doi.org/10.2147/btt.s37606,Anti-tumor necrosis factor (TNF) drugs for the treatment of psoriatic arthritis: an indirect comparison meta-analysis,"To evaluate the comparative effectiveness of available tumor necrosis factor-α inhibitors (anti-TNFs) for the management of psoriatic arthritis (PsA) in patients with an inadequate response to disease-modifying antirheumatic drugs (DMARDs).We used an exhaustive search strategy covering randomized clinical trials, systematic reviews and health technology assessments (HTA) published on anti-TNFs for PsA. We performed indirect comparisons of the available anti-TNFs (adalimumab, etanercept, golimumab, and infliximab) measuring relative risks (RR) for the psoriatic arthritis response criteria (PsARC), mean differences (MDs) for improvements from baseline for the Health Assessment Questionnaire (HAQ) by PsARC responders and non-responders, and MD for the improvements from baseline for the psoriasis area and severity index (PASI). When the reporting of data on intervention group response rates and improvements were incomplete, we used straightforward conversions based on the available data.We retrieved data from 20 publications representing seven trials, as well as two HTAs. All anti-TNFs were significantly better than control, but the indirect comparison did not reveal any statistically significant difference between the anti-TNFs. For PsARC response, golimumab yielded the highest RR and etanercept the second highest; adalimumab and infliximab both yielded notably smaller RRs. For HAQ improvement, etanercept and infliximab yielded the largest MD among PsARC responders. For PsARC nonresponders, etanercept, infliximab, and golimumab yielded similar MDs, and adalimumab a notably lower MD. For PASI improvement, infliximab yielded the largest MD and golimumab the second largest, while etanercept yielded the smallest MD. In some instances, the estimated magnitudes of effect were notably different from the estimates of previous HTA indirect comparisons.There is insufficient statistical evidence to demonstrate differences in effectiveness between available anti-TNFs for PsA. Effect estimates seem sensitive to the analytic approach, and this uncertainty should be taken into account in future economic evaluations.",0
https://doi.org/10.1080/10705511.2014.915379,A Nonlinear Structural Equation Mixture Modeling Approach for Nonnormally Distributed Latent Predictor Variables,"Structural equation models with interaction and quadratic effects have become a standard tool for testing nonlinear hypotheses in the social sciences. Most of the current approaches assume normally distributed latent predictor variables. In this article, we describe a nonlinear structural equation mixture approach that integrates the strength of parametric approaches (specification of the nonlinear functional relationship) and the flexibility of semiparametric structural equation mixture approaches for approximating the nonnormality of latent predictor variables. In a comparative simulation study, the advantages of the proposed mixture procedure over contemporary approaches [Latent Moderated Structural Equations approach (LMS) and the extended unconstrained approach] are shown for varying degrees of skewness of the latent predictor variables. Whereas the conventional approaches show either biased parameter estimates or standard errors of the nonlinear effects, the proposed mixture approach provides unbias...",0
https://doi.org/10.1177/0899764010362114,Making Civil Society Work: Models of Democracy and Their Impact on Civic Engagement,"This article evaluates the influence of different models of democracy on individual volunteering in associations and organizations. More precisely, we investigate the extent to which the degree of liberal and participatory conceptions of democracy respectively shapes the conditions under which voluntary engagement thrives. We apply multilevel analysis—a method that corresponds well to the central hypothesis of institutional approaches and is most suitable for modeling the relationship between the democratic context and individual volunteering. We show that both a representative conception of democracy, as well as strong direct democracy, leads to advantageous conditions for civic engagement. In contrast, if the two models of democracy are combined, the two different logics of the democratic process disturb one another, resulting in less voluntary engagement.",0
https://doi.org/10.1207/s15328007sem1003_1,Effect of the Number of Variables on Measures of Fit in Structural Equation Modeling,"There has been relatively little systematic investigation of the effect of the number of variables on measures of model fit in structural equation modeling. There is conflicting evidence as to whether measures of fit tend to improve or decline as more variables are added to the model. We consider 3 different types of specification error: minor factors, 2-factor models, and method errors. Using a formal method based on the noncentrality parameter (NCP), we find that root mean squared error of approximation (RMSEA) tends to improve regardless of the type of specification error and that the comparative fit index (CFI) and Tucker-Lewis Index (TLI), generally, though not always, tend to worsen as the number of variables in the model increases. The formal method that we develop can be used to investigate other measures of fit and other types of misspecification.",0
https://doi.org/10.1111/bmsp.12040,A threshold theory account of psychometric functions with response confidence under the balance condition,"The study of thresholds for discriminability has been of long-standing interest in psychophysics. While threshold theories embrace the concept of discrete-state thresholds, signal detection theory discounts such a concept. In this paper we concern ourselves with the concept of thresholds from the discrete-state modelling viewpoint. In doing so, we find it necessary to clarify some fundamental issues germane to the psychometric function (PF), which is customarily constructed using psychophysical methods with a binary-response format. We challenge this response format and argue that response confidence also plays an important role in the construction of PFs, and thus should have some impact on threshold estimation. We motivate the discussion by adopting a three-state threshold theory for response confidence proposed by Krantz (1969, Psychol. Rev., 76, 308–324), which is a modification of Luce's (1963, Psychol. Rev., 70, 61–79) low-threshold theory. In particular, we discuss the case in which the practice of averaging over order (or position) is enforced in data collection. Finally, we illustrate the fit of the Luce–Krantz model to data from a line-discrimination task with response confidence.",0
https://doi.org/10.1890/12-0156.1,Temporal shifts in top-down vs. bottom-up control of epiphytic algae in a seagrass ecosystem,"In coastal marine food webs, small invertebrate herbivores (mesograzers) have long been hypothesized to occupy an important position facilitating dominance of habitat-forming macrophytes by grazing competitively superior epiphytic algae. Because of the difficulty of manipulating mesograzers in the field, however, their impacts on community organization have rarely been rigorously documented. Understanding mesograzer impacts has taken on increased urgency in seagrass systems due to declines in seagrasses globally, caused in part by widespread eutrophication favoring seagrass overgrowth by faster-growing algae. Using cage-free field experiments in two seasons (fall and summer), we present experimental confirmation that mesograzer reduction and nutrients can promote blooms of epiphytic algae growing on eelgrass (Zostera marina). In this study, nutrient additions increased epiphytes only in the fall following natural decline of mesograzers. In the summer, experimental mesograzer reduction stimulated a 447% increase in epiphytes, appearing to exacerbate seasonal dieback of eelgrass. Using structural equation modeling, we illuminate the temporal dynamics of complex interactions between macrophytes, mesograzers, and epiphytes in the summer experiment. An unexpected result emerged from investigating the interaction network: drift macroalgae indirectly reduced epiphytes by providing structure for mesograzers, suggesting that the net effect of macroalgae on seagrass depends on macroalgal density. Our results show that mesograzers can control proliferation of epiphytic algae, that top-down and bottom-up forcing are temporally variable, and that the presence of macroalgae can strengthen top-down control of epiphytic algae, potentially contributing to eelgrass persistence.",0
https://doi.org/10.1080/17405629.2015.1040757,The Achievement Emotions Questionnaire: Validation for Pre-Adolescent Students,"The Achievement Emotions Questionnaire (AEQ) is a self-report instrument developed to measure the emotions of students in academic situations. The main purpose of this research was to adapt and validate this questionnaire to assess pre-adolescents' class- and test-related emotions towards mathematics. The participants were 1515 Portuguese students from grades 5 and 7 (age range 10–13 years). Confirmatory factor analyses and descriptive statistics confirm the reliability and internal validity of the AEQ for Pre-Adolescents (AEQ-PA), providing evidence that the AEQ-PA is an effective instrument to assess pre-adolescent achievement emotions towards mathematics classes and tests.",0
https://doi.org/10.1080/01621459.1992.10475289,Facilitating the Gibbs Sampler: The Gibbs Stopper and the Griddy-Gibbs Sampler,"Abstract The article briefly reviews the history, literature, and form of the Gibbs sampler. An importance sampling device is proposed for converting the output of the Gibbs sampler to a sample from the exact posterior. This Gibbs stopper technique is also useful for assessing convergence of the Gibbs sampler for moderate sized problems. Also presented is an approach for implementing the Gibbs sampler in nonconjugate situations. The basic idea is to approximate the true cdf of each conditional distribution by a piecewise linear function and then sample from the approximation. Questions relating to the number of nodes in the approximation, gap size between successive nodes, and the treatment of unbounded intervals for a given conditional are discussed. The methodology is illustrated using a genetic linkage model, a nonlinear regression model, and the Cox model.",0
https://doi.org/10.1016/j.obhdp.2011.09.004,The pursuit of missing information in negotiation,A large body of research has focused on how people exchange and use information during the negotiation process. This work tends to treat information as if it all were readily available upon request. The current research investigated how delays in the pursuit of missing information can influence people's ex-ante priorities and the final settlements they reach. Study 1 found that negotiators achieved more value on an issue after seeking missing information about that issue compared to when the same information was readily accessible. Study 2 found that the effect of searching for information on outcomes was mediated by changes in how important negotiators perceived the issue to be. Theoretical and practical implications are discussed.,0
https://doi.org/10.1037//0021-9010.86.5.837,Job search and employment: A personality-motivational analysis and meta-analytic review.,"A motivational, self-regulatory conceptualization of job search was used to organize and investigate the relationships between personality, expectancies, self, social, motive, and biographical variables and individual differences in job search behavior and employment outcomes. Meta-analytic results indicated that all antecedent variables, except optimism, were significantly related to job search behavior, with estimated population correlations ranging from -.15 to .46. As expected, job search behavior was significantly and positively related to finding employment. Several antecedents of job search were also significantly related to employment success, although the size of these relationships was consistently smaller than those obtained for job search. Moderator analyses showed significant differences in the size of variable relationships for type of job search measure (effort vs. intensity) and sample type (job loser vs. employed job seeker vs. new entrant).",0
https://doi.org/10.1002/sim.6163,Objective Bayesian meta-analysis for sparse discrete data,"This paper presents a Bayesian model for meta-analysis of sparse discrete binomial data, which are out of the scope of the usual hierarchical normal random-effect models. Treatment effectiveness data are often of this type. The crucial linking distribution between the effectiveness conditional on the healthcare center and the unconditional effectiveness is constructed from specific bivariate classes of distributions with given marginals. This assures coherency between the marginal and conditional prior distributions utilized in the analysis. Further, we impose a bivariate class of priors that is able to accommodate a wide range of heterogeneity degrees between the multicenter clinical trials involved. Applications to real multicenter data are given and compared with previous meta-analysis. Copyright © 2014 John Wiley & Sons, Ltd.",0
https://doi.org/10.1037/a0015730,Complex span versus updating tasks of working memory: The gap is not that deep.,"How to best measure working memory capacity is an issue of ongoing debate. Besides established complex span tasks, which combine short-term memory demands with generally unrelated secondary tasks, there exists a set of paradigms characterized by continuous and simultaneous updating of several items in working memory, such as the n-back, memory updating, or alpha span tasks. With a latent variable analysis (N = 96) based on content-heterogeneous operationalizations of both task families, the authors found a latent correlation between a complex span factor and an updating factor that was not statistically different from unity (r = .96). Moreover, both factors predicted fluid intelligence (reasoning) equally well. The authors conclude that updating tasks measure working memory equally well as complex span tasks. Processes involved in building, maintaining, and updating arbitrary bindings may constitute the common working memory ability underlying performance on reasoning, complex span, and updating tasks.",0
https://doi.org/10.1037//0021-9010.86.1.145,Risk propensity differences between entrepreneurs and managers: A meta-analytic review.,"Research examining the relative risk-taking propensities of entrepreneurs and managers has produced conflicting findings and no consensus, posing an impediment to theory development. To overcome the limitations of narrative reviews, the authors used psychometric meta-analysis to mathematically cumulate the literature concerning risk propensity differences between entrepreneurs and managers. Results indicate that the risk propensity of entrepreneurs is greater than that of managers. Moreover, there are larger differences between entrepreneurs whose primary goal is venture growth versus those whose focus is on producing family income. Results also underscore the importance of precise construct definitions and rigorous measurement.",0
https://doi.org/10.1002/sim.2211,Inference of nested variance components in a longitudinal myopia intervention trial,"This paper was motivated by a double-blind randomized clinical trial of myopia intervention. In addition to the primary goal of comparing treatment effects, we are concerned with the modelling of correlation that may come from two possible sources, one among the longitudinal observations and the other between measurements taken from both eyes per subject. The data are nested repeated measurements. We suggest three models for analysis. Each one expresses the correlation differently in various covariance structures. We articulate their differences and describe the implementations in estimation using commercial statistical software. The computer output can be further utilized to perform model selection with Schwarz criterion. Simulation studies are conducted to evaluate the performance under each model. Data of the myopia intervention trial are reanalysed with these models for illustration. The results indicate that atropine is more effective in reducing the progression rate, the rates are homogeneous across subjects, and, among the suggested models, the one with independent random effects of two eyes fits best. We conclude that model selection is a crucial step before making inference with estimates; otherwise the correlation may be attributed incorrectly to a different mechanism. The same conclusion applies to other variance components as well.",0
https://doi.org/10.1007/bf02289856,Local influence analysis of nonlinear structural equation models,"By regarding the latent random vectors as hypothetical missing data and based on the conditional expectation of the complete-data log-likelihood function in the EM algorithm, we investigate assessment of local influence of various perturbation schemes in a nonlinear structural equation model. The basic building blocks of local influence analysis are computed via observations of the latent variables generated by the Metropolis-Hastings algorithm, while the diagnostic measures are obtained via the conformal normal curvature. Seven perturbation schemes, including some perturbation schemes on latent vectors, are investigated. The proposed procedure is illustrated by a simulation study and a real example. © 2004 The Psychometric Society.",0
https://doi.org/10.1080/10705510701758281,The Impact of Misspecifying Class-Specific Residual Variances in Growth Mixture Models,"The purpose of this study was to examine the impact of misspecifying a growth mixture model (GMM) by assuming that Level-1 residual variances are constant across classes, when they do, in fact, vary in each subpopulation. Misspecification produced bias in the within-class growth trajectories and variance components, and estimates were substantially less precise than those obtained from a correctly specified GMM. Bias and precision became worse as the ratio of the largest to smallest Level-1 residual variances increased, class proportions became more disparate, and the number of class-specific residual variances in the population increased. Although the Level-1 residuals are typically of little substantive interest, these results suggest that researchers should carefully estimate and report these parameters in published GMM applications.",0
https://doi.org/10.1016/j.jval.2013.09.006,An Integrated Approach to Evaluating Alternative Risk Prediction Strategies: A Case Study Comparing Alternative Approaches for Preventing Invasive Fungal Disease,"This article proposes an integrated approach to the development, validation, and evaluation of new risk prediction models illustrated with the Fungal Infection Risk Evaluation study, which developed risk models to identify non-neutropenic, critically ill adult patients at high risk of invasive fungal disease (IFD).Our decision-analytical model compared alternative strategies for preventing IFD at up to three clinical decision time points (critical care admission, after 24 hours, and end of day 3), followed with antifungal prophylaxis for those judged ""high"" risk versus ""no formal risk assessment."" We developed prognostic models to predict the risk of IFD before critical care unit discharge, with data from 35,455 admissions to 70 UK adult, critical care units, and validated the models externally. The decision model was populated with positive predictive values and negative predictive values from the best-fitting risk models. We projected lifetime cost-effectiveness and expected value of partial perfect information for groups of parameters.The risk prediction models performed well in internal and external validation. Risk assessment and prophylaxis at the end of day 3 was the most cost-effective strategy at the 2% and 1% risk threshold. Risk assessment at each time point was the most cost-effective strategy at a 0.5% risk threshold. Expected values of partial perfect information were high for positive predictive values or negative predictive values (£11 million-£13 million) and quality-adjusted life-years (£11 million).It is cost-effective to formally assess the risk of IFD for non-neutropenic, critically ill adult patients. This integrated approach to developing and evaluating risk models is useful for informing clinical practice and future research investment.",0
https://doi.org/10.1007/bf02296195,MCMC estimation and some model-fit analysis of multidimensional IRT models,A Bayesian procedure to estimate the three-parameter normal ogive model and a generalization of the procedure to a model with multidimensional ability parameters are presented. The procedure is a generalization of a procedure by Albert (1992) for estimating the two-parameter normal ogive model. The procedure supports analyzing data from multiple populations and incomplete designs. It is shown that restrictions can be imposed on the factor matrix for testing specific hypotheses about the ability structure. The technique is illustrated using simulated and real data.,0
https://doi.org/10.1002/sim.1142,Small-sample adjustments in using the sandwich variance estimator in generalized estimating equations,"The generalized estimating equation (GEE) approach is widely used in regression analyses with correlated response data. Under mild conditions, the resulting regression coefficient estimator is consistent and asymptotically normal with its variance being consistently estimated by the so-called sandwich estimator. Statistical inference is thus accomplished by using the asymptotic Wald chi-squared test. However, it has been noted in the literature that for small samples the sandwich estimator may not perform well and may lead to much inflated type I errors for the Wald chi-squared test. Here we propose using an approximate t- or F-test that takes account of the variability of the sandwich estimator. The level of type I error of the proposed t- or F-test is guaranteed to be no larger than that of the Wald chi-squared test. The satisfactory performance of the proposed new tests is confirmed in a simulation study. Our proposal also has some advantages when compared with other new approaches based on direct modifications of the sandwich estimator, including the one that corrects the downward bias of the sandwich estimator. In addition to hypothesis testing, our result has a clear implication on constructing Wald-type confidence intervals or regions.",0
https://doi.org/10.1037/0021-9010.71.2.302,On seeking moderator variables in the meta-analysis of correlational data: A Monte Carlo investigation of statistical power and resistance to Type I error.,"A series of Monte Carlo computer simulations was conducted to investigate (a) the likelihood that meta-analysis will detect true differences in effect sizes rather than attributing differences to methodological artifact and (b) the likelihood that meta-analysis will suggest the presence of moderator variables when in fact differences in effect sizes are due to methodological artifact. The simulations varied the magnitude of the true population differences between correlations, the number of studies included in the meta-analysis, and the average sample size. Simulations were run both correcting for and not correcting for measurement error. The power of three indexes-the Schmidt-Hunter ratio of expected to observed variance, the Callender-Osburn procedure, and a chi-square test-to detect true differences was investigated. Small true differences will not be detected regardless of sample size and number of studies, and moderate true differences will not be detected with small numbers of studies or small sample sizes. Hence there is a need for caution in attributing observed variation across studies to artifact. Ã‚Â© 1986 American Psychological Association.",0
https://doi.org/10.1007/s11121-009-0125-1,Evaluating Mediation in Longitudinal Multivariate Data: Mediation Effects for the Aban Aya Youth Project Drug Prevention Program,"This study illustrates a method to evaluate mediational mechanisms in a longitudinal prevention trial, the Aban Aya Youth Project (AAYP). In previous studies, interventions of AAYP were found to be effective in reducing the growth of violence, substance use and unsafe sex among African American adolescents. In this article, we hypothesized that the effects of the interventions in reducing the growth of substance use behavior were achieved through their effects in changing intermediate processes such as behavioral intentions, attitudes toward the behavior, estimates of peers' behaviors, best friends' behaviors, and peer group pressure. In evaluating these mediational mechanisms, difficulties arise because the growth trajectories of the substance use outcome variable and some of the mediating variables were curvilinear. In addition, all of the multivariate mediational measures had planned missing data so that a score from the multiple items for a mediator could not be formed easily. In this article, we introduce a latent growth modeling (LGM) approach; namely, a two-domain LGM mediation model, in which the growth curves of the outcome and the mediator are simultaneously modeled and the mediation effects are evaluated. Results showed that the AAYP intervention effects on adolescent drug use were mediated by normative beliefs of prevalence estimates, friends' drug use behavior, perceived friends' encouragement to use, and attitudes toward the behavior. Â© 2009 Society for Prevention Research.",0
https://doi.org/10.1177/0146621605279761,Longitudinal Rasch Modeling in the Context of Psychotherapy Outcomes Assessment,"The present study illustrates an extension of Kamata's (2001) restricted form of the hierarchical generalized linear model that provides a multilevel longitudinal Rasch measurement model appropriate for use with polytomous responses. This extension can be used to assess average and interindividual change in the latent trait of interest, concurrently with an assessment of the invariance of item locations over time. Responses of 1,353 college students to three subscales (namely, measures of depression/ anxiety, stress, and well-being) of the Outcome Questionnaire were used for demonstration purposes. Interpretation of the results is provided, and the benefits of using a multilevel item response theory model with longitudinal data are discussed.",0
https://doi.org/10.1080/10705511.2013.769391,Sample Size Limits for Estimating Upper Level Mediation Models Using Multilevel SEM,"This simulation study investigated use of the multilevel structural equation model (MLSEM) for handling measurement error in both mediator and outcome variables (M and Y) in an upper level multilevel mediation model. Mediation and outcome variable indicators were generated with measurement error. Parameter and standard error bias, confidence interval coverage, and power to detect the ab mediated effect using Empirical-M confidence interval estimates were assessed for the correct MLSEM versus a conventional multilevel model (MM) that used composite scores for M and Y. The following conditions were manipulated: level 1 and 2 sample sizes, intraclass correlation, degree of measurement error in M, and the true value of ab. The MLSEM more accurately recovered the ab effect's value, but serious convergence issues were encountered with MLSEM estimates based on fewer than 80 clusters. More power for detecting a nonzero ab was found for MM than for MLSEM estimates.",0
https://doi.org/10.1016/s0160-2896(02)00115-0,The multiple faces of working memory,"Investigated the distinctiveness of working memory functions and their components against the background of a multi-facet model. 133 university students (mean age 26 years) performed a series of specially constructed working memory tasks. Each task represented an operationalization of specific cells of the proposed taxonomy of working memory functional and content-related facets. Dependent variables included recall performance and reaction time. Structural equation modeling yielded 3 distinct working memory functions: (1) simultaneous storage of information in the context of processing, (2) supervision, and (3) coordination of elements into structures. Further analyses allowed for a more detailed subdivision of each function into specific components. Only a minimal portion of the variance associated with working memory functions is specific to the verbal or the spatial domain. Overall, the findings demonstrate that working memory is best characterized as a highly interrelated collection of cognitive functions.",0
https://doi.org/10.1371/journal.pone.0106528,The Influence of Compositional and Contextual Factors on Non-Receipt of Basic Vaccines among Children of 12-23-Month Old in India: A Multilevel Analysis,"Children unreached by vaccination are at higher risk of poor health outcomes and India accounts for nearly a quarter of unvaccinated children worldwide. The objective of this study was to investigate compositional and contextual determinants of non-receipt of childhood vaccines in India using multilevel modelling.We studied characteristics of unvaccinated children using the District Level Health and Facility Survey 3, a nationally representative probability sample containing 65 617 children aged 12-23 months from 34 Indian states and territories. We developed four-level Bayesian binomial regression models to examine the determinants of non-vaccination. The analysis considered two outcomes: completely unvaccinated (CUV) children who had not received any of the eight vaccine doses recommended by India's Universal Immunization Programme, and children who had not received any dose from routine immunisation services (no RI). The no RI category includes CUV children and those who received only polio doses administered via mass campaigns. Overall, 4.83% (95% CI: 4.62-5.06) of children were CUV while 12.01% (11.68-12.35) had received no RI. Individual compositional factors strongly associated with CUV were: non-receipt of tetanus immunisation for mothers during pregnancy (OR = 3.65 [95% CrI: 3.30-4.02]), poorest household wealth index (OR = 2.44 [1.81-3.22] no maternal schooling (OR = 2.43 [1.41-4.05]) and no paternal schooling (OR = 1.83 [1.30-2.48]). In rural settings, the influence of maternal illiteracy disappeared whereas the role of household wealth index was reinforced. Factors associated with no RI were similar to those for CUV, but effect sizes for individual compositional factors were generally larger. Low maternal education was the strongest risk factor associated with no RI in all models. All multilevel models found significant variability at community, district, and state levels net of compositional factors.Non-vaccination in India is strongly related to compositional characteristics and is geographically distinct. Tailored strategies are required to overcome current barriers to immunisation.",0
https://doi.org/10.1086/208721,Conjoint Analysis in Consumer Research: Issues and Outlook,Since 1971 conjoint analysis has been applied to a wide variety of problems in consumer research. This paper discusses various issues involved in implementing conjoint analysis and describes some new technical developments and application areas for the methodology.,0
https://doi.org/10.1207/s15328007sem0901_3,Latent Variable Interaction Modeling,Latent variable interaction modeling with continuous observed variables is presented using 2 different approaches. The 1st approach analyzes data using a LISREL 8.30 program where the latent interaction variable is defined by multiplying pairs of observed variables. The 2nd approach analyzes data using PRELIS2 and SIMPLIS programs where the latent interaction variable is defined by multiplying the latent variable scores of the exogeneous latent independent variables. The programs used to create the multivariate normal observed variables and conduct the analyses for the 2 different approaches are given in the appendixes. The product indicant and latent variable score approach produced similar gamma coefficients in their hypothesized models but differed in their standard errors for the gamma coefficients. The latent variable score approach holds the promise of being easier to implement and can be applied to more complex latent variable interaction models.,0
https://doi.org/10.3758/bf03194550,Slope bias of psychometric functions derived from adaptive data,"Several investigators have fit psychometric functions to data from adaptive procedures for threshold estimation. Although the threshold estimates are in general quite correct, one encounters a slope bias that has not been explained up to now. The present paper demonstrates slope bias for parametric and nonparametric maximum-likelihood fits and for Spearman-Kärber analysis of adaptive data. The examples include staircase and stochastic approximation procedures. The paper then presents an explanation of slope bias based on serial data dependency in adaptive procedures. Data dependency is first illustrated with simple two-trial examples and then extended to realistic adaptive procedures. Finally, the paper presents an adaptive staircase procedure designed to measure threshold and slope directly. In contrast to classical adaptive threshold-only procedures, this procedure varies both a threshold and a spread parameter in response to double trials.",0
https://doi.org/10.3200/jexe.73.3.221-248,A Primer for Using and Understanding Weights With National Datasets,"Using data from the National Study of Postsecondary Faculty and the Early Childhood Longitudinal Study—Kindergarten Class of 1998-99, the author provides guidelines for incorporating weights and design effects in single-level analysis using Windows-based SPSS and AM software. Examples of analyses that do and do not employ weights and design effects are also provided to illuminate the differential results of key parameter estimates and standard errors using varying degrees of using or not using the weighting and design effect continuum. The author gives recommendations on the most appropriate weighting options, with specific reference to employing a strategy to accommodate both oversampled groups and cluster sampling (i.e., using weights and design effects) that leads to the most accurate parameter estimates and the decreased potential of committing a Type I error. However, using a design effect adjusted weight in SPSS may produce underestimated standard errors when compared with accurate estimates produce...",0
https://doi.org/10.1002/9780470712993.ch4,"Psychometric Models, Test Designs and Item Types for the Next Generation of Educational and Psychological Tests",,0
https://doi.org/10.1177/0049124194022003007,The Bilevel Reticular Action Model for Path Analysis with Latent Variables,"A two-level (hierarchical) model for path analysis with latent variables is described, together with some properties of a computer program written to implement the model. A simple illustrative example is given.",0
https://doi.org/10.1207/s15328007sem1204_1,A Semiparametric Approach to Modeling Nonlinear Relations Among Latent Variables,"To date, finite mixtures of structural equation models (SEMMs) have been developed and applied almost exclusively for the purpose of providing model-based cluster analyses. This type of analysis constitutes a direct application of the model wherein the estimated component distributions of the latent classes are thought to represent the characteristics of distinct unobserved subgroups of the population. This article instead considers an indirect application of the SEMM in which the latent classes are estimated only in the service of more flexibly modeling the characteristics of the aggregate population as a whole. More specifically, the SEMM is used to semiparametrically model nonlinear latent variable regression functions. This approach is first developed analytically and then demonstrated empirically through analyses of simulated and real data.",0
https://doi.org/10.1080/16184742.2013.837942,The influence of professional athlete philanthropy on donation intentions,"Athlete-supported charities and foundations have become increasingly popular forms of philanthropy in the professional sport landscape. However, a paucity of research has examined whether the high-profile status of a professional athlete can influence third-party donations to the charitable cause the athlete supports. Based on theory and prior literature, the image of, and identification with, a professional National Basketball Association athlete were hypothesized to influence donation intentions from potential donors in the Orlando, Florida area. In addition, the intervening effects of personal involvement with the athlete's cause and trust toward the athlete were examined. The results suggest that athlete identification, image of the athlete, trust toward the athlete, and cause involvement significantly influenced donation intentions. The results also indicate that athlete trust mediated the relationship between identification, image, and donation intentions. Finally, cause involvement moderated the re...",0
https://doi.org/10.1080/10705511.2011.557329,Alternative Methods for Assessing Mediation in Multilevel Data: The Advantages of Multilevel SEM,"Multilevel modeling (MLM) is a popular way of assessing mediation effects with clustered data. Two important limitations of this approach have been identified in prior research and a theoretical rationale has been provided for why multilevel structural equation modeling (MSEM) should be preferred. However, to date, no empirical evidence of MSEM's advantages relative to MLM approaches for multilevel mediation analysis has been provided. Nor has it been demonstrated that MSEM performs adequately for mediation analysis in an absolute sense. This study addresses these gaps and finds that the MSEM method outperforms 2 MLM-based techniques in 2-level models in terms of bias and confidence interval coverage while displaying adequate efficiency, convergence rates, and power under a variety of conditions. Simulation results support prior theoretical work regarding the advantages of MSEM over MLM for mediation in clustered data.",0
https://doi.org/10.1093/acprof:oso/9780199216093.003.0014,Categorization as nonparametric Bayesian density estimation,"Abstract The authors apply the state of the art techniques from machine learning and statistics to reconceptualize the problem of unsupervised category learning, and to relate it to previous psychologically motivated models, especially Anderson's rational analysis of categorization. The resulting analysis provides a deeper understanding of the motivations underlying the classic models of category representation, based on prototypes or exemplars, as well as shedding new light on the empirical data. Exemplar models assume that a category is represented by a set of stored exemplars, and categorizing new stimuli involves comparing these stimuli to the set of exemplars in each category. Prototype models assume that a category is associated with a single prototype and categorization involves comparing new stimuli to these prototypes. These approaches to category learning correspond to different strategies for density estimation used in statistics, being nonparametric and parametric density estimation respectively.",0
https://doi.org/10.1146/annurev.psych.58.110405.085520,Analysis of Nonlinear Patterns of Change with Random Coefficient Models,"Abstract Nonlinear patterns of change arise frequently in the analysis of repeated measures from longitudinal studies in psychology. The main feature of nonlinear development is that change is more rapid in some periods than in others. There generally also are strong individual differences, so although there is a general similarity of patterns for different persons over time, individuals exhibit substantial heterogeneity in their particular response. To describe data of this kind, researchers have extended the random coefficient model to accommodate nonlinear trajectories of change. It can often produce a statistically satisfying account of subject-specific development. In this review we describe and illustrate the main ideas of the nonlinear random coefficient model with concrete examples.",0
https://doi.org/10.1080/01621459.1990.10475312,Approaches for Empirical Bayes Confidence Intervals,"Abstract Parametric empirical Bayes (EB) methods of point estimation date to the landmark paper by James and Stein (1961). Interval estimation through parametric empirical Bayes techniques has a somewhat shorter history, which was summarized by Laird and Louis (1987). In the exchangeable case, one obtains a “naive” EB confidence interval by simply taking appropriate percentiles of the estimated posterior distribution of the parameter, where the estimation of the prior parameters (“hyperparameters”) is accomplished through the marginal distribution of the data. Unfortunately, these “naive” intervals tend to be too short, since they fail to account for the variability in the estimation of the hyperparameters. That is, they do not attain the desired coverage probability in the EB sense defined by Morris (1983a, b). They also provide no statement of conditional calibration (Rubin 1984). In this article we propose a conditional bias correction method for developing EM intervals that corrects these deficiencies...",0
https://doi.org/10.1214/009053605000000039,Testing for monotone increasing hazard rate,"A test of the null hypothesis that a hazard rate is monotone nondecreasing, versus the alternative that it is not, is proposed. Both the test statistic and the means of calibrating it are new. Unlike previous approaches, neither is based on the assumption that the null distribution is exponential. Instead, empirical information is used to effectively identify and eliminate from further consideration parts of the line where the hazard rate is clearly increasing; and to confine subsequent attention only to those parts that remain. This produces a test with greater apparent power, without the excessive conservatism of exponential-based tests. Our approach to calibration borrows from ideas used in certain tests for unimodality of a density, in that a bandwidth is increased until a distribution with the desired properties is obtained. However, the test statistic does not involve any smoothing, and is, in fact, based directly on an assessment of convexity of the distribution function, using the conventional empirical distribution. The test is shown to have optimal power properties in difficult cases, where it is called upon to detect a small departure, in the form of a bump, from monotonicity. More general theoretical properties of the test and its numerical performance are explored.",0
https://doi.org/10.1093/biomet/80.2.267,Maximum likelihood estimation via the ECM algorithm: A general framework,"Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated",0
https://doi.org/10.1207/s15327906mbr3903_3,Evaluating Small Sample Approaches for Model Test Statistics in Structural Equation Modeling,"Through Monte Carlo simulation, small sample methods for evaluating overall data-model fit in structural equation modeling were explored. Type I error behavior and power were examined using maximum likelihood (ML), Satorra-Bentler scaled and adjusted (SB; Satorra & Bentler, 1988, 1994), residual-based (Browne, 1984), and asymptotically distribution free (ADF; Browne, 1982, 1984) test statistics. To accommodate small sample sizes the ML and SB statistics were adjusted using a k-factor correction (Bartlett, 1950); the residual-based and ADF statistics were corrected using modified x2 and F statistics (Yuan & Bentler, 1998, 1999). Design characteristics include model type and complexity, ratio of sample size to number of estimated parameters, and distributional form. The k-factor-corrected SB scaled test statistic was especially stable at small sample sizes with both normal and nonnormal data. Methodologists are encouraged to investigate its behavior under a wider variety of models and distributional forms.",0
https://doi.org/10.1007/s11266-012-9347-0,Eliciting Donor Preferences,"Most charity organizations depend on contributions from the general public, but little research is conducted on donor preferences. Do donors have geographical, recipient, or thematic preferences? We designed a conjoint analysis experiment in which people rated development aid projects by donating money in dictator games. We find that our sample show strong age, gender, regional, and thematic preferences. Furthermore, we find significant differences between segments. The differences in donations are consistent with differences in donors’ attitudes toward development aid and their beliefs about differences in poverty and vulnerability of the recipients. The method here used for development projects can easily be adapted to elicit preferences for other kinds of projects that rely on gifts from private donors.",0
https://doi.org/10.1111/bmsp.12028,Statistical mediation analysis with a multicategorical independent variable,"Virtually all discussions and applications of statistical mediation analysis have been based on the condition that the independent variable is dichotomous or continuous, even though investigators frequently are interested in testing mediation hypotheses involving a multicategorical independent variable (such as two or more experimental conditions relative to a control group). We provide a tutorial illustrating an approach to estimation of and inference about direct, indirect, and total effects in statistical mediation analysis with a multicategorical independent variable. The approach is mathematically equivalent to analysis of (co)variance and reproduces the observed and adjusted group means while also generating effects having simple interpretations. Supplementary material available online includes extensions to this approach and Mplus, SPSS, and SAS code that implements it.",0
https://doi.org/10.1161/circoutcomes.111.960724,Bayesian Hierarchical Modeling and the Integration of Heterogeneous Information on the Effectiveness of Cardiovascular Therapies,"When making therapeutic decisions for an individual patient or formulating treatment guidelines on a population level, it is often necessary to utilize information arising from different study designs, settings, or treatments. In clinical practice, heterogeneous information is frequently synthesized qualitatively, whereas in comparative effectiveness research and guideline development, it is imperative that heterogeneous data are integrated quantitatively and in a manner that accurately captures the true uncertainty in the results. Bayesian hierarchical modeling is a technique that utilizes all available information from multiple sources and naturally yields a revised estimate of the treatment effect associated with each source. A hierarchical model consists of multiple levels (ie, a hierarchy) of probability distributions that represent relationships between information arising within single populations or trials, as well as relationships between information arising from different populations or trials. We describe the structure of Bayesian hierarchical models and discuss their advantages over simpler models when multiple information sources are relevant. Two examples are presented that illustrate this technique: a meta-analysis of immunosuppressive therapy in idiopathic dilated cardiomyopathy and a subgroup analysis of the National Institute of Neurological Disorders and Stroke Intravenous Tissue Plasminogen Activator Stroke Trial.",0
https://doi.org/10.1007/978-0-387-73186-5_8,Non-Hierarchical Multilevel Models,"In the models discussed in this book so far we have assumed that the structures of the populations from which the data have been drawn are hierarchical. This assumption is sometimes not justified. In this chapter two main types of non-hierarchical model are considered. Firstly, cross-classified models. The notion of cross-classification is probably reasonably familiar to most readers. Secondly, we consider multiple membership models, where lower level units are influenced by more than one higher-level unit from the same classification. For example, some pupils may attend more than one school. We also consider situations that contain a mixture of hierarchical, crossed and multiple membership relationships. Â© 2008 Springer Science+Business Media, LLC.",0
https://doi.org/10.1177/0146621612469720,The Random-Threshold Generalized Unfolding Model and Its Application of Computerized Adaptive Testing,"The random-threshold generalized unfolding model (RTGUM) was developed by treating the thresholds in the generalized unfolding model as random effects rather than fixed effects to account for the subjective nature of the selection of categories in Likert items. The parameters of the new model can be estimated with the JAGS (Just Another Gibbs Sampler) freeware, which adopts a Bayesian approach for estimation. A series of simulations was conducted to evaluate the parameter recovery of the new model and the consequences of ignoring the randomness in thresholds. The results showed that the parameters of RTGUM were recovered fairly well and that ignoring the randomness in thresholds led to biased estimates. Computerized adaptive testing was also implemented on RTGUM, where the Fisher information criterion was used for item selection and the maximum a posteriori method was used for ability estimation. The simulation study showed that the longer the test length, the smaller the randomness in thresholds, and the more categories in an item, the more precise the ability estimates would be.",0
https://doi.org/10.1007/s11222-012-9343-7,Likelihood-based and Bayesian methods for Tweedie compound Poisson linear mixed models,"The Tweedie compound Poisson distribution is a subclass of the exponential dispersion family with a power variance function, in which the value of the power index lies in the interval (1,2). It is well known that the Tweedie compound Poisson density function is not analytically tractable, and numerical procedures that allow the density to be accurately and fast evaluated did not appear until fairly recently. Unsurprisingly, there has been little statistical literature devoted to full maximum likelihood inference for Tweedie compound Poisson mixed models. To date, the focus has been on estimation methods in the quasi-likelihood framework. Further, Tweedie compound Poisson mixed models involve an unknown variance function, which has a significant impact on hypothesis tests and predictive uncertainty measures. The estimation of the unknown variance function is thus of independent interest in many applications. However, quasi-likelihood-based methods are not well suited to this task. This paper presents several likelihood-based inferential methods for the Tweedie compound Poisson mixed model that enable estimation of the variance function from the data. These algorithms include the likelihood approximation method, in which both the integral over the random effects and the compound Poisson density function are evaluated numerically; and the latent variable approach, in which maximum likelihood estimation is carried out via the Monte Carlo EM algorithm, without the need for approximating the density function. In addition, we derive the corresponding Markov Chain Monte Carlo algorithm for a Bayesian formulation of the mixed model. We demonstrate the use of the various methods through a numerical example, and conduct an array of simulation studies to evaluate the statistical properties of the proposed estimators. Â© 2012 Springer Science+Business Media, LLC.",0
https://doi.org/10.1002/sim.6776,Bayesian meta-analytical methods to incorporate multiple surrogate endpoints in drug development process,"A number of meta-analytical methods have been proposed that aim to evaluate surrogate endpoints. Bivariate meta-analytical methods can be used to predict the treatment effect for the final outcome from the treatment effect estimate measured on the surrogate endpoint while taking into account the uncertainty around the effect estimate for the surrogate endpoint. In this paper, extensions to multivariate models are developed aiming to include multiple surrogate endpoints with the potential benefit of reducing the uncertainty when making predictions. In this Bayesian multivariate meta-analytic framework, the between-study variability is modelled in a formulation of a product of normal univariate distributions. This formulation is particularly convenient for including multiple surrogate endpoints and flexible for modelling the outcomes which can be surrogate endpoints to the final outcome and potentially to one another. Two models are proposed, first, using an unstructured between-study covariance matrix by assuming the treatment effects on all outcomes are correlated and second, using a structured between-study covariance matrix by assuming treatment effects on some of the outcomes are conditionally independent. While the two models are developed for the summary data on a study level, the individual-level association is taken into account by the use of the Prentice's criteria (obtained from individual patient data) to inform the within study correlations in the models. The modelling techniques are investigated using an example in relapsing remitting multiple sclerosis where the disability worsening is the final outcome, while relapse rate and MRI lesions are potential surrogates to the disability progression.",0
https://doi.org/10.1016/j.addbeh.2005.04.006,The development of alcohol use and outcome expectancies among American Indian young adults: A growth mixture model,"Adolescence is an important developmental period for understanding alcohol use. American Indian youth are a group for whom various preventive interventions focusing on alcohol use have been implemented but have not necessarily been widely successful, highlighting the need to further refine our approaches. In the work reported here, we followed 464 14- to 18-year-old American Indian youth annually for seven years. We examined the development of alcohol use and positive alcohol outcome expectancies in parallel, using growth mixture modeling to identify classes with different trajectories of alcohol use and expectancies. We found five classes; the largest (n = 198) was made up of youth who experienced increases in both alcohol use and positive outcome expectancies. Initial levels of outcome expectancies were related to subsequent changes in alcohol use, while the reverse was not true, suggesting that interventions focusing on outcome expectancies are appropriate for a substantial number of youth. However, class heterogeneity in the relationships between the two processes pointed out that changes in expectancies may not always precede changes in alcohol use. Thus, intervention design implications are discussed for the class structures.",0
https://doi.org/10.1016/j.csda.2008.12.013,An improved approximation to the precision of fixed effects from restricted maximum likelihood,"An approximate small sample variance estimator for fixed effects from the multivariate normal linear model, together with appropriate inference tools based on a scaled F pivot, is now well established in practice and there is a growing literature on its properties in a variety of settings. Although effective under linear covariance structures, there are examples of nonlinear structures for which it does not perform as well. The cause of this problem is shown to be a missing term in the underlying Taylor series expansion which accommodates the bias in the estimators of the parameters of the covariance structure. The form of this missing term is derived, and then used to adjust the small sample variance estimator. The behaviour of the resulting estimator is explored in terms of invariance under transformation of the covariance parameters and also using a simulation study. It is seen to perform successfully in the way predicted from its derivation.",0
https://doi.org/10.1515/9783110267709,Multilevel Models,"This book covers a broad range of topics about multilevel modeling. The goal is to help readersto understand the basic concepts, theoretical frameworks, and application methods of multilevel modeling. Itis at a level also accessible to non-mathematicians, focusing on the methods and applications of various multilevel models and using the widely used statistical software SAS.Examples are drawn from analysis of real-world research data. Ã‚Â© 2012 Higher Education Press and Walter de Gruyter GmbH & Co. KG, Berlin/Boston.",0
https://doi.org/10.1177/0049124112460380,Distinguishing Between Cross- and Cluster-Level Mediation Processes in the Cluster Randomized Trial,"For the cluster randomized trial, the treatment, implemented at the cluster level, may be hypothesized to impact a final outcome of interest via a mediating variable, with the mediator and outcome being measured typically at the participant level. Two opposing views—one that allows the mediation process to flow through a participant-level mediator and one that does not—have been presented in the literature. We integrate these perspectives and argue that it is the theoretical model and the nature of the mediating variable that determines whether the treatment effect may be mediated by a participant-level or only a cluster-level mediator in this design. An accompanying simulation study shows that when the mediation process is unnecessarily restricted to the cluster level, the power to detect the presence of mediation is substantially reduced.",0
https://doi.org/10.3389/fnbot.2014.00014,Time changes with feeling of speed: an embodied perspective,"The speed of moving stimuli can bias duration perception. Here, we investigated whether words describing different speeds influence subjective duration estimation in a temporal bisection task. Duration estimations of two different types of speed words (fast- vs. slow-speed words) were compared. We found that the time bisection point was significantly lower for fast-speed words than for slow-speed words, suggesting that the durations of fast-speed words were overestimated compared to the slow-speed words. In contrast, fast- and slow-speed words did not significantly differ in just noticeable differences and Weber fractions, indicating that the types of speed words did not influence the sensitivity of duration estimation. These results provide new evidence to support the theory of embodied cognition in the context of implicit meaning of a speed word.",0
https://doi.org/10.1037/a0025688,A time-lagged momentary assessment study on daily life physical activity and affect.,"Novel study designs using within-subject methodology and frequent and prospective measurements are required to unravel direction of causality and dynamic processes of behavior over time. The current study examined the effects of physical activity on affective state. A primary and within-study replication sample was derived from twin pairs.Female twins (n = 504) participated in an experience sampling method study at baseline. Positive and negative affective changes were examined before and following daily life increases in physical activity. Neuroticism was measured at baseline and depressive symptoms were assessed at baseline and at each of four follow-up assessments. Diagnoses, derived by Structured Clinical Interview for Diagnostic and Statistical Manual for Mental Health-IV axis I disorders, (A. P. A., 1994) were obtained at baseline.A significant increase in positive affect (PA) following the moment of increase in physical activity was replicated across both samples up to 180 min after physical activity. There was no effect of physical activity on negative affect (NA). Across the two samples, a history of fulfilling diagnostic criteria for depression at least once moderated the effect of physical activity on PA, in that the effect was lost more rapidly.The study supports a causal effect of physical activity on PA. However, people with past experience of clinical depression may benefit less from the PA-inducing effect of physical activity. These findings have implications for the use of physical exercise in clinical practice.",0
https://doi.org/10.2307/1403259,A Review of Inference Procedures for the Intraclass Correlation Coefficient in the One-Way Random Effects Model,"Summary Recent theory and methodology for inferences concerning the intraclass correlation coefficient are reviewed, under the assumption of an underlying random effects model. Topics discussed include point and interval estimation, significance-testing for nonzero values of the intraclass correlation, and inference procedures in multiple samples.",0
https://doi.org/10.1080/10618600.2013.785732,On a Class of Shrinkage Priors for Covariance Matrix Estimation,"We propose a flexible class of models based on scale mixture of uniform distributions to construct shrinkage priors for covariance matrix estimation. This new class of priors enjoys a number of advantages over the traditional scale mixture of normal priors, including its simplicity and flexibility in characterizing the prior density. We also exhibit a simple, easy to implement Gibbs sampler for posterior simulation which leads to efficient estimation in high dimensional problems. We first discuss the theory and computational details of this new approach and then extend the basic model to a new class of multivariate conditional autoregressive models for analyzing multivariate areal data. The proposed spatial model flexibly characterizes both the spatial and the outcome correlation structures at an appealing computational cost. Examples consisting of both synthetic and real-world data show the utility of this new framework in terms of robust estimation as well as improved predictive performance.",0
https://doi.org/10.12738/estp.2015.6.0102,Effects of Calibration Sample Size and Item Bank Size on Ability Estimation in Computerized Adaptive Testing,"This study aimed to investigate the effects of calibration sample size and item bank size on examinee ability estimation in computerized adaptive testing (CAT). For this purpose, a 500-item bank pre-calibrated using the three-parameter logistic model with 10,000 examinees was simulated. Calibration samples of varying sizes (150, 250, 350, 500, 750, 1,000, 2,000, 3,000, and 5,000) were selected from the parent sample, and item banks that represented small (100) and medium size (200 and 300) banks were drawn from the 500-item bank. Items in these banks were recalibrated using the drawn samples, and their estimated parameters were used in post-hoc simulations to re-estimate ability parameters for the simulated 10,000 examinees. The findings showed that ability estimates in CAT are robust against fluctuations in item parameter estimation and that accurate ability parameter estimates can be obtained with a calibration sample of 150 examinees. Moreover, a 200-item bank pre-calibrated with as few as 150 examinees can be used for some purposes in CAT as long as it has sufficient information at targeted ability levels.",0
https://doi.org/10.1126/science.1158023,Dynamic Shifts of Limited Working Memory Resources in Human Vision,"Our ability to remember what we have seen is very limited. Most current views characterize this limit as a fixed number of items—only four objects—that can be held in visual working memory. We show that visual memory capacity is not fixed by the number of objects, but rather is a limited resource that is shared out dynamically between all items in the visual scene. This resource can be shifted flexibly between objects, with allocation biased by selective attention and toward targets of upcoming eye movements. The proportion of resources allocated to each item determines the precision with which it is remembered, a relation that we show is governed by a simple power law, allowing quantitative estimates of resource distribution in a scene.",0
https://doi.org/10.2753/mtp1069-6679160402,Structural Equation Modeling in Marketing: Some Practical Reminders,"The authors review issues related to the application of structural equation modeling (SEM) in marketing. The discussion begins by considering issues related to the process of applying SEM in empirical research, including model specification, identification, estimation, evaluation, and respecification, and reporting of results. In addition to these process issues, a number of other issues, such as formulation of multiple theoretical models, model error versus sampling error, and relating study objectives to the capabilities of SEM, are considered, and suggestions offered regarding ways that SEM applications might be improved.",0
https://doi.org/10.1167/11.6.1,Differential attentional modulation of cortical responses to S-cone and luminance stimuli,"Neural signals driven by short-wave-sensitive (S) cones are, to a large degree, anatomically and functionally separate from the achromatic luminance pathway until at least one synapse into V1. Attentional mechanisms that act at an anatomically early stage in V1 may, therefore, affect S-cone and luminance signals differently. Here, we used a steady-state visually evoked potential (SSVEP) paradigm combined with electrical source imaging to study the effects of contrast and attention on neural responses to both chromatic S-cone isolating and achromatic stimuli in five human visual areas including V1. The responses to these gratings were affected very differently by changes in contrast and attention. Increasing cone contrast increased the response amplitude for both types of stimulus. For the S-cone-defined stimuli, we also observed a systematic decrease in the response phase of the first harmonic with increasing stimulus contrast, but there was no corresponding change in phase for the first harmonic of the luminance probes. Attending to the contrast of the grating increased the amplitude and phase of luminance-driven responses but had no effect on S-cone-driven responses. We conclude that while attentional modulation can be observed in achromatic pathways as early as V1, attention may not affect SSVEP signals generated by S-cone stimuli.",0
https://doi.org/10.1177/0013161x11400185,How to Improve Teaching Practices,"Purpose: Although it is expected that building schoolwide capacity for teacher learning will improve teaching practices, there is little systematic evidence to support this claim. This study aimed to examine the relative impact of transformational leadership practices, school organizational conditions, teacher motivational factors, and teacher learning on teaching practices. Research Design: Data were collected from a survey of 502 teachers from 32 elementary schools in the Netherlands. A structural model was tested on the within-school covariance matrix and a chi-square test taking into account nonindependence of observations. Findings: Results suggest that teachers’ engagement in professional learning activities, in particular experimenting and reflection, is a powerful predictor for teaching practices. Teachers’ sense of self-efficacy appeared to be the most important motivational factor for explaining teacher learning and teaching practices. Motivational factors also mediate the effects of school organizational conditions and leadership practices on teacher learning and teaching practices. Finally, transformational leadership practices stimulate teachers’ professional learning and motivation and improve school organizational conditions. Conclusions: For school leaders, to foster teacher learning and improve teaching practices a combination of transformational leadership behaviors is required. Further research is needed to examine the relative effects of transformational leadership dimensions on school organizational conditions, teacher motivation, and professional learning in schools. Finally, conditions for school improvement were examined at one point in time. Longitudinal studies to school improvement are required to model changes in schools’ capacities and growth and their subsequent effects on teaching practices.",0
,POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS VIA REALIZED DISCREPANCIES,"This paper considers Bayesian counterparts of the classical tests for good- ness of fit and their use in judging the fit of a single Bayesian model to the observed data. We focus on posterior predictive assessment, in a framework that also includes conditioning on auxiliary statistics. The Bayesian formulation facilitates the con- struction and calculation of a meaningful reference distribution not only for any (classical) statistic, but also for any parameter-dependent statistic or discrep- ancy. The latter allows us to propose the realized discrepancy assessment of model fitness, which directly measures the true discrepancy between data and the posited model, for any aspect of the model which we want to explore. The computation required for the realized discrepancy assessment is a straightforward byproduct of the posterior simulation used for the original Bayesian analysis. We illustrate with three applied examples. The first example, which serves mainly to motivate the work, illustrates the difficulty of classical tests in assessing the fitness of a Poisson model to a positron emission tomography image that is constrained to be nonnegative. The second and third examples illustrate the details of the posterior predictive approach in two problems: estimation in a model with inequality constraints on the parameters, and estimation in a mixture model. In all three examples, standard test statistics (either a χ 2 or a likelihood ratio) are not pivotal: the difficulty is not just how to compute the reference distribution for the test, but that in the classical framework no such distribution exists, independent of the unknown model parameters.",0
https://doi.org/10.1002/jts.22039,Temporal Associations Among Chronic PTSD Symptoms in U.S. Combat Veterans,"The present study examined fluctuation over time in symptoms of posttraumatic stress disorder (PTSD) among 34 combat veterans (28 with diagnosed PTSD, 6 with subclinical symptoms) assessed every 2 weeks for up to 2 years (range of assessments = 13-52). Temporal relationships were examined among four PTSD symptom clusters (reexperiencing, avoidance, emotional numbing, and hyperarousal) with particular attention to the influence of hyperarousal. Multilevel cross-lagged random coefficients autoregression for intensive time series data analyses were used to model symptom fluctuation decades after combat experiences. As anticipated, hyperarousal predicted subsequent fluctuations in the 3 other PTSD symptom clusters (reexperiencing, avoidance, emotional numbing) at subsequent 2-week intervals (rs = .45, .36, and .40, respectively). Additionally, emotional numbing influenced later reexperiencing and avoidance, and reexperiencing influenced later hyperarousal (rs = .44, .40, and .34, respectively). These findings underscore the important influence of hyperarousal. Furthermore, results indicate a bidirectional relationship between hyperarousal and reexperiencing as well as a possible chaining of symptoms (hyperarousal → emotional numbing → reexperiencing → hyperarousal) and establish potential internal, intrapersonal mechanisms for the maintenance of persistent PTSD symptoms. Results suggested that clinical interventions targeting hyperarousal and emotional numbing symptoms may hold promise for PTSD of long duration.",0
https://doi.org/10.1177/01466216980223006,A Model for Optimal Constrained Adaptive Testing,"A model for constrained computerized adaptive testing is proposed in which the information in the test at the trait level (0) estimate is maximized subject to a number of possible constraints on the content of the test. At each item-selection step, a full test is assembled to have maximum information at the current 0 estimate, fixing the items already administered. Then the item with maximum in-formation is selected. All test assembly is optimal because a linear programming (LP) model is used that automatically updates to allow for the attributes of the items already administered and the new value of the 0 estimator. The LP model also guarantees that each adaptive test always meets the entire set of constraints. A simulation study using a bank of 753 items from the Law School Admission Test showed that the 0 estimator for adaptive tests of realistic lengths did not suffer any loss of efficiency from the presence of 433 constraints on the item selection process.",0
https://doi.org/10.1002/sim.1524,Separation of individual-level and cluster-level covariate effects in regression analysis of correlated data,"The focus of this paper is regression analysis of clustered data. Although the presence of intracluster correlation (the tendency for items within a cluster to respond alike) is typically viewed as an obstacle to good inference, the complex structure of clustered data offers significant analytic advantages over independent data. One key advantage is the ability to separate effects at the individual (or item-specific) level and the group (or cluster-specific) level. We review different approaches for the separation of individual-level and cluster-level effects on response, their appropriate interpretation and give recommendations for model fitting based on the intent of the data analyst. Unlike many earlier papers on this topic, we place particular emphasis on the interpretation of the cluster-level covariate effect. The main ideas of the paper are highlighted in an analysis of the relationship between birth weight and IQ using sibling data from a large birth cohort study.",0
https://doi.org/10.1017/s0266466600008884,Testing for Second-Order Stochastic Dominance of Two Distributions,"A distribution function F is said to stochastically dominate another distribution function G in the second-order sense if , for all x . Second-order stochastic dominance plays an important role in economics, finance, and accounting. Here a statistical test has been constructed to test , for some x ∈ [ a , b ], against the hypothesis , for all x ∈ [ a , b ], where a and b are any two real numbers. The test has been shown to be consistent and has an upper bound α on the asymptotic size. The test is expected to have usefulness for comparison of random prospects for risk averters.",0
https://doi.org/10.1002/sim.3067,Authors' Reply,,0
https://doi.org/10.1002/(sici)1099-0720(199808)12:4<339::aid-acp571>3.0.co;2-d,Modelling clustered data in autobiographical memory research: the multilevel approach,"Much memory research involves recording several autobiographical memories for each of several people. These memories are not independent of each other, an assumption of the statistical procedures used in many cognitive psychology papers. In recent years there have been both statistical and computational advances for modelling these hierarchical data structures. This is often called multilevel modelling. Using data from recent memory research (Burt et al., 1995), I describe this approach and show how it compares favourably with traditional approaches. © 1998 John Wiley & Sons, Ltd.",0
https://doi.org/10.5194/aab-58-277-2015,Comparison of inference methods of genetic parameters with an application to body weight in broilers,"Abstract. REML (restricted maximum likelihood) has become the standard method of variance component estimation in animal breeding. Inference in Bayesian animal models is typically based upon Markov chain Monte Carlo (MCMC) methods, which are generally flexible but time-consuming. Recently, a new Bayesian computational method, integrated nested Laplace approximation (INLA), has been introduced for making fast non-sampling-based Bayesian inference for hierarchical latent Gaussian models. This paper is concerned with the comparison of estimates provided by three representative programs (ASReml, WinBUGS and the R package AnimalINLA) of the corresponding methods (REML, MCMC and INLA), with a view to their applicability for the typical animal breeder. Gaussian and binary as well as simulated data were used to assess the relative efficiency of the methods. Analysis of 2319 records of body weight at 35 days of age from a broiler line suggested a purely additive animal model, in which the heritability estimates ranged from 0.31 to 0.34 for the Gaussian trait and from 0.19 to 0.36 for the binary trait, depending on the estimation method. Although in need of further development, AnimalINLA seems a fast program for Bayesian modeling, particularly suitable for the inference of Gaussian traits, while WinBUGS appeared to successfully accommodate a complicated structure between the random effects. However, ASReml remains the best practical choice for the serious animal breeder.",0
https://doi.org/10.3758/bf03196750,An introduction to Bayesian hierarchical models with an application in the theory of signal detection,"Although many nonlinear models of cognition have been proposed in the past 50 years, there has been little consideration of corresponding statistical techniques for their analysis. In analyses with nonlinear models, unmodeled variability from the selection of items or participants may lead to asymptotically biased estimation. This asymptotic bias, in turn, renders inference problematic. We show, for example, that a signal detection analysis of recognition memory data leads to asymptotic underestimation of sensitivity. To eliminate asymptotic bias, we advocate hierarchical models in which participant variability, item variability, and measurement error are modeled simultaneously. By accounting for multiple sources of variability, hierarchical models yield consistent and accurate estimates of participant and item effects in recognition memory. This article is written in tutorial format; we provide an introduction to Bayesian statistics, hierarchical modeling, and Markov chain Monte Carlo computational techniques.",0
https://doi.org/10.1016/b978-044452044-9/50005-7,Advances in Analysis of Mean and Covariance Structure when Data are Incomplete,"Missing data arise in many areas of empirical research. One such area is in the context of structural equation models (SEM). A review is presented of the methodological advances in fitting data to SEM and, more generally, to mean and covariance structure models when there is missing data. This encompasses common missing data mechanisms and some widely used methods for handling missing data. The methods fall under the classifications of ad-hoc, likelihood-based, and simulation-based. Also included are the results of some of the published simulation studies. In order to encourage further research, a method is proposed for performing sensitivity analysis, which up to now has been seemingly lacking. A simulation study was done to demonstrate the method using a three-factor factor analysis model, focusing on MCAR and MNAR data. Parameter estimates from samples of all available data, in the form of box plots, are compared with parameter estimates from only the complete data. The results indicate a possible distinction for determining missing data mechanisms.",0
https://doi.org/10.1002/per.1988,"Felt Security in Daily Interactions as a Mediator of the Effect of Attachment on Relationship Satisfaction†Some of the Results Reported in this Article Were Previously Presented at the Annual Meeting of the Society for Interpersonal Theory and Research in Montreal, Canada (May 2012).","This study examined how felt security in interpersonal situations with one's romantic partner mediated the effect of global (dispositional) attachment on relationship satisfaction. Felt security was measured using an event–contingent recording (ECR) methodology with a sample of 93 cohabiting couples who reported their social interactions with each other during a 20–day period. Global attachment was measured at the beginning of the ECR procedure. Relationship satisfaction was measured at the end of the ECR procedure (T1) and approximately 7 months after the ECR procedure (T2). Results confirmed the established links between attachment and relationship satisfaction such that higher attachment avoidance and attachment anxiety were associated with decline in satisfaction over time. Results also indicated that attachment avoidance but not attachment anxiety was negatively related to felt security, both within–partner and across–partners. As expected, lower felt security exerted a negative effect on relationship satisfaction at T1 and T2, and partly mediated the effect of attachment avoidance on relationship satisfaction at T1 and T2, both within–partner and across–partners. Partners’ gender emerged as a moderator of these results. Findings suggest higher attachment avoidance leads to less felt security in daily social interactions, which leads to less satisfaction with the romantic relationship. Copyright © 2015 European Association of Personality Psychology",0
https://doi.org/10.1080/10705510701575511,Lower Level Mediation Effect Analysis in Two-Level Studies: A Note on a Multilevel Structural Equation Modeling Approach,"This article presents a didactic discussion of a multilevel covariance structure modeling approach to estimation of lowest level mediation effect indexes in two-level studies. The procedure is useful when addressing questions about relations among total and indirect effects between variables of interest while accounting for the hierarchical structure of analyzed data. The discussed method also permits interval estimation and hypothesis tests with respect to related quantities of relevance when evaluating mediated effects with clustered data, and is illustrated on a two-level data set.",0
https://doi.org/10.1093/biomet/82.1.81,Bias correction in generalised linear mixed models with a single component of dispersion,"SUMMARY General expressions are derived for the asymptotic biases in three approximate estimators of regression coefficients and variance component, for small values of the variance component, in generalised linear mixed models with canonical link function and a single source of extraneous variation. The estimators involve first and second order Laplace expansions of the integrated likelihood and a related procedure known as penalised quasilikelihood. Numerical studies of a series of matched pairs of binary outcomes show that the first order estimators of the variance component are seriously biased. Easily computed correction factors produce satisfactory estimators of small variance components, comparable to those obtained with a second order Laplace expansion, and markedly improve the asymptotic performance for larger values. For a series of matched pairs of binomial observations, the variance correction factors rapidly approach one as the binomial denominators increase. These results greatly extend the range of parameter values for which the approximate estimation procedures have satisfactory asymptotic properties.",0
https://doi.org/10.1016/j.aap.2015.06.002,Estimating the safety effects of lane widths on urban streets in Nebraska using the propensity scores-potential outcomes framework,"A sufficient understanding of the safety impact of lane widths in urban areas is necessary to produce geometric designs that optimize safety performance for all users. The overarching trend found in the research literature is that as lane widths narrow, crash frequency increases. However, this trend is inconsistent and is the result of multiple cross-sectional studies that have issues related to lack of control for potential confounding variables, unobserved heterogeneity or omitted variable bias, or endogeneity among independent variables, among others. Using ten years of mid-block crash data on urban arterials and collectors from four cities in Nebraska, crash modification factors (CMFs) were estimated for various lane widths and crash types. These CMFs were developed using the propensity scores-potential outcomes methodology. This method reduces many of the issues associated with cross-sectional regression models when estimating the safety effects of infrastructure-related design features. Generalized boosting, a non-parametric modeling technique, was used to estimate the propensity scores. Matching was performed using both Nearest Neighbor and Mahalanobis matching techniques. CMF estimation was done using mixed-effects negative binomial or Poisson regression with the matched data. Lane widths included in the analysis included 9ft, 10ft, 11ft, and 12ft. Some of the estimated CMFs were point estimates while others were functions of traffic volume (i.e., the CMF changed depending on the traffic volume). Roadways with 10ft travel lanes were found to experience the highest crash frequency relative to other lane widths. Meanwhile, roads with 9ft travel lanes were found to experience the lowest relative crash frequency. While this may be due to increased driver caution when traveling on narrow lanes, it is possible that unobserved factors influenced this result. CMFs for target crash types (sideswipe same-direction and sideswipe opposite-direction) were consistent with the values currently used in the Highway Safety Manual (HSM).",0
https://doi.org/10.1080/00401706.1979.10489726,Tables of the Studentized Maximum Modulus Distribution and an Application to Multiple Comparisons Among Means,"Tables of the upper α-points m α; k*, v of the studentized maximum modulus distribution with parameter k* and v degrees of freedom are given for α = .01, .05, 10, and .20, twelve values 01 V, and k* = k(k − 1)/2 for k = 3(1)20. The tables are of use in applying Hochberg's GT2-Method [6] of multiple comparison and extend the use of this method to cases with k ≤ 20 means. As conjectured in [15]. the GT2-Method is in almost all cases the strongest of several competitors of Spodtvoll and Stoline's T′-Method [11] for carrying out the set of k* pairwise comparisons between the means of k groups with arbitrary sample sizes. The auxiliary tables of [15], in which the T′-Method and other procedures are compared, are extended here. These tables should aid the user in choosing between the T′ and GT2-Methods. The results of the comparisons continue to support the fact that the T-Method is preferred if the unbalance of the sample sizes is slight and that T becomes a stronger competitor as α increases.",0
https://doi.org/10.1054/jpai.2001.26174,"Systemic administration of CNI-1493, a p38 mitogen-activated protein kinase inhibitor, blocks intrathecal human immunodeficiency virus-1 gp120-induced enhanced pain states in rats","Intrathecal administration of the human immunodeficiency virus-1 envelope glycoprotein, gp120, activates astrocytes and microglia to release products that induce thermal hyperalgesia and mechanical allodynia. Both pain states are disrupted by intrathecal CNI-1493, a p38 mitogen-activated protein (MAP) kinase inhibitor. Whether CNI-1493, or any other p38 MAP kinase inhibitor, can cross the blood-brain barrier to affect spinal cord function is unknown. Given that several such drugs are in clinical trials, it is of interest to determine whether they may be potentially useful in treating centrally mediated pain. The aim of the present studies was to determine whether systemic CNI-1493 could block intrathecal gp120-induced thermal hyperalgesia and/or mechanical allodynia. Because p38 MAP kinase inhibition would be expected to prevent proinflammatory cytokine release and/or signal transduction, we sought to determine from the same animals the likely mechanism by which CNI-1493 blocks gp120-induced pain states. These studies show that systemic CNI-1493 blocks intrathecal gp120-induced thermal hyperalgesia and mechanical allodynia. Because CNI-1493 did not block proinflammatory cytokine release, this may suggest disruption at the level of signal transduction. These studies provide the first evidence that systemic p38 MAP kinase inhibitors can prevent centrally mediated exaggerated pain states. Thus, CNI-1493 may provide a novel therapeutic approach for the treatment of pain.",0
https://doi.org/10.1111/j.2044-8317.1989.tb00911.x,Balanced versus unbalanced designs for linear structural relations in two-level data,"A general two-level model for multivariate data is described and illustrated by specializations to a common factor model and a path model with latent variables. The problem of estimation is treated, and in the special case of a balanced sampling design, a likelihood-based discrepancy function and a test of goodness of fit are obtained in terms of some simple sufficient statistics.",0
https://doi.org/10.1007/s11266-014-9496-4,Environmental Philanthropy and Environmental Behavior in Five Countries: Is There Convergence Among Youth?,"This paper compares and contrasts environmental philanthropy, environmental behavior, and their determinants among university students in five countries: Canada, Germany, Israel, South Korea, and the United States. The paperÃ¢â‚¬â„¢s unique contribution to the nonprofit literature is its focus on environmental philanthropy as an unexplored form of philanthropic behavior, and the ability to test environmental philanthropy in an international setting and in comparison to other modes of environmental behavior. By environmental philanthropy, we mean giving to, and volunteering in, various environmental non-governmental organizations, and by environmental behavior, we refer to daily behaviors in the private sphere with ecological implications. We hypothesize that although the five countries vary on several characteristics, the student populationsÃ¢â‚¬â€ who are young, educated, and exposed to global ideas and normsÃ¢â‚¬â€ will be relatively similar to each other in their environmental and philanthropic behavior and in the determinants of such behavior. To test this hypothesis, a standardized questionnaire was administered to 8,477 students on five campuses. Results show significant differences between students in their environmental philanthropic behavior, as well as in the demographic and attitudinal determinants of such behaviors. Ã‚Â© 2014, International Society for Third-Sector Research and The Johns Hopkins University.",0
https://doi.org/10.1109/iccv.2005.77,Discovering objects and their location in images,"We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing only one object per image. We also extend the bag-of-words vocabulary to include 'doublets' which encode spatially local co-occurring regions. It is demonstrated that this extended vocabulary gives a cleaner image segmentation. Finally, the classification and segmentation methods are applied to a set of images containing multiple objects per image. These results demonstrate that we can successfully build object class models from an unsupervised analysis of images.",0
https://doi.org/10.3758/app.72.4.1179,DLs in reminder and 2AFC tasks: Data and models,"García-Pérez and Alcalá-Quintana (2010) dispute the conclusion of Lapid, Ulrich, and Rammsayer (2008) that the two-alternative forced choice (2AFC) task yields meaningfully larger estimates of the difference limen (DL) than does the reminder task. García-Pérez and Alcalá-Quintana overlook, however, fundamental properties of 2AFC psychometric functions and Type B order errors in their reanalysis. In addition, their favored theory (i.e., the difference model with guessing) does not provide a plausible account for why the 2AFC task tends to yield larger DLs (by about 50%) than does the reminder task. In trying to clarify these issues, I hope to advance the proper assessment of discrimination performance in 2AFC tasks.",0
https://doi.org/10.1007/978-94-010-1436-6_6,Confidence Intervals vs Bayesian Intervals,"For many years, statistics textbooks have followed this ‘canonical’ procedure: (1) the reader is warned not to use the discredited methods of Bayes and Laplace, (2) an orthodox method is extolled as superior and applied to a few simple problems, (3) the corresponding Bayesian solutions are not worked out or described in any way. The net result is that no evidence whatsoever is offered to substantiate the claim of superiority of the orthodox method.",0
https://doi.org/10.1093/biostatistics/kxh022,Using a Bayesian latent growth curve model to identify trajectories of positive affect and negative events following myocardial infarction,"Positive and negative affect data are often collected over time in psychiatric care settings, yet no generally accepted means are available to relate these data to useful diagnoses or treatments. Latent class analysis attempts data reduction by classifying subjects into one of K unobserved classes based on observed data. Latent class models have recently been extended to accommodate longitudinally observed data. We extend these approaches in a Bayesian framework to accommodate trajectories of both continuous and discrete data. We consider whether latent class models might be used to distinguish patients on the basis of trajectories of observed affect scores, reported events, and presence or absence of clinical depression.",0
https://doi.org/10.1002/9781119170174.epcn506,Cultural Consensus Theory,,0
https://doi.org/10.1207/s15327906mbr4002_2,Comparison of Two Procedures for Analyzing Small Sets of Repeated Measures Data,"This article compares two methods for analyzing small sets of repeated measures data under normal and non-normal heteroscedastic conditions: a mixed model approach with the Kenward-Roger correction and a multivariate extension of the modified Brown-Forsythe (BF) test. These procedures differ in their assumptions about the covariance structure of the data and in the method of estimation of the parameters defining the mean structure. Simulation results show that the BF test outperformed its competitor, in terms of Type I errors, particularly when the total sample size was small, and the data were normally distributed. Under non-normal distributions the BF test tended to err on the side of conservatism. Results also indicate that neither method was uniformly more powerful. With very few exceptions, the power differences between these two methods depended on the population covariance structure, the nature of the pairing of covariance matrices and group sizes, and the relationship between mean vectors and dispersion matrices.",0
https://doi.org/10.2307/2532317,Generalized Linear Models with Random Effects; Salamander Mating Revisited,"In recent years much effort has been devoted to extending regression methodology to non-Gaussian data, where responses are not independent. These methods for dependent responses are suitable for data from longitudinal studies or nested designs. However, use of these methods for crossed designs seems to have serious limitations due to the intensive computations involved because of the intractable nature of the joint distribution. In this paper, we cast the problem in a Bayesian framework and use a Monte Carlo method, the Gibbs sampler, to avoid current computational limitations. The flexibility of this approach is illustrated by analyzing the interesting salamander mating data reported by McCullagh and Nelder (1989, Generalized Linear Models, 2nd edition, London: Chapman and Hall).",0
https://doi.org/10.1016/j.cognition.2009.03.008,A Bayesian framework for word segmentation: Exploring the effects of context,"Since the experiments of Saffran et al. [Saffran, J., Aslin, R., & Newport, E. (1996). Statistical learning in 8-month-old infants. Science, 274, 1926-1928], there has been a great deal of interest in the question of how statistical regularities in the speech stream might be used by infants to begin to identify individual words. In this work, we use computational modeling to explore the effects of different assumptions the learner might make regarding the nature of words--in particular, how these assumptions affect the kinds of words that are segmented from a corpus of transcribed child-directed speech. We develop several models within a Bayesian ideal observer framework, and use them to examine the consequences of assuming either that words are independent units, or units that help to predict other units. We show through empirical and theoretical results that the assumption of independence causes the learner to undersegment the corpus, with many two- and three-word sequences (e.g. what's that, do you, in the house) misidentified as individual words. In contrast, when the learner assumes that words are predictive, the resulting segmentation is far more accurate. These results indicate that taking context into account is important for a statistical word segmentation strategy to be successful, and raise the possibility that even young infants may be able to exploit more subtle statistical patterns than have usually been considered.",0
https://doi.org/10.1017/s1740925x0700035x,The glial modulatory drug AV411 attenuates mechanical allodynia in rat models of neuropathic pain,"Abstract Controlling neuropathic pain is an unmet medical need and we set out to identify new therapeutic candidates. AV411 (ibudilast) is a relatively nonselective phosphodiesterase inhibitor that also suppresses glial-cell activation and can partition into the CNS. Recent data strongly implicate activated glial cells in the spinal cord in the development and maintenance of neuropathic pain. We hypothesized that AV411 might be effective in the treatment of neuropathic pain and, hence, tested whether it attenuates the mechanical allodynia induced in rats by chronic constriction injury (CCI) of the sciatic nerve, spinal nerve ligation (SNL) and the chemotherapeutic paclitaxel (Taxol ¯ ). Twice-daily systemic administration of AV411 for multiple days resulted in a sustained attenuation of CCI-induced allodynia. Reversal of allodynia was of similar magnitude to that observed with gabapentin and enhanced efficacy was observed in combination. We further show that multi-day AV411 reduces SNL-induced allodynia, and reverses and prevents paclitaxel-induced allodynia. Also, AV411 cotreatment attenuates tolerance to morphine in nerve-injured rats. Safety pharmacology, pharmacokinetic and initial mechanistic analyses were also performed. Overall, the results indicate that AV411 is effective in diverse models of neuropathic pain and support further exploration of its potential as a therapeutic agent for the treatment of neuropathic pain.",0
https://doi.org/10.1080/00031305.1985.10479400,An Introduction to Empirical Bayes Data Analysis,"Abstract Empirical Bayes methods have been shown to be powerful data-analysis tools in recent years. The empirical Bayes model is much richer than either the classical or the ordinary Bayes model and often provides superior estimates of parameters. An introduction to some empirical Bayes methods is given, and these methods are illustrated with two examples.",0
https://doi.org/10.1016/s0927-5398(03)00007-0,Improved estimation of the covariance matrix of stock returns with an application to portfolio selection,"This paper proposes to estimate the covariance matrix of stock returns by an optimally weighted average of two existing estimators: the sample covariance matrix and single-index covariance matrix. This method is generally known as shrinkage, and it is standard in decision theory and in empirical Bayesian statistics. Our shrinkage estimator can be seen as a way to account for extra-market covariance without having to specify an arbitrary multi-factor structure. For NYSE and AMEX stock returns from 1972 to 1995, it can be used to select portfolios with significantly lower out-of-sample variance than a set of existing estimators, including multi-factor models.",0
https://doi.org/10.1177/0146621603260678,Order-Constrained Bayes Inference for Dichotomous Models of Unidimensional Nonparametric IRT,"This study introduces an order-constrained Bayes inference framework useful for analyzing data containing dichotomous-scored item responses, under the assumptions of either the monotone homogeneity model or the double monotonicity model of nonparametric item response theory (NIRT). The framework involves the implementation of Gibbs sampling to estimate order-constrained parameters, followed by inference with the posterior-predictive distribution to test the monotonicity, invariant item ordering, and local independence assumptions of NIRT. The Bayes framework is demonstrated through the analysis of real test data, and possible extensions of it are discussed.",0
https://doi.org/10.1016/0010-0285(80)90005-5,A feature-integration theory of attention,"A new hypothesis about the role of focused attention is proposed. The feature-integration theory of attention suggests that attention must be directed serially to each stimulus in a display whenever conjunctions of more than one separable feature are needed to characterize or distinguish the possible objects presented. A number of predictions were tested in a variety of paradigms including visual search, texture segregation, identification and localization, and using both separable dimensions (shape and color) and local elements or parts of figures (lines, curves, etc. in letters) as the features to be integrated into complex wholes. The results were in general consistent with the hypothesis. They offer a new set of criteria for distinguishing separable from integral features and a new rationale for predicting which tasks will show attention limits and which will not.",0
https://doi.org/10.1371/journal.pgen.1005650,"A Flexible, Efficient Binomial Mixed Model for Identifying Differential DNA Methylation in Bisulfite Sequencing Data","Identifying sources of variation in DNA methylation levels is important for understanding gene regulation. Recently, bisulfite sequencing has become a popular tool for investigating DNA methylation levels. However, modeling bisulfite sequencing data is complicated by dramatic variation in coverage across sites and individual samples, and because of the computational challenges of controlling for genetic covariance in count data. To address these challenges, we present a binomial mixed model and an efficient, sampling-based algorithm (MACAU: Mixed model association for count data via data augmentation) for approximate parameter estimation and p-value computation. This framework allows us to simultaneously account for both the over-dispersed, count-based nature of bisulfite sequencing data, as well as genetic relatedness among individuals. Using simulations and two real data sets (whole genome bisulfite sequencing (WGBS) data from Arabidopsis thaliana and reduced representation bisulfite sequencing (RRBS) data from baboons), we show that our method provides well-calibrated test statistics in the presence of population structure. Further, it improves power to detect differentially methylated sites: in the RRBS data set, MACAU detected 1.6-fold more age-associated CpG sites than a beta-binomial model (the next best approach). Changes in these sites are consistent with known age-related shifts in DNA methylation levels, and are enriched near genes that are differentially expressed with age in the same population. Taken together, our results indicate that MACAU is an efficient, effective tool for analyzing bisulfite sequencing data, with particular salience to analyses of structured populations. MACAU is freely available at www.xzlab.org/software.html.",0
https://doi.org/10.1016/j.jmva.2009.05.002,Shrinkage estimators for large covariance matrices in multivariate real and complex normal distributions under an invariant quadratic loss,"The problem of estimating large covariance matrices of multivariate real normal and complex normal distributions is considered when the dimension of the variables is larger than the number of samples. The Stein–Haff identities and calculus on eigenstructure for singular Wishart matrices are developed for real and complex cases, respectively. By using these techniques, the unbiased risk estimates for certain classes of estimators for the population covariance matrices under invariant quadratic loss functions are obtained for real and complex cases, respectively. Based on the unbiased risk estimates, shrinkage estimators which are counterparts of the estimators due to Haff [L.R. Haff, Empirical Bayes estimation of the multivariate normal covariance matrix, Ann. Statist. 8 (1980) 586–697] are shown to improve upon the best scalar multiple of the empirical covariance matrix under the invariant quadratic loss functions for both real and complex multivariate normal distributions in the situation where the dimension of the variables is larger than the number of samples.",0
https://doi.org/10.1007/bf02294246,A dynamic factor model for the analysis of multivariate time series,"As a method to ascertain the structure of intra-individual variation, P-technique has met difficulties in the handling of a lagged covariance structure. A new statistical technique, coined dynamic factor analysis, is proposed, which accounts for the entire lagged covariance function of an arbitrary second order stationary time series. Moreover, dynamic factor analysis is shown to be applicable to a relatively short stretch of observations and therefore is considered worthwhile for psychological research. At several places the argumentation is clarified through the use of examples. Ã‚Â© 1985 The Psychometric Society.",0
https://doi.org/10.2307/2532834,High-Dimensional Multivariate Probit Analysis,"A computationally practical form of probit analysis for multiple response variables based on an assumed common factor model for the latent tolerances is proposed. Numerical integration over the factor space provides maximum likelihood estimation of the probit regression parameters and of the probabilities of response combinations under the model. The procedure is applied to five variables from the Pneumoconiosis Field Trial, two variables of which were previously analyzed by Ashford and Sowden (1970, Biometrics 26, 535-546).",0
https://doi.org/10.1080/01621459.1990.10474968,Illustration of Bayesian Inference in Normal Data Models Using Gibbs Sampling,"Abstract The use of the Gibbs sampler as a method for calculating Bayesian marginal posterior and predictive densities is reviewed and illustrated with a range of normal data models, including variance components, unordered and ordered means, hierarchical growth curves, and missing data in a crossover trial. In all cases the approach is straightforward to specify distributionally and to implement computationally, with output readily adapted for required inference summaries.",0
https://doi.org/10.1198/108571102726,Adequacy of approximations to distributions of test statistics in complex mixed linear models,"A recent study of lady beetle antennae was a small sample repeated measures design involving a complex covariance structure. Distributions of test statistics based on mixed models fitted to such data are unknown, but two recently developed methods for approximating the distributions of test statistics in mixed linear models have been included as options in the latest release of the MIXED procedure of SASÃ‚Â®. One method (FC, from Fai and Cornelius) computes degrees of freedom of an approximating F distribution for the test statistic using spectral decomposition of the hypothesis matrix together with repeated application of a method for single-degree-of-freedom tests. The other method (KR, from Kenward and Roger) adjusts the estimated covariance matrix of the parameter estimates, computes a scale adjustment to the test statistic, and computes the degrees of freedom of an approximating F distribution. Using the two methods, p values for a hypothesis of interest in the lady beetle study were quite different. Simulation studies on the Proc MIXED implementation of these methods showed that Type I error rates of both methods are affected by covariance structure complexity, sample size, and imbalance. Nonetheless, the KR method performs well in situations with fairly complicated covariance structures when sample sizes are moderate to small and the design is reasonably balanced. The KR method should be used in preference to the FC method, although it had inflated Type I error rates for complex covariance structures combined with small sample sizes. Ã‚Â© 2002 American Statistical Association and the International Biometric Society.",0
https://doi.org/10.1007/s00439-007-0445-9,Methods for meta-analysis in genetic association studies: a review of their potential and pitfalls,"Meta-analysis offers the opportunity to combine evidence from retrospectively accumulated or prospectively generated data. Meta-analyses may provide summary estimates and can help in detecting and addressing potential inconsistency between the combined datasets. Application of meta-analysis in genetic associations presents considerable potential and several pitfalls. In this review, we present basic principles of meta-analytic methods, adapted for human genome epidemiology. We describe issues that arise in the retrospective or the prospective collection of relevant data through various sources, common traps to consider in the appraisal of evidence and potential biases that may interfere. We describe the relative merits and caveats for common methods used to trace inconsistency across studies along with possible reasons for non-replication of proposed associations. Different statistical models may be employed to combine data and some common misconceptions may arise in the process. Several meta-analysis diagnostics are often applied or misapplied in the literature, and we comment on their use and limitations. An alternative to overcome limitations arising from retrospective combination of data from published studies is to create networks of research teams working in the same field and perform collaborative meta-analyses of individual participant data, ideally on a prospective basis. We discuss the advantages and the challenges inherent in such collaborative approaches. Meta-analysis can be a useful tool in dissecting the genetics of complex diseases and traits, provided its methods are properly applied and interpreted. Ã‚Â© Springer-Verlag 2007.",0
https://doi.org/10.1016/s0304-4076(98)00055-4,Marketing models of consumer heterogeneity,"The distribution of consumer preferences plays a central role in many marketing activities. Pricing and product design decisions, for example, are based on an understanding of the differences among consumers in price sensitivity and valuation of product attributes. In addition, marketing activities which target specific households require household level parameter estimates. Thus, the modeling of consumer heterogeneity is the central focus of many statistical marketing applications. In contrast, heterogeneity is often regarded as an ancillary nuisance problem in much of the applied econometrics literature which must be dealt with but is not the focus of the investigation. The focus is instead on estimating average effects of policy variables. In this paper, we discuss various approaches to modeling consumer heterogeneity and evaluate the utility of these approaches for marketing applications.",0
https://doi.org/10.2307/2669749,P Values for Composite Null Models,,0
https://doi.org/10.1007/s13253-013-0137-y,Objective Bayesian Analysis of Geometrically Anisotropic Spatial Data,"Anisotropic models are often used in spatial statistics to analyze spatially referenced data. Within a Bayesian framework we develop default priors for the anisotropic Gaussian random field model with and without including a nugget parameter accounting for the effects of microscale variations and measurement errors. We present Jeffreys priors and a reference prior and study their posterior propriety. Moreover, we obtain that the predictive distributions at ungauged locations have finite variance. We also show that the seemingly uninformative uniform prior for the anisotropy parameters, ratio and angle, yields an improper posterior. Finally, we find that the proposed priors have good frequentist properties and we illustrate our approach by analyzing two data sets for which we discuss model choice as well as predictions and uncertainty estimates. Â© 2013 International Biometric Society.",0
https://doi.org/10.1016/j.ijresmar.2004.01.002,A cross-validity comparison of rating-based and choice-based conjoint analysis models,"Abstract This paper compares OLS, hierarchical Bayes (HB), and latent segment, rating-based conjoint models to HB and latent segment choice-based conjoint models. The biggest systematic difference between rating- and choice-based parameters is consistent with a compatibility effect. This leads to a stronger prominence effect for rating-based models. The HB rating-based model has the highest hit rate and choice share validations and the OLS model has the second highest. Within both rating- and choice-based models, hierarchical Bayes models have higher hit rate and choice share validations than latent segment models. Using a choice simulator, the profiles predicted to be optimal with an HB choice-based model are similar to those predicted to be optimal with an HB rating-based model.",0
,Markov Chain Monte Carlo Maximum Likelihood,"http://conservancy.umn.edu/handle/11299/58440 Markov chain Monte Carlo (e. g., the Metropolis algorithm and Gibbs sampler) is a general tool for simulation of complex stochastic processes useful in many types of statistical inference. The basics of Markov chain Monte Carlo are reviewed, including choice of algorithms and variance estimation, and some new methods are introduced. The use of Markov chain Monte Carlo for maximum likelihood estimation is explained, and its performance is compared with maximum pseudo likelihood estimation.",0
https://doi.org/10.1002/(sici)1097-0258(19971030)16:20<2349::aid-sim667>3.0.co;2-e,Using the general linear mixed model to analyse unbalanced repeated measures and longitudinal data,"The general linear mixed model provides a useful approach for analysing a wide variety of data structures which practising statisticians often encounter. Two such data structures which can be problematic to analyse are unbalanced repeated measures data and longitudinal data. Owing to recent advances in methods and software, the mixed model analysis is now readily available to data analysts. The model is similar in many respects to ordinary multiple regression, but because it allows correlation between the observations, it requires additional work to specify models and to assess goodness-of-fit. The extra complexity involved is compensated for by the additional flexibility it provides in model fitting. The purpose of this tutorial is to provide readers with a sufficient introduction to the theory to understand the method and a more extensive discussion of model fitting and checking in order to provide guidelines for its use. We provide two detailed case studies, one a clinical trial with repeated measures and dropouts, and one an epidemiological survey with longitudinal follow-up.",0
https://doi.org/10.1186/s13643-016-0186-8,The effectiveness and safety of treatments used for acute diarrhea and acute gastroenteritis in children: protocol for a systematic review and network meta-analysis,"Acute diarrhea and acute gastroenteritis (AD/AGE) are common among children in low- and middle-income countries (LMIC) and high-income countries (HIC). Supportive therapy including maintaining feeding, prevention of dehydration, and use of oral rehydration solution (ORS), is the mainstay of treatment in all children. Several additional treatments aiming to reduce the episode duration have been compared to placebo, but the differences in effectiveness among them are unknown.We will conduct a systematic review of all randomized controlled trials evaluating the use of zinc, vitamin A, probiotics, prebiotics, synbiotics, racecadotril, smectite, and fermented and lactose-free milk/formula for AD/AGE treatment in children. The primary outcomes are diarrhea duration and mortality. Secondary outcomes are diarrhea lasting 3 or 7 days, stool frequency, treatment failure, hospitalizations, and adverse events. We will search MEDLINE, Ovid EMBASE, CINAHL, the Cochrane Central Register of Controlled Trials (CENTRAL), and LILACS through Ovid, as well as grey literature resources. Two reviewers will independently screen titles and abstracts, review full texts, extract information, and assess the risk of bias (ROB) and the confidence in the estimate (with the grading of recommendations, assessment, development, and evaluation [GRADE] approach). Results will be summarized narratively and statistically. Subgroup analysis according to HIC vs. LMIC, age, nutrition status, and ROB is planned. We will perform a Bayesian network meta-analysis to combine the pooled direct and indirect treatment effect estimates for each outcome, if adequate data is available.This is the first systematic review and network meta-analysis that aims to determine the relative effectiveness of pharmacological and nutritional treatments for reducing the duration of AD/AGE in children. The results will help to reduce the uncertainty of the effectiveness of the interventions, find knowledge gaps, and/or encourage further research for other therapeutic options.PROSPERO registration number: CRD42015023778.",0
https://doi.org/10.1093/biomet/73.3.755,A note on A. Albert and J. A. Anderson's conditions for the existence of maximum likelihood estimates in logistic regression models,"SUMMARY This note expands the paper by Albert & Anderson (1984) on the existence and uniqueness of maximum likelihood estimates in logistic regression models. Their three possible mutually exclusive data pattems: (i) overlap, (ii) complete separation, and (iii) quasiseparation are considered. The maximum likelihood estimate exists only in (i). Modifications of the statement and proofs of Albert & Anderson's results are given for (ii) and (iii). The identifiability for a more general model arising in the study of (iii) is discussed together with the maximization of the corresponding likelihood. A linear program is presented which determines whether data is of type (i), (ii) or (iii), and in the case of (iii) identifies Albert & Anderson's minimal set Qm.",0
https://doi.org/10.1177/014662167700100209,A Use of the Information Function in Tailored Testing,"Several important and useful implications in latent trait theory, with direct implications for individualized adaptive or tailored testing, are pointed out. A way of using the information function in tailored testing in connection with the standard error of estimation of the ability level using maximum likelihood estimation is suggested. It is emphasized that the standard error of estimation should be considered as the major index of dependability, as opposed to the reliability of a test. The concept of weak parallel forms is expanded to testing procedures in which different sets of items are presented to different examinees. Examples are given. Researchers have tended to use latent trait theory rather than classical test theory in research on individualized adaptive or tailored testing. This is quite natural, since latent trait theory has definite merits over classical test theory in many crucial matters. Because of the lack of opportunities to really learn the theory, however, these researchers tend to overlook some important implications in latent trait theory. As a result, its full use has not yet materialized. Not only are information functions seldom used to maximum advantage, but also those who have tried to use latent trait theory still use some popular concepts in classical test theory, such as reliability. expanded to testing procedures in which different sets of items are presented to different examinees. Examples are given.  In  this paper, the author points out some important implications in latent trait theory which are not fully understood and appreciated among researchers, and gives some practical suggestions for its use",0
https://doi.org/10.1177/0038040715588603,Secondary Education Systems and the General Skills of Less- and Intermediate-educated Adults,"We investigate the impact of external differentiation and vocational orientation of (lower and upper) secondary education on country variation in the mean numeracy skills of, and skills gaps between, adults with low and intermediate formal qualifications. We use data on 30- to 44-year-olds in 18 countries from the 2011–12 round of the Program for the International Assessment of Adult Competencies. We find that higher levels of external differentiation (tracking) amplify skills gaps between less- and intermediate-educated adults. This is mainly due to lower mean skills achievement of less-educated adults. By contrast, greater emphasis on vocational skills in upper-secondary education is positively related to numeracy skills for both less- and intermediate-educated adults. Gains are larger for the less educated, so the gap in numeracy skills tends to fall with the degree of vocational orientation. We discuss implications of our findings for research on educational and labor market inequalities.",0
https://doi.org/10.1002/sim.2639,Trying to be precise about vagueness,"A previous investigation by Lambert et al., which used computer simulation to examine the influence of choice of prior distribution on inferences from Bayesian random effects meta-analysis, is critically examined from a number of viewpoints. The practical example used is shown to be problematic. The various prior distributions are shown to be unreasonable in terms of what they imply about the joint distribution of the overall treatment effect and the random effects variance. An alternative form of prior distribution is tentatively proposed. Finally, some practical recommendations are made that stress the value both of fixed effect analyses and of frequentist approaches as well as various diagnostic investigations.",0
https://doi.org/10.1016/j.jmp.2008.03.002,Bayes factors: Prior sensitivity and model generalizability,"Model selection is a central issue in mathematical psychology. One useful criterion for model selection is generalizability; that is, the chosen model should yield the best predictions for future data. Some researchers in psychology have proposed that the Bayes factor can be used for assessing model generalizability. An alternative method, known as the generalization criterion, has also been proposed for the same purpose. We argue that these two methods address different levels of model generalizability (local and global), and will often produce divergent conclusions. We illustrate this divergence by applying the Bayes factor and the generalization criterion to a comparison of retention functions. The application of alternative model selection criteria will also be demonstrated within the framework of model generalizability.",0
https://doi.org/10.1201/b10905-8,Implementing MCMC,"The reversible jump Markov chain Monte Carlo (MCMC) sampler (Green, 1995) provides a general framework for Markov chain Monte Carlo simulation in which the dimension of the parameter space can vary between iterates of the Markov chain. The reversible jump sampler can be viewed as an extension of the Metropolis-Hastings algorithm onto more general state spaces. To understand this in a Bayesian modeling context, suppose that for observed data x wehavea countable collectionof candidatemodelsM = {M1,M2, . . .} indexedbyaparameter k ∈ K. The index k can be considered as an auxiliarymodel indicator variable, such thatMk′ denotes themodel where k = k′. EachmodelMk has an nk-dimensional vector of unknown parameters, θk ∈ Rnk , where nk can take different values for different models k ∈ K. The joint posterior distribution of (k, θk) given observed data, x, is obtained as the product of the likelihood, L(x | k, θk), and the joint prior, p(k, θk) = p(θk | k)p(k), constructed from the prior distribution of θk under model Mk , and the prior for the model indicator k (i.e. the prior for model Mk). Hence, the joint posterior isπ(k, θk | x) = L(x | k, θk)p(θk | k)p(k)∑ k′∈K∫ R nk′ L(x | k′, θ′k′)p(θ′k′ | k′)p(k′)dθ′k′. (3.1)The reversible jump algorithm uses the joint posterior distribution in Equation 3.1 as the target of an MCMC sampler over the state space Θ = ⋃k∈K({k} × Rnk ), where the states of the Markov chain are of the form (k, θk), the dimension of which can vary over the state space. Accordingly, from the output of a single Markov chain sampler, the user is able to obtain a full probabilistic description of the posterior probabilities of each model having observed the data, x, in addition to the posterior distributions of the individual models. This chapter aims to provide an overview of the reversible jump sampler. Wewill outlinethe sampler’s theoretical underpinnings, present the latest and most popular techniques for enhancing algorithmperformance, and discuss the analysis of sampler output. Through the use of numerous worked examples it is hoped that the reader will gain a broad appreciation of the issues involved in multi-model simulation, and the conﬁdence to implement reversible jump samplers in the course of their own studies.",0
https://doi.org/10.1287/mksc.1040.0100,Generalized Robust Conjoint Estimation,"We introduce methods from statistical learning theory to the field of conjoint analysis for preference modeling. We present a method for estimating preference models that can be highly nonlinear and robust to noise. Like recently developed polyhedral methods for conjoint analysis, our method is based on computationally efficient optimization techniques. We compare our method with standard logistic regression, hierarchical Bayes, and the polyhedral methods using standard, widely used simulation data. The experiments show that the proposed method handles noise significantly better than both logistic regression and the recent polyhedral methods and is never worse than the best method among the three mentioned above. It can also be used for estimating nonlinearities in preference models faster and better than all other methods. Finally, a simple extension for handling heterogeneity shows promising results relative to hierarchical Bayes. The proposed method can therefore be useful, for example, for analyzing large amounts of data that are noisy or for estimating interactions among product features.",0
https://doi.org/10.2307/2112482,A Hierarchical Model for Studying School Effects,"When researchers investigate how school policies, practices, or climates affect student outcomes, they use multilevel, hierarchical data. Though methodologists have consistently warned of the formidable inferential problems such data pose for traditional statistical methods, no comprehensive alternative analytic strategy has been available. This paper presents a general statistical methodology for such hierarchically structured data and illustrates its use by reexamining the High School and Beyond data and the controversy over the effectiveness of public and Catholic schools. The model enables the researcher to utilize mean achievement and certain structural parameters that characterize the equity in the social distribution of achievement as multivariate outcomes for each school. Variation in these school-level outcomes is then explained as a function of school characteristics.",0
https://doi.org/10.3102/10769986028002169,Performance of Empirical Bayes Estimators of Level-2 Random Parameters in Multilevel Analysis: A Monte Carlo Study for Longitudinal Designs,"Multilevel analysis is a useful technique for analyzing longitudinal data. To describe a person’s development across time, the quality of the estimates of the random coefficients, which relate time to individual changes in a relevant dependent variable, is of importance. The present study compares three estimators of the random coefficients: the Bayes estimator (BE), the empirical Bayes estimator (EBE), and the ordinary least squares estimator (OLSE). Using MLwiN, Monte Carlo simulations are carried out to study the performance of the estimators, while systematically varying the size of the sample as well as the number of measurement occasions. First, we examine for normally distributed random coefficients to what extent the EBE performs better than the OLSE and to what extent the EBE preserves the good properties of the BE. Second, we examine the robustness of the EBE which is based on a normal distribution of the random parameters, by comparing its performance to the OLSE for data being generated from two distributions other than the normal distribution: a modified t-distribution and a modified exponential distribution. As performance criteria we examine the Bayes risk as well as a criterion based on the frequentist notion of mean squared error.",0
https://doi.org/10.1080/10705511.2015.1014041,A Poor Person’s Posterior Predictive Checking of Structural Equation Models,"Posterior predictive model checking (PPMC) is a Bayesian model checking method that compares the observed data to (plausible) future observations from the posterior predictive distribution. We propose an alternative to PPMC in the context of structural equation modeling, which we term the poor person’s PPMC (PP-PPMC), for the situation wherein one cannot afford (or is unwilling) to draw samples from the full posterior. Using only by-products of likelihood-based estimation (maximum likelihood estimate and information matrix), the PP-PPMC offers a natural method to handle parameter uncertainty in model fit assessment. In particular, a coupling relationship between the classical p values from the model fit chi-square test and the predictive p values from the PP-PPMC method is carefully examined, suggesting that PP-PPMC might offer an alternative, principled approach for model fit assessment. We also illustrate the flexibility of the PP-PPMC approach by applying it to case-influence diagnostics.",0
https://doi.org/10.1038/jes.2015.44,Retrospective benzene exposure assessment for a multi-center case-cohort study of benzene-exposed workers in China,"Quality of exposure assessment has been shown to be related to the ability to detect risk of lymphohematopoietic disorders in epidemiological investigations of benzene, especially at low levels of exposure. We set out to build a statistical model for reconstructing exposure levels for 2898 subjects from 501 factories that were part of a nested case-cohort study within the NCI-CAPM cohort of more than 110,000 workers. We used a hierarchical model to allow for clustering of measurements by factory, workshop, job, and date. To calibrate the model we used historical routine monitoring data. Measurements below the limit of detection were accommodated by constructing a censored data likelihood. Potential non-linear and industry-specific time-trends and predictor effects were incorporated using regression splines and random effects. A partial validation of predicted exposures in 2004/2005 was performed through comparison with full-shift measurements from an exposure survey in facilities that were still open. Median cumulative exposure to benzene at age 50 for subjects that ever held an exposed job (n=1175) was 509 mg/m 3 years. Direct comparison of model estimates with measured full-shift personal exposure in the 2004/2005 survey showed moderate correlation and a potential downward bias at low (<1 mg/m 3) exposure estimates. The modeling framework enabled us to deal with the data complexities generally found in studies using historical exposure data in a comprehensive way and we therefore expect to be able to investigate effects at relatively low exposure levels. Ã‚Â© 2016 Nature America, Inc.",0
https://doi.org/10.1167/10.2.11,Rod and cone contrast gains derived from reaction time distribution modeling,"Contrast gain reflects the rapidity of response amplitude increase with increase in stimulus contrast. In physiology, contrast gain can be measured directly as the initial slope of cell contrast response function. In psychophysics, contrast gain estimation is not straightforward. Further, rod and cone contrast gains have not been measured psychophysically at mesopic light levels where both rods and cones are active, due to the difficulty in producing stimuli that excite rods and cones separately at the same adaptation level. Here, we estimated rod and contrast gains by fitting reaction time distributions measured at a light level in which rods alone (scotopic), rods and cones (mesopic), or cones alone (photopic) mediate vision. The reaction time distributions were modeled by two different strategies, a simplified diffusion model that assumes a stochastic accumulation process and a model we developed that begins with sensory input based on early visual processing impulse response functions and assumes the reaction time variability originates in the response criterion. Estimates of contrast gain from both models were comparable and consistent with primate physiology measurements.",0
https://doi.org/10.1016/s0895-4356(03)00007-6,A comparison between traditional methods and multilevel regression for the analysis of multicenter intervention studies,"This article reviews three traditional methods for the analysis of multicenter trials with persons nested within clusters, i.e., centers, namely naïve regression (persons as units of analysis), fixed effects regression, and the use of summary measures (clusters as units of analysis), and compares these methods with multilevel regression. The comparison is made for continuous (quantitative) outcomes, and is based on the estimator of the treatment effect and its standard error, because these usually are of main interest in intervention studies. When the results of the experiment have to be valid for some larger population of centers, the centers in the intervention study have to present a random sample from this population and multilevel regression may be used. It is shown that the treatment effect and especially its standard error, are generally incorrectly estimated by the traditional methods, which should, therefore, not in general be used as an alternative to multilevel regression.",0
https://doi.org/10.1111/j.1530-0277.2007.00596.x,"Where, With Whom, and How Much Alcohol Is Consumed on Drinking Events Involving Aggression? Event-Level Associations in a Canadian National Survey of University Students","Epidemiological research using event-level data can provide a better understanding of the association between alcohol consumption, characteristics of drinking contexts, and the likelihood of aggressive behavior. The present research assessed whether alcohol intake and characteristics of the drinking context were associated with the likelihood of aggression within individuals across 3 drinking events based on a national sample of university students, taking into account individual characteristics and university level variables. Additionally, we determined whether individual characteristics, particularly drinking pattern, were associated with alcohol-related aggression controlling for drinking event characteristics, and whether relations of aggression to alcohol and drinking contexts differed by gender.Secondary analyses of the 2004 Canadian Campus Survey (CCS), a national survey of 6,282 university students (41% response rate) at 40 Canadian universities, were conducted. Respondents were asked about their three most recent drinking events, including whether they were in an argument or fight with someone, number of drinks consumed, and characteristics of the drinking context as well as their usual drinking frequency and heavy episodic drinking. We used multi-level analyses to account for the nested structure of the data (i.e., drinking events nested within individuals who were nested within universities).The number of drinks consumed was positively associated with aggression. Drinking contexts found to be positively associated with aggression included being at a party, at a fraternity/sorority and/or residence, at three or more drinking places (versus 1 or 2), and having a partner present whereas having a meal reduced the likelihood of aggression. A significant interaction was found between gender and being at a party, with a significant effect found for women but not for men.These results support experimental evidence indicating a direct role of alcohol in aggression and point to characteristics of the drinking context that might be targeted in future prevention initiatives.",0
https://doi.org/10.1037/a0012869,"The multilevel latent covariate model: A new, more reliable approach to group-level effects in contextual studies.","In multilevel modeling (MLM), group-level (L2) characteristics are often measured by aggregating individual-level (L1) characteristics within each group so as to assess contextual effects (e.g., group-average effects of socioeconomic status, achievement, climate). Most previous applications have used a multilevel manifest covariate (MMC) approach, in which the observed (manifest) group mean is assumed to be perfectly reliable. This article demonstrates mathematically and with simulation results that this MMC approach can result in substantially biased estimates of contextual effects and can substantially underestimate the associated standard errors, depending on the number of L1 individuals per group, the number of groups, the intraclass correlation, the sampling ratio (the percentage of cases within each group sampled), and the nature of the data. To address this pervasive problem, the authors introduce a new multilevel latent covariate (MLC) approach that corrects for unreliability at L2 and results in unbiased estimates of L2 constructs under appropriate conditions. However, under some circumstances when the sampling ratio approaches 100%, the MMC approach provides more accurate estimates. Based on 3 simulations and 2 real-data applications, the authors evaluate the MMC and MLC approaches and suggest when researchers should most appropriately use one, the other, or a combination of both approaches.",0
https://doi.org/10.1093/oxfordhb/9780199286546.003.0026,Multilevel Models,"Abstract This article addresses multilevel models in which units are nested within one another. The focus is primarily two-level models. It also describes cross-unit heterogeneity. Moreover, it assesses the fixed and random effects from the multilevel model. It generally tries to convey the scope of multilevel models but in a very compact way. Multilevel models provide great promise for exploiting information in hierarchical data structures. There are a range of alternatives for such data and it bears repeating that sometimes, simpler-to-apply correctives are best.",0
https://doi.org/10.1002/sim.6791,A tutorial on Bayesian bivariate meta-analysis of mixed binary-continuous outcomes with missing treatment effects,"Bivariate random-effects meta-analysis (BVMA) is a method of data synthesis that accounts for treatment effects measured on two outcomes. BVMA gives more precise estimates of the population mean and predicted values than two univariate random-effects meta-analyses (UVMAs). BVMA also addresses bias from incomplete reporting of outcomes. A few tutorials have covered technical details of BVMA of categorical or continuous outcomes. Limited guidance is available on how to analyze datasets that include trials with mixed continuous-binary outcomes where treatment effects on one outcome or the other are not reported. Given the advantages of Bayesian BVMA for handling missing outcomes, we present a tutorial for Bayesian BVMA of incompletely reported treatment effects on mixed bivariate outcomes. This step-by-step approach can serve as a model for our intended audience, the methodologist familiar with Bayesian meta-analysis, looking for practical advice on fitting bivariate models. To facilitate application of the proposed methods, we include our WinBUGS code. As an example, we use aggregate-level data from published trials to demonstrate the estimation of the effects of vitamin K and bisphosphonates on two correlated bone outcomes, fracture, and bone mineral density. We present datasets where reporting of the pairs of treatment effects on both outcomes was 'partially' complete (i.e., pairs completely reported in some trials), and we outline steps for modeling the incompletely reported data. To assess what is gained from the additional work required by BVMA, we compare the resulting estimates to those from separate UVMAs. We discuss methodological findings and make four recommendations. Copyright © 2015 John Wiley & Sons, Ltd.",0
https://doi.org/10.1080/10439463.2013.878344,"Policy spillover and the policing of protest in New York City, 1960–2006","Scholars have suggested that the policing of protest have become more permissive in Western democracies since the 1960s. While widely accepted, studies affirming a softening of police conduct have focused on national trends despite awareness that police tactics have unevenly diffused across the USA. This study examines the temporal trends in protest policing in New York City to evaluate how and why the dominant strategies of protest control have changed over time. Drawing on the widespread privatisation of public space in New York during the 1980s coupled with the adoption of Broken Windows crime control strategies, I develop an alternative explanation of the temporal dynamics of protest policing that is based on policy spillover, or the unintentional spillover effects that policy decisions unrelated to protest policing may nonetheless have on police conduct. Using a sample of 6147 protest events occurring in New York between 1960 and 2006, I confirm that the prevalence of arrests and other forms of polic...",0
https://doi.org/10.1037/0033-295x.112.3.662,Bayesian statistical inference in psychology: Comment on Trafimow (2003).,"D. Trafimow (2003) presented an analysis of null hypothesis significance testing (NHST) using Bayes's theorem. Among other points, he concluded that NHST is logically invalid, but that logically valid Bayesian analyses are often not possible. The latter conclusion reflects a fundamental misunderstanding of the nature of Bayesian inference. This view needs correction, because Bayesian methods have an important role to play in many psychological problems where standard techniques are inadequate. This comment, with the help of a simple example, explains the usefulness of Bayesian inference for psychology.",0
https://doi.org/10.1002/sim.3680,"The BUGS project: Evolution, critique and future directions","BUGS is a software package for Bayesian inference using Gibbs sampling. The software has been instrumental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the BUGS project and some future perspectives.",0
https://doi.org/10.1007/s11336-005-1350-6,Signal Detection Models with Random Participant and Item Effects,"The theory of signal detection is convenient for measuring mnemonic ability in recognition memory paradigms. In these paradigms, randomly selected participants are asked to study randomly selected items. In practice, researchers aggregate data across items or participants or both. The signal detection model is nonlinear; consequently, analysis with aggregated data is not consistent. In fact, mnemonic ability is underestimated, even in the large-sample limit. We present two hierarchical Bayesian models that simultaneously account for participant and item variability. We show how these models provide for accurate estimation of participants' mnemonic ability as well as the memorability of items. The model is benchmarked with a simulation study and applied to a novel data set. Â© 2007 The Psychometric Society.",0
https://doi.org/10.1109/icssbe.2012.6396604,Guessing-testlet response model,"The psychometric standard of fairness can be violated if the guessing effect is improperly handled. The two most common ways of handling guessing effect are through item design and guessing effect modeling. Items with lower priori guessing probability helps to reduce guessing effect. This paper proposes a two-parameter logistic guessing-testlet response model to model such items. The proposed model is an extended testlet response model where items of the same guessing priori guessing probability are grouped in the same testlet. To reduce the priori guessing probabilities, the items are designed to have multiple-correct responses and the number of correct responses is varying across items. Simulation result shows that the proposed model outperforms the two-parameter logistic item response model in model fit. The proposed guessing-testlet merits ability with no guessing but penalizes ability with guessing.",0
https://doi.org/10.1080/10705510903203573,Factor Analysis with Ordinal Indicators: A Monte Carlo Study Comparing DWLS and ULS Estimation,"Factor analysis models with ordinal indicators are often estimated using a 3-stage procedure where the last stage involves obtaining parameter estimates by least squares from the sample polychoric correlations. A simulation study involving 324 conditions (1,000 replications per condition) was performed to compare the performance of diagonally weighted least squares (DWLS) and unweighted least squares (ULS) in the procedure's third stage. Overall, both methods provided accurate and similar results. However, ULS was found to provide more accurate and less variable parameter estimates, as well as more precise standard errors and better coverage rates. Nevertheless, convergence rates for DWLS are higher. Our recommendation is therefore to use ULS, and, in the case of nonconvergence, to use DWLS, as this method might converge when ULS does not.",0
https://doi.org/10.1007/978-3-642-69746-3_2,From Intentions to Actions: A Theory of Planned Behavior,"There appears to be general agreement among social psychologists that most human behavior is goal-directed (e. g., Heider, 1958 ; Lewin, 1951). Being neither capricious nor frivolous, human social behavior can best be described as following along lines of more or less well-formulated plans. Before attending a concert, for example, a person may extend an invitation to a date, purchase tickets, change into proper attire, call a cab, collect the date, and proceed to the concert hall. Most, if not all, of these activities will have been designed in advance; their execution occurs as the plan unfolds. To be sure, a certain sequence of actions can become so habitual or routine that it is performed almost automatically, as in the case of driving from home to work or playing the piano. Highly developed skills of this kind typically no longer require conscious formulation of a behavioral plan. Nevertheless, at least in general outline, we are normally well aware of the actions required to attain a certain goal. Consider such a relatively routine behavior as typing a letter. When setting this activity as a goal, we anticipate the need to locate a typewriter, insert a sheet of paper, adjust the margins, formulate words and sentences, strike the appropriate keys, and so forth. Some parts of the plan are more routine, and require less conscious thought than others, but without an explicit or implicit plan to guide the required sequence of acts, no letter would get typed.",0
https://doi.org/10.2307/2530695,Unbalanced Repeated-Measures Models with Structured Covariance Matrices,"The question of how to analyze unbalanced or incomplete repeated-measures data is a common problem facing analysts. We address this problem through maximum likelihood analysis using a general linear model for expected responses and arbitrary structural models for the within-subject covariances. Models that can be fit include standard univariate and multivariate models with incomplete data, random-effects models, and models with time-series and factor-analytic error structures. We describe Newton-Raphson and Fisher scoring algorithms for computing maximum likelihood estimates, and generalized EM algorithms for computing restricted and unrestricted maximum likelihood estimates. An example fitting several models to a set of growth data is included.",0
https://doi.org/10.1214/07-aoas138,In-season prediction of batting averages: A field test of empirical Bayes and Bayes methodologies,"Batting average is one of the principle performance measures for an individual baseball player. It is natural to statistically model this as a binomial-variable proportion, with a given (observed) number of qualifying attempts (called “at-bats”), an observed number of successes (“hits”) distributed according to the binomial distribution, and with a true (but unknown) value of pi that represents the player’s latent ability. This is a common data structure in many statistical applications; and so the methodological study here has implications for such a range of applications. We look at batting records for each Major League player over the course of a single season (2005). The primary focus is on using only the batting records from an earlier part of the season (e.g., the first 3 months) in order to estimate the batter’s latent ability, pi, and consequently, also to predict their batting-average performance for the remainder of the season. Since we are using a season that has already concluded, we can then validate our estimation performance by comparing the estimated values to the actual values for the remainder of the season. The prediction methods to be investigated are motivated from empirical Bayes and hierarchical Bayes interpretations. A newly proposed nonparametric empirical Bayes procedure performs particularly well in the basic analysis of the full data set, though less well with analyses involving more homogeneous subsets of the data. In those more homogeneous situations better performance is obtained from appropriate versions of more familiar methods. In all situations the poorest performing choice is the naïve predictor which directly uses the current average to predict the future average. One feature of all the statistical methodologies here is the preliminary use of a new form of variance stabilizing transformation in order to transform the binomial data problem into a somewhat more familiar structure involving (approximately) Normal random variables with known variances. This transformation technique is also used in the construction of a new empirical validation test of the binomial model assumption that is the conceptual basis for all our analyses.",0
https://doi.org/10.1177/0146621605282772,Markov Chain Monte Carlo Estimation of Item Parameters for the Generalized Graded Unfolding Model,"The authors present a Markov Chain Monte Carlo (MCMC) parameter estimation procedure for the generalized graded unfolding model (GGUM) and compare it to the marginal maximum likelihood (MML) approach implemented in the GGUM2000 computer program, using simulated and real personality data. In the simulation study, test length, number of response options, and sample size were manipulated. Results indicate that the two methods are comparable in terms of item parameter estimation accuracy. Although the MML estimates exhibit slightly smaller bias than MCMC estimates, they also show greater variability, which results in larger root mean squared errors. Of the two methods, only MCMC provides reasonable standard error estimates for all items.",0
https://doi.org/10.1093/aje/kwq472,Implementation of G-Computation on a Simulated Data Set: Demonstration of a Causal Inference Technique,The growing body of work in the epidemiology literature focused on G-computation includes theoretical explanations of the method but very few simulations or examples of application. The small number of G-computation analyses in the epidemiology literature relative to other causal inference approaches may be partially due to a lack of didactic explanations of the method targeted toward an epidemiology audience. The authors provide a step-by-step demonstration of G-computation that is intended to familiarize the reader with this procedure. The authors simulate a data set and then demonstrate both G-computation and traditional regression to draw connections and illustrate contrasts between their implementation and interpretation relative to the truth of the simulation protocol. A marginal structural model is used for effect estimation in the G-computation example. The authors conclude by answering a series of questions to emphasize the key characteristics of causal inference techniques and the G-computation procedure in particular.,0
https://doi.org/10.1080/02664760802191397,Bayesian assessment of times to diagnosis in breast cancer screening,"Breast cancer is one of the diseases with the most profound impact on health in developed countries and mammography is the most popular method for detecting breast cancer at a very early stage. This paper focuses on the waiting period from a positive mammogram until a confirmatory diagnosis is carried out in hospital. Generalized linear mixed models are used to perform the statistical analysis, always within the Bayesian reasoning. Markov chain Monte Carlo algorithms are applied for estimation by simulating the posterior distribution of the parameters and hyperparameters of the model through the free software WinBUGS.",0
https://doi.org/10.1348/000711001159546,Bayesian estimation and test for factor analysis model with continuous and polytomous data in several populations,"The main purpose of this paper is to develop a Bayesian approach for the multisample factor analysis model with continuous and polytomous variables. Joint Bayesian estimates of the thresholds, the factor scores and the structural parameters subjected to some simple constraints across groups are obtained simultaneously. The Gibbs sampler is used to produce the joint Bayesian estimates. It is shown that the conditional distributions involved in the implementation are the familiar uniform, gamma, normal, univariate truncated normal and Wishart distributions. The Bayes factor is introduced to test hypotheses involving constraints among the structural parameters of the factor analysis models across groups. Two procedures for computing the test statistics are developed, one based on the Schwarz criterion (or Bayesian information criterion), while the other computes the posterior densities and likelihood ratios by means of draws from the appropriate conditional distributions via the Gibbs sampler. The empirical performance of the proposed Bayesian procedure and its sensitivity to prior distributions are illustrated by some simulation results and two real-life examples.",0
https://doi.org/10.2501/s1470785309200402,The Truth is out there! how External Validity can Lead to Better Marketing Decisions,"Marketing managers typically have to use and integrate many pieces of data and marketing intelligence when taking decisions such as whether to launch a product and, if so, at what price. Conjoint experiments and analysis remain popular marketing research tools with business practitioners to test and measure how the market will react to different actions. There is a growing body of work that focuses on, first, how to construct the experiments so that they better represent real market conditions and, second, the use of sophisticated model specifications that provide information on consumers' responses. The market researcher typically uses internal validation for model validity - a comparison of model prediction and within-sample holdout data. We contend in this paper that customers and users of market research information need to adopt a different and wider meaning of validity, referred to as external validity, to facilitate improved decision making. In this research, a case study is used as an example to demonstrate how marketing managers can use the information from a choice-based conjoint derived choice model differently depending on the manner in which the model validation is carried out. Ã‚Â© 2009 The Market Research Society.",0
https://doi.org/10.1371/journal.pone.0071608,Recalibration of the Multisensory Temporal Window of Integration Results from Changing Task Demands,"The notion of the temporal window of integration, when applied in a multisensory context, refers to the breadth of the interval across which the brain perceives two stimuli from different sensory modalities as synchronous. It maintains a unitary perception of multisensory events despite physical and biophysical timing differences between the senses. The boundaries of the window can be influenced by attention and past sensory experience. Here we examined whether task demands could also influence the multisensory temporal window of integration. We varied the stimulus onset asynchrony between simple, short-lasting auditory and visual stimuli while participants performed two tasks in separate blocks: a temporal order judgment task that required the discrimination of subtle auditory-visual asynchronies, and a reaction time task to the first incoming stimulus irrespective of its sensory modality. We defined the temporal window of integration as the range of stimulus onset asynchronies where performance was below 75% in the temporal order judgment task, as well as the range of stimulus onset asynchronies where responses showed multisensory facilitation (race model violation) in the reaction time task. In 5 of 11 participants, we observed audio-visual stimulus onset asynchronies where reaction time was significantly accelerated (indicating successful integration in this task) while performance was accurate in the temporal order judgment task (indicating successful segregation in that task). This dissociation suggests that in some participants, the boundaries of the temporal window of integration can adaptively recalibrate in order to optimize performance according to specific task demands.",0
https://doi.org/10.1111/j.1745-3984.1991.tb00363.x,Multilevel Factor Analysis of Class and Student Achievement Components,"Multilevel Factor Analysis of Class and Student Achievement Components Author(s): Bengt O. Muthen Source: Journal of Educational Measurement, Vol. 28, No. 4 (Winter, 1991), pp. 338-354 Published by: National Council on Measurement in Education Stable URL: http://www.jstor.org/stable/1434897 . Accessed: 18/05/2011 18:39 Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at . http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at . http://www.jstor.org/action/showPublisher?publisherCode=ncme. . Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. National Council on Measurement in Education is collaborating with JSTOR to digitize, preserve and extend access to Journal of Educational Measurement. http://www.jstor.org",0
https://doi.org/10.1007/bf02288391,The theory of the estimation of test reliability,"The theoretically best estimate of the reliability coefficient is stated in terms of a precise definition of the equivalence of two forms of a test. Various approximations to this theoretical formula are derived, with reference to several degrees of completeness of information about the test and to special assumptions. The familiar Spearman-Brown Formula is shown to be a special case of the general formulation of the problem of reliability. Reliability coefficients computed in various ways are presented for comparative purposes. Â© 1937 Psychometric Society.",0
https://doi.org/10.3102/1076998615621299,Using Data-Dependent Priors to Mitigate Small Sample Bias in Latent Growth Models,"Mixed-effects models (MEMs) and latent growth models (LGMs) are often considered interchangeable save the discipline-specific nomenclature. Software implementations of these models, however, are not interchangeable, particularly with small sample sizes. Restricted maximum likelihood estimation that mitigates small sample bias in MEMs has not been widely developed for LGMs, and fully Bayesian methods, while not dependent on asymptotics, can encounter issues because the choice for the factor covariance matrix prior distribution has substantial influence with small samples. This tutorial discusses differences between LGMs and MEMs and demonstrates how data-dependent priors, an established class of methods that blend frequentist and Bayesian paradigms, can be implemented within M plus 7.1 to abate the small sample bias that is prevalent with LGM software while keeping additional programming to the bare minimum.",1
https://doi.org/10.1111/j.1541-0420.2011.01617.x,Split-Plot Designs for Robotic Serial Dilution Assays,"This article explores effective implementation of split-plot designs in serial dilution bioassay using robots. We show that the shortest path for a robot to fill plate wells for a split-plot design is equivalent to the shortest common supersequence problem in combinatorics. We develop an algorithm for finding the shortest common supersequence, provide an R implementation, and explore the distribution of the number of steps required to implement split-plot designs for bioassay through simulation. We also show how to construct collections of split plots that can be filled in a minimal number of steps, thereby demonstrating that split-plot designs can be implemented with nearly the same effort as strip-plot designs. Finally, we provide guidelines for modeling data that result from these designs.",0
https://doi.org/10.1080/02664763.2015.1010491,Comparison of three-level cluster randomized trials using quantile dispersion graphs,"The purpose of this article is to evaluate and compare several three-level cluster randomized designs on the basis of their power functions. The power function of cluster designs depends on the intracluster correlations (ICCs), which are generally unknown at the planning stage. Thus, to compare these designs a prior knowledge of the ICCs is required. Three interval estimation methods are proposed for assigning joint confidence intervals to the two ICCs (corresponding to each cluster level). A detailed simulation study comparing the confidence intervals attained by the different techniques is given. The technique of quantile dispersion graphs is used for comparing the three-level cluster designs. For a given design, quantiles of the power function, are obtained for various effect sizes. These quantiles are functions of the unknown ICC coefficients. To address the dependence of the quantiles on the correlations, a 100(1−α)% confidence region is computed, and used as a parameter space. A three-level nested d...",0
,A digitally delivered and fully automated internet- and mobile-based smoking cessation programme : a randomized controlled trial,,0
https://doi.org/10.1177/2158244015625445,Two Cross-Platform Programs for Inferences and Interval Estimation About Indirect Effects in Mediational Models,"In this article, we describe two new programs that compute both p-values and confidence intervals (CI) for the indirect effect in mediational models, including (a) a p-value based on the partial posterior method, which we refer to as p 3 computed across the posterior distribution of the regression coefficients; (b) a variant of p 3 that uses a normal approximation for the posterior distributions, p 3N ; (c) Hierarchical Bayesian CIs (CI HB ) based on the posterior distributions of the regression coefficients; and (d) CIs based on the Monte Carlo method (CI MC ). These programs do not require access to raw data as do resampling methods. Similar to Sobel’s test, p 3 and p 3N constitute a single p-value for the indirect effect while performing substantially better in terms of Type I and II error rates. Furthermore, we include a memory efficient computational algorithm for CI HB and CI MC that allows for precision beyond that in existing alternative implementations. The underlying programs can utilize multicore processors, and their performance is tested through a simulation study. Finally, the use of these programs is illustrated with an empirical example.",0
https://doi.org/10.1002/sim.7347,Multidimensional latent trait linear mixed model: an application in clinical studies with multivariate longitudinal outcomes,"Multilevel item response theory (MLIRT) models have been widely used to analyze the multivariate longitudinal data of mixed types (e.g., categorical and continuous) in clinical studies. The MLIRT models often have unidimensional assumption, that is, the multiple outcomes are clinical manifestations of a univariate latent variable. However, the unidimensional assumption may be unrealistic because some diseases may be heterogeneous and characterized by multiple impaired domains with variable clinical symptoms and disease progressions. We relax this assumption and propose a multidimensional latent trait linear mixed model (MLTLMM) to allow multiple latent variables and within-item multidimensionality (one outcome can be a manifestation of more than one latent variable). We conduct extensive simulation studies to assess the unidimensional MLIRT model and the proposed MLTLMM model. The simulation studies suggest that the MLTLMM model outperforms unidimensional model when the multivariate longitudinal outcomes are manifested by multiple latent variables. The proposed model is applied to two motivating studies of amyotrophic lateral sclerosis: a clinical trial of ceftriaxone and the Pooled Resource Open-Access ALS Clinical Trials database. Copyright © 2017 John Wiley & Sons, Ltd.",0
https://doi.org/10.1007/s11121-014-0505-z,Preventing Weight Gain and Obesity: Indirect Effects of the Family Check-Up in Early Childhood,"The early signs of obesity are observable in early childhood. Although the most promising prevention approaches are family-centered, few relevant early prevention programs exist. This study evaluated the effects of an evidence-based, home-visiting intervention, the Family Check-Up (FCU), on the trajectory of children's weight gain. The FCU was designed to prevent the development of behavior problems by improving family management practices; children's weight has not been an explicit target. On the basis of previous research and conceptual models, we hypothesized that intervention effects on parenting practices, specifically caregivers' use of positive behavior support (PBS) strategies in toddlerhood, would mediate improvements in children's weight trajectories. A total of 731 indigent caregiver-child dyads from a multisite randomized intervention trial were examined. Observational assessment of parenting and mealtime behaviors occurred from age 2-5 years. The child's body mass index (BMI) was assessed yearly from age 5-9.5 years. Path analysis with a latent growth model revealed a significant indirect effect of the FCU on the trajectory of BMI in later childhood. Improvements in caregivers' PBS in toddlerhood, which was related to the nutritional quality of the meals caregivers served to the child during the mealtime task, served as the intervening process. Furthermore, findings indicate that the FCU prevents progression to overweight and obese status amongst at-risk children. These study results add to existing evidence that has demonstrated that family-based interventions aimed at improving general family management skills are effective at preventing weight gain. Future directions are discussed.",0
https://doi.org/10.1007/s11336-012-9257-5,Application of a Multidimensional Nested Logit Model to Multiple-Choice Test Items,"Nested logit models have been presented as an alternative to multinomial logistic models for multiple-choice test items (Suh and Bolt in Psychometrika 75:454-473, 2010) and possess a mathematical structure that naturally lends itself to evaluating the incremental information provided by attending to distractor selection in scoring. One potential concern in attending to distractors is the possibility that distractor selection reflects a different trait/ability than that underlying the correct response. This paper illustrates a multidimensional extension of a nested logit item response model that can be used to evaluate such distinctions and also defines a new framework for incorporating collateral information from distractor selection when differences exist. The approach is demonstrated in application to questions faced by a university testing center over whether to incorporate distractor selection into the scoring of its multiple-choice tests. Several empirical examples are presented. Â© 2012 The Psychometric Society.",0
https://doi.org/10.1177/0272989x09341752,Bayesian Hierarchical Models for Cost-Effectiveness Analyses that Use Data from Cluster Randomized Trials,"Cost-effectiveness analyses (CEA) may be undertaken alongside cluster randomized trials (CRTs) where randomization is at the level of the cluster (for example, the hospital or primary care provider) rather than the individual. Costs (and outcomes) within clusters may be correlated so that the assumption made by standard bivariate regression models, that observations are independent, is incorrect. This study develops a flexible modeling framework to acknowledge the clustering in CEA that use CRTs. The authors extend previous Bayesian bivariate models for CEA of multicenter trials to recognize the specific form of clustering in CRTs. They develop new Bayesian hierarchical models (BHMs) that allow mean costs and outcomes, and also variances, to differ across clusters. They illustrate how each model can be applied using data from a large (1732 cases, 70 primary care providers) CRT evaluating alternative interventions for reducing postnatal depression. The analyses compare cost-effectiveness estimates from BHMs with standard bivariate regression models that ignore the data hierarchy. The BHMs show high levels of cost heterogeneity across clusters (intracluster correlation coefficient, 0.17). Compared with standard regression models, the BHMs yield substantially increased uncertainty surrounding the cost-effectiveness estimates, and altered point estimates. The authors conclude that ignoring clustering can lead to incorrect inferences. The BHMs that they present offer a flexible modeling framework that can be applied more generally to CEA that use CRTs.",0
https://doi.org/10.1080/01650250444000414,Gender effects in peer nominations for aggression and social status,"Little prior research has examined children’s interpersonal perceptions of peers from a social relations model framework. This study examines the degree of actor and partner variances, as well as generalised and dyadic reciprocities, in a sample of 351 sixth graders’ peer nominations of different forms and functions of aggression and aspects of social status. Gender differences in these nominations are also explored. Results indicate significant actor and partner variances for all measures, and generalised reciprocity in social status perceptions. Clear gender differences were noted in rates of nominations, such that more same-sex than cross-sex nominations were generally given for both positive and negative aspects; however, we found mixed evidence of gender differences in the variance partitioning and reciprocity correlation estimates.",0
https://doi.org/10.1191/1471082x06st119oa,A Bayesian approach to inequality constrained linear mixed models: estimation and model selection,Constrained parameter problems arise in a wide variety of applications. This article deals with estimation and model selection in linear mixed models with inequality constraints on the parameters. It is shown that different theories can be translated into statistical models by putting constraints on the model parameters yielding a set of competing models. A new approach based on the principle of encompassing priors is proposed and used to compute Bayes factors and subsequently posterior model probabilities. Model selection is based on posterior model probabilities. The approach is illustrated using a longitudinal data set.,0
https://doi.org/10.1002/sim.3848,Meta-regression with partial information on summary trial or patient characteristics,"We present a model for meta-regression in the presence of missing information on some of the study level covariates, obtaining inferences using Bayesian methods. In practice, when confronted with missing covariate data in a meta-regression, it is common to carry out a complete case or available case analysis. We propose to use the full observed data, modelling the joint density as a factorization of a meta-regression model and a conditional factorization of the density for the covariates. With the inclusion of several covariates, inter-relations between these covariates are modelled. Under this joint likelihood-based approach, it is shown that the lesser assumption of the covariates being Missing At Random is imposed, instead of the more usual Missing Completely At Random (MCAR) assumption. The model is easily programmable in WinBUGS, and we examine, through the analysis of two real data sets, sensitivity and robustness of results to the MCAR assumption.",0
https://doi.org/10.1016/j.bbr.2011.08.048,Post-conditioning experience with acute or chronic inflammatory pain reduces contextual fear conditioning in the rat,"There is evidence that pain can impact cognitive function in people. The present study evaluated whether Pavlovian fear conditioning in rats would be reduced if conditioning were followed by persistent inflammatory pain induced by a subcutaneous injection of dilute formalin or complete Freund's adjuvant (CFA) on the dorsal lumbar surface of the back. Formalin-induced pain specifically impaired contextual fear conditioning but not auditory cue conditioning (Experiment 1A). Moreover, formalin pain only impaired contextual fear conditioning if it was initiated within 1h of conditioning and did not have a significant effect if initiated 2, 8 or 32 h after (Experiments 1A and 1B). Experiment 2 showed that formalin pain initiated after a session of context pre-exposure reduced the ability of that pre-exposure to facilitate contextual fear when the rat was limited to a brief exposure to the context during conditioning. Similar impairments in context- but not CS-fear conditioning were also observed if the rats received an immediate post-conditioning injection with CFA (Experiment 3). Finally, we confirmed that formalin and CFA injected s.c. on the back induced pain-indicative behaviours, hyperalgesia and allodynia with a similar timecourse to intraplantar injections (Experiment 4). These results suggest that persistent pain impairs learning in a hippocampus-dependent task, and may disrupt processes that encode experiences into long-term memory.",0
https://doi.org/10.2165/11539870-000000000-00000,Comparing Methods of Data Synthesis,"Background: Cost-effectiveness models should always be amendable to updating once new data on important model parameters become available. However, several methods of synthesizing data exist and the choice of method may affect the cost-effectiveness estimates. Objectives: To investigate the impact of the different methods of metaanalysis on final estimates of cost effectiveness from a probabilistic Markov model for chronic obstructive pulmonary disease (COPD). Methods: We compared four different methods to synthesize data for the parameters of a cost-effectiveness model for COPD: frequentist and Bayesian fixed-effects (FE) and random-effects (RE) meta-analyses. These methods were applied to obtain new transition probabilities between stable disease states and new event probabilities. Results: The four methods resulted in different estimates of probabilities and their standard errors (SE). The effects of using different synthesis techniques were most prominent in the estimation of the SEs. We found up to 9-fold differences in SEs of the exacerbation probabilities and up to almost 3-fold differences in SEs of the transition probabilities. We found that the frequentist FE model produced the lowest SEs, whereas the Bayesian RE model produced the highest for all parameters. The estimates of differences between treatments in total costs, QALYs and cost-effectiveness acceptability curves (CEAC) also varied depending on the synthesis method. The CEAC was 15% lower with a Bayesian RE model than with any of the other models. Conclusions: Health economic modellers should be aware that the choice of synthesis technique can affect resulting model parameters considerably, which can in turn affect estimates of cost effectiveness and the uncertainty around them. Â© 2011 Adis Data Information BV. All rights reserved.",0
https://doi.org/10.1037/10099-006,Interindividual differences in intraindividual change.,,0
https://doi.org/10.1016/j.pain.2012.02.015,"Intrathecal cannabilactone CB2R agonist, AM1710, controls pathological pain and restores basal cytokine levels","Spinal glial and proinflammatory cytokine actions are strongly implicated in pathological pain. Spinal administration of the anti-inflammatory cytokine interleukin (IL)-10 abolishes pathological pain and suppresses proinflammatory IL-1β and tumor necrosis factor alpha (TNF-α). Drugs that bind the cannabinoid type-2 receptor (CB(2)R) expressed on spinal glia reduce mechanical hypersensitivity. To better understand the CB(2)R-related anti-inflammatory profile of key anatomical nociceptive regions, we assessed mechanical hypersensitivity and protein profiles following intrathecal application of the cannabilactone CB(2)R agonist, AM1710, in 2 animal models; unilateral sciatic nerve chronic constriction injury (CCI), and spinal application of human immunodeficiency virus-1 glycoprotein 120 (gp120), a model of peri-spinal immune activation. In CCI animals, lumbar dorsal spinal cord and corresponding dorsal root ganglia (DRG) were evaluated by immunohistochemistry for expression of IL-10, IL-1β, phosphorylated p38-mitogen-activated-kinase (p-p38MAPK), a pathway associated with proinflammatory cytokine production, glial cell markers, and degradative endocannabinoid enzymes, including monoacylglycerol lipase (MAGL). AM1710 reversed bilateral mechanical hypersensitivity. CCI revealed decreased IL-10 expression in dorsal spinal cord and DRG, while AM1710 resulted in increased IL-10, comparable to controls. Adjacent DRG and spinal sections revealed increased IL-1β, p-p38MAPK, glial markers, and/or MAGL expression, while AM1710 suppressed all but spinal p-p38MAPK and microglial activation. In spinal gp120 animals, AM1710 prevented bilateral mechanical hypersensitivity. For comparison to immunohistochemistry, IL-1β and TNF-α protein quantification from lumbar spinal and DRG homogenates was determined, and revealed increased DRG IL-1β protein levels from gp120, that was robustly prevented by AM1710 pretreatment. Cannabilactone CB(2)R agonists are emerging as anti-inflammatory agents with pain therapeutic implications.",0
https://doi.org/10.1198/106186006x137047,Optimal Full Matching and Related Designs via Network Flows,"In the matched analysis of an observational study, confounding on covariates X is addressed by comparing members of a distinguished group (Z = 1) to controls (Z = 0) only when they belong to the same matched set. The better matchings, therefore, are those whose matched sets exhibit both dispersion in Z and uniformity in X. For dispersion in Z, pair matching is best, creating matched sets that are equally balanced between the groups; but actual data place limits, often severe limits, on matched pairs' uniformity in X. At the other extreme is full matching, the matched sets of which are as uniform in X as can be, while often so poorly dispersed in Z as to sacrifice efficiency.This article presents an algorithm for exploring the intermediate territory. Given requirements on matched sets' uniformity in X and dispersion in Z, the algorithm first decides the requirements' feasibility. In feasible cases, it furnishes a match that is optimal for X-uniformity among matches with Z-dispersion as stipulated. To illus...",0
https://doi.org/10.1093/oxfordhb/9780195394399.013.0012,Social Support and Immunity,"Abstract Social support has been reliably related to lower rates of morbidity and mortality across a number of diseases. However, little is known about the more specific pathways and mechanisms responsible for such links. In this chapter, we argue that part of the link between social support and health is explained by immune-system alternations that, in turn, influence broad-based disease outcomes. Recent studies suggest that social support is related to lower IL-6 and better immune function in biologically relevant contexts (e.g., vaccinations, cancer patients). The implications of these findings are discussed in light of a broad model hypothesizing that social support may influence health outcomes via behavioral (e.g., health behaviors), psychological (e.g., stress appraisals), and neuroendocrine-immune mechanisms. Important future research areas are also emphasized, especially the need to uncover the psychological pathways by which social support may be health-promoting.",0
https://doi.org/10.1002/sim.2393,Bayesian implementation of a genetic model-free approach to the meta-analysis of genetic association studies,"A genetic model-free method for the meta-analysis of genetic association studies is described that estimates the mode of inheritance from the data rather than assuming that it is known. For a bi-allelic polymorphism, with G as risk allele and g as wild-type, the genetic model depends on the ratio of the two log odds ratios, lambda = log OR(Gg)/log OR(GG), where OR(GG) compares GG with gg and OR(Gg) compares Gg with gg. Modelling log OR(GG) as a random effect creates a hierarchical model that can be implemented within a Bayesian framework. In Bayesian modelling, vague prior distributions have to be specified for all unknown parameters when no external information is available. When the data are sparse even supposedly vague prior distributions may have an influence on the posterior estimates. We investigate the impact of different vague prior distributions for the between-study standard deviation of log OR(GG) and for lambda, by considering three published meta-analyses and associated simulations. Our results show that depending on the characteristics of the meta-analysis the results may indeed be sensitive to the choice of vague prior distribution for either parameter. Genetic association studies usually use a case-control design that should be analysed by the corresponding retrospective likelihood. However, under some circumstances the prospective likelihood has been shown to produce identical results and it is usually preferred for its simplicity. In our meta-analyses the two likelihoods give very similar results.",0
https://doi.org/10.1111/j.1600-0706.2013.00946.x,"Plant damage and herbivore performance change with latitude for two old-field plant species, but rarely as predicted","A long standing hypothesis in biogeography is that latitudinal gradients in plant defenses (LGPD) should arise because selection for plant defenses is greater in the tropics compared to temperate areas. Previous studies have focused on plant traits thought to confer resistance, yet many traits may not actually confer resistance (putative resistance) or interact to influence herbivore performance. In this study, I used a multi-trophic approach to examine relationships between latitude, herbivore pressure, and plant resistance (measured as the growth rates of herbivores) of two old-field plant species (Solanum carolinense and Solidago altissima) using a field survey across a 12 degree gradient in the eastern US combined with laboratory bioassays measuring the performance of generalist and specialist herbivores. I used structural equation modeling to examine the direct and indirect pathways by which latitude influences herbivore pressure and plant resistance. A latitudinal gradient in plant damage was observed in the expected direction for S. caroliense (damage decreased with latitude), but the opposite relationship was observed for S. altissima. Damage to both plant species was mediated by herbivore abundances, which was in turn influenced by predator abundances. Resistance to herbivores also varied with latitude but the form of the relationship was dependent on herbivore and plant species. There were direct, non-linear relationships between latitude and resistance (for Spodoptera exigua and Schistocerca americana feeding on S. altissima; S. exigua and Manduca sexta feeding on S. carolinense). Herbivore growth rates were also mediated by the density of S. carolinense for Leptinotarsa juncta and S. americana feeding on S. carolinense. There was no relationship between plant resistance and herbivore pressure and no indication of feedbacks. Results from this study indicate that latitudinal variation in plant resistance is complex, possibly constrained by resource availability and tradeoffs in plant defenses.",0
https://doi.org/10.1007/s11135-014-0058-z,Simulation studies of structural equation models with covariates in a redundancy analysis framework,"A recent approach to structural equation modelling based on so-called extended redundancy analysis (ERA) has been proposed in the literature, enhanced with the added characteristic of generalizing redundancy analysis (GRA) models for more than two blocks. In this approach, the relationships between the observed exogenous variables and the observed endogenous variables are moderated by the presence of unobservable composites, estimated as linear combinations of exogenous variables. However, in presence of direct effects linking exogenous and endogenous variables, the composite scores are estimated by ignoring the presence of the specified direct effects. In this paper, we generalize the ERA, extending this new class of models to allow for external covariate effects. In particular, covariates are allowed to affect endogenous indicators indirectly through the composites and/or directly. The method proposed herein is called GRA, which allows us to specify and fit a variety of relationships among composites and endogenous variables. In the paper we propose two simulation studies aimed to illustrate the advantages of GRA over ERA in terms of recovery of the original underlying structure of the data in small samples. Moreover, other than the proposal of this new methodology, a second aspect of originality of this paper is that, to our knowledge, no existing empirical research addresses the behaviour of ERA with external covariate effect in simulation studies. Â© 2014, Springer Science+Business Media Dordrecht.",0
https://doi.org/10.1080/00949659208811388,On some small sample properties of generalized estimating equationEstimates for multivariate dichotomous outcomes,"(1992). On some small sample properties of generalized estimating equationEstimates for multivariate dichotomous outcomes. Journal of Statistical Computation and Simulation: Vol. 41, No. 1-2, pp. 19-29.",0
,Imposing structure on an unstructured environment : Ontogenetic changes in the ability to form rules of behavior under conditions of low environmental predictability,,0
https://doi.org/10.1177/1094428107300344,Tests of the Three-Path Mediated Effect,"In a three-path mediational model, two mediators intervene in a series between an independent and a dependent variable. Methods of testing for mediation in such a model are generalized from the more often used single-mediator model. Six such methods are introduced and compared in a Monte Carlo study in terms of their Type I error, power, and coverage. Based on its results, the joint significance test is preferred when only a hypothesis test is of interest. The percentile bootstrap and bias-corrected bootstrap are preferred when a confidence interval on the mediated effect is desired, with the latter having more power but also slightly inflated Type I error in some conditions.",0
https://doi.org/10.1080/00273171.2017.1342203,Bayesian Estimation for Item Factor Analysis Models with Sparse Categorical Indicators,"Psychometric models for item-level data are broadly useful in psychology. A recurring issue for estimating item factor analysis (IFA) models is low-item endorsement (item sparseness), due to limited sample sizes or extreme items such as rare symptoms or behaviors. In this paper, I demonstrate that under conditions characterized by sparseness, currently available estimation methods, including maximum likelihood (ML), are likely to fail to converge or lead to extreme estimates and low empirical power. Bayesian estimation incorporating prior information is a promising alternative to ML estimation for IFA models with item sparseness. In this article, I use a simulation study to demonstrate that Bayesian estimation incorporating general prior information improves parameter estimate stability, overall variability in estimates, and power for IFA models with sparse, categorical indicators. Importantly, the priors proposed here can be generally applied to many research contexts in psychology, and they do not impact results compared to ML when indicators are not sparse. I then apply this method to examine the relationship between suicide ideation and insomnia in a sample of first-year college students. This provides an important alternative for researchers who may need to model items with sparse endorsement.",0
https://doi.org/10.1037/a0026802,Bayesian structural equation modeling: A more flexible representation of substantive theory.,"This article proposes a new approach to factor analysis and structural equation modeling using Bayesian analysis. The new approach replaces parameter specifications of exact zeros with approximate zeros based on informative, small-variance priors. It is argued that this produces an analysis that better reflects substantive theories. The proposed Bayesian approach is particularly beneficial in applications where parameters are added to a conventional model such that a nonidentified model is obtained if maximum-likelihood estimation is applied. This approach is useful for measurement aspects of latent variable modeling, such as with confirmatory factor analysis, and the measurement part of structural equation modeling. Two application areas are studied, cross-loadings and residual correlations in confirmatory factor analysis. An example using a full structural equation model is also presented, showing an efficient way to find model misspecification. The approach encompasses 3 elements: model testing using posterior predictive checking, model estimation, and model modification. Monte Carlo simulations and real data are analyzed using Mplus. The real-data analyses use data from Holzinger and Swineford's (1939) classic mental abilities study, Big Five personality factor data from a British survey, and science achievement data from the National Educational Longitudinal Study of 1988.",0
https://doi.org/10.1037/a0014877,Evaluating cognitive theory: A joint modeling approach using responses and response times.,"In current psychological research, the analysis of data from computer-based assessments or experiments is often confined to accuracy scores. Response times, although being an important source of additional information, are either neglected or analyzed separately. In this article, a new model is developed that allows the simultaneous analysis of accuracy scores and response times of cognitive tests with a rule-based design. The model is capable of simultaneously estimating ability and speed on the person side as well as difficulty and time intensity on the task side, thus dissociating information that is often confounded in current analysis procedures. Further, by integrating design matrices on the task side, it becomes possible to assess the effects of design parameters (e.g., cognitive processes) on both task difficulty and time intensity, offering deeper insights into the task structure. A Bayesian approach, using Markov Chain Monte Carlo methods, has been developed to estimate the model. An application of the model in the context of educational assessment is illustrated using a large-scale investigation of figural reasoning ability.",0
https://doi.org/10.1037/a0013269,A double-structure structural equation model for three-mode data.,"Structural equation models are commonly used to analyze 2-mode data sets, in which a set of objects is measured on a set of variables. The underlying structure within the object mode is evaluated using latent variables, which are measured by indicators coming from the variable mode. Additionally, when the objects are measured under different conditions, 3-mode data arise, and with this, the simultaneous study of the correlational structure of 2 modes may be of interest. In this article the authors present a model with a simultaneous latent structure for 2 of the 3 modes of such a data set. They present an empirical illustration of the method using a 3-mode data set (person by situation by response) exploring the structure of anger and irritation across different interpersonal situations as well as across persons.",0
https://doi.org/10.1111/j.2044-8317.1998.tb00679.x,Bayesian sampling-based approach for factor analysis models with continuous and polytomous data,"Factor analysis is an important technique in behavioural science research in explaining the interdependence and assessing causations and correlations of the observed variables and the latent factors. Recently, generalization of the model to handle polytomous variables has received a lot of attention. In this paper, a Bayesian approach to analysing the model with continuous and polytomous variables is developed. In the posterior analysis, the observed continuous and polytomous data are augmented with the latent factor scores and the unobserved measurements underlying the polytomous variables. Random observations from the posterior distributions are simulated via the Gibbs sampler algorithm. It is shown that the conditional distributions involved in the implementation of the algorithm are the familiar distributions, hence the simulation is rather straightforward. Joint Bayesian estimates of the unknown thresholds, structural parameters and the factor scores are produced simultaneously. The efficiency and accuracy of our approach are demonstrated by a real-life example and a simulation study.",0
https://doi.org/10.1207/s15326985ep3903_2,Beyond Individual Differences: Exploring School Effects on SAT Scores,"This article explores the complex, hierarchical relation among school characteristics, individual differences in academic achievement, extracurricular activities, and socioeconomic background on performance on the verbal and mathematics Scholastic Aptitude Test (SAT). Using multilevel structural equation models (SEMs) with latent means, we analyzed data from a national sample of college-bound high school students. A nested series of SEMs were fit simultaneously to eight subgroups (disaggregated by both gender and ethnicity) of high school students. Our analyses suggest that (a) multilevel SEMs provide a reasonably good fit to the data, (b) family background influences SAT scores directly and indirectly, learning opportunities in and outside the school curriculum are related to SAT performance, and (c) the characteristics of the schools matter when it comes to performance on the SAT. We argue that context matters and that researchers ought to move beyond analyses of individual differences when attempting t...",0
https://doi.org/10.1016/j.electstud.2010.04.009,Stumbling block or stepping stone? The influence of direct democracy on individual participation in parliamentary elections,"This paper evaluates whether direct democracy supplements or undermines traditional representative democracy. While a first approach assumes that a culture of active direct democracy stimulates citizens’ political interest and ultimately bolsters participation in parliamentary elections, a competing hypothesis proposes a negative relationship between the frequency of ballot measures and electoral participation due to voter fatigue and decreased significance of elections. Our multilevel analysis of the 26 Swiss cantons challenges recent studies conducted for the U.S. states: In the Swiss context, where direct democracy is more important in the political process than the less salient parliamentary elections, greater use of direct democratic procedures is associated with a lower individual probability to participate in elections. Furthermore, by distinguishing between short and long-term effects of direct democracy, we show that the relationship observed is of a long-term nature and can therefore be seen as a result of adaptive learning processes rather than of instantaneous voter fatigue.",0
https://doi.org/10.1177/0049124187016001006,Direct and Indirect Effects in Linear Structural Equation Models,"This article discusses total indirect effects in linear structural equation models. First, I define these effects. Second, I show how the delta method may be used to obtain the standard errors of the sample estimates of these effects and test hypotheses about the magnitudes of the indirect effects. To keep matters simple, I focus throughout on a particularly simple linear structural equation system; for a treatment of the general case, see Sobel (1986). To illustrate the ideas and results, a detailed example is presented.",0
,Statistical Inference in Factor Analysis,"In this paper we discuss some methods of factor analysis. The entire discussion is centered around one general probability model. We consider some mathematical problems of the model, such as whether certain kinds of observed data determine the model uniquely. We treat the statistical problems of estimation and tests of certain hypotheses. For these purposes the asymptotic distribution theory of some statistics is treated. The primary aim of this paper is to give a unified exposition of this part of factor analysis from the viewpoint of the mathematical statistician. The literature on factor analysis is scattered; moreover, the many papers and books have been written from many different points of view. By confining ourselves to one model and by emphasizing statistical inferences for this model we hope to present a clear picture to the statistician. The development given here is expected to point up features of model-building and statistical inference that occur in other areas where statistical theories are being developed. For example, nearly all of the problems met in factor analysis are met in latent structure analysis. There are also some new results given in this paper. The proofs of these are mainly given in a technical Part II of the paper. In confining.ourselves to the mathematical and statistical aspects of one model, we are leaving out of consideration many important and interesting topics. We shall not consider how useful this model may be nor in what substantive areas one may expect to find data (and problems) that fit the model. We also do not consider methods based on other models. In doing this, we do not mean to imply that the model considered here is the most useful or important. It seems that this model has some usefulness and importance, it has been studied considerably, and one can give a fairly unified exposition of it. Extensive discussion of the purposes and applications (as well as other developments) of factor analysis is given in books by psychologists (for example, Holzinger and Harmon [10], Thomson [23], Thurstone [24]). Some general discussion of statistical inference has been given in papers by Bartlett [9] and Kendall [12].",0
https://doi.org/10.1177/0146621613517165,Expediting Clinical and Translational Research via Bayesian Instrument Development,"Developing valid and reliable instruments is crucial, but costly and time-consuming in health care research and evaluation. The Food and Drug Administration (FDA) and the National Institutes of Health (NIH) have set up guidelines for developing patient-reported outcome (PRO) instruments. However, the guidelines are not applicable to cases of small sample sizes. Instead of using an exact estimation procedure to examine psychometric properties, the Bayesian Instrument Development (BID) method integrates expert data and participant data into a single seamless analysis. Using a novel set of priors, simulated data were used to compare BID to classical instrument development procedures and test the stability of BID. To display BID to non-statisticians, a graphical user interface (GUI) based on R and WINBUGS is developed and demonstrated with data on a small sample of heart failure patients. Costs were saved by eliminating the need for unnecessary continuation of data collection for larger samples as required by the classical instrument development approach.",0
https://doi.org/10.1207/s15324818ame1904_3,Simultaneous Use of Multiple Answer Copying Indexes to Improve Detection Rates,"Many of the currently available statistical indexes to detect answer copying lack sufficient power at small α levels or when the amount of copying is relatively small. Furthermore, there is no one index that is uniformly best. Depending on the type or amount of copying, certain indexes are better than others. The purpose of this article was to explore the utility of simultaneously using multiple copying indexes to detect different types and amounts of answer copying. This study compared eight copying indexes: S1 and S2 (Sotaridona & Meijer, 2003), K¯ 2 (Sotaridona & Meijer, 2002), ω (Wollack, 1997),B and H (Angoff, 1974), and new indexes Runs and MaxStrings, plus all possible pairs and triplets of the 8 indexes using multiple comparison procedures (Dunn, 1961) to adjust the critical α level for each index in a pair or triplet. Empirical Type-I error rates and power of all indexes, pairs, and triplets were examined in a real data simulation (i.e., where actual examinee responses to items [rather than gener...",0
https://doi.org/10.1111/insr.12067,On Some Principles of Statistical Inference,"Summary  Statistical theory aims to provide a foundation for studying the collection and interpretation of data, a foundation that does not depend on the particular details of the substantive field in which the data are being considered. This gives a systematic way to approach new problems, and a common language for summarising results; ideally, the foundations and common language ensure that statistical aspects of one study, or of several studies on closely related phenomena, can be broadly accessible. We discuss some principles of statistical inference, to outline how these are, or could be, used to inform the interpretation of results, and to provide a greater degree of coherence for the foundations of statistics.",0
https://doi.org/10.1198/jasa.2011.ap09653,A Hierarchical Model for Quantifying Forest Variables Over Large Heterogeneous Landscapes With Uncertain Forest Areas,"We are interested in predicting one or more continuous forest variables (e.g., biomass, volume, age) at a fine resolution (e.g., pixel level) across a specified domain. Given a definition of forest/nonforest, this prediction is typically a two-step process. The first step predicts which locations are forested. The second step predicts the value of the variable for only those forested locations. Rarely is the forest/nonforest status predicted without error. However, the uncertainty in this prediction is typically not propagated through to the subsequent prediction of the forest variable of interest. Failure to acknowledge this error can result in biased estimates of forest variable totals within a domain. In response to this problem, we offer a modeling framework that will allow propagation of this uncertainty. Here we envision two latent processes generating the data. The first is a continuous spatial process while the second is a binary spatial process. The continuous spatial process controls the spatial association structure of the forest variable of interest, while the binary process indicates presence of a possible nonzero value for the forest variable at a given location. The proposed models are applied to georeferenced National Forest Inventory (NFI) data and spatially coinciding remotely sensed predictor variables. Due to the large number of observed locations in this dataset we seek dimension reduction not just in the likelihood, but also for unobserved stochastic processes. We demonstrate how a low-rank predictive process can be adapted to our setting and reduce the dimensionality of the data and ease the computational burden.",0
https://doi.org/10.1177/0146621608329891,Addressing Score Bias and Differential Item Functioning Due to Individual Differences in Response Style,"A multidimensional item response theory model that accounts for response style factors is presented. The model, a multidimensional extension of Bock's nominal response model, is shown to allow for the study and control of response style effects in ordered rating scale data so as to reduce bias in measurement of the intended trait. In the current application, the model is also used to investigate response style as an underlying cause of differential item functioning. The approach is illustrated using the item responses of cigarette smokers to the Wisconsin Inventory of Smoking Dependence Motives, a self-report measure of tobacco dependence.",0
https://doi.org/10.3102/1076998609346967,A Model for Teacher Effects From Longitudinal Data Without Assuming Vertical Scaling,"There is an increasing interest in using longitudinal measures of student achievement to estimate individual teacher effects. Current multivariate models assume each teacher has a single effect on student outcomes that persists undiminished to all future test administrations (complete persistence [CP]) or can diminish with time but remains perfectly correlated (variable persistence [VP]). However, when state assessments do not use a vertical scale or the evolution of the mix of topics present across a sequence of vertically aligned assessments changes as students advance in school, these assumptions of persistence may not be consistent with the achievement data. We develop the “generalized persistence” (GP) model, a Bayesian multivariate model for estimating teacher effects that accommodates longitudinal data that are not vertically scaled by allowing less than perfect correlation of a teacher’s effects across test administrations. We illustrate the model using mathematics assessment data.",0
https://doi.org/10.1111/j.2517-6161.1979.tb01078.x,Bayesian Inference for a Normal Dispersion Matrix and its Application to Stochastic Multiple Regression Analysis,"SUMMARY This paper discusses Bayesian inference procedures for a normal dispersion matrix. Structural information for the prior mean of the dispersion matrix is incorporated into the analysis through a Normal-Wishart prior distribution. Many of the resulting Bayes estimates are invariant, consistent and asymptotically efficient. Using this procedure, a coherent Bayesian argument for stochastic multiple regression analysis is developed, where dependent and independent variables are jointly random, without experimental control. It is shown that ordinary least squares, a general form of ridge regression, and factor analysis regression methods can be represented as special cases within this general framework. The preliminary report of a simulation study indicates that the Bayesian regression techniques demonstrate substantial improvements over least squares, even using frequentist criteria. Finally, the related problem of joint estimation of the normal population mean and dispersion matrix is briefly discussed in Section 5. The resulting Bayes estimate for the population mean is shown to be closely related to Stein estimates.",0
https://doi.org/10.1111/1467-8624.00571,A Longitudinal Study of Mathematical Competencies in Children With Specific Mathematics Difficulties Versus Children With Comorbid Mathematics and Reading Difficulties,"Mathematical competencies of 180 children were examined at 4 points between 2nd and 3rd grades (age range between 7 and 9 years). Children were initially classified into one of 4 groups: math difficulties but normal reading (MD only), math and reading difficulties (MD-RD), reading difficulties but normal math (RD only), and normal achievement in math and reading (NA). The groups did not differ significantly in rate of development. However, at the end of 3rd grade the MD only group performed better than the MD-RD group in problem solving but not in calculation. The NA and RD only groups performed better than the MD-RD group in most areas. Deficiencies in fact mastery and calculation fluency, in particular, are defining features of MD, with or without RD.",0
https://doi.org/10.1002/gepi.21960,Bayesian Variable Selection in Multilevel Item Response Theory Models with Application in Genomics,"The goal of this paper is to present an implementation of stochastic search variable selection (SSVS) to multilevel model from item response theory (IRT). As experimental settings get more complex and models are required to integrate multiple (and sometimes massive) sources of information, a model that can jointly summarize and select the most relevant characteristics can provide better interpretation and a deeper insight into the problem. A multilevel IRT model recently proposed in the literature for modeling multifactorial diseases is extended to perform variable selection in the presence of thousands of covariates using SSVS. We derive conditional distributions required for such a task as well as an acceptance-rejection step that allows for the SSVS in high dimensional settings using a Markov Chain Monte Carlo algorithm. We validate the variable selection procedure through simulation studies, and illustrate its application on a study with genetic markers associated with the metabolic syndrome.",0
https://doi.org/10.1177/0146621602026001007,A General Bayesian Model for Testlets: Theory and Applications,"The need for more realistic and richer forms of assessment in educational tests has led to the inclusion (in many tests) of polytomously scored items, multiple items based on a single stimulus (a “testlet”), and the increased use of a generalized mixture of binary and polytomous item formats. In this paper, the authors extend earlier work on the modeling of testlet-based response data to include the situation in which a test is composed, partially or completely, of polytomously scored items and/or testlets. The model they propose, a modified version of commonly employed item response models, is embedded within a fully Bayesian framework, and inferences under the model are obtained using Markov chain Monte Carlo techniques. The authors demonstrate its use within a designed series of simulations and by analyzing operational data from the North Carolina Test of Computer Skills and the Educational Testing Service’s Test of Spoken English. Their empirical findings suggest that the North Carolina Test of Computer Skills exhibits significant testlet effects, indicating significant dependence of item scores obtained from common stimuli, whereas the Test of Spoken English does not.",0
https://doi.org/10.1198/000313006x90305,A Spatial Analysis of Basketball Shot Chart Data,"Basketball coaches at all levels use shot charts to study shot locations and outcomes for their own teams as well as upcoming opponents. Shot charts are simple plots of the location and result of each shot taken during a game. Although shot chart data are rapidly increasing in richness and availability, most coaches still use them purely as descriptive summaries. However, a team's ability to defend a certain player could potentially be improved by using shot data to make inferences about the player's tendencies and abilities. This article develops hierarchical spatial models for shot-chart data, which allow for spatially varying effects of covariates. Our spatial models permit differential smoothing of the fitted surface in two spatial directions, which naturally correspond to polar coordinates: distance to the basket and angle from the line connecting the two baskets. We illustrate our approach using the 2003–2004 shot chart data for Minnesota Timberwolves guard Sam Cassell.",0
https://doi.org/10.1198/106186006x160681,A New Algorithm for Simulating a Correlation Matrix Based on Parameter Expansion and Reparameterization,"The correlation matrix (denoted by R) plays an important role in many statistical models. Unfortunately, sampling the correlation matrix in Markov chain Monte Carlo (MCMC) algorithms can be problematic. In addition to the positive definite constraint of covariance matrices, correlation matrices have diagonal elements fixed at one. In this article, we propose an efficient two-stage parameter expanded reparameterization and Metropolis-Hastings (PX-RPMH) algorithm for simulating R. Using this algorithm, we draw all elements of R simultaneously by first drawing a covariance matrix from an inverse Wishart distribution, and then translating it back to a correlation matrix through a reduction function and accepting it based on a Metropolis-Hastings acceptance probability. This algorithm is illustrated using multivariate probit (MVP) models and multivariate regression (MVR) models with a common correlation matrix across groups. Via both a simulation study and a real data example, the performance of the PX-RPMH al...",0
https://doi.org/10.1037/h0037350,Estimating causal effects of treatments in randomized and nonrandomized studies.,"A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. Recent psychological and educational literature has included extensive criticism of the use of nonrandomized studies to estimate causal effects of treatments (e.g., Campbell & Erlebacher, 1970). The implication in much of this literature is that only properly randomized experiments can lead to useful estimates of causal effects. If taken as applying to all fields of study, this position is untenable. Since the extensive use of randomized experiments is limited to the last half century,8 and in fact is not used in much scientific investigation today,4 one is led to the conclusion that most scientific truths have been established without using randomized experiments. In addition, most of us successfully determine the causal effects of many of our everyday actions, even interpersonal behaviors, without the benefit of randomization. Even if the position that causal effects of treatments can only be well established from randomized experiments is taken as applying only to the social sciences in which",0
https://doi.org/10.1007/s00221-010-2286-3,Auditory temporal modulation of the visual Ternus effect: the influence of time interval,"Research on multisensory interactions has shown that the perceived timing of a visual event can be captured by a temporally proximal sound. This effect has been termed 'temporal ventriloquism effect.' Using the Ternus display, we systematically investigated how auditory configurations modulate the visual apparent-motion percepts. The Ternus display involves a multielement stimulus that can induce either of two different percepts of apparent motion: 'element motion' or 'group motion'. We found that two sounds presented in temporal proximity to, or synchronously with, the two visual frames, respectively, can shift the transitional threshold for visual apparent motion (Experiments 1 and 3). However, such effects were not evident with single-sound configurations (Experiment 2). A further experiment (Experiment 4) provided evidence that time interval information is an important factor for crossmodal interaction of audiovisual Ternus effect. The auditory interval was perceived as longer than the same physical visual interval in the sub-second range. Furthermore, the perceived audiovisual interval could be predicted by optimal integration of the visual and auditory intervals. Â© 2010 Springer-Verlag.",0
https://doi.org/10.1016/j.bbi.2006.10.012,Intrathecal interleukin-10 gene therapy attenuates paclitaxel-induced mechanical allodynia and proinflammatory cytokine expression in dorsal root ganglia in rats,"Paclitaxel is a commonly used cancer chemotherapy drug that frequently causes painful peripheral neuropathies. The mechanisms underlying this dose-limiting side effect are poorly understood. Growing evidence supports that proinflammatory cytokines, such as interleukin-1 (IL-1) and tumor necrosis factor (TNF), released by activated spinal glial cells and within the dorsal root ganglia (DRG) are critical in enhancing pain in various animal models of neuropathic pain. Whether these cytokines are involved in paclitaxel-induced neuropathy is unknown. Here, using a rat neuropathic pain model induced by repeated systemic paclitaxel injections, we examined whether paclitaxel upregulates proinflammatory cytokine gene expression, and whether these changes and paclitaxel-induced mechanical allodynia can be attenuated by intrathecal IL-1 receptor antagonist (IL-1ra) or intrathecal delivery of plasmid DNA encoding the anti-inflammatory cytokine, interleukin-10 (IL-10). The data show that paclitaxel treatment induces mRNA expression of IL-1, TNF, and immune cell markers in lumbar DRG. Intrathecal IL-1ra reversed paclitaxel-induced allodynia and intrathecal IL-10 gene therapy both prevented, and progressively reversed, this allodynic state. Moreover, IL-10 gene therapy resulted in increased IL-10 mRNA levels in lumbar DRG and meninges, measured 2 weeks after initiation of therapy, whereas paclitaxel-induced expression of IL-1, TNF, and CD11b mRNA in lumbar DRG was markedly decreased. Taken together, these data support that paclitaxel-induced neuropathic pain is mediated by proinflammatory cytokines, possibly released by activated immune cells in the DRG. We propose that targeting the production of proinflammatory cytokines by intrathecal IL-10 gene therapy may be a promising therapeutic strategy for the relief of paclitaxel-induced neuropathic pain.",0
https://doi.org/10.1016/j.jmva.2009.05.001,Normal distribution based pseudo ML for missing data: With applications to mean and covariance structure analysis,"When missing data are either missing completely at random (MCAR) or missing at random (MAR), the maximum likelihood (ML) estimation procedure preserves many of its properties. However, in any statistical modeling, the distribution specification for the likelihood function is at best only an approximation to the real world. In particular, since the normal-distribution-based ML is typically applied to data with heterogeneous marginal skewness and kurtosis, it is necessary to know whether such a practice still generates consistent parameter estimates. When the manifest variables are linear combinations of independent random components and missing data are MAR, this paper shows that the normal-distribution-based MLE is consistent regardless of the distribution of the sample. Examples also show that the consistency of the MLE is not guaranteed for all nonnormally distributed samples. When the population follows a confirmatory factor model, and data are missing due to the magnitude of the factors, the MLE may not be consistent even when data are normally distributed. When data are missing due to the magnitude of measurement errors/uniqueness, MLEs for many of the covariance parameters related to the missing variables are still consistent. This paper also identifies and discusses the factors that affect the asymptotic biases of the MLE when data are not missing at random. In addition, the paper also shows that, under certain data models and MAR mechanism, the MLE is asymptotically normally distributed and the asymptotic covariance matrix is consistently estimated by the commonly used sandwich-type covariance matrix. The results indicate that certain formulas and/or conclusions in the existing literature may not be entirely correct.",0
https://doi.org/10.1093/biomet/57.1.97,Monte Carlo sampling methods using Markov chains and their applications,"SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed. For numerical problems in a large number of dimensions, Monte Carlo methods are often more efficient than conventional numerical methods. However, implementation of the Monte Carlo methods requires sampling from high dimensional probability distributions and this may be very difficult and expensive in analysis and computer time. General methods for sampling from, or estimating expectations with respect to, such distributions are as follows. (i) If possible, factorize the distribution into the product of one-dimensional conditional distributions from which samples may be obtained. (ii) Use importance sampling, which may also be used for variance reduction. That is, in order to evaluate the integral J = X) p(x)dx = Ev(f), where p(x) is a probability density function, instead of obtaining independent samples XI, ..., Xv from p(x) and using the estimate J, = Zf(xi)/N, we instead obtain the sample from a distribution with density q(x) and use the estimate J2 = Y{f(xj)p(x1)}/{q(xj)N}. This may be advantageous if it is easier to sample from q(x) thanp(x), but it is a difficult method to use in a large number of dimensions, since the values of the weights w(xi) = p(x1)/q(xj) for reasonable values of N may all be extremely small, or a few may be extremely large. In estimating the probability of an event A, however, these difficulties may not be as serious since the only values of w(x) which are important are those for which x -A. Since the methods proposed by Trotter & Tukey (1956) for the estimation of conditional expectations require the use of importance sampling, the same difficulties may be encountered in their use. (iii) Use a simulation technique; that is, if it is difficult to sample directly from p(x) or if p(x) is unknown, sample from some distribution q(y) and obtain the sample x values as some function of the corresponding y values. If we want samples from the conditional dis",0
https://doi.org/10.1177/014662169201600111,Equating Tests Under the Graded Response Model,"The Stocking and Lord (1983) procedure for computing equating coefficients for tests having dichotomously scored items is extended to the case of graded response items. A system of equations for obtaining the equating coefficients under Samejima's (1969, 1972) graded response model is derived. These equations are used to compute equating coefficients in two related situations. Under the first, the equating coefficients are obtained by matching, on an examinee by examinee basis, the true scores on two tests. In the second case, the equating coefficients are obtained by matching the test characteristic curves (TCCS) of the two tests. Several examples of computing equating coefficients in these two situations are provided. The TCC matching ap proach was much less demanding computationally and yielded equating coefficients that differed little from those obtained through the true score distribution matching approach.",0
https://doi.org/10.2307/2348941,A Language and Program for Complex Bayesian Modelling,"Gibbs sampling has enormous potential for analysing complex data sets. However, routine use of Gibbs sampling has been hampered by the lack of general purpose software for its implementation. Until now all applications have involved writing one-off computer code in low or intermediate level languages such as C or Fortran. We describe some general purpose software that we are currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling). The BUGS system comprises three components: first, a natural language for specifying complex models; second, an 'expert system' for deciding appropriate methods for obtaining samples required by the Gibbs sampler; third, a sampling module containing numerical routines to perform the sampling. S objects are used for data input and output. BUGS is written in Modula-2 and runs under both DOS and UNIX.",0
https://doi.org/10.1136/bmjopen-2015-010251,Comparative safety and effectiveness of cognitive enhancers for Alzheimer's dementia: protocol for a systematic review and individual patient data network meta-analysis,"Alzheimer's dementia (AD) is the most common cause of dementia, and several organisations, such as the National Institute for Health and Care Excellence, suggest that management of patients with AD should be tailored to their needs. To date, little research has been conducted on the treatment effect in different subgroups of patients with AD. The aim of this study is to examine the comparative effectiveness and safety of cognitive enhancers for different patient characteristics.We will update our previous literature search from January 2015 forward, using the same terms and electronic databases (eg, MEDLINE) from our previous review. We will additionally search grey literature and scan the reference lists of the included studies. Randomised clinical trials of any duration conducted at any time comparing cognitive enhancers alone or in any combination against other cognitive enhancers, or placebo in adults with AD will be eligible. The outcomes of interest are cognition according to the Mini-Mental State Examination, and overall serious adverse events. For each outcome and treatment comparison, we will perform a Bayesian hierarchical random-effects meta-analysis combining the individual patient data (IPD) from each eligible study. If the identified treatment comparisons form a connected network diagram, we will perform an IPD network meta-analysis (NMA) to estimate subgroup effects for patients with different characteristics, such as AD severity and sex. We will combine aggregated data from studies that we will not be able to obtain IPD, with the IPD provided by the original authors, in a single model. We will use the PRISMA-IPD and PRISMA-NMA statements to report our findings.The findings of this study will be of interest to stakeholders, including decision makers, guideline developers, clinicians, methodologists and patients, and they will help to improve guidelines for the management of patients with AD.CRD42015023507.",0
https://doi.org/10.1007/s11336-010-9174-4,Bayesian Semiparametric Structural Equation Models with Latent Variables,"Structural equation models (SEMs) with latent variables are widely useful for sparse covariance structure modeling and for inferring relationships among latent variables. Bayesian SEMs are appealing in allowing for the incorporation of prior information and in providing exact posterior distributions of unknowns, including the latent variables. In this article, we propose a broad class of semiparametric Bayesian SEMs, which allow mixed categorical and continuous manifest variables while also allowing the latent variables to have unknown distributions. In order to include typical identifiability restrictions on the latent variable distributions, we rely on centered Dirichlet process (CDP) and CDP mixture (CDPM) models. The CDP will induce a latent class model with an unknown number of classes, while the CDPM will induce a latent trait model with unknown densities for the latent traits. A simple and efficient Markov chain Monte Carlo algorithm is developed for posterior computation, and the methods are illustrated using simulated examples, and several applications. © 2010 The Psychometric Society.",0
https://doi.org/10.1159/000360485,Multilevel Modeling in the Context of Growth Modeling,"Multilevel modeling is a flexible approach for the analysis of nested data structures, such as those encountered in longitudinal studies with repeated measures of an outcome of interest taken across time and nested within subjects. The baseline score on the outcome and rate of change vary across subjects, and subject level predictor variables may be used to explain part of the between-subject variability. This contribution shows how to formulate linear and logistic models for continuous and binary outcomes. A study of the effect of growth hormone in adolescents with short stature is used as an illustrative example to demonstrate the use of these models and to aid in the interpretation of model parameter estimates. Attention is also paid to sufficient sample sizes, and two methods to explore the relation between sample size and power of statistical tests are discussed.",0
https://doi.org/10.1016/0010-0285(72)90016-3,Subjective probability: A judgment of representativeness,"This paper explores a heuristic — representativeness — according to which the subjective probability of an event, or a sample, is determined by the degree to which it: (i) is similar in essential characteristics to its parent population; and (ii) reflects the salient features of the process by which it is generated. This heuristic is explicated in a series of empirical examples demonstrating predictable and systematic errors in the evaluation of uncertain events. In particular, since sample size does not represent any property of the population, it is expected to have little or no effect on judgment of likelihood. This prediction is confirmed in studies showing that subjective sampling distributions and posterior probability judgments are determined by the most salient characteristic of the sample (e.g., proportion, mean) without regard to the size of the sample. The present heuristic approach is contrasted with the normative (Bayesian) approach to the analysis of the judgment of uncertainty.",0
https://doi.org/10.1037/0021-9010.66.2.166,Task differences as moderators of aptitude test validity in selection: A red herring.,"This article describes results of two studies, based on a total sample size of nearly 400,000, examining the traditional belief that between-job task differences cause aptitude tests to be valid for some jobs but not for others. Results indicate that aptitude tests are valid across jobs. The moderating effect of tasks is negligible even when jobs differ grossly in task makeup and is probably nonexistent when task differences are less extreme. These results have important implications for validity generalization, for the use of task-oriented job analysis in selection research, for criterion construction, for moderator research, and for proper interpretation of the Uniform Guidelines on Employee Selection Procedures. The philosophy of science and methodological assumptions historically underlying belief in the hypothesis that tasks are important moderators of test validities are examined and critiqued. It is concluded that the belief in this hypothesis can be traced to behaviorist assumptions introduced into personnel psychology in the early 1960s and that, in retrospect, these assumptions can be seen to be false.",0
https://doi.org/10.1111/jasp.12298,On feeling good at work: the role of regulatory mode and passion in psychological adjustment,"Abstract The major postulate of this work is that regulatory modes inﬂuence the type ofpassion people experience with regard to an activity,which in turn inﬂuences theirpsychological adjustment. Integrating regulatory mode theory and the dualisticmodelof passion,wehypothesizedthatlocomotion—associatedwithintrinsicandautonomousmotivations—wouldpositivelypredictharmoniouspassion,whichinturn would enhance workers’ psychological adjustment. In contrast, we hypoth-esized that assessment—associated with extrinsic and non-autonomousmotivations—would positively predict obsessive passion, which in turn wouldreduce workers’ psychological adjustment. Two ﬁeld studies supported thesehypotheseswithpsychologicaladjustmentmeasuresofstress(Study1)andburnout(Study2)indifferentworkcontexts.SusanandClaireworkforthesameﬁnancialdepartmentofahardware supply company.At work,they assist their manag-ers in making strategic decisions and ensure that the comp-any’s operations run smoothly. Susan is a“doer,”she bustlesabout all day and works relentlessly to get the job done,whereasClaireismoredetail-orientedandalwaysensuresheisdoing“therightthing.”Despite,havingsimilarresponsibil-ities and being equally committed to their work, Susan andClaire’s work experience is much different. Susan is con-stantly full of energy and copes very well with stress.After aday’s work, she can easily let go of her responsibilities andenjoy other activities in her life. Claire, on the other hand,feels emotionally drained and constantly preoccupied withherwork,evenoutsideworkinghours.Howcanthisbe?Inthelastdecades,muchresearchhasfocusedonworker’spsychological adjustment: a billion dollar problem drainingorganizational proﬁt by increasing turnover rates (Gupta B Schaufeli & Salanova,2007). For instance, extensive work has been conductedon topics such as supervisor–subordinate relationships(Landeweerd & Boumans, 1994; Tepper, 2000) and socialsupport at work (for a review, see Viswesvaran, Sanchez, &Fisher, 1999).However,as the preceding example illustrates,one important question that remains is how can peopleexperience diametrically opposed psychological adjustmentdespite working in similar environment? The presentresearch addresses this theoretical question by investigatingthe interface between regulatory mode theory (Kruglanskiet al., 2000) and the dualistic model of passion (Vallerandet al.,2003).Inthepresentarticle,weproposethatonepivotalfactorforpsychologicaladjustmentcanbefoundinworkers’attitudetowardtheirwork.Speciﬁcally,wesuggestthatbasicself-regulatory processes such as locomotion (i.e., being a“doer”) and assessment (i.e., “doing the right thing”) canimpact individuals’ psychological adjustment by inﬂuencingthe type of passion individuals entertain for an importantactivity(e.g.,work).Inturn,thetypeofpassion(harmoniousvs.obsessive)onedevelopsforanactivitycaneitherfacilitateorpreventtheoccurrenceof stressandburnoutatwork.",0
https://doi.org/10.1037/0022-3514.42.6.1051,Perception of out-group homogeneity and levels of social categorization: Memory for the subordinate attributes of in-group and out-group members.,"Four experiments were conducted to explore the hypothesis that in-group members perceive their own group as more variegated and complex than do out-group members (the out-group homogeneity principle). The first three experiments were designed to demonstrate this effect in a symmetric manner for both parties of the in-group-out-group dichotomy, and the fourth experiment tested one particular theoretical account of this phenomenon. In Experiments 1 and 2, men and women subjects estimated the proportion of men or women who would endorse a variety of personality/attitude items. The items were constructed to vary on the dimensions of stereotypic meaning (masculinity-femininity) and social desirability (favorable-unfavorable). It was predicted and found that outgroup members viewed a group as endorsing more stereotypic and fewer counterstereotypic items than, did in-group members. These findings were interpreted as support for the out-group homogeneity principle, and it was argued that since this effect was general across items varying in social desirability, the phenomenon was independent of traditional ethnocentrism effects. Experiment 3 asked members of three campus sororities to directly judge the degree of intragroup similarity for their own group and two other groups. Again, each group judged its own members to be more dissimilar to one another than did out-group judges. In Experiment 4 a theory was proposed suggesting that different levels of social categorization are used to encode in-group and out-group members' behavior and that this process could account for the perception of out-group homogeneity. It was predicted and found that men and women were more likely to remember the subordinate attributes of an in-group member than of an out-group member, which provides some evidence for the theoretical model.",0
https://doi.org/10.1016/j.ejpoleco.2005.10.005,The determinants of individual attitudes towards immigration,"The paper formulates hypotheses and reports on individual attitudes towards immigration based on data for 24 countries on socioeconomic position, sociodemographic characteristics and political attitudes. The results are consistent with the predictions of factor proportions trade theory, but also suggest that a range of other economic and cultural factors influence attitudes towards immigration.",0
https://doi.org/10.1016/j.ecolmodel.2011.11.001,Performance of a Bayesian state-space model of semelparous species for stock-recruitment data subject to measurement error,"Abstract Measurement errors in spawner abundance create problems for fish stock assessment scientists. To deal with measurement error, we develop a Bayesian state-space model for stock-recruitment data that contain measurement error in spawner abundance, process error in recruitment, and time series bias. Through extensive simulations across numerous scenarios, we compare the statistical performance of the Bayesian state-space model with that of standard regression for a traditional stock-recruitment model that only considers process error. Performance varies depending on the information content in data, as determined by stock productivity, types of harvest situations, and amount of measurement error. Overall, in terms of estimating optimal spawner abundance S MSY , the Ricker density-dependence parameter β , and optimal harvest rate h MSY , the Bayesian state-space model works best for informative data from low and variable harvest rate situations for high-productivity salmon stocks. The traditional stock-recruitment model (TSR) may be used for estimating α and h MSY for low-productivity stocks from variable and high harvest rate situations. However, TSR can severely overestimate S MSY when spawner abundance is measured with large error in low and variable harvest rate situations. We also found that there is substantial merit in using h MSY (or benchmarks derived from it) instead of S MSY as a management target.",0
https://doi.org/10.1016/j.jpain.2004.06.006,Spinal gap junctions: Potential involvement in pain facilitation,"Glia are now recognized as important contributors in pathological pain creation and maintenance. Spinal cord glia exhibit extensive gap junctional connectivity, raising the possibility that glia are involved in the contralateral spread of excitation resulting in mirror image pain. In the present experiments, the gap junction decoupler carbenoxolone was administered intrathecally after induction of neuropathic pain in response to sciatic nerve inflammation (sciatic inflammatory neuropathy) or partial nerve injury (chronic constriction injury). In both neuropathic pain models, a low dose of carbenoxolone reversed mirror image mechanical allodynia, while leaving ipsilateral mechanical allodynia unaffected. Ipsilateral thermal hyperalgesia was briefly attenuated. Critically, blockade of mechanical allodynia and thermal hyperalgesia was not observed in response to intrathecal glycyrrhizic acid, a compound similar to carbenoxolone in all respects but it does not decouple gap junctions. Thus, blockade of mechanical allodynia and thermal hyperalgesia by carbenoxolone does appear to reflect an effect on gap junctions. Examination of carbenoxolone's effects on intrathecal human immunodeficiency virus type 1 gp120 showed that blockade of pain facilitation might result, at least in part, via suppression of interleukin-1 and, in turn, interleukin-6. These data provide the first suggestion that spread of excitation via gap junctions might contribute importantly to inflammatory and traumatic neuropathic pain.The current studies provide evidence for involvement of gap junctions in spinal cord pain facilitation. Intrathecal carbenoxolone, a gap junction decoupler, reversed neuropathy-induced mirror image pain and intrathecal gp120-induced allodynia. In addition, it decreased gp120-induced proinflammatory cytokines. This suggests gap junction activation might lead to proinflammatory cytokine release by distantly activated glia.",0
https://doi.org/10.1080/14697680903369500,Bayesian analysis of multi-group nonlinear structural equation models with application to behavioral finance,"Structural equation models (SEMs) have been widely used to determine the relationships among certain observed and latent variables in behavioral finance. The purpose of this paper is to develop a Bayesian approach for analysing multi-group nonlinear SEMs. Using recently developed tools in statistical computing, such as the Gibbs sampler, we propose an efficient method to estimate parameters and select an appropriate model. The proposed method is used to investigate the relationships among all identified influential factors that have an impact on the motivation for insider trading within the framework of behavioral finance.",0
https://doi.org/10.1287/mksc.1030.0032,"A Choice Model with Conjunctive, Disjunctive, and Compensatory Screening Rules","Many theories of consumer behavior involve thresholds and discontinuities. In this paper, we investigate consumers' use of screening rules as part of a discrete-choice model. Alternatives that pass the screen are evaluated in a manner consistent with random utility theory; alternatives that do not pass the screen have a zero probability of being chosen. The proposed model accommodates conjunctive, disjunctive, and compensatory screening rules. We estimate a model that reflects a discontinuous decision process by employing the Bayesian technique of data augmentation and using Markov-chain Monte Carlo methods to integrate over the parameter space. The approach has minimal information requirements and can handle a large number of choice alternatives. The method is illustrated using a conjoint study of cameras. The results indicate that 92% of respondents screen alternatives on one or more attributes.",0
https://doi.org/10.1080/00273171.2016.1168279,Bayesian Factor Analysis as a Variable-Selection Problem: Alternative Priors and Consequences,"Factor analysis is a popular statistical technique for multivariate data analysis. Developments in the structural equation modeling framework have enabled the use of hybrid confirmatory/exploratory approaches in which factor-loading structures can be explored relatively flexibly within a confirmatory factor analysis (CFA) framework. Recently, Muthén & Asparouhov proposed a Bayesian structural equation modeling (BSEM) approach to explore the presence of cross loadings in CFA models. We show that the issue of determining factor-loading patterns may be formulated as a Bayesian variable selection problem in which Muthén and Asparouhov's approach can be regarded as a BSEM approach with ridge regression prior (BSEM-RP). We propose another Bayesian approach, denoted herein as the Bayesian structural equation modeling with spike-and-slab prior (BSEM-SSP), which serves as a one-stage alternative to the BSEM-RP. We review the theoretical advantages and disadvantages of both approaches and compare their empirical performance relative to two modification indices-based approaches and exploratory factor analysis with target rotation. A teacher stress scale data set is used to demonstrate our approach.",0
https://doi.org/10.3141/2392-04,Does Prior Specification Matter in Hotspot Identification and Before–After Road Safety Studies?,"This study investigated the effects of prior assumptions in applications of full Bayes methods in road safety analysis. The effect of prior choice was evaluated in the accuracy of model parameters, hotspot identification, goodness of fit, and a treatment effectiveness index in before–after studies. Particular attention was devoted to conditions with a lack of data, which were referenced as the low-mean and small-sample problem. In this research, informative, semi-informative, and noninformative priors were determined on the basis of past published studies. A simulation framework was used to evaluate various scenarios of sample size and crash occurrence mean. Quasi-simulated data were generated on the basis of two empirical databases of divided and undivided rural highway segments in New York and Texas. Various sample mean values were obtained on the basis of time period (number of years) and classifying accidents in fatal–injury and total accidents. The outcomes under low-mean and small-sample conditions were found to be significantly biased. However, the introduction of informative priors can make observational before–after studies feasible when a few observations from treatment or comparison sites are used. Informative priors can help provide more accurate estimates of the effectiveness of the treatment. Finally, in accordance with previous works, the inverse dispersion parameter was significantly affected by prior specifications; nevertheless, regression parameters, goodness of fit, and hotspot identification were less sensitive to prior choices.",0
https://doi.org/10.1080/00401706.1984.10487921,"Monte Carlo Comparison of ANOVA, MIVQUE, REML, and ML Estimators of Variance Components","For the one-way classification random model with unbalanced data, we compare five estimators of σ2 a and σ2 e , the among- and within-treatments variance components: analysis of variance (ANOVA), maximum likelihood (ML), restricted maximum likelihood (REML), and two minimum variance quadratic unbiased (MIVQUE) estimators. MIVQUE(0) is MIVQUE with a priori values = 0 and = 1; MIVQUE(A) is MIVQUE with the ANOVA estimates used as a priori's, We enforce nonnegativity for all estimators, setting any negative estimate to zero in accord with usual practice. The estimators are compared through their biases and MSE's, estimated by Monte Carlo simulation. Our results indicate that the ANOVA estimators perform well, except with seriously unbalanced data when σ2 a /σ2 e > 1; ML is excellent when σ2 a /σ2 e < 0.5, and MIVQUE(A) is adequate; further iteration to the REML estimates is unnecessary. When σ2 a /σ2 e ≥ 1, MIVQUE(0) (the default for SASS PROCEDURE VARCOMP) is poor for estimating σ2 a and very poor for σ2 e ,...",0
https://doi.org/10.1111/j.1467-985x.2008.00552.x,A re-evaluation of random-effects meta-analysis,"Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of 'set shifting' ability in people with eating disorders.",0
https://doi.org/10.1111/j.2044-8317.2011.02023.x,The impact of ignoring multiple membership data structures in multilevel models,"This study compared the use of the conventional multilevel model (MM) with that of the multiple membership multilevel model (MMMM) for handling multiple membership data structures. Multiple membership data structures are commonly encountered in longitudinal educational data sets in which, for example, mobile students are members of more than one higher-level unit (e.g., school). While the conventional MM requires the user either to delete mobile students' data or to ignore prior schools attended, MMMM permits inclusion of mobile students' data and models the effect of all schools attended on student outcomes. The simulation study identified underestimation of the school-level predictor coefficient, as well as underestimation of the level-two variance component with corresponding overestimation of the level-one variance when multiple membership data structures were ignored. Results are discussed along with limitations and ideas for future MMMM methodological research as well as implications for applied researchers.",0
,"Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis","Preface. About this Book 1. Introduction to The New Statistics 2. From Null Hypothesis Significance Testing to Effect Sizes 3. Confidence Intervals 4. Confidence Intervals, Error Bars, and p Values 5. Replication 6. Two Simple Designs 7. Meta-Analysis 1: Introduction and Forest Plots 8. Meta-Analysis 2: Models 9. Meta-Analysis 3: Large-Scale Analyses 10. The Noncentral t Distribution 11. Cohen's d 12. Power 13. Precision for Planning 14. Correlations, Proportions, and Further Effect Size Measures 15. More Complex Designs and The New Statistics in Practice. Glossary. Appendixes A. Loading and Using ESCI. B. ESCI for the Normal and t Distributions, and Values of z and t. C. Guide to the ESCI Modules and Pages",0
https://doi.org/10.1509/jmr.10.0395,Model-Based Segmentation Featuring Simultaneous Segment-Level Variable Selection,"The authors propose a new Bayesian latent structure regression model with variable selection to solve various commonly encountered marketing problems related to market segmentation and heterogeneity. The proposed procedure simultaneously performs segmentation and regression analysis within the derived segments, in addition to determining the optimal subset of independent variables per derived segment. The authors present comparative analyses contrasting the performance of the proposed methodology against standard latent class regression and traditional Bayesian finite mixture regression. They demonstrate that their proposed Bayesian model compares favorably with these traditional benchmark models. They then present an actual commercial customer satisfaction study performed for an electric utility company in the southeastern United States, in which they examine the heterogeneous drivers of perceived quality. Finally, they discuss limitations of the research and provide several directions for further research.",0
https://doi.org/10.1007/bf02294341,Round-robin analysis of social interaction: Exact and estimated standard errors,Kenny has proposed a variance-components model for dyadic social interaction. His Social Relations model estimates variances and covariances from a round-robin of two-person interactions. The current paper presents a matrix formulation of the Social Relations model. It uses the formulation to derive exact and estimated standard errors for round-robin estimates of Social Relations parameters.,0
https://doi.org/10.1080/01621459.1989.10478825,Approximate Bayesian Inference in Conditionally Independent Hierarchical Models (Parametric Empirical Bayes Models),"Abstract We consider two-stage models of the kind used in parametric empirical Bayes (PEB) methodology, calling them conditionally independent hierarchical models. We suppose that there are k “units,” which may be experimental subjects, cities, study centers, etcetera. At the first stage, the observation vectors Yi for units i = 1, …, k are independently distributed with densities p(yi | θi ), or more generally, p(yi | θi, λ). At the second stage, the unit-specific parameter vectors θi are iid with densities p(θi | λ). The PEB approach proceeds by regarding the second-stage distribution as a prior and noting that, if λ were known, inference about θ could be based on its posterior. Since λ is not known, the simplest PEB methods estimate the parameter λ by maximum likelihood or some variant, and then treat λ as if it were known to be equal to this estimate. Although this procedure is sometimes satisfactory, a well-known defect is that it neglects the uncertainty due to the estimation of λ. In this article w...",0
https://doi.org/10.1002/(sici)1097-0258(19970415)16:7<753::aid-sim494>3.0.co;2-g,INCORPORATING VARIABILITY IN ESTIMATES OF HETEROGENEITY IN THE RANDOM EFFECTS MODEL IN META-ANALYSIS,"When combining results from separate investigations in a meta-analysis, random effects methods enable the modelling of differences between studies by incorporating a heterogeneity parameter τ2 that accounts explicitly for across-study variation. We develop a simple form for the variance of Cochran's homogeneity statistic Q, leading to interval estimation of τ2 utilizing an approximating distribution for Q; this enables us to extend the point estimation of DerSimonian and Laird. We also develop asymptotic likelihood methods and compared them with this method. We then use these approximating distributions to give a new method of calculating the weight given to the individual studies’ results when estimating the overall mean which takes into account variation in these point estimates of τ2. Two examples illustrate the methods presented, where we show that the new weighting scheme is between the standard fixed and random effects models in down-weighting the results of large studies and up-weighting those of small studies. © 1997 by John Wiley & Sons, Ltd.",0
https://doi.org/10.1037/1082-989x.12.1.58,Item factor analysis: Current approaches and future directions.,"The rationale underlying factor analysis applies to continuous and categorical variables alike; however, the models and estimation methods for continuous (i.e., interval or ratio scale) data are not appropriate for item-level data that are categorical in nature. The authors provide a targeted review and synthesis of the item factor analysis (IFA) estimation literature for ordered-categorical data (e.g., Likert-type response scales) with specific attention paid to the problems of estimating models with many items and many factors. Popular IFA models and estimation methods found in the structural equation modeling and item response theory literatures are presented. Following this presentation, recent developments in the estimation of IFA parameters (e.g., Markov chain Monte Carlo) are discussed. The authors conclude with considerations for future research on IFA, simulated examples, and advice for applied researchers.",0
https://doi.org/10.1177/0272989x02239653,Use of Bayesian Markov Chain Monte Carlo Methods to Model Cost-of-Illness Data,"It is well known that the modeling of cost data is often problematic due to the distribution of such data. Commonly observed problems include 1) a strongly right-skewed data distribution and 2) a significant percentage of zero-cost observations. This article demonstrates how a hurdle model can be implemented from a Bayesian perspective by means of Markov Chain Monte Carlo simulation methods using the freely available software WinBUGS. Assessment of model fit is addressed through the implementation of two cross-validation methods. The relative merits of this Bayesian approach compared to the classical equivalent are discussed in detail. To illustrate the methods described, patient-specific non-health-care resource-use data from a prospective longitudinal study and the Norfolk Arthritis Register (NOAR) are utilized for 218 individuals with early inflammatory polyarthritis (IP). The NOAR database also includes information on various patient-level covariates.",0
https://doi.org/10.1080/00949659608811724,Reparameterizing the generalized linear model to accelerate gibbs sampler convergence,"Albert and Chib introduced a complete Bayesian method to analyze data arising from the generalized linear model in which they used the Gibbs sampling algorithm facilitated by latent variables. Recently, Cowles proposed an alternative algorithm to accelerate the convergence of the Albert-Chib algorithm. The novelty in this latter algorithm is achieved by using a Hastings algorithm to generate latent variables and bin boundary parameters jointly instead of individually from their respective full conditionals. In the same spirit, we reparameterize the cumulative-link generalized linear model to accelerate the convergence of Cowles’ algorithm even further. One important advantage of our method is that for the three-bin problem it does not require the Hastings algorithm. In addition, for problems with more than three bins, while the Hastings algorithm is required, we provide a proposal density based on the Dirichlet distribution which is more natural than the truncated normal density used in the competing algo...",0
https://doi.org/10.3758/s13428-012-0244-7,Assessing the evidence for response time mixture distributions,"I describe a technique for comparing two simple accounts of a distribution of response times: A mixture model and a generalized-shift model. In the mixture model, a target distribution is assumed to be a mixture of response times from two other (reference) distributions. In the generalized-shift model, the target distribution is assumed to be a quantile average of the reference distributions. In order to distinguish these two possibilities, quantiles for the target distribution are estimated from the quantiles of the reference distributions assuming either a shift or a mixture, and the predicted quantiles are used to calculate the multinomial likelihood of the obtained data. Monte Carlo simulations reported here demonstrate that the index is relatively unbiased, is effective with moderate sample sizes and modest spreads between the reference distributions, is relatively unaffected by changes in the number of bins or by data trimming, can be used with data aggregated across subjects, and is relatively insensitive to a range of subject variations in distribution shape and in mixture or shift proportion. As an illustration, the index is applied to the interpretation of three effects from distinct paradigms: residual switch costs in the task-switching paradigm, the psychological refractory period effect, and sequential effects in the Simon task. I conclude that the multinomial likelihood index provides a useful and easily applied tool for the interpretation of effects on response time distributions.",0
https://doi.org/10.1111/j.1745-3992.2007.00107.x,Estimating Item Response Theory Models Using Markov Chain Monte Carlo Methods,"The purpose of this ITEMS module is to provide an introduction to Markov chain Monte Carlo (MCMC) estimation for item response models. A brief description of Bayesian inference is followed by an overview of the various facets of MCMC algorithms, including discussion of prior specification, sampling procedures, and methods for evaluating chain convergence. Model comparison and fit issues in the context of MCMC are also considered. Finally, an illustration is provided in which a two-parameter logistic (2PL) model is fit to item response data from a university mathematics placement test through MCMC using the WINBUGS 1.4 software. While MCMC procedures are often complex and can be easily misused, it is suggested that they offer an attractive methodology for experimentation with new and potentially complex IRT models, as are frequently needed in real-world applications in educational measurement.",0
https://doi.org/10.1111/j.1745-3984.2010.00109.x,Stratified and Maximum Information Item Selection Procedures in Computer Adaptive Testing,"In this study we evaluated and compared three item selection procedures: the maximum Fisher information procedure (F), the a-stratified multistage computer adaptive testing (CAT) (STR), and a refined stratification procedure that allows more items to be selected from the high a strata and fewer items from the low a strata (USTR), along with completely random item selection (RAN). The comparisons were with respect to error variances, reliability of ability estimates and item usage through CATs simulated under nine test conditions of various practical constraints and item selection space. The results showed that F had an apparent precision advantage over STR and USTR under unconstrained item selection, but with very poor item usage. USTR reduced error variances for STR under various conditions, with small compromises in item usage. Compared to F, USTR enhanced item usage while achieving comparable precision in ability estimates; it achieved a precision level similar to F with improved item usage when items were selected under exposure control and with limited item selection space. The results provide implications for choosing an appropriate item selection procedure in applied settings.",0
https://doi.org/10.1111/j.1467-9868.2008.00663.x,Gaussian predictive process models for large spatial data sets,"With scientific data available at geocoded locations, investigators are increasingly turning to spatial process models for carrying out statistical inference. Over the last decade, hierarchical models implemented through Markov chain Monte Carlo methods have become especially popular for spatial modelling, given their flexibility and power to fit models that would be infeasible with classical methods as well as their avoidance of possibly inappropriate asymptotics. However, fitting hierarchical spatial models often involves expensive matrix decompositions whose computational complexity increases in cubic order with the number of spatial locations, rendering such models infeasible for large spatial data sets. This computational burden is exacerbated in multivariate settings with several spatially dependent response variables. It is also aggravated when data are collected at frequent time points and spatiotemporal process models are used. With regard to this challenge, our contribution is to work with what we call predictive process models for spatial and spatiotemporal data. Every spatial (or spatiotemporal) process induces a predictive process model (in fact, arbitrarily many of them). The latter models project process realizations of the former to a lower dimensional subspace, thereby reducing the computational burden. Hence, we achieve the flexibility to accommodate non-stationary, non-Gaussian, possibly multivariate, possibly spatiotemporal processes in the context of large data sets. We discuss attractive theoretical properties of these predictive processes. We also provide a computational template encompassing these diverse settings. Finally, we illustrate the approach with simulated and real data sets.",0
https://doi.org/10.1177/1094428109342899,Identifying and Analyzing Extremes: Illustrated by CEOs’ Pay and Performance,"This article presents statistical methods for identifying outcomes in a given sample that can be inferred as plausible extreme and whether the extremes on two variables are associated. Applications to CEO pay and performance of 50 top-paid CEOs illustrate these methodologies. Thresholds between extremes and nonextremes are found using high probability intervals under the probability distributions that govern sampling variations of the sample extremes. A Bayesian approach is used to compute odds on the association between the extremes of the two variables. The extreme pay—performance analysis of 50 top-paid CEOs reveals astonishing odds in favor of a company being extreme high only on one of the two versus on both variables. The result is considered decisive evidence for a negative association between extreme on CEO pay and extreme on performance among such top-paid CEOs. By contrast, analysis of the nonextreme CEOs yielded no evidence of any association between CEO pay and performance.",0
https://doi.org/10.1016/j.ecolmodel.2009.06.035,"Assessing and testing prediction uncertainty for single tree-based models: A case study applied to northern hardwood stands in southern Québec, Canada","Abstract Estimating prediction uncertainty for a single tree-based model is hindered by the complex structure of these models. In this paper, we addressed this issue with a case study applied to northern hardwood stands in Quebec, Canada. SaMARE is a stochastic single tree-based model that was designed for these types of stands. Using a Monte Carlo approach, the model can provide a mean predicted value and its confidence limits for some plot-level attributes. The mean predicted values were compared to observed values in terms of bias and accuracy. In addition to these common statistics, we compared nominal coverage of Monte Carlo-simulated confidence intervals with real (observed) coverage to verify the adequacy of the simulated uncertainty. A comparison was made using several plot-level attributes, which exhibited an increasing discriminative complexity. This complexity ranges from coarse attributes, such as all-species basal area, up to more complex ones, such as basal area for stems of a particular species and with sawlog potential. The results showed that in terms of absolute value, biases were small, but could be relatively high with respect to the average observed value when the discriminative complexity of the attribute increased. The comparison between nominal and real coverage of confidence intervals gave satisfactory results for all-species plot-level attributes. However, for some species-specific attributes, the Monte Carlo-simulated confidence intervals overestimated the real coverage.",0
https://doi.org/10.1177/0146621603260916,Kernel-Smoothing Estimation of Item Characteristic Functions for Continuous Personality Items: An Empirical Comparison with the Linear and the Continuous-Response Models,"This study used kernel-smoothing procedures to estimate the item characteristic functions (ICFs) of a set of continuous personality items. The nonparametric ICFs were compared with the ICFs estimated (a) by the linear model and (b) by Samejima’s continuous-response model. The study was based on a conditioned approach and used an error-in-variables kernel model. The assumptions of the model were approximately correct in the present conditions. The results obtained with the kernel procedure suggested that (a) the ICFs of these items were essentially sigmoid curves, as is usually assumed in parametric item response theory, and (b) for most of the items, these curves were well approximated by both the linear and the continuous-response model.",0
https://doi.org/10.1007/s11336-006-1177-1,Bayesian Analysis of Nonlinear Structural Equation Models with Nonignorable Missing Data,"A Bayesian approach is developed for analyzing nonlinear structural equation models with nonignorable missing data. The nonignorable missingness mechanism is specified by a logistic regression model. A hybrid algorithm that combines the Gibbs sampler and the Metropolis - Hastings algorithm is used to produce the joint Bayesian estimates of structural parameters, latent variables, parameters in the nonignorable missing model, as well as their standard errors estimates. A goodness-of-fit statistic for assessing the plausibility of the posited nonlinear structural equation model is introduced, and a procedure for computing the Bayes factor for model comparison is developed via path sampling. Results obtained with respect to different missing data models, and different prior inputs are compared via simulation studies. In particular, it is shown that in the presence of nonignorable missing data, results obtained by the proposed method with a nonignorable missing data model are significantly better than those that are obtained under the missing at random assumption. A real example is presented to illustrate the newly developed Bayesian methodologies. © 2006 The Psychometric Society.",0
https://doi.org/10.1371/journal.pone.0120025,Exploratory Movement Generates Higher-Order Information That Is Sufficient for Accurate Perception of Scaled Egocentric Distance,"Body movement influences the structure of multiple forms of ambient energy, including optics and gravito-inertial force. Some researchers have argued that egocentric distance is derived from inferential integration of visual and non-visual stimulation. We suggest that accurate information about egocentric distance exists in perceptual stimulation as higher-order patterns that extend across optics and inertia. We formalize a pattern that specifies the egocentric distance of a stationary object across higher-order relations between optics and inertia. This higher-order parameter is created by self-generated movement of the perceiver in inertial space relative to the illuminated environment. For this reason, we placed minimal restrictions on the exploratory movements of our participants. We asked whether humans can detect and use the information available in this higher-order pattern. Participants judged whether a virtual object was within reach. We manipulated relations between body movement and the ambient structure of optics and inertia. Judgments were precise and accurate when the higher-order optical-inertial parameter was available. When only optic flow was available, judgments were poor. Our results reveal that participants perceived egocentric distance from the higher-order, optical-inertial consequences of their own exploratory activity. Analysis of participants' movement trajectories revealed that self-selected movements were complex, and tended to optimize availability of the optical-inertial pattern that specifies egocentric distance. We argue that accurate information about egocentric distance exists in higher-order patterns of ambient energy, that self-generated movement can generate these higher-order patterns, and that these patterns can be detected and used to support perception of egocentric distance that is precise and accurate.",0
,A UNIFIED THEORY OF STATISTICAL ANALYSIS AND INFERENCE FOR VARIANCE COMPONENT MODELS FOR DYADIC DATA,"Using the covariance structure induced by the exchangeability of sampling units, a unified approach to the analysis of dyadic data is proposed. Dyadic data, encountered in diallel designs in genetics and other substantive scientific fields, arise when pairs of sampling units are studied. The problem has been addressed independently in a number of different areas of study. This paper argues that dyadic data structures involve the same statistical elements as those of ordinary analysis of variance and multivariate analysis. In addition to a synthesis of the available literature, the article provides a closed form expression of the Gaussian likelihood, the sufficient statistics and their joint distributions, and outlines for EM and ECM algorithms for handling missing data and other complications. The approach is illustrated with an applied example. The objective is to show that the analysis of dyadic data can be developed as a standard statistical method not unlike the analysis of variance, albeit with a multivariate twist. Dyadic data structures can be treated similarly to ordinary factorial structures and have the potential to be more widely used.",0
https://doi.org/10.1002/(sici)1097-0258(19971215)16:23<2741::aid-sim703>3.0.co;2-0,Investigating underlying risk as a source of heterogeneity in meta-analysis,"In a meta-analysis of clinical trials, an important issue is whether the treatment benefit varies according to the underlying risk of the patients in the different trials. The usual naive analyses employed to investigate this question use either the observed risk of events in the control groups, or the average risk in the control and treatment groups, as a measure of underlying risk. These analyses are flawed and can produce seriously misleading results. We show how their biases depend on three components of variability, the within-trial and between-trial variances of the control group risks, and the between-trial variance of the treatment effects. We propose a Bayesian solution to the problem which can be carried out using the BUGS implementation of Gibbs sampling. The analysis is illustrated for a meta-analysis of bleeding and mortality data in trials of sclerotherapy for patients with cirrhosis, and the results contrasted with those from the naive approaches. Comparisons with other methods recently proposed for this problem are also made. We conclude that the Bayesian solution presented in this paper is not only more appropriate than other proposed methods, but is also sufficiently easy to implement that it can be used by applied researchers undertaking meta-analyses.",0
https://doi.org/10.1037/a0035889,"Empirical Bayes MCMC estimation for modeling treatment processes, mechanisms of change, and clinical outcomes in small samples.","The current analysis demonstrates the use of empirical Bayes (EB) estimation methods with data-derived prior parameters for studying clinically intricate process-mechanism-outcome linkages using structural equation modeling (SEM) with small samples.The data were obtained from a small subsample of 23 families receiving Functional Family Therapy (FFT) for adolescent substance abuse during a completed randomized clinical trial. Two or 3 video-recorded FFT sessions were randomly selected for each family. The middle 20-min portion of each session was observed and coded. An SEM examining the influence of a select set of observed therapist behaviors on pre- to posttreatment change in mother reports of family functioning and, in turn, pre- to posttreatment change in adolescent reports of adolescent marijuana use and delinquent behavior was specified. The SEM was implemented using EB estimation with data-derived maximum likelihood (ML) prior parameters and Markov Chain Monte Carlo (MCMC) estimation of the joint posterior distribution.The EB SEM results indicated that a relatively high proportion of individually focused general interventions (i.e., seek information, acknowledge) as well as relationally focused meaning change interventions by therapists during sessions of FFT were predictive of pre- to posttreatment increases in levels of family functioning as reported by mothers in families of substance-abusing adolescents. In turn, increases in mother-reported family functioning were predictive of reductions in levels of adolescent-reported delinquent behavior.EB MCMC methods produced more stable results than did ML, especially regarding the variances on the change factors in the SEM. EB MCMC estimation is a viable alternative to ML estimation of SEMs in clinical research with prohibitively small samples.",0
https://doi.org/10.1287/mksc.17.2.91,Similarities in Choice Behavior Across Product Categories,"Differences between consumers in sensitivity to marketing mix variables have been extensively documented in the scanner panel data. All studies of consumer heterogeneity focus on a specific category of products and ignore the fact that the purchase behavior of panel households is often observed simultaneously in multiple categories. If sensitivity to marketing mix variables is a common consumer trait, then one should expect to see similarities in sensitivity across multiple categories. The goal in this paper is to measure the covariance of both observed (linked to measured characteristics of households) and unobserved heterogeneity in marketing mix sensitivity across multiple categories. Measurement of correlation in sensitivities across categories will serve to guide the interpretation of the literature on household heterogeneity. If there is a large correlation, one can be more confident that sensitivity to marketing variables is a fundamental household property and not simply a category-specific anomaly. Detection of correlation in sensitivities across categories requires an appropriate methodology that can handle the high dimensional covariance structures and properly account for uncertainty in estimation. For example, a simple approach might be to fit a brand choice model to each of the available categories in turn, ignoring the data in the other categories. For each category, household parameter estimates could be obtained for the parameters corresponding to price, display, and feature sensitivity. These parameter estimates could be viewed as data and the correlations across categories could be computed. Such a procedure could induce a downward bias in the estimation of correlation due to the independent sampling errors, which are present in each parameter estimate. We develop a hierarchical model structure that introduces an explicit correlation structure across categories and utilizes the data in multiple categories at the same time. To reduce the size of the covariance matrix, we use a variance components approach. We introduce household-specific demographic variables to decompose the correlation across categories into that which can be ascribed to observable and unobservable sources. Shopping behavior variables such as shopping frequency and market basket size as well as intensity of shopping in a category are also included in the model. Using data on five categories, we find substantial and statistically important correlations ranging from .32 for price sensitivities to .58 for feature sensitivity. These correlations are much larger than the correlations obtained with the state-of-the-art techniques available prior to our work. We attribute our ability to detect substantial correlations to our method, which involves the joint use of multiple category data in a parsimonious and efficient manner. Unlike previous studies with panel data, household demographic variables are found to be strongly related to price sensitivity. Higher income households are less price sensitive and large families are more price sensitive. Shopping behavior variables are also important in explaining price sensitivity. Households that visit the store often are more price sensitive. Households with larger market baskets are less price sensitive, confirming the view of Bell and Lattin (1998). Heavy user households tend to be both less price sensitive and less display sensitive. The evidence presented here of substantial correlations validates, in part, the notion that sensitivity to marketing mix variables is a consumer trait and is not unique to specific product categories. It also opens the possibility of using information across categories in making inferences about consumer brand preference and marketing mix sensitivity, providing a richer source of information for target marketing.",0
https://doi.org/10.1093/chemse/bjt073,Fitting Psychometric Functions Using a Fixed-Slope Parameter: An Advanced Alternative for Estimating Odor Thresholds With Data Generated by ASTM E679,"Psychometric functions are predominately used for estimating detection thresholds in vision and audition. However, the requirement of large data quantities for fitting psychometric functions (>30 replications) reduces their suitability in olfactory studies because olfactory response data are often limited (<4 replications) due to the susceptibility of human olfactory receptors to fatigue and adaptation. This article introduces a new method for fitting individual-judge psychometric functions to olfactory data obtained using the current standard protocol-American Society for Testing and Materials (ASTM) E679. The slope parameter of the individual-judge psychometric function is fixed to be the same as that of the group function; the same-shaped symmetrical sigmoid function is fitted only using the intercept. This study evaluated the proposed method by comparing it with 2 available methods. Comparison to conventional psychometric functions (fitted slope and intercept) indicated that the assumption of a fixed slope did not compromise precision of the threshold estimates. No systematic difference was obtained between the proposed method and the ASTM method in terms of group threshold estimates or threshold distributions, but there were changes in the rank, by threshold, of judges in the group. Overall, the fixed-slope psychometric function is recommended for obtaining relatively reliable individual threshold estimates when the quantity of data is limited.",0
https://doi.org/10.1080/00031305.1985.10479383,Constructing a Control Group Using Multivariate Matched Sampling Methods That Incorporate the Propensity Score,"Matched sampling is a method for selecting units from a large reservoir of potential controls to produce a control group of modest size that is similar to a treated group with respect to the distribution of observed covariates. We illustrate the use of multivariate matching methods in an observational study of the effects of prenatal exposure to barbiturates on subsequent psychological development. A key idea is the use of the propensity score as a distinct matching variable. Â© 1985 Taylor & Francis Group, LLC.",0
https://doi.org/10.1007/bf02294565,Constrained maximum likelihood estimation of two-level covariance structure model via EM type algorithms,"In this paper, the constrained maximum likelihood estimation of a two-level covariance structure model with unbalanced designs is considered. The two-level model is reformulated as a single-level model by treating the group level latent random vectors as hypothetical missing-data. Then, the popular EM algorithm is extended to obtain the constrained maximum likelihood estimates. For general nonlinear constraints, the multiplier method is used at the M-step to find the constrained minimum of the conditional expectation. An accelerated EM gradient procedure is derived to handle linear constraints. The empirical performance of the proposed EM type algorithms is illustrated by some artifical and real examples.",0
https://doi.org/10.1177/0013164414537491,A Note on the Specification of Error Structures in Latent Interaction Models,"Latent interaction models have motivated a great deal of methodological research, mainly in the area of estimating such models. Product-indicator methods have been shown to be competitive with other methods of estimation in terms of parameter bias and standard error accuracy, and their continued popularity in empirical studies is due, in part, to their straightforward implementation and relative ease of estimation in mainstream structural equation modeling software. In recent years, the impact of different specifications of the mean structure of the structural model has been the focus of a fair amount of investigation in this area. Yet the effects of misspecification of the error structure of the observed variables implied by the model have not been investigated. In this study, the authors demonstrate algebraically the ramifications of misspecifying these error structures for the unconstrained product-indicator approach. Recommendations to practitioners based on these results are discussed.",0
https://doi.org/10.1523/jneurosci.21-08-02808.2001,Intrathecal HIV-1 Envelope Glycoprotein gp120 Induces Enhanced Pain States Mediated by Spinal Cord Proinflammatory Cytokines,"Perispinal (intrathecal) injection of the human immunodeficiency virus-1 (HIV-1) envelope glycoprotein gp120 creates exaggerated pain states. Decreases in response thresholds to both heat stimuli (thermal hyperalgesia) and light tactile stimuli (mechanical allodynia) are rapidly induced after gp120 administration. gp120 is the portion of HIV-1 that binds to and activates microglia and astrocytes. These glial cells have been proposed to be key mediators of gp120-induced hyperalgesia and allodynia because these pain changes are blocked by drugs thought to affect glial function preferentially. The aim of the present series of studies was to determine whether gp120-induced pain changes involve proinflammatory cytokines [interleukin-1β (IL-1) and tumor necrosis factor-α (TNF-α)], substances released from activated glia. IL-1 and TNF antagonists each prevented gp120-induced pain changes. Intrathecal gp120 produced time-dependent, site-specific increases in TNF and IL-1 protein release into lumbosacral CSF; parallel cytokine increases in lumbar dorsal spinal cord were also observed. Intrathecal administration of fluorocitrate (a glial metabolic inhibitor), TNF antagonist, and IL-1 antagonist each blocked gp120-induced increases in spinal IL-1 protein. These results support the concept that activated glia in dorsal spinal cord can create exaggerated pain states via the release of proinflammatory cytokines.",0
https://doi.org/10.1007/bf02293108,"Computer methods for sampling from gamma, beta, poisson and bionomial distributions","Accurate computer methods are evaluated which transform uniformly distributed random numbers into quantities that follow gamma, beta, Poisson, binomial and negative-binomial distributions. All algorithms are designed for variable parameters. The known convenient methods are slow when the parameters are large. Therefore new procedures are introduced which can cope efficiently with parameters of all sizes. Some algorithms require sampling from the normal distribution as an intermediate step. In the reported computer experiments the normal deviates were obtained from a recent method which is also described. Â© 1974 Springer-Verlag.",0
https://doi.org/10.1016/j.indmarman.2015.04.014,The influence of leadership on product and process innovations in China: The contingent role of knowledge acquisition capability,"Abstract Building upon upper echelon theory and a dynamic capability perspective, this study investigates the relative effectiveness of two types of leadership on product and process innovations in emerging economies. The authors found that in China transformational-charismatic (TC) leadership has a stronger effect on product innovation, while transactional leadership has a stronger effect on process innovation. The authors further study the boundary conditions of leadership and empirically examine the contingent effects of organizational level capability on the relationships between leadership and innovation. The moderating effects are intriguing: knowledge acquisition capability strengthens the effect of TC leadership on process innovation and that of transactional leadership on product innovation. However, knowledge acquisition capability attenuates the positive relationship between TC leadership and product innovation as well as the positive relationship between transactional leadership and process innovation.",0
https://doi.org/10.1080/10543406.2015.1008516,Inference for Surrogate Endpoint Validation in the Binary Case,"Surrogate endpoint validation for a binary surrogate endpoint and a binary true endpoint is investigated using the criteria of proportion explained (PE) and the relative effect (RE). The concepts of generalized confidence intervals and fiducial intervals are used for computing confidence intervals for PE and RE. The numerical results indicate that the proposed confidence intervals are satisfactory in terms of coverage probability, whereas the intervals based on Fieller's theorem and the delta method fall short in this regard. Our methodology can also be applied to interval estimation problems in a causal inference-based approach to surrogate endpoint validation.",0
https://doi.org/10.1371/journal.pone.0147040,Autism and Overcoming Job Barriers: Comparing Job-Related Barriers and Possible Solutions in and outside of Autism-Specific Employment,"The aim of this study was to discover how individuals with autism succeed in entering the job market. We therefore sought to identify expected and occurred barriers, keeping them from taking up and staying in employment as well as to identify the solutions used to overcome these barriers. Sixty-six employed individuals with autism--17 of them with autism-specific employment--participated in an online survey. Results showed a variety of possible barriers. Individuals in autism-specific employment named formality problems--problems with organizational and practical process-related aspects of the job entry--most frequently while individuals in non-autism-specific employment mentioned social problems--obstacles concerning communication and human interaction--most. In terms of solutions, both groups used their own resources as much as external help, but differed in their specific strategies. In addition, correlations of an autism-specific employment with general and occupational self-efficacy as well as life and job satisfaction were examined. Possible implications of the results are discussed with regard to problem solving behavior and the use of strengths.",0
https://doi.org/10.1080/10705511.2011.607723,Bayesian Data-Model Fit Assessment for Structural Equation Modeling,"Bayesian approaches to modeling are receiving an increasing amount of attention in the areas of model construction and estimation in factor analysis, structural equation modeling (SEM), and related...",0
https://doi.org/10.4324/9780203813409,Current Topics in the Theory and Application of Latent Variable Models,"M. C. Edwards, R. C. MacCallum, Introduction: Complexity and Meaning in Latent Variable Modeling. Part I. Complexities in Latent Variable Modeling. R. Cudeck, J. R. Harring, Estimating the Correlation between Two Variables when Individuals are Measured Repeatedly. R. Gonzalez, D. Griffin, Deriving Estimators and Their Standard Errors in Dyadic Data Analysis: Examples Using a Symbolic Computation Program. P. F. Craigmile, M. Peruggia, T. Van Zandt, A Bayesian Hierarchical Model for Response Time Data Providing Evidence for Criteria Changes Over Time. I. Moustaki, A Review of Estimation Methods for Latent Variable Models. G. Zhang, C.T. Lee, Standard Errors for Ordinary Least Squares Estimates of Parameters in Structural Equation Modeling. L. Cai, Three Cheers for the Asymptotically Distribution Free Theory of Estimation and Inference: Some Recent Applications in Linear and Nonlinear Latent Variable Modeling. K. A. Duncan, S. N. MacEachern, Nonparametric Bayesian Modeling of Item Response Curves with a Three Parameter Logistic Prior Mean. W. A. Nicewander, Exact Solutions for IRT Latent Regression Slopes and Latent Variable Intercorrelations. S. du Toit, Analysis of Structural Equation Models Based on a Mixture of Continuous and Ordinal Random Variables in the Case of Complex Survey Data. Part II. Drawing Meaning from Latent Variable Models. R. E. Millsap, A Simulation Paradigm for Evaluating Approximate Fit. R. C. MacCallum, T. Lee, M. W. Browne, Fungible Parameter Values in Latent Curve Models. A. Shapiro, Statistical Inference of Moment/Covariance Structures. J. L. Rodgers, W. H. Beasley, Fisher, Gosset, and Alternative Hypothesis Significance Testing (AHST): Using the Bootstrap to Test Scientific Hypotheses about the Multiple Correlation. S. M. Boker, M. Martin, On The Equilibrium Dynamics of Meaning. K. Tateneni, M. Schiller, Applying Components Analysis to Attitudinal Segmentation.",0
https://doi.org/10.1057/cep.2013.16,Cantonal variations of integration policy and their impact on immigrant educational inequality,"Migration policy regimes are mainly analysed from an international comparative perspective, whereas subnational policy variations, although they are particularly pronounced in federal states, remain largely neglected. By transferring an international concept of integration policy to Switzerland’s cantonal level, we show that cantonal variations of integration policy are not only considerable but even occasionally exceed international variance. Subsequent outcome analyses further undermine the relevance of cantonal integration polices, suggesting that liberal and culturally pluralist policies moderate immigrant educational inequality in schools. As the results of our Bayesian multilevel analyses show, a combination of different policy aspects representing inclusive cantonal integration policies has the greatest potential to ameliorate immigrants’ equal opportunities in school. Accounting for immigrants’ heterogeneity in terms of linguistic and social background moreover reveals that integration policy differently affects various groups of immigrants.",0
https://doi.org/10.1111/j.1745-3984.1998.tb00533.x,Simultaneous Assembly of Multiple Test Forms,"An algorithm for the assembly of multiple test forms is proposed in which the multiple-form problem is reduced to a series of computationally less intensive two-form problems. At each step, one form is assembled to its true specifications; the other form is a dummy assembled only to maintain a balance between the quality of the current form and the remaining forms. It is shown how the method can be implemented using the technique of O-1 linear programming. Two empirical examples using a former item pool from the LSAT are given - one in which a set of parallel forms is assembled and another in which the targets for the information functions of the forms are shifted systematically.",0
https://doi.org/10.1167/15.3.2,The cost of misremembering: Inferring the loss function in visual working memory,"Visual working memory (VWM) is a highly limited storage system. A basic consequence of this fact is that visual memories cannot perfectly encode or represent the veridical structure of the world. However, in natural tasks, some memory errors might be more costly than others. This raises the intriguing possibility that the nature of memory error reflects the costs of committing different kinds of errors. Many existing theories assume that visual memories are noise-corrupted versions of afferent perceptual signals. However, this additive noise assumption oversimplifies the problem. Implicit in the behavioral phenomena of visual working memory is the concept of a loss function: a mathematical entity that describes the relative cost to the organism of making different types of memory errors. An optimally efficient memory system is one that minimizes the expected loss according to a particular loss function, while subject to a constraint on memory capacity. This paper describes a novel theoretical framework for characterizing visual working memory in terms of its implicit loss function. Using inverse decision theory, the empirical loss function is estimated from the results of a standard delayed recall visual memory experiment. These results are compared to the predicted behavior of a visual working memory system that is optimally efficient for a previously identified natural task, gaze correction following saccadic error. Finally, the approach is compared to alternative models of visual working memory, and shown to offer a superior account of the empirical data across a range of experimental datasets.",0
https://doi.org/10.1111/rssa.12012,Multilevel factor analytic models for assessing the relationship between nurse-reported adverse events and patient safety,"Summary  We explore health outcomes and nurse staffing data that are multivariate multilevel structured. These data can be used to relate latent constructs such as patient safety to hospital, nursing unit, nurse and patient characteristics by using factor analytic models. It is important that the multilevel nature of the data is taken into account; otherwise it can lead to invalid inferences. We explore the relationship between patient safety and nurse-reported adverse events from the Belgian chapter of the Europe Nurse Forecasting Survey. The data were split into a learning and a validation data set. Since no a priori factor structure has been proposed in the literature, we establish the factor structure by using a frequentist exploratory factor analysis on the learning data set and validate the factor structure proposed by using a Bayesian confirmatory factor analysis on the validation data set. Multivariate analysis-of-variance discrepancy measures were used to assess the need for multilevel factor analysis. We establish that there was substantial between-nursing-unit, but not between-hospital, variability to warrant the use of multilevel factor analyses. The final model was a two-level (nurse level and nursing unit level) factor analytic model with two factors at both levels. The Bayesian approach offers more flexibility in fitting the multilevel confirmatory factor analysis. To avoid double usage of the data the validation and learning data sets were used to fit and assess the goodness of fit of the multilevel confirmatory factor analysis respectively.",0
https://doi.org/10.1007/s11336-013-9328-2,A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models,"Group-level variance estimates of zero often arise when fitting multilevel or hierarchical linear models, especially when the number of groups is small. For situations where zero variances are implausible a priori, we propose a maximum penalized likelihood approach to avoid such boundary estimates. This approach is equivalent to estimating variance parameters by their posterior mode, given a weakly informative prior distribution. By choosing the penalty from the log-gamma family with shape parameter greater than 1, we ensure that the estimated variance will be positive. We suggest a default log-gamma(2,λ) penalty with λ → 0, which ensures that the maximum penalized likelihood estimate is approximately one standard error from zero when the maximum likelihood estimate is zero, thus remaining consistent with the data while being nondegenerate. We also show that the maximum penalized likelihood estimator with this default penalty is a good approximation to the posterior median obtained under a noninformative prior.Our default method provides better estimates of model parameters and standard errors than the maximum likelihood or the restricted maximum likelihood estimators. The log-gamma family can also be used to convey substantive prior information. In either case-pure penalization or prior information-our recommended procedure gives nondegenerate estimates and in the limit coincides with maximum likelihood as the number of groups increases.",0
https://doi.org/10.1016/j.jag.2012.04.007,"Hierarchical Bayesian spatial models for predicting multiple forest variables using waveform LiDAR, hyperspectral imagery, and large inventory datasets","a b s t r a c t In this paper we detail a multivariate spatial regression model that couples LiDAR, hyperspectral and for- est inventory data to predict forest outcome variables at a high spatial resolution. The proposed model is used to analyze forest inventory data collected on the US Forest Service Penobscot Experimental Forest (PEF), ME, USA. In addition to helping meet the regression model's assumptions, results from the PEF analysis suggest that the addition of multivariate spatial random effects improves model fit and pre- dictive ability, compared with two commonly applied modeling approaches. This improvement results from explicitly modeling the covariation among forest outcome variables and spatial dependence among observations through the random effects. Direct application of such multivariate models to even moder- ately large datasets is often computationally infeasible because of cubic order matrix algorithms involved in estimation. We apply a spatial dimension reduction technique to help overcome this computational hurdle without sacrificing richness in modeling.",0
https://doi.org/10.1002/per.1997,"The Social Consequences and Mechanisms of Personality: How to Analyse Longitudinal Data from Individual, Dyadic, Round–Robin and Network Designs","There is a growing interest among personality psychologists in the processes underlying the social consequences of personality. To adequately tackle this issue, complex designs and sophisticated mathematical models must be employed. In this article, we describe established and novel statistical approaches to examine social consequences of personality for individual, dyadic and group (round–robin and network) data. Our overview includes response surface analysis (RSA), autoregressive path models and latent growth curve models for individual data; actor–partner interdependence models and dyadic RSAs for dyadic data; and social relations and social network analysis for round–robin and network data. Altogether, our goal is to provide an overview of various analytical approaches, the situations in which each can be employed and a first impression about how to interpret their results. Three demo data sets and scripts show how to implement the approaches in R. Copyright © 2015 European Association of Personality Psychology",0
https://doi.org/10.1007/978-3-319-20585-4_10,Can Psychometric Measurement Models Inform Behavior Genetic Models? A Bayesian Model Comparison Approach,"As methodologists have increasingly noted, the role of psychometrics in operationalizing a construct is often overlooked when evaluating research claims (Borsboom, 2006). In a related vein, others have noted that psychological research appears to move away from assessment and interpretation of a single a priori statistical model to a more nuanced comparison of models which assess the trade-off between a model’s parsimony and complexity in explaining behavior (e.g., Rodgers, 2010). The genetic factor model is one such statistical model often used to estimate the relative contributions of genetic and environmental components of observed behavior in genetically informative designs (Heath, Neale, Hewitt, Eaves, & Fulker, 1989; Martin, Eaves et al., 1977; Neale & Cardon, 1992). Mathematically, the genetic factor model decomposes observed phenotypic variability into additive genetic (A), common (C), and unique (E) environmental components and is, for that reason, often referred to as the ACE model.",0
https://doi.org/10.4324/9781315091655-7,Dynamic Structural Equation Modeling in Longitudinal Experimental Studies,,0
https://doi.org/10.3310/hta17590,"The Percutaneous shunting in Lower Urinary Tract Obstruction (PLUTO) study and randomised controlled trial: evaluation of the effectiveness, cost-effectiveness and acceptability of percutaneous vesicoamniotic shunting for lower urinary tract obstruction","Congenital lower urinary tract obstruction (LUTO) is a disease associated with high perinatal mortality and childhood morbidity. Fetal vesicoamniotic shunting (VAS) bypasses the obstruction with the potential to improve outcome.To determine the effectiveness, cost-effectiveness and patient acceptability of VAS for fetal LUTO.A multicentre, randomised controlled trial incorporating a prospective registry, decision-analytic health economic model and preplanned Bayesian analysis using elicited opinions. Patient acceptability was evaluated by interview in a qualitative study.Fetal medicine departments in the UK, Ireland and the Netherlands.Pregnant women with a male singleton fetus with LUTO.In utero percutaneous VAS compared with conservative care.The primary outcome was survival to 28 days. Secondary outcome measures were survival and renal function at 1 year of age, cost of care and cost per additional life-year and per disability-free survival at the end of 1 year.The trial stopped early with 31 women randomised because of difficulties in recruitment. Of those randomised to VAS and conservative management, 3/16 (19%) and 2/15 (13%), respectively, did not receive their allocated intervention. Based on intention-to-treat analysis, survival at 28 days was higher if allocated VAS (50%) than conservative management (27%) [relative risk (RR) 1.88, 95% confidence interval (CI) 0.71 to 4.96, p = 0.27]. At 12 months survival was 44% in the VAS arm and 20% in the conservative arm (RR 2.19, 95% CI 0.69 to 6.94, p = 0.25). Neither difference was statistically significant. Of survivors at 1 year, two in the VAS arm had no evidence of renal impairment and four in the VAS arm and two in the conservative arm required medical management. One baby in the conservative arm had end-stage renal failure at 1 year. VAS was more expensive because of additional surgery and intensive care. VAS cost £15,500 per survivor at 1 year and £43,900 per disability-free year. Elicited expert opinions showed uncertainty in the effect of VAS at 28 days. In a Bayesian analysis combining elicited opinion with the results, uncertainty of the benefit of VAS remained (RR 1.31, 95% credible interval 0.84 to 2.18). The acceptability study identified visualisation of the fetus during ultrasound scanning, perceiving a personal benefit, and altruism as positive influences on recruitment. Fear of the VAS procedure and the perceived severity of LUTO influenced non-participation. The need for more detailed information about the condition and its implications during pregnancy and following delivery was a further important finding of this research. Recruitment was hampered by logistical and regulatory difficulties, a lower incidence of LUTO and lower antenatal diagnosis rate [estimated to be 3.34 (95% CI 2.95 to 3.72) per 10,000 total births and 47%, respectively, in an associated epidemiological study] and high termination of pregnancy rates. In the registry women also demonstrated a clear preference for conservative management.Survival to 28 days and 1 year appears to be higher with VAS than with conservative management, but it is not possible to prove benefit beyond reasonable doubt. Notably, prognosis in both arms for survival and renal function is poor. VAS was substantially more costly and unlikely to be regarded as cost-effective based on the 1-year data. Parents should be counselled about the risks of pregnancy loss with or without VAS insertion. The National Institute for Health and Care Excellence interventional procedures guidance (IPG 202) should be updated to reflect this new evidence. Babies in the PLUTO trial should be followed up long term for the different outcomes.ISRCTN53328556.This project was funded by the NIHR Health Technology Assessment programme and will be published in full in Health Technology Assessment ; Vol. 17, No. 59. See the NIHR Journals Library website for further project information.",0
https://doi.org/10.2307/2983637,Statistical Problems in Agricultural Experimentation,,0
https://doi.org/10.1177/1368430215603457,Emergence of internal and external motivations to respond without prejudice in White children,"This research examined White U.S. children’s internal and external motivations to respond without prejudice (IMS and EMS). Study 1 examined the psychometric properties of IMS and EMS scales adapted from Plant and Devine (1998) in a predominately White sample of children ( N = 123, ages 7–12, M age = 9.50, SD = 1.66). Among children aged 8 and older, both scales were reliable and correlated with theoretically relevant constructs. Study 2 examined whether IMS and EMS mediate the inverse relation between age and explicit prejudice in a sample of White children ( N = 145, ages 8–10, M age = 8.98, SD = 0.82). When ethnic labeling was present, EMS mediated the inverse relation between age and explicit ethnic bias. When ethnic labeling was absent, IMS mediated the inverse relation between age and explicit ethnic bias. Results indicate IMS and EMS emerge in childhood and bear implications for theories of ethnic attitude development.",0
https://doi.org/10.1037/a0029252,A general and flexible approach to estimating the social relations model using Bayesian methods.,"The social relations model (SRM) is a conceptual, methodological, and analytical approach that is widely used to examine dyadic behaviors and interpersonal perception within groups. This article introduces a general and flexible approach to estimating the parameters of the SRM that is based on Bayesian methods using Markov chain Monte Carlo techniques. The Bayesian approach overcomes several statistical problems that have plagued SRM researchers. First, it provides a single unified approach to estimating SRM parameters that can be easily extended to more specialized models (e.g., measurement models, moderator variables, categorical outcome variables). Second, sampling-based Bayesian methods allow statistically reliable inferences to be made about variance components and correlations, even with small sample sizes. Third, the Bayesian approach is able to handle designs with missing data. In a simulation study, the statistical properties (bias, root-mean-square error, coverage rate) of the parameter estimates produced by the Bayesian approach are compared with those of the method of moment estimates that have been used in previous research. A data example is presented to illustrate how discrete person moderators can be included in SRM analyses using the Bayesian approach. Finally, further extensions of the SRM are discussed, and suggestions for applied research are made.",0
https://doi.org/10.2202/1544-6115.1414,Orthology-Based Multilevel Modeling of Differentially Expressed Mouse and Human Gene Pairs,"There is great interest in finding human genes expressed through pharmaceutical intervention, thus opening a genomic window into benefit and side-effect profiles of a drug. Human insight gained from FDA-required animal experiments has historically been limited, but in the case of gene expression measurements, proposed biological orthologies between mouse and human genes provide a foothold for animal-to-human extrapolation. We have investigated a five-component, multilevel, bivariate normal mixture model that incorporates mouse, as well as human, gene expression data. The goal is two-fold: to increase human differential gene-finding power; and to find a subclass of gene pairs for which there is a direct exploitable relationship between animal and human genes. In simulation studies, the dual-species model boasted impressive gains in differential gene-finding power over a related marginal model using only human data. Bias in parameter estimation was problematic, however, and occasionally led to failures in control of the false discovery rate. Though it was considerably more difficult to find species-extrapolative gene-pairs (than differentially expressed human genes), simulation experiments deemed it to be possible, especially when traditional FDR controls are relaxed and under hypothetical parameter configurations.",0
https://doi.org/10.1002/sim.1544,Bayesian analysis of structural equation models with dichotomous variables,"Structural equation modelling has been used extensively in the behavioural and social sciences for studying interrelationships among manifest and latent variables. Recently, its uses have been well recognized in medical research. This paper introduces a Bayesian approach to analysing general structural equation models with dichotomous variables. In the posterior analysis, the observed dichotomous data are augmented with the hypothetical missing values, which involve the latent variables in the model and the unobserved continuous measurements underlying the dichotomous data. An algorithm based on the Gibbs sampler is developed for drawing the parameters values and the hypothetical missing values from the joint posterior distributions. Useful statistics, such as the Bayesian estimates and their standard error estimates, and the highest posterior density intervals, can be obtained from the simulated observations. A posterior predictive p-value is used to test the goodness-of-fit of the posited model. The methodology is applied to a study of hypertensive patient non-adherence to medication.",0
https://doi.org/10.1523/jneurosci.1850-04.2004,"A Role for Proinflammatory Cytokines and Fractalkine in Analgesia, Tolerance, and Subsequent Pain Facilitation Induced by Chronic Intrathecal Morphine","The present experiments examined the role of spinal proinflammatory cytokines [interleukin-1β (IL-1)] and chemokines (fractalkine) in acute analgesia and in the development of analgesic tolerance, thermal hyperalgesia, and tactile allodynia in response to chronic intrathecal morphine. Chronic (5 d), but not acute (1 d), intrathecal morphine was associated with a rapid increase in proinflammatory cytokine protein and/or mRNA in dorsal spinal cord and lumbosacral CSF. To determine whether IL-1 release modulates the effects of morphine, intrathecal morphine was coadministered with intrathecal IL-1 receptor antagonist (IL-1ra). This regimen potentiated acute morphine analgesia and inhibited the development of hyperalgesia, allodynia, and analgesic tolerance. Similarly, intrathecal IL-1ra administered after the establishment of morphine tolerance reversed hyperalgesia and prevented the additional development of tolerance and allodynia. Fractalkine also appears to modulate the effects of intrathecal morphine because coadministration of morphine with intrathecal neutralizing antibody against the fractalkine receptor (CX3CR1) potentiated acute morphine analgesia and attenuated the development of tolerance, hyperalgesia, and allodynia. Fractalkine may be exerting these effects via IL-1 because fractalkine (CX3CL1) induced the release of IL-1 from acutely isolated dorsal spinal cord in vitro . Finally, gene therapy with an adenoviral vector encoding for the release of the anti-inflammatory cytokine IL-10 also potentiated acute morphine analgesia and attenuated the development of tolerance, hyperalgesia, and allodynia. Taken together, these results suggest that IL-1 and fractalkine are endogenous regulators of morphine analgesia and are involved in the increases in pain sensitivity that occur after chronic opiates.",0
https://doi.org/10.1207/s15328007sem1203_5,Application of Structural Equation Models to Quality of Life,"Quality of life (QOL) has become an important concept for health care. As QOL is a multidimensional concept that is best evaluated by a number of latent constructs, it is well recognized that latent variable models, such as exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) are useful tools for analyzing QOL data. Recently, QOL researchers have realized the potential of structural equation modeling (SEM), which is a generalization of EFA and CFA in formulating a regression type equation in the model for studying the effects of the latent constructs to the QOL or health-related QOL. However, as the items in a QOL questionnaire are usually measured on an ordinal categorical scale, standard methods in SEM that are based on the normal distribution may produce misleading results. In this article, we propose an approach that uses a threshold specification to handle the ordinal categorical variables. Then, on the basis of observed ordinal categorical data, a maximum likelihood (ML) approach...",0
https://doi.org/10.1177/1065912914534534,"Happy Medium, Happy Citizens","Institutional and behavioral theories of democracy abound but rarely intersect. Do executive lawmaking power and prowess condition democratic regime support in presidential democracies? We develop theoretical expectations linking the lawmaking powers of the president and mass regime support and test them by analyzing survey data from eighteen Latin American countries over ten years. As hypothesized, results indicate that preference for, and satisfaction with, democracy is highest where presidents have moderate legislative powers and success and lowest where presidents either dominate policymaking or face gridlock. Hence, a “happy medium” of presidential power promotes the attitudinal foundations of democracy.",0
https://doi.org/10.1177/0165025409343765,Methods and Measures: Growth mixture modeling: A method for identifying differences in longitudinal change among unobserved groups,"Growth mixture modeling (GMM) is a method for identifying multiple unobserved sub-populations, describing longitudinal change within each unobserved sub-population, and examining differences in change among unobserved sub-populations. We provide a practical primer that may be useful for researchers beginning to incorporate GMM analysis into their research. We briefly review basic elements of the standard latent basis growth curve model, introduce GMM as an extension of multiple-group growth modeling, and describe a four-step approach to conducting a GMM analysis. Example data from a cortisol stress-response paradigm are used to illustrate the suggested procedures.",0
https://doi.org/10.1016/j.jsp.2013.12.002,An effect size measure and Bayesian analysis of single-case designs,"This article describes a linear modeling approach for the analysis of single-case designs (SCDs). Effect size measures in SCDs have been defined and studied for the situation where there is a level change without a time trend. However, when there are level and trend changes, effect size measures are either defined in terms of changes in R(2) or defined separately for changes in slopes and intercept coefficients. We propose an alternate effect size measure that takes into account changes in slopes and intercepts in the presence of serial dependence and provides an integrated procedure for the analysis of SCDs through estimation and inference based directly on the effect size measure. A Bayesian procedure is described to analyze the data and draw inferences in SCDs. A multilevel model that is appropriate when several subjects are available is integrated into the Bayesian procedure to provide a standardized effect size measure comparable to effect size measures in a between-subjects design. The applicability of the Bayesian approach for the analysis of SCDs is demonstrated through an example.",0
https://doi.org/10.1016/0749-596x(91)90025-f,A process dissociation framework: Separating automatic from intentional uses of memory,"Abstract This paper begins by considering problems that have plagued investigations of automatic or unconscious influences of perception and memory. A process dissociation procedure that provides an escape from those problems is introduced. The process dissociation procedure separates the contributions of different types of processes to performance of a task, rather than equating processes with tasks. Using that procedure, I provide new evidence in favor of a two-factor theory of recognition memory; one factor relies on automatic processes and the other relies on intentional processes. Recollection (an intentional use of memory) is hampered when attention is divided, rather than full, at the time of test. In contrast, the use of familiarity as a basis for recognition memory judgments (an automatic use of memory) is shown to be invariant across full versus divided attention, manipulated at test. Process dissociation procedures provide a general framework for separating automatic from intentional forms of processing in a variety of domains; including perception, memory, and thought.",0
https://doi.org/10.3389/fpsyg.2015.01680,The curvilinear relationship between work pressure and momentary task performance: the role of state and trait core self-evaluations,"Whereas several studies have demonstrated that core self-evaluations (CSE)-or one's appraisals about one's own self-worth, capabilities, and competences-relate to job outcomes, less is known about the mechanisms underlying these relationships. In the present study, we address this issue by examining the role of within- and between-person variation in CSE in the relationship between work pressure and task performance. We hypothesized that (a) work pressure relates to task performance in a curvilinear way, (b) state CSE mediates the curvilinear relationship between work pressure and task performance, and (c) the relationship between work pressure and state CSE is moderated by trait CSE. Our hypotheses were tested via a 10-day daily diary study with 55 employees in which trait CSE was measured at baseline, while work pressure, task performance, and state CSE were assessed on a daily basis. Bayesian multilevel path analysis showed that work pressure affects task performance via state CSE, with state CSE increasing as long as the employee feels that (s)he is able to handle the work pressure, while it decreases when the level of work pressure exceeds the employees' coping abilities. Moreover, we found that for people low on trait CSE, the depleting effect of work pressure via state CSE happens for low levels of work pressure, while for people high in trait CSE the depleting effect is located at high levels of work pressure. Together, our findings suggest that the impact of work pressure on task performance is driven by a complex interplay of between- and within-person differences in CSE.",0
https://doi.org/10.1198/sbr.2009.0070,Evaluating the Proportion of Treatment Effect Explained by a Continuous Surrogate Marker in Logistic or Probit Regression Models,"Using surrogate endpoints in clinical trials is desirable for drug development because the trials can be shortened and therefore more cost-effective. Validating a surrogate for the clinical endpoint is critical in this context. One of the key steps in statistical validation of a surrogate for a single trial is to estimate the proportion of treatment effect explained (PTE or PE) by a surrogate. Often the measure for PTE is estimated from the difference in coefficients of treatment from two models with or without adjusting for the surrogate for clinical endpoint. Inherent problems with the method are: the two models may not be valid simultaneously; and the estimate can often lie outside the interval [0, 1]. In this article, we provide alternative measures for evaluating the proportion of treatment effect explained by a surrogate in logistic or probit regression models. Our measures can be estimated easily with any statistical programs capable of binary linear regression modeling, and the interpretation of the measures can be illustrated using Ordinal Dominance (OD) curves. The concept can be visually understood by any practical user. Simulation shows our alternative measures yield more accurate estimates which are less biased, less variable, and with narrower confidence intervals. A clinical trial example is provided.",0
https://doi.org/10.22237/jmasm/1241137860,A Comparison of Maximum Likelihood and Expected A Posteriori Estimation for Polychoric Correlation Using Monte Carlo Simulation,"This study aims to compare the maximum likelihood (ML) and expected a posterior (EAP) estimation for polychoric correlation (PCC) under diverse conditions, especially when considering a sample size. As the ML is the classical solution to estimate PCC, the EAP is a new method based on Bayes’ theorem. Different types of prior distributions are also adapted to investigate the sensitivity of prior distribution onto the PCC estimate for the EAP case. The Monte Carlo simulation is used for this comparison by a specialized program code in MATLAB.",0
https://doi.org/10.3758/bf03194023,An EZ-diffusion model for response time and accuracy,"The EZ-diffusion model for two-choice response time tasks takes mean response time, the variance of response time, and response accuracy as inputs. The model transforms these data via three simple equations to produce unique values for the quality of information, response conservativeness, and nondecision time. This transformation of observed data in terms of unobserved variables addresses the speed-accuracy trade-off and allows an unambiguous quantification of performance differences in two-choice response time tasks. The EZ-diffusion model can be applied to data-sparse situations to facilitate individual subject analysis. We studied the performance of the EZ-diffusion model in terms of parameter recovery and robustness against misspecification by using Monte Carlo simulations. The EZ model was also applied to a real-world data set.",0
https://doi.org/10.1093/biostatistics/2.4.485,Empirical Bayes Gibbs sampling,"The wide applicability of Gibbs sampling has increased the use of more complex and multi-level hierarchical models. To use these models entails dealing with hyperparameters in the deeper levels of a hierarchy. There are three typical methods for dealing with these hyperparameters: specify them, estimate them, or use a 'flat' prior. Each of these strategies has its own associated problems. In this paper, using an empirical Bayes approach, we show how the hyperparameters can be estimated in a way that is both computationally feasible and statistically valid.",0
https://doi.org/10.1037/0022-3514.92.6.990,Misunderstanding the affective consequences of everyday social interactions: the hidden benefits of putting one's best face forward.,"Positive self-presentation may have beneficial consequences for mood that are typically overlooked. Across a series of studies, participants underestimated how good they would feel in situations that required them to put their best face forward. In Studies 1 and 2A, participants underestimated the emotional benefits of interacting with an opposite sex stranger versus the benefits of interacting with a romantic partner. In Study 2B, participants who were instructed to engage in self-presentation felt happier after interacting with their romantic partner than participants who were not given this instruction, although other participants serving as forecasters did not anticipate such benefits. Increasing the generalizability of this self-presentation effect across contexts, the authors demonstrated that participants also underestimated how good they would feel before and after being evaluated by another person (Studies 3 and 4). This failure to recognize the affective benefits of putting one's best face forward may underlie forecasting errors regarding the emotional consequences of the most common forms of social interactions.",0
https://doi.org/10.1287/mksc.2014.0859,Partner Selection in Brand Alliances: An Empirical Investigation of the Drivers of Brand Fit,"We investigate whether partners in a brand alliance should be similar or dissimilar in brand image to foster favorable perceptions of brand fit. Using a Bayesian nonlinear structural equation model and evaluations of 1,200 brand alliances, we find that the conceptual coherence in brand personality profiles predicts attitudes towards a brand alliance. More specifically, we find that similarity in Sophistication and Ruggedness and moderate dissimilarity in Sincerity and Competence result in more favorable brand alliance evaluations. Overall, we find that similarity effects are more pronounced than dissimilarity effects. Implications for brand alliance strategies and marketing managers are discussed.",0
https://doi.org/10.1348/000711009x430906,Investigating the relationship between item exposure and test overlap: Item sharing and item pooling,"To date, exposure control procedures that are designed to control item exposure and test overlap simultaneously are based on the assumption of item sharing between pairs of examinees. However, examinees may obtain test information from more than one examinee in practice. This larger scope of information sharing needs to be taken into account in refining exposure control procedures. To control item exposure and test overlap among a group of examinees larger than two, the relationship between the two indices needs to be identified first. The purpose of this paper is to analytically derive the relationships between item exposure rate and each of the two forms of test overlap, item sharing and item pooling, for fixed-length computerized adaptive tests. Item sharing is defined as the number of common items shared by all examinees in a group, while item pooling is the number of overlapping items that an examinee has with a group of examinees. The accuracy of the derived relationships was verified using numerical examples. The relationships derived will lay the foundation for future development of procedures to simultaneously control item exposure and item sharing or item pooling among a group of examinees larger than two.",0
https://doi.org/10.5849/forsci.11-146,Volume and Error Variance Estimation Using Integrated Stem Taper Models,"In this study, we propose two volume and error variance estimators based on an integrated nonlinear mixed-effects stem taper model. The estimators rely either on a first- or a second-order Taylor series expansion. They were first tested through Monte Carlo simulations. The accuracy of the volume and error variance estimates was then tested against more than 1,000 observations. Empirical and nominal coverage of the confidence intervals were also compared under the assumption of a Gaussian distribution. For the volume estimators, results showed that the first-order estimator tends to slightly underestimate the volume, mainly because the stem taper model had random effects specified in a nonlinear manner. The second-order estimator was more accurate with neither under- nor overestimations of volume. For both the first- and the second-order variance estimators, the confidence intervals had empirical coverage that closely matches nominal coverage for probability levels >0.9. Although the proposed estimators require the stem taper model to predict the squared diameter of the cross section, they have the benefit of providing a tractable estimate of the variance. The covariances between different stem sections are quickly estimated because there is no need for repeated numerical integrations. Ã‚Â© 2013 by the Society of American Foresters.",0
https://doi.org/10.1348/000711099159080,Statistical analysis of nonlinear factor analysis models,"This paper presents a Bayesian analysis of a general nonlinear factor analysis model. Both the non-informative and conjugate prior distributions are considered. A hybrid algorithm that combines the Metropolis-Hastings algorithm and the Gibbs sampler is implemented to produce direct estimates of the latent factor scores and the structural parameters. Standard errors estimates as well as a goodness-of-fit statistic for assessing the posited model are presented. To illustrate the methodology, results from a simulation study and a real example are reported.",0
https://doi.org/10.1111/j.2044-8317.1992.tb00977.x,Prior distribution for item response curves,"The paper examines the use of the ordered Dirichlet prior for binary logistic item response models. This prior is based on the investigator's prior information about the probabilities of correct response to items from examinees at several ability levels. The effect of the prior is examined in terms of the posterior mode of the item parameters computed via the EM algorithm. An illustration is used to explore the application of a 1981 ACT math test to form a prior that is then used on a similar 1987 test. A novel feature of the paper is the emphasis placed on the entire item response curves, rather than the item parameters per se. Detailed computational expressions for the three-parameter logistic model are summarized in an Appendix.",0
https://doi.org/10.1111/j.0006-341x.2001.01198.x,Small-Sample Adjustments for Wald-Type Tests Using Sandwich Estimators,"The sandwich estimator of variance may be used to create robust Wald-type tests from estimating equations that are sums of K independent or approximately independent terms. For example, for repeated measures data on K individuals, each term relates to a different individual. These tests applied to a parameter may have greater than nominal size if K is small, or more generally if the parameter to be tested is essentially estimated from a small number of terms in the estimating equation. We offer some practical modifications to these robust Wald-type tests, which asymptotically approach the usual robust Wald-type tests. We show that one of these modifications provides exact coverage for a simple case and examine by simulation the modifications applied to the generalized estimating equations of Liang and Zeger (1986), conditional logistic regression, and the Cox proportional hazard model.",0
https://doi.org/10.1080/1369183x.2014.915192,Over-education among Immigrants in Europe: The Value of Civic Involvement,"The present study investigates the effect of social connectedness through associational involvement on over-education among immigrants in 19 European societies, addressing the often-claimed importance of weak ties. Based on the idea that the effectiveness of social networks in distributing relevant information may depend on the degree to which such networks are linked to other networks, it thereby specifically distinguishes between associations that are connected to the wider community and those that are rather isolated. Using pooled individual-level data from the European Social Survey, empirical analyses indeed reveal active participation in associations to be related to lower risk of experiencing over-education. Moreover, the effect appears type specific, indicating access to multiple associations which are widely connected to different parts of society to be of particular benefit, especially in case of more recent migration.",0
https://doi.org/10.1111/j.2044-8317.1995.tb01067.x,A two-stage estimation of structural equation models with continuous and polytomous variables,"This paper develops a computationally efficient procedure for analysis of structural equation models with continuous and polytomous variables. A partition maximum likelihood approach is used to obtain the first stage estimates of the thresholds and the polyserial and polychoric correlations in the underlying correlation matrix. Then, based on the joint asymptotic distribution of the first stage estimator and an appropriate weight matrix, a generalized least squares approach is employed to estimate the structural parameters in the correlation structure. Asymptotic properties of the estimators are derived. Some simulation studies are conducted to study the empirical behaviours and robustness of the procedure, and compare it with some existing methods.",0
https://doi.org/10.1016/j.csda.2003.08.006,The influence of violations of assumptions on multilevel parameter estimates and their standard errors,"A crucial problem in the statistical analysis of hierarchically structured data is the dependence of the observations at the lower levels. Multilevel modeling programs account for this dependence and in recent years these programs have been widely accepted. One of the major assumptions of the tests of significance used in the multilevel programs is normality of the error distributions involved. Simulations were used to assess how important this assumption is for the accuracy of multilevel parameter estimates and their standard errors. Simulations varied the number of groups, the group size, and the intraclass correlation, with the second level residual errors following one of three non-normal distributions. In addition asymptotic maximum likelihood standard errors are compared to robust (Huber/White) standard errors. The results show that non-normal residuals at the second level of the model have little or no effect on the parameter estimates. For the fixed parameters, both the maximum likelihood-based standard errors and the robust standard errors are accurate. For the parameters in the random part of the model, the maximum likelihood-based standard errors at the lowest level are accurate, while the robust standard errors are often overcorrected. The standard errors of the variances of the level-two random effects are highly inaccurate, although the robust errors do perform better than the maximum likelihood errors. For good accuracy, robust standard errors need at least 100 groups. Thus, using robust standard errors as a diagnostic tool seems to be preferable to simply relying on them to solve the problem.",0
https://doi.org/10.3102/1076998606298042,Multilevel Structural Equation Models for the Analysis of Comparative Data on Educational Performance,The Programme for International Student Assessment comparative study of reading performance among 15-year-olds is reanalyzed using statistical procedures that allow the full complexity of the data ...,0
https://doi.org/10.1023/a:1020371312283,,"We consider methods for estimating causal effects of treatments when treatment assignment is unconfounded with outcomes conditional on a possibly large set of covariates. Robins and Rotnitzky (1995) suggested combining regression adjustment with weighting based on the propensity score (Rosenbaum and Rubin, 1983). We adopt this approach, allowing for a flexible specification of both the propensity score and the regression function. We apply these methods to data on the effects of right heart catheterization (RHC) studied in Connors et al (1996), and we find that our estimator gives stable estimates over a wide range of values for the two parameters governing the selection of variables.",0
https://doi.org/10.1037/0022-006x.65.4.599,"Toward terminological, conceptual, and statistical clarity in the study of mediators and moderators: Examples from the child-clinical and pediatric psychology literatures.","Numerous recent attempts to identify mediated and moderated effects in child-clinical and pediatric research on child adjustment have been characterized by terminological, conceptual, and statistical inconsistencies. To promote greater clarity, the terms mediating and moderating are defined and differentiated. Recommended statistical strategies that can be used to test for these effects are reviewed (i.e., multiple regression and structural equation modeling techniques). The distinction between mediated and indirect effects is also discussed. Examples of troublesome and appropriate uses of these terms in the child-clinical and pediatric psychology literatures are highlighted.",0
https://doi.org/10.1191/096228001682157616,"Multilevel models for meta-analysis, and their application to absolute risk differences","Meta-analysis can be considered a multilevel statistical problem, since information within studies is combined in the presence of potential heterogeneity between studies. Here a general multilevel model framework is developed for meta-analysis to combine either summary data or individual patient outcome data from each study, and to include either study or individual level covariates that might explain heterogeneity. Classical and Bayesian approaches to estimation are contrasted. These methods are applied to a meta-analysis of trials of thrombolytic therapy after myocardial infarction. Subgroups within the trials were available, categorized by the time delay until treatment, so that a three-level random effects model that includes time delay as a covariate is proposed. In addition it was desired to represent the treatment effect as an absolute risk reduction, rather than the conventional odds ratio. We show how this can be achieved within a Bayesian analysis, while still recognizing the binary nature of the original outcome data.",0
https://doi.org/10.4324/9780203852279,Multilevel Analysis,"This practical introduction helps readers apply multilevel techniques to their research. Noted as an accessible introduction, the book also includes advanced extensions, making it useful as both an introduction and as a reference to students, researchers, and methodologists. Basic models and examples are discussed in non-technical terms with an emphasis on understanding the methodological and statistical issues involved in using these models. The estimation and interpretation of multilevel models is demonstrated using realistic examples from various disciplines. For example, readers will find data sets on stress in hospitals, GPA scores, survey responses, street safety, epilepsy, divorce, and sociometric scores, to name a few. The data sets are available on the website in SPSS, HLM, MLwiN, LISREL and/or Mplus files. Readers are introduced to both the multilevel regression model and multilevel structural models. Highlights of the second edition include:  Two new chapters—one on multilevel models for ordinal and count data (Ch. 7) and another on multilevel survival analysis (Ch. 8). Thoroughly updated chapters on multilevel structural equation modeling that reflect the enormous technical progress of the last few years. The addition of some simpler examples to help the novice, whilst the more complex examples that combine more than one problem have been retained. A new section on multivariate meta-analysis (Ch. 11). Expanded discussions of covariance structures across time and analyzing longitudinal data where no trend is expected. Expanded chapter on the logistic model for dichotomous data and proportions with new estimation methods. An updated website at http://www.joophox.net/ with data sets for all the text examples and up-to-date screen shots and PowerPoint slides for instructors. Ideal for introductory courses on multilevel modeling and/or ones that introduce this topic in some detail taught in a variety of disciplines including: psychology, education, sociology, the health sciences, and business. The advanced extensions also make this a favorite resource for researchers and methodologists in these disciplines. A basic understanding of ANOVA and multiple regression is assumed. The section on multilevel structural equation models assumes a basic understanding of SEM.",0
https://doi.org/10.1093/biomet/86.2.365,Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes,"SUMMARY We study the combined impact that all-or-none compliance and subsequent missing outcomes can have on the estimation of the intention-to-treat effect of assignment in randomised studies. In this setting, a standard analysis, which drops subjects with missing outcomes and ignores compliance information, can be biased for the intention-to-treat effect. To address all-or-none compliance that is followed by missing outcomes, we construct a new estimation procedure for the intention-to-treat effect that maintains good randomisation-based properties under more plausible, nonignorable noncompliance and nonignorable missing-outcome conditions: the 'compound exclusion restriction' on the effect of assignment and the 'latent ignorability' of the missing data mechanism. We present both theoretical results and a simulation study. Moreover, we show how the two key concepts of compound exclusion and latent ignorability are relevant in more complicated settings, such as right censoring of a time-to-event outcome.",0
https://doi.org/10.1177/0146621614540514,MCMC GGUM,Link_to_subscribed_fulltex,0
https://doi.org/10.1111/j.1460-9568.2012.08264.x,Proactive inhibitory control varies with task context,"The goal of executive control is to adjust our behaviour to the environment. It involves not only the continuous planning and adaptation of actions but also the inhibition of inappropriate movements. Recently, a proactive form of inhibitory control has been shown, demonstrating that actions can be withheld, in an uncertain environment, thanks to the proactive locking of the mechanism by which motor commands are triggered (e.g. while waiting at traffic lights in a dense pedestrian zone, one will refrain in anticipation of a brisk acceleration when the green light comes on). However, little is known about this executive function and it remains unclear whether the overall amount of inhibitory control can be modulated as a function of the context. Here, we show that the level of this control varies parametrically as a function of the exogenous and endogenous factors setting the task context. We also show that the level of implemented proactive inhibitory control is dynamically readjusted to match the implicit temporal structure of the environment. These observations are discussed in relation to possible underlying functional substrates and related neurological and psychiatric pathologies.",0
,xtmixed and Denominator Degrees of Freedom: Myth or Magic,A review of issues and controversy surrounding F-ratio denominator degrees of freedom (DDF) in linear mixed models. The presentation will look at the history of denominator degrees of freedom as well survey their use in various statistical packages.,0
https://doi.org/10.1016/j.jamcollsurg.2013.07.386,Antibiotic Prophylaxis for the Prevention of Surgical Site Infection after Tension-Free Hernia Repair: A Bayesian and Frequentist Meta-Analysis,"Efficacy of antibiotic prophylaxis for the prevention of surgical site infection (SSI) after open tension-free hernia repair remains controversial. In light of additional data, the aim of this study was to determine whether antibiotic prophylaxis reduces SSI after hernia repair.We conducted a systematic review and meta-analysis to identify randomized controlled trials comparing antibiotic prophylaxis and the subsequent incidence of SSI after inguinal or femoral hernia repair. The primary outcomes measure was the incidence of SSI. Subgroup analysis was evaluated by stratifying the categories of SSI. The meta-analysis was performed using Bayesian and frequentist methods.Twelve studies were included in this meta-analysis; 1,902 patients received antibiotic prophylaxis and the other 1,936 patients were allocated to the control group. Incidence of SSI was 47 (pooled rate 3.0%) in the antibiotic group and 91 (6.0%) in the control group. The number needed to treat to prevent 1 episode of SSI is 41. The Bayesian meta-analysis yielded a significant reduction of SSI in the antibiotic group (odds ratio = 0.49; 95% credible interval 0.25-0.81). Subgroup analysis showed that an antibiotic prophylaxis was beneficial for the prevention of superficial SSI (odds ratio = 0.40; 95% credible interval 0.12-0.98), but not beneficial for prevention of deep SSI (odds ratio = 0.59; 95% credible interval 0.11-3.20). Also, the results were similar to those with frequentist methods.This meta-analysis suggests that antibiotic prophylaxis is efficacious for the prevention of SSI after open mesh hernia repair.",0
https://doi.org/10.3102/1076998607313593,Marginal Maximum Likelihood Estimation of a Latent Variable Model With Interaction,"There has been considerable interest in nonlinear latent variable models specifying interaction between latent variables. Although it seems to be only slightly more complex than linear regression without the interaction, the model that includes a product of latent variables cannot be estimated by maximum likelihood assuming normality. Consequently, many approximate methods have been proposed. Recently, a maximum likelihood method of estimation based on the expectation–maximization algorithm has been suggested that is optimum if the distribution assumptions are true. In this article, the authors outline an alternative marginal maximum likelihood estimator using numerical quadrature. A key feature of the approach is that in the marginal distribution of the manifest variables the complicated integration can be reduced, often to a single dimension. This allows a direct approach to maximizing the log-likelihood and makes the method relatively straightforward to use.",0
https://doi.org/10.1002/brb3.44,"Immunofluorescent spectral analysis reveals the intrathecal cannabinoid agonist, AM1241, produces spinal anti‐inflammatory cytokine responses in neuropathic rats exhibiting relief from allodynia","During pathological pain, the actions of the endocannabinoid system, including the cannabinoid 2 receptor (CB(2)R), leads to effective anti-allodynia and modifies a variety of spinal microglial and astrocyte responses. Here, following spinal administration of the CB(2)R compound, AM1241, we examined immunoreactive alterations in markers for activated p38 mitogen-activated protein kinase, interleukin-1β (IL-1β), the anti-inflammatory cytokine, interleukin-10 (IL-10) as well as degradative endocannabinoid enzymes, and markers for altered glial responses in neuropathic rats. In these studies, the dorsal horn of the spinal cord and dorsal root ganglia were examined. AM1241 produced profound anti-allodynia with corresponding immunoreactive levels of p38 mitogen-activated kinase, IL-1β, IL-10, the endocannabinoid enzyme monoacylglycerol lipase, and astrocyte activation markers that were similar to nonneuropathic controls. In contrast, spinal AM1241 did not suppress the increased microglial responses observed in neuropathic rats. The differences in fluorescent markers were determined within discrete anatomical regions by applying spectral analysis methods, which virtually eliminated nonspecific signal during the quantification of specific immunofluorescent intensity. These data reveal expression profiles that support the actions of intrathecal AM1241 control pathological pain through anti-inflammatory mechanisms by modulating critical glial factors, and additionally decrease expression levels of endocannabinoid degradative enzymes.",0
https://doi.org/10.1002/sim.7572,A Bayesian semiparametric latent variable approach to causal mediation,"In assessing causal mediation effects in randomized studies, a challenge is that the direct and indirect effects can vary across participants due to different measured and unmeasured characteristics. In that case, the population effect estimated from standard approaches implicitly averages over and does not estimate the heterogeneous direct and indirect effects. We propose a Bayesian semiparametric method to estimate heterogeneous direct and indirect effects via clusters, where the clusters are formed by both individual covariate profiles and individual effects due to unmeasured characteristics. These cluster-specific direct and indirect effects can be estimated through a set of regression models where specific coefficients are clustered by a stick-breaking prior. To let clustering be appropriately informed by individual direct and indirect effects, we specify a data-dependent prior. We conduct simulation studies to assess performance of the proposed method compared to other methods. We use this approach to estimate heterogeneous causal direct and indirect effects of an expressive writing intervention for patients with renal cell carcinoma.",0
https://doi.org/10.1037//0022-006x.70.4.976,A finite mixture model of growth trajectories of adolescent alcohol use: Predictors and consequences.,"The current study sought to identify classes of growth trajectories of adolescent alcohol use and to examine the predictors and outcomes associated with the classes. Alcohol use was assessed from Grades 7 to 12 in a school-based sample. Latent growth mixture modeling was used, and results indicated 5 discrete longitudinal drinking patterns. The 2 most common drinking patterns included occasional very light drinking from Grades 7 to 12 and moderate escalation in both quantity and frequency of alcohol use. One group drank infrequently but at high levels throughout the study period. Another group exhibited rapid escalation in both quantity and frequency. The final group started at high levels of frequency and quantity in Grade 7 and showed rapid de-escalation in frequency. Emotional distress and risk taking distinguished the classes, and all classes, particularly rapid escalators, showed elevated levels of alcohol-related problems relative to occasional very light drinkers.",0
https://doi.org/10.1177/0146167215626706,Motivational Affordance and Risk-Taking Across Decision Domains,"We propose a motivational affordance account to explain both stability and variability in risk-taking propensity in major decision domains. We draw on regulatory focus theory to differentiate two types of motivation (prevention, promotion) that play a key role in predicting risk-taking. Study 1 demonstrated that prevention motivation is negatively associated with risk-taking across six key decision domains, including health/safety, ethics, recreation, gambling, investment, and social. In contrast, promotion motivation is positively associated with risk-taking in the social and investment domains. Study 2 replicated the same pattern and provided direct evidence that promotion motivation is a strong predictor of risk-taking only in domains where there is true potential for gains. Study 3 manipulated promotion (vs. prevention) motivation experimentally to demonstrate that motivational affordance is a critical mechanism for understanding risk-taking behaviors.",0
https://doi.org/10.1146/annurev.psych.58.110405.085542,Mediation Analysis,"Abstract Mediating variables are prominent in psychological theory and research. A mediating variable transmits the effect of an independent variable on a dependent variable. Differences between mediating variables and confounders, moderators, and covariates are outlined. Statistical methods to assess mediation and modern comprehensive approaches are described. Future directions for mediation analysis are discussed.",0
https://doi.org/10.3168/jds.2007-0621,"Cow, Farm, and Herd Management Factors in the Dry Period Associated with Raised Somatic Cell Counts in Early Lactation","This study investigated cow characteristics, farm facilities, and herd management strategies during the dry period to examine their joint influence on somatic cell counts (SCC) in early lactation. Data from 52 commercial dairy farms throughout England and Wales were collected over a 2-yr period. For the purpose of analysis, cows were separated into those housed for the dry period (6,419 cow-dry periods) and those at pasture (7,425 cow-dry periods). Bayesian multilevel models were specified with 2 response variables: ln SCC (continuous) and SCC >199,000 cells/mL (binary), both within 30 d of calving. Cow factors associated with an increased SCC after calving were parity, an SCC >199,000 cells/mL in the 60 d before drying off, increasing milk yield 0 to 30 d before drying off, and reduced DIM after calving at the time of SCC estimation. Herd management factors associated with an increased SCC after calving included procedures at drying off, aspects of bedding management, stocking density, and method of pasture grazing. Posterior predictions were used for model assessment, and these indicated that model fit was generally good. The research demonstrated that specific dry-period management strategies have an important influence on SCC in early lactation.",0
https://doi.org/10.1037/0021-843x.89.2.203,Social competence and depression: The role of illusory self-perceptions.,"Attempted to evaluate the role in clinical depression of 2 personal variables: social competencies (as perceived by others) and their encoding or self-perception by the individual. Both self-ratings and ratings from observers were obtained for 71 depressed, 59 psychiatric control, and 73 normal control individuals following a group interaction at different times in the course of treatment. As expected, the depressed Ss initially rated themselves and were rated by others as less socially competent than the 2 control groups, and their self-perceptions improved with treatment. Surprisingly, the depressed were more realistic in their self-perceptions than the controls. Specifically, the controls perceived themselves more positively than others saw them, whereas the depressed saw themselves as they were seen. This realism of the depressed tended to decrease in the course of treatment. The theoretical implications of a possibly illusory flow for appropriate affect and self-regulation are discussed. (20 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). Â© 1980 American Psychological Association.",0
https://doi.org/10.1111/j.1559-1816.2002.tb01434.x,The Influence of Affective and Instrumental Beliefs on Exercise Intentions and Behavior: A Longitudinal Analysis,"Programs encouraging exercise might reduce coronary illness. Ajzen's theory of planned behavior is a useful model for understanding exercise motivation. The current study investigated the contribution of the instrumental and affective components of attitude. As part of a community-based study of exercise behavior, 424 men and 572 women completed questionnaires, with 365 participants providing 6-month follow-up data. Regressions highlighted the affective component as a much more powerful predictor of intention compared to the instrumental component. After controlling for prior exercise, intention was not predictive of later exercise, although the affective component was. The implications for exercise promotion are discussed.",0
https://doi.org/10.1214/aos/1043351257,A unified jackknife theory for empirical best prediction with M-estimation,"The paper presents a unified jackknife theory for a fairly general class of mixed models which includes some of the widely used mixed linear models and generalized linear mixed models as special cases. The paper develops jackknife theory for the important, but so far neglected, prediction problem for the general mixed model. For estimation of fixed parameters, a jackknife method is considered for a general class of M-estimators which includes the maximum likelihood, residual maximum likelihood and ANOVA estimators for mixed linear models and the recently developed method of simulated moments estimators for generalized linear mixed models. For both the prediction and estimation problems, a jackknife method is used to obtain estimators of the mean squared errors (MSE). Asymptotic unbiasedness of the MSE estimators is shown to hold essentially under certain moment conditions. Simulation studies undertaken support our theoretical results.",0
https://doi.org/10.1080/00031305.2015.1111260,The Central Role of Bayes’ Theorem for Joint Estimation of Causal Effects and Propensity Scores,"Although propensity scores have been central to the estimation of causal effects for over 30 years, only recently has the statistical literature begun to consider in detail methods for Bayesian estimation of propensity scores and causal effects. Underlying this recent body of literature on Bayesian propensity score estimation is an implicit discordance between the goal of the propensity score and the use of Bayes theorem. The propensity score condenses multivariate covariate information into a scalar to allow estimation of causal effects without specifying a model for how each covariate relates to the outcome. Avoiding specification of a detailed model for the outcome response surface is valuable for robust estimation of causal effects, but this strategy is at odds with the use of Bayes theorem, which presupposes a full probability model for the observed data that adheres to the likelihood principle. The goal of this paper is to explicate this fundamental feature of Bayesian estimation of causal effects with propensity scores in order to provide context for the existing literature and for future work on this important topic.",0
https://doi.org/10.1111/j.1600-0447.2006.00914.x,A framework for evidence-based mental health care and policy,"Care planning integrates a growing number of disciplines, research fields and analysis techniques. A framework of the main areas of interest with regard to evidence-based health care in mental health is provided here.The framework is based on the experience of working with data analysts and health and social decision makers at the PSICOST/RIRAG network, a Spanish research association which includes psychiatrists, health economists and health policy experts, as well as on a review of the literature.Three main areas have been identified and described here: outcomes management, knowledge discovery from data, and decision support systems. Their use in mental health care is reviewed.It is important to promote bridging strategies among these new fields in order to enhance communication and information transfer between the different parts involved in mental health decision making: i) clinicians and epidemiologists, ii) data analysts, iii) care policy makers and other end-users.",0
https://doi.org/10.1002/ejsp.2420250405,"The theory of planned behaviour and exercise: An investigation into the role of prior behaviour, behavioural intentions and attitude variability","This paper reports a prospective study which applied the Theory of Planned Behaviour (TPB) to the prediction of exercise behaviour over a six-month period. The study addressed a number of issues which have been identified in the literature on the TPB; these being the role of prior behaviour in the TPB, the distinction between desires and self-predictions, and the question of attitude variability. The findings showed prior behaviour to be the strongest predictor of exercise behaviour at six months. Contrary to expectations, the self-prediction measure was not found to be a better predictor of behaviour than the desire measure. Attitude variability was found to be related to perceptions of control. However, attitude variability was not found to moderate relationships between components of the TPB. The implications of the results for the development of the TPB are discussed.",0
https://doi.org/10.1198/000313008x331530,A Note Concerning a Selection “Paradox” of Dawid's,"This article briefly reviews a selection “paradox” of Dawid's, whereby Bayesian inference appears to be unchanged whether or not treatments have been selected for inspection on the basis of extreme values. The problem is recast in terms of a hierarchical model. This offers an alternative explanation of the paradox but also reveals a disturbing dependence of inference on prior specification. The example may also be used to deepen students' understanding of the implications of using conjugate nonhierarchical priors in Bayesian analysis. To illustrate, some simulations are presented.",0
https://doi.org/10.1136/ip.2006.014571,"Maternal depression, child behavior, and injury","<h3>Summary</h3> <h3>Background and objectives</h3> Recent interest has focused on wait listing patients without pretreating coronary artery disease to expedite transplantation. Our practice is to offer coronary revascularization before transplantation if indicated. <h3>Design, setting, participants, &amp; measurements</h3> Between 2006 and 2009, 657 patients (427 men, 230 women; ages, 56.5 ± 9.94 years) underwent pretransplant assessment with coronary angiography. 573 of 657 (87.2%) patients were wait listed; 247 of 573 (43.1%) patients were transplanted during the follow-up period, 30.09 ± 11.67 months. <h3>Results</h3> Patient survival for those not wait listed was poor, 83.2% and 45.7% at 1 and 3 years, respectively. In wait-listed patients, survival was 98.9% and 95.3% at 1 and 3 years, respectively. 184 of 657 (28.0%) patients were offered revascularization. Survival in patients (<i>n</i> = 16) declining revascularization was poor: 75% survived 1 year and 37.1% survived 3 years. Patients undergoing revascularization followed by transplantation (<i>n</i> = 51) had a 98.0% and 88.4% cardiac event–free survival at 1 and 3 years, respectively. Cardiac event–free survival for patients revascularized and awaiting deceased donor transplantation was similar: 94.0% and 90.0% at 1 and 3 years, respectively. <h3>Conclusions</h3> Our data suggest pre-emptive coronary revascularization is not only associated with excellent survival rates in patients subsequently transplanted, but also in those patients waiting on dialysis for a deceased donor transplant.",0
,Principles and selected applications of item response theory.,,0
https://doi.org/10.1007/bf02294248,"Nonconvergence, improper solutions, and starting values in lisrel maximum likelihood estimation","In the framework of a robustness study on maximum likelihood estimation with LISREL three types of problems are dealt with: nonconvergence, improper solutions, and choice of starting values. The purpose of the paper is to illustrate why and to what extent these problems are of importance for users of LISREL. The ways in which these issues may affect the design and conclusions of robustness research is also discussed.",0
https://doi.org/10.1080/00461520.2012.670488,Classroom Climate and Contextual Effects: Conceptual and Methodological Issues in the Evaluation of Group-Level Effects,"Classroom context and climate are inherently classroom-level (L2) constructs, but applied researchers sometimes—inappropriately—represent them by student-level (L1) responses in single-level models rather than more appropriate multilevel models. Here we focus on important conceptual issues (distinctions between climate and contextual variables; use of classroom L2 rather than student-level L1 measures) and more appropriate multilevel models. To illustrate these issues, we consider the effects of two L2 classroom climate variables and one L2 classroom contextual variable on two L1 student-level outcomes for 2261 students in 128 classes. Through this example, we illustrate how to apply evolving doubly latent multilevel models to (a) evaluate the factor structure of L1 and L2 constructs based on multiple indicators of classroom climate and context measures, (b) control measurement error at L1 and L2, (c) control sampling error in the aggregation of L1 responses to form L2 constructs (the average of student-l...",0
https://doi.org/10.1007/bf02293801,Marginal maximum likelihood estimation of item parameters: Application of an EM algorithm,"Maximum likelihood estimation of item parameters in the marginal distribution, integrating over the distribution of ability, becomes practical when computing procedures based on an EM algorithm are used. By characterizing the ability distribution empirically, arbitrary assumptions about its form are avoided. The Em procedure is shown to apply to general item-response models lacking simple sufficient statistics for ability. This includes models with more than one latent dimension. Â© 1981 The Psychometric Society.",0
https://doi.org/10.1006/jmps.1993.1032,Information Processing Models Generating Lognormally Distributed Reaction Times,"Abstract The lognormal distribution has been fitted successfully to empirical reaction times (RTs), yet mechanisms that might generate lognormally distributed (RTs are unknown. Thus, the lognormal has the status of an ad-hoc distribution in RT research. In this paper we describe mechanisms that could generate the lognormal distribution and show how specific RT models can be constructed within the framework of general mathematical properties of the lognormal. It is demonstrated that various conceptually different mechanisms could produce lognormally distributed RTs.",0
https://doi.org/10.1177/1094428103257358,"Apples and Oranges (and Pears, Oh My!): The Search for Moderators in Meta-Analysis","The purpose of this article is to review current practices with respect to detection and estimation of moderators in meta-analysis and to develop recommendations that are driven by the results of this review and previous research. The first purpose was accomplished through a review of the meta-analyses published in Journal of Applied Psychology from 1978 to 1997. Results show, first, that practices with respect to both the execution of and the reporting of results from searches for moderators are highly variable and, second, that findings relevant for detection of moderators (e.g., percentage variance attributable to artifacts, SDρ, etc.) are often highly inconsistent with what has been suggested in the past. These practices held regardless of time of publication, specificity of the question addressed in the paper, and content area. Detailed suggestions for modifications of current practices are offered.",0
https://doi.org/10.1037/a0013971,Mediators of change for multisystemic therapy with juvenile sexual offenders.,"The mediators of favorable multisystemic therapy (MST) outcomes achieved at 12 months postrecruitment were examined within the context of a randomized effectiveness trial with 127 juvenile sexual offenders and their caregivers. Outcome measures assessed youth delinquency, substance use, externalizing symptoms, and deviant sexual interest/risk behaviors; hypothesized mediators included measures of parenting and peer relations. Data were collected at pretreatment, 6 months postrecruitment, and 12 months postrecruitment. Consistent with the MST theory of change and the small extant literature in this area of research, analyses showed that favorable MST effects on youth antisocial behavior and deviant sexual interest/risk behaviors were mediated by increased caregiver follow-through on discipline practices as well as decreased caregiver disapproval of and concern about the youth's bad friends during the follow-up. These findings have important implications for the community-based treatment of juvenile sexual offenders.",0
https://doi.org/10.1111/j.1460-9568.2005.04057.x,"Controlling pathological pain by adenovirally driven spinal production of the anti-inflammatory cytokine, interleukin-10","Gene therapy for the control of pain has, to date, targeted neurons. However, recent evidence supports that spinal cord glia are critical to the creation and maintenance of pain facilitation through the release of proinflammatory cytokines. Because of the ability of interleukin-10 (IL-10) to suppress proinflammatory cytokines, we tested whether an adenoviral vector encoding human IL-10 (AD-h-IL10) would block and reverse pain facilitation. Three pain models were examined, all of which are mediated by spinal pro-inflammatory cytokines. Acute intrathecal administration of rat IL-10 protein itself briefly reversed chronic constriction injury-induced mechanical allodynia and thermal hyperalgesia. The transient reversal caused by IL-10 protein paralleled the half-life of human IL-10 protein in the intrathecal space (t1/2 ∼ 2 h). IL-10 gene therapy both prevented and reversed thermal hyperalgesia and mechanical allodynia, without affecting basal responses to thermal or mechanical stimuli. Extra-territorial, as well as territorial, pain changes were reversed by this treatment. Intrathecal AD-h-IL10 injected over lumbosacral spinal cord led to elevated lumbosacral cerebrospinal fluid (CSF) levels of human IL-10, with far less human IL-10 observed in cervical CSF. In keeping with IL-10's known anti-inflammatory actions, AD-h-IL10 lowered CSF levels of IL-1, relative to control AD. These studies support that this gene therapy approach provides an alternative to neuronally focused drug and gene therapies for clinical pain control.",0
https://doi.org/10.3389/fpsyg.2015.01963,Psychometric Evaluation of the Overexcitability Questionnaire-Two Applying Bayesian Structural Equation Modeling (BSEM) and Multiple-Group BSEM-Based Alignment with Approximate Measurement Invariance,"The Overexcitability Questionnaire-Two (OEQ-II) measures the degree and nature of overexcitability, which assists in determining the developmental potential of an individual according to Dabrowski's Theory of Positive Disintegration. Previous validation studies using frequentist confirmatory factor analysis, which postulates exact parameter constraints, led to model rejection and a long series of model modifications. Bayesian structural equation modeling (BSEM) allows the application of zero-mean, small-variance priors for cross-loadings, residual covariances, and differences in measurement parameters across groups, better reflecting substantive theory and leading to better model fit and less overestimation of factor correlations. Our BSEM analysis with a sample of 516 students in higher education yields positive results regarding the factorial validity of the OEQ-II. Likewise, applying BSEM-based alignment with approximate measurement invariance, the absence of non-invariant factor loadings and intercepts across gender is supportive of the psychometric quality of the OEQ-II. Compared to males, females scored significantly higher on emotional and sensual overexcitability, and significantly lower on psychomotor overexcitability.",0
https://doi.org/10.1080/08957347.2012.714686,Item Selection and Ability Estimation Procedures for a Mixed-Format Adaptive Test,"In this study we compared five item selection procedures using three ability estimation methods in the context of a mixed-format adaptive test based on the generalized partial credit model. The item selection procedures used were maximum posterior weighted information, maximum expected information, maximum posterior weighted Kullback-Leibler information, and maximum expected posterior weighted Kullback-Leibler information procedures. The ability estimation methods investigated were maximum likelihood estimation (MLE), weighted likelihood estimation (WLE), and expected a posteriori (EAP). Results suggested that all item selection procedures, regardless of the information functions on which they were based, performed equally well across ability estimation methods. The principal conclusions drawn about the ability estimation methods are that MLE is a practical choice and WLE should be considered when there is a mismatch between pool information and the population ability distribution. EAP can serve as a viab...",0
https://doi.org/10.1080/07474938.2012.690653,Stochastic Dominance with Ordinal Variables: Conditions and a Test,"A re-emerging literature on robustness in multidimensional welfare and poverty comparisons has revived interest in multidimensional stochastic dominance. Considering the widespread use of ordinal variables in wellbeing measurement, and particularly in composite indices, I derive multivariate stochastic dominance conditions for ordinal variables. These are the analogues of the conditions for continuous variables (e.g., Bawa, 1975, and Atkinson and Bourguignon, 1982). The article also derives mixed-order-of-dominance conditions for any type of variable. Then I propose an extension of Anderson's nonparametric test in order to test these conditions for ordinal variables. In addition, I propose the use of vectors and matrices of positions in order to handle multivariate, multinomial distributions. An empirical application to multidimensional wellbeing in Peru illustrates these tests.",0
https://doi.org/10.1371/journal.pone.0058369,Bayesian Inference for Generalized Linear Mixed Model Based on the Multivariate t Distribution in Population Pharmacokinetic Study,"This article provides a fully bayesian approach for modeling of single-dose and complete pharmacokinetic data in a population pharmacokinetic (PK) model. To overcome the impact of outliers and the difficulty of computation, a generalized linear model is chosen with the hypothesis that the errors follow a multivariate Student t distribution which is a heavy-tailed distribution. The aim of this study is to investigate and implement the performance of the multivariate t distribution to analyze population pharmacokinetic data. Bayesian predictive inferences and the Metropolis-Hastings algorithm schemes are used to process the intractable posterior integration. The precision and accuracy of the proposed model are illustrated by the simulating data and a real example of theophylline data.",0
https://doi.org/10.3758/bf03257252,A hierarchical model for estimating response time distributions,"We present a statistical model for inference with response time (RT) distributions. The model has the following features. First, it provides a means of estimating the shape, scale, and location (shift) of RT distributions. Second, it is hierarchical and models between-subjects and within-subjects variability simultaneously. Third, inference with the model is Bayesian and provides a principled and efficient means of pooling information across disparate data from different individuals. Because the model efficiently pools information across individuals, it is particularly well suited for those common cases in which the researcher collects a limited number of observations from several participants. Monte Carlo simulations reveal that the hierarchical Bayesian model provides more accurate estimates than several popular competitors do. We illustrate the model by providing an analysis of the symbolic distance effect in which participants can more quickly ascertain the relationship between nonadjacent digits than that between adjacent digits.",0
